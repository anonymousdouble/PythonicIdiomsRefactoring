file_html,method_content,file_name,lineno,old_code,new_code
https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_lookahead_swap.py,"def test_lookahead_swap_hang_in_min_case(self):
        """"""Verify LookaheadSwap does not stall in minimal case.""""""
        # ref: https://github.com/Qiskit/qiskit-terra/issues/2171

        qr = QuantumRegister(14, ""q"")
        qc = QuantumCircuit(qr)
        qc.cx(qr[0], qr[13])
        qc.cx(qr[1], qr[13])
        qc.cx(qr[1], qr[0])
        qc.cx(qr[13], qr[1])
        dag = circuit_to_dag(qc)

        cmap = CouplingMap(FakeMelbourne().configuration().coupling_map)

        out = LookaheadSwap(cmap, search_depth=4, search_width=4).run(dag)

        self.assertIsInstance(out, DAGCircuit)",_1606.py,9,"qc.cx(qr[1], qr[0])",qc.cx(*qr[1::-1])
https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_lookahead_swap.py,"def test_lookahead_swap_hang_in_min_case(self):
        """"""Verify LookaheadSwap does not stall in minimal case.""""""
        # ref: https://github.com/Qiskit/qiskit-terra/issues/2171

        qr = QuantumRegister(14, ""q"")
        qc = QuantumCircuit(qr)
        qc.cx(qr[0], qr[13])
        qc.cx(qr[1], qr[13])
        qc.cx(qr[1], qr[0])
        qc.cx(qr[13], qr[1])
        dag = circuit_to_dag(qc)

        cmap = CouplingMap(FakeMelbourne().configuration().coupling_map)

        out = LookaheadSwap(cmap, search_depth=4, search_width=4).run(dag)

        self.assertIsInstance(out, DAGCircuit)",_1606.py,10,"qc.cx(qr[13], qr[1])",qc.cx(*qr[13::-12])
https://github.com/inkstitch/inkstitch/tree/master/lib/svg/guides.py,"def _parse(self):
        self.label = self.node.get(INKSCAPE_LABEL, """")

        doc_size = self.svg.get_page_bbox()

        # inkscape's Y axis is reversed from SVG's, and the guide is in inkscape coordinates
        self.position = Point(*string_to_floats(self.node.get('position')))
        self.position.y = doc_size.y.size - self.position.y

        # convert units to px
        unit = self.svg.unit
        self.position.y = convert_unit(self.position.y, 'px', unit)

        # This one baffles me.  I think inkscape might have gotten the order of
        # their vector wrong?
        parts = string_to_floats(self.node.get('orientation'))
        self.direction = Point(parts[1], parts[0])",_7484.py,17,"Point(parts[1], parts[0])",Point(*parts[1::-1])
https://github.com/pyglet/pyglet/tree/master/pyglet/math.py,"def heading(self):
        """"""The angle of the vector in radians.

        :type: float
        """"""
        return _math.atan2(self[1], self[0])",_7854.py,6,"_math.atan2(self[1], self[0])",_math.atan2(*self[1::-1])
https://github.com/raiden-network/raiden/tree/master/raiden/storage/sqlite.py,"def get_snapshots(self) -> List[SnapshotEncodedRecord]:
        cursor = self.conn.cursor()
        cursor.execute(
            ""SELECT identifier, statechange_qty, statechange_id, data FROM state_snapshot""
        )

        return [
            SnapshotEncodedRecord(snapshot[0], snapshot[1], snapshot[2], snapshot[3])
            for snapshot in cursor
        ]",_48.py,8,"snapshot[0], snapshot[1], snapshot[2], snapshot[3]",*snapshot[:4]
https://github.com/EntySec/Ghost/tree/master/ghost/modules/upload.py,"def run(self, argc, argv):
        self.device.upload(argv[1], argv[2])",_143.py,2,"argv[1], argv[2]",*argv[1:3]
https://github.com/google/TensorNetwork/tree/master/tensornetwork/quantum/quantum.py,"def eliminate_identities(nodes: Collection[AbstractNode]) -> Tuple[dict, dict]:
  """"""Eliminates any connected CopyNodes that are identity matrices.

  This will modify the network represented by `nodes`.
  Only identities that are connected to other nodes are eliminated.

  Args:
    nodes: Collection of nodes to search.
  Returns:
    nodes_dict: Dictionary mapping remaining Nodes to any replacements.
    dangling_edges_dict: Dictionary specifying all dangling-edge replacements.
  """"""
  nodes_dict = {}
  dangling_edges_dict = {}
  for n in nodes:
    if isinstance(
        n, CopyNode) and n.get_rank() == 2 and not (n[0].is_dangling() and
                                                    n[1].is_dangling()):
      old_edges = [n[0], n[1]]
      _, new_edges = remove_node(n)
      if 0 in new_edges and 1 in new_edges:
        e = connect(new_edges[0], new_edges[1])
      elif 0 in new_edges:  # 1 was dangling
        dangling_edges_dict[old_edges[1]] = new_edges[0]
      elif 1 in new_edges:  # 0 was dangling
        dangling_edges_dict[old_edges[0]] = new_edges[1]
      else:
        # Trace of identity, so replace with a scalar node!
        d = n.get_dimension(0)
        # NOTE: Assume CopyNodes have numpy dtypes.
        nodes_dict[n] = Node(np.array(d, dtype=n.dtype), backend=n.backend)
    else:
      for e in n.get_all_dangling():
        dangling_edges_dict[e] = e
      nodes_dict[n] = n

  return nodes_dict, dangling_edges_dict",_154.py,22,"new_edges[0], new_edges[1]",*new_edges[:2]
https://github.com/pandas-dev/pandas/tree/master/pandas/io/formats/style.py,"def to_latex(
        self,
        buf: FilePath | WriteBuffer[str] | None = None,
        *,
        column_format: str | None = None,
        position: str | None = None,
        position_float: str | None = None,
        hrules: bool | None = None,
        clines: str | None = None,
        label: str | None = None,
        caption: str | tuple | None = None,
        sparse_index: bool | None = None,
        sparse_columns: bool | None = None,
        multirow_align: str | None = None,
        multicol_align: str | None = None,
        siunitx: bool = False,
        environment: str | None = None,
        encoding: str | None = None,
        convert_css: bool = False,
    ) -> str | None:
        r""""""
        Write Styler to a file, buffer or string in LaTeX format.

        .. versionadded:: 1.3.0

        Parameters
        ----------
        buf : str, path object, file-like object, or None, default None
            String, path object (implementing ``os.PathLike[str]``), or file-like
            object implementing a string ``write()`` function. If None, the result is
            returned as a string.
        column_format : str, optional
            The LaTeX column specification placed in location:

            \\begin{tabular}{<column_format>}

            Defaults to 'l' for index and
            non-numeric data columns, and, for numeric data columns,
            to 'r' by default, or 'S' if ``siunitx`` is ``True``.
        position : str, optional
            The LaTeX positional argument (e.g. 'h!') for tables, placed in location:

            ``\\begin{table}[<position>]``.
        position_float : {""centering"", ""raggedleft"", ""raggedright""}, optional
            The LaTeX float command placed in location:

            \\begin{table}[<position>]

            \\<position_float>

            Cannot be used if ``environment`` is ""longtable"".
        hrules : bool
            Set to `True` to add \\toprule, \\midrule and \\bottomrule from the
            {booktabs} LaTeX package.
            Defaults to ``pandas.options.styler.latex.hrules``, which is `False`.

            .. versionchanged:: 1.4.0
        clines : str, optional
            Use to control adding \\cline commands for the index labels separation.
            Possible values are:

              - `None`: no cline commands are added (default).
              - `""all;data""`: a cline is added for every index value extending the
                width of the table, including data entries.
              - `""all;index""`: as above with lines extending only the width of the
                index entries.
              - `""skip-last;data""`: a cline is added for each index value except the
                last level (which is never sparsified), extending the widtn of the
                table.
              - `""skip-last;index""`: as above with lines extending only the width of the
                index entries.

            .. versionadded:: 1.4.0
        label : str, optional
            The LaTeX label included as: \\label{<label>}.
            This is used with \\ref{<label>} in the main .tex file.
        caption : str, tuple, optional
            If string, the LaTeX table caption included as: \\caption{<caption>}.
            If tuple, i.e (""full caption"", ""short caption""), the caption included
            as: \\caption[<caption[1]>]{<caption[0]>}.
        sparse_index : bool, optional
            Whether to sparsify the display of a hierarchical index. Setting to False
            will display each explicit level element in a hierarchical key for each row.
            Defaults to ``pandas.options.styler.sparse.index``, which is `True`.
        sparse_columns : bool, optional
            Whether to sparsify the display of a hierarchical index. Setting to False
            will display each explicit level element in a hierarchical key for each
            column. Defaults to ``pandas.options.styler.sparse.columns``, which
            is `True`.
        multirow_align : {""c"", ""t"", ""b"", ""naive""}, optional
            If sparsifying hierarchical MultiIndexes whether to align text centrally,
            at the top or bottom using the multirow package. If not given defaults to
            ``pandas.options.styler.latex.multirow_align``, which is `""c""`.
            If ""naive"" is given renders without multirow.

            .. versionchanged:: 1.4.0
        multicol_align : {""r"", ""c"", ""l"", ""naive-l"", ""naive-r""}, optional
            If sparsifying hierarchical MultiIndex columns whether to align text at
            the left, centrally, or at the right. If not given defaults to
            ``pandas.options.styler.latex.multicol_align``, which is ""r"".
            If a naive option is given renders without multicol.
            Pipe decorators can also be added to non-naive values to draw vertical
            rules, e.g. ""\|r"" will draw a rule on the left side of right aligned merged
            cells.

            .. versionchanged:: 1.4.0
        siunitx : bool, default False
            Set to ``True`` to structure LaTeX compatible with the {siunitx} package.
        environment : str, optional
            If given, the environment that will replace 'table' in ``\\begin{table}``.
            If 'longtable' is specified then a more suitable template is
            rendered. If not given defaults to
            ``pandas.options.styler.latex.environment``, which is `None`.

            .. versionadded:: 1.4.0
        encoding : str, optional
            Character encoding setting. Defaults
            to ``pandas.options.styler.render.encoding``, which is ""utf-8"".
        convert_css : bool, default False
            Convert simple cell-styles from CSS to LaTeX format. Any CSS not found in
            conversion table is dropped. A style can be forced by adding option
            `--latex`. See notes.

        Returns
        -------
        str or None
            If `buf` is None, returns the result as a string. Otherwise returns `None`.

        See Also
        --------
        Styler.format: Format the text display value of cells.

        Notes
        -----
        **Latex Packages**

        For the following features we recommend the following LaTeX inclusions:

        ===================== ==========================================================
        Feature               Inclusion
        ===================== ==========================================================
        sparse columns        none: included within default {tabular} environment
        sparse rows           \\usepackage{multirow}
        hrules                \\usepackage{booktabs}
        colors                \\usepackage[table]{xcolor}
        siunitx               \\usepackage{siunitx}
        bold (with siunitx)   | \\usepackage{etoolbox}
                              | \\robustify\\bfseries
                              | \\sisetup{detect-all = true}  *(within {document})*
        italic (with siunitx) | \\usepackage{etoolbox}
                              | \\robustify\\itshape
                              | \\sisetup{detect-all = true}  *(within {document})*
        environment           \\usepackage{longtable} if arg is ""longtable""
                              | or any other relevant environment package
        hyperlinks            \\usepackage{hyperref}
        ===================== ==========================================================

        **Cell Styles**

        LaTeX styling can only be rendered if the accompanying styling functions have
        been constructed with appropriate LaTeX commands. All styling
        functionality is built around the concept of a CSS ``(<attribute>, <value>)``
        pair (see `Table Visualization <../../user_guide/style.ipynb>`_), and this
        should be replaced by a LaTeX
        ``(<command>, <options>)`` approach. Each cell will be styled individually
        using nested LaTeX commands with their accompanied options.

        For example the following code will highlight and bold a cell in HTML-CSS:

        >>> df = pd.DataFrame([[1,2], [3,4]])
        >>> s = df.style.highlight_max(axis=None,
        ...                            props='background-color:red; font-weight:bold;')
        >>> s.to_html()  # doctest: +SKIP

        The equivalent using LaTeX only commands is the following:

        >>> s = df.style.highlight_max(axis=None,
        ...                            props='cellcolor:{red}; bfseries: ;')
        >>> s.to_latex()  # doctest: +SKIP

        Internally these structured LaTeX ``(<command>, <options>)`` pairs
        are translated to the
        ``display_value`` with the default structure:
        ``\<command><options> <display_value>``.
        Where there are multiple commands the latter is nested recursively, so that
        the above example highlighted cell is rendered as
        ``\cellcolor{red} \bfseries 4``.

        Occasionally this format does not suit the applied command, or
        combination of LaTeX packages that is in use, so additional flags can be
        added to the ``<options>``, within the tuple, to result in different
        positions of required braces (the **default** being the same as ``--nowrap``):

        =================================== ============================================
        Tuple Format                           Output Structure
        =================================== ============================================
        (<command>,<options>)               \\<command><options> <display_value>
        (<command>,<options> ``--nowrap``)  \\<command><options> <display_value>
        (<command>,<options> ``--rwrap``)   \\<command><options>{<display_value>}
        (<command>,<options> ``--wrap``)    {\\<command><options> <display_value>}
        (<command>,<options> ``--lwrap``)   {\\<command><options>} <display_value>
        (<command>,<options> ``--dwrap``)   {\\<command><options>}{<display_value>}
        =================================== ============================================

        For example the `textbf` command for font-weight
        should always be used with `--rwrap` so ``('textbf', '--rwrap')`` will render a
        working cell, wrapped with braces, as ``\textbf{<display_value>}``.

        A more comprehensive example is as follows:

        >>> df = pd.DataFrame([[1, 2.2, ""dogs""], [3, 4.4, ""cats""], [2, 6.6, ""cows""]],
        ...                   index=[""ix1"", ""ix2"", ""ix3""],
        ...                   columns=[""Integers"", ""Floats"", ""Strings""])
        >>> s = df.style.highlight_max(
        ...     props='cellcolor:[HTML]{FFFF00}; color:{red};'
        ...           'textit:--rwrap; textbf:--rwrap;'
        ... )
        >>> s.to_latex()  # doctest: +SKIP

        .. figure:: ../../_static/style/latex_1.png

        **Table Styles**

        Internally Styler uses its ``table_styles`` object to parse the
        ``column_format``, ``position``, ``position_float``, and ``label``
        input arguments. These arguments are added to table styles in the format:

        .. code-block:: python

            set_table_styles([
                {""selector"": ""column_format"", ""props"": f"":{column_format};""},
                {""selector"": ""position"", ""props"": f"":{position};""},
                {""selector"": ""position_float"", ""props"": f"":{position_float};""},
                {""selector"": ""label"", ""props"": f"":{{{label.replace(':','ยง')}}};""}
            ], overwrite=False)

        Exception is made for the ``hrules`` argument which, in fact, controls all three
        commands: ``toprule``, ``bottomrule`` and ``midrule`` simultaneously. Instead of
        setting ``hrules`` to ``True``, it is also possible to set each
        individual rule definition, by manually setting the ``table_styles``,
        for example below we set a regular ``toprule``, set an ``hline`` for
        ``bottomrule`` and exclude the ``midrule``:

        .. code-block:: python

            set_table_styles([
                {'selector': 'toprule', 'props': ':toprule;'},
                {'selector': 'bottomrule', 'props': ':hline;'},
            ], overwrite=False)

        If other ``commands`` are added to table styles they will be detected, and
        positioned immediately above the '\\begin{tabular}' command. For example to
        add odd and even row coloring, from the {colortbl} package, in format
        ``\rowcolors{1}{pink}{red}``, use:

        .. code-block:: python

            set_table_styles([
                {'selector': 'rowcolors', 'props': ':{1}{pink}{red};'}
            ], overwrite=False)

        A more comprehensive example using these arguments is as follows:

        >>> df.columns = pd.MultiIndex.from_tuples([
        ...     (""Numeric"", ""Integers""),
        ...     (""Numeric"", ""Floats""),
        ...     (""Non-Numeric"", ""Strings"")
        ... ])
        >>> df.index = pd.MultiIndex.from_tuples([
        ...     (""L0"", ""ix1""), (""L0"", ""ix2""), (""L1"", ""ix3"")
        ... ])
        >>> s = df.style.highlight_max(
        ...     props='cellcolor:[HTML]{FFFF00}; color:{red}; itshape:; bfseries:;'
        ... )
        >>> s.to_latex(
        ...     column_format=""rrrrr"", position=""h"", position_float=""centering"",
        ...     hrules=True, label=""table:5"", caption=""Styled LaTeX Table"",
        ...     multirow_align=""t"", multicol_align=""r""
        ... )  # doctest: +SKIP

        .. figure:: ../../_static/style/latex_2.png

        **Formatting**

        To format values :meth:`Styler.format` should be used prior to calling
        `Styler.to_latex`, as well as other methods such as :meth:`Styler.hide`
        for example:

        >>> s.clear()
        >>> s.table_styles = []
        >>> s.caption = None
        >>> s.format({
        ...    (""Numeric"", ""Integers""): '\${}',
        ...    (""Numeric"", ""Floats""): '{:.3f}',
        ...    (""Non-Numeric"", ""Strings""): str.upper
        ... })  # doctest: +SKIP
                        Numeric      Non-Numeric
                  Integers   Floats    Strings
        L0    ix1       $1   2.200      DOGS
              ix2       $3   4.400      CATS
        L1    ix3       $2   6.600      COWS

        >>> s.to_latex()  # doctest: +SKIP
        \begin{tabular}{llrrl}
        {} & {} & \multicolumn{2}{r}{Numeric} & {Non-Numeric} \\
        {} & {} & {Integers} & {Floats} & {Strings} \\
        \multirow[c]{2}{*}{L0} & ix1 & \\$1 & 2.200 & DOGS \\
         & ix2 & \$3 & 4.400 & CATS \\
        L1 & ix3 & \$2 & 6.600 & COWS \\
        \end{tabular}

        **CSS Conversion**

        This method can convert a Styler constructured with HTML-CSS to LaTeX using
        the following limited conversions.

        ================== ==================== ============= ==========================
        CSS Attribute      CSS value            LaTeX Command LaTeX Options
        ================== ==================== ============= ==========================
        font-weight        | bold               | bfseries
                           | bolder             | bfseries
        font-style         | italic             | itshape
                           | oblique            | slshape
        background-color   | red                cellcolor     | {red}--lwrap
                           | #fe01ea                          | [HTML]{FE01EA}--lwrap
                           | #f0e                             | [HTML]{FF00EE}--lwrap
                           | rgb(128,255,0)                   | [rgb]{0.5,1,0}--lwrap
                           | rgba(128,0,0,0.5)                | [rgb]{0.5,0,0}--lwrap
                           | rgb(25%,255,50%)                 | [rgb]{0.25,1,0.5}--lwrap
        color              | red                color         | {red}
                           | #fe01ea                          | [HTML]{FE01EA}
                           | #f0e                             | [HTML]{FF00EE}
                           | rgb(128,255,0)                   | [rgb]{0.5,1,0}
                           | rgba(128,0,0,0.5)                | [rgb]{0.5,0,0}
                           | rgb(25%,255,50%)                 | [rgb]{0.25,1,0.5}
        ================== ==================== ============= ==========================

        It is also possible to add user-defined LaTeX only styles to a HTML-CSS Styler
        using the ``--latex`` flag, and to add LaTeX parsing options that the
        converter will detect within a CSS-comment.

        >>> df = pd.DataFrame([[1]])
        >>> df.style.set_properties(
        ...     **{""font-weight"": ""bold /* --dwrap */"", ""Huge"": ""--latex--rwrap""}
        ... ).to_latex(convert_css=True)  # doctest: +SKIP
        \begin{tabular}{lr}
        {} & {0} \\
        0 & {\bfseries}{\Huge{1}} \\
        \end{tabular}

        Examples
        --------
        Below we give a complete step by step example adding some advanced features
        and noting some common gotchas.

        First we create the DataFrame and Styler as usual, including MultiIndex rows
        and columns, which allow for more advanced formatting options:

        >>> cidx = pd.MultiIndex.from_arrays([
        ...     [""Equity"", ""Equity"", ""Equity"", ""Equity"",
        ...      ""Stats"", ""Stats"", ""Stats"", ""Stats"", ""Rating""],
        ...     [""Energy"", ""Energy"", ""Consumer"", ""Consumer"", """", """", """", """", """"],
        ...     [""BP"", ""Shell"", ""H&M"", ""Unilever"",
        ...      ""Std Dev"", ""Variance"", ""52w High"", ""52w Low"", """"]
        ... ])
        >>> iidx = pd.MultiIndex.from_arrays([
        ...     [""Equity"", ""Equity"", ""Equity"", ""Equity""],
        ...     [""Energy"", ""Energy"", ""Consumer"", ""Consumer""],
        ...     [""BP"", ""Shell"", ""H&M"", ""Unilever""]
        ... ])
        >>> styler = pd.DataFrame([
        ...     [1, 0.8, 0.66, 0.72, 32.1678, 32.1678**2, 335.12, 240.89, ""Buy""],
        ...     [0.8, 1.0, 0.69, 0.79, 1.876, 1.876**2, 14.12, 19.78, ""Hold""],
        ...     [0.66, 0.69, 1.0, 0.86, 7, 7**2, 210.9, 140.6, ""Buy""],
        ...     [0.72, 0.79, 0.86, 1.0, 213.76, 213.76**2, 2807, 3678, ""Sell""],
        ... ], columns=cidx, index=iidx).style

        Second we will format the display and, since our table is quite wide, will
        hide the repeated level-0 of the index:

        >>> styler.format(subset=""Equity"", precision=2)
        ...       .format(subset=""Stats"", precision=1, thousands="","")
        ...       .format(subset=""Rating"", formatter=str.upper)
        ...       .format_index(escape=""latex"", axis=1)
        ...       .format_index(escape=""latex"", axis=0)
        ...       .hide(level=0, axis=0)  # doctest: +SKIP

        Note that one of the string entries of the index and column headers is ""H&M"".
        Without applying the `escape=""latex""` option to the `format_index` method the
        resultant LaTeX will fail to render, and the error returned is quite
        difficult to debug. Using the appropriate escape the ""&"" is converted to ""\\&"".

        Thirdly we will apply some (CSS-HTML) styles to our object. We will use a
        builtin method and also define our own method to highlight the stock
        recommendation:

        >>> def rating_color(v):
        ...     if v == ""Buy"": color = ""#33ff85""
        ...     elif v == ""Sell"": color = ""#ff5933""
        ...     else: color = ""#ffdd33""
        ...     return f""color: {color}; font-weight: bold;""
        >>> styler.background_gradient(cmap=""inferno"", subset=""Equity"", vmin=0, vmax=1)
        ...       .applymap(rating_color, subset=""Rating"")  # doctest: +SKIP

        All the above styles will work with HTML (see below) and LaTeX upon conversion:

        .. figure:: ../../_static/style/latex_stocks_html.png

        However, we finally want to add one LaTeX only style
        (from the {graphicx} package), that is not easy to convert from CSS and
        pandas does not support it. Notice the `--latex` flag used here,
        as well as `--rwrap` to ensure this is formatted correctly and
        not ignored upon conversion.

        >>> styler.applymap_index(
        ...     lambda v: ""rotatebox:{45}--rwrap--latex;"", level=2, axis=1
        ... )  # doctest: +SKIP

        Finally we render our LaTeX adding in other options as required:

        >>> styler.to_latex(
        ...     caption=""Selected stock correlation and simple statistics."",
        ...     clines=""skip-last;data"",
        ...     convert_css=True,
        ...     position_float=""centering"",
        ...     multicol_align=""|c|"",
        ...     hrules=True,
        ... )  # doctest: +SKIP
        \begin{table}
        \centering
        \caption{Selected stock correlation and simple statistics.}
        \begin{tabular}{llrrrrrrrrl}
        \toprule
         &  & \multicolumn{4}{|c|}{Equity} & \multicolumn{4}{|c|}{Stats} & Rating \\
         &  & \multicolumn{2}{|c|}{Energy} & \multicolumn{2}{|c|}{Consumer} &
        \multicolumn{4}{|c|}{} &  \\
         &  & \rotatebox{45}{BP} & \rotatebox{45}{Shell} & \rotatebox{45}{H\&M} &
        \rotatebox{45}{Unilever} & \rotatebox{45}{Std Dev} & \rotatebox{45}{Variance} &
        \rotatebox{45}{52w High} & \rotatebox{45}{52w Low} & \rotatebox{45}{} \\
        \midrule
        \multirow[c]{2}{*}{Energy} & BP & {\cellcolor[HTML]{FCFFA4}}
        \color[HTML]{000000} 1.00 & {\cellcolor[HTML]{FCA50A}} \color[HTML]{000000}
        0.80 & {\cellcolor[HTML]{EB6628}} \color[HTML]{F1F1F1} 0.66 &
        {\cellcolor[HTML]{F68013}} \color[HTML]{F1F1F1} 0.72 & 32.2 & 1,034.8 & 335.1
        & 240.9 & \color[HTML]{33FF85} \bfseries BUY \\
         & Shell & {\cellcolor[HTML]{FCA50A}} \color[HTML]{000000} 0.80 &
        {\cellcolor[HTML]{FCFFA4}} \color[HTML]{000000} 1.00 &
        {\cellcolor[HTML]{F1731D}} \color[HTML]{F1F1F1} 0.69 &
        {\cellcolor[HTML]{FCA108}} \color[HTML]{000000} 0.79 & 1.9 & 3.5 & 14.1 &
        19.8 & \color[HTML]{FFDD33} \bfseries HOLD \\
        \cline{1-11}
        \multirow[c]{2}{*}{Consumer} & H\&M & {\cellcolor[HTML]{EB6628}}
        \color[HTML]{F1F1F1} 0.66 & {\cellcolor[HTML]{F1731D}} \color[HTML]{F1F1F1}
        0.69 & {\cellcolor[HTML]{FCFFA4}} \color[HTML]{000000} 1.00 &
        {\cellcolor[HTML]{FAC42A}} \color[HTML]{000000} 0.86 & 7.0 & 49.0 & 210.9 &
        140.6 & \color[HTML]{33FF85} \bfseries BUY \\
         & Unilever & {\cellcolor[HTML]{F68013}} \color[HTML]{F1F1F1} 0.72 &
        {\cellcolor[HTML]{FCA108}} \color[HTML]{000000} 0.79 &
        {\cellcolor[HTML]{FAC42A}} \color[HTML]{000000} 0.86 &
        {\cellcolor[HTML]{FCFFA4}} \color[HTML]{000000} 1.00 & 213.8 & 45,693.3 &
        2,807.0 & 3,678.0 & \color[HTML]{FF5933} \bfseries SELL \\
        \cline{1-11}
        \bottomrule
        \end{tabular}
        \end{table}

        .. figure:: ../../_static/style/latex_stocks.png
        """"""
        obj = self._copy(deepcopy=True)  # manipulate table_styles on obj, not self

        table_selectors = (
            [style[""selector""] for style in self.table_styles]
            if self.table_styles is not None
            else []
        )

        if column_format is not None:
            # add more recent setting to table_styles
            obj.set_table_styles(
                [{""selector"": ""column_format"", ""props"": f"":{column_format}""}],
                overwrite=False,
            )
        elif ""column_format"" in table_selectors:
            pass  # adopt what has been previously set in table_styles
        else:
            # create a default: set float, complex, int cols to 'r' ('S'), index to 'l'
            _original_columns = self.data.columns
            self.data.columns = RangeIndex(stop=len(self.data.columns))
            numeric_cols = self.data._get_numeric_data().columns.to_list()
            self.data.columns = _original_columns
            column_format = """"
            for level in range(self.index.nlevels):
                column_format += """" if self.hide_index_[level] else ""l""
            for ci, _ in enumerate(self.data.columns):
                if ci not in self.hidden_columns:
                    column_format += (
                        (""r"" if not siunitx else ""S"") if ci in numeric_cols else ""l""
                    )
            obj.set_table_styles(
                [{""selector"": ""column_format"", ""props"": f"":{column_format}""}],
                overwrite=False,
            )

        if position:
            obj.set_table_styles(
                [{""selector"": ""position"", ""props"": f"":{position}""}],
                overwrite=False,
            )

        if position_float:
            if environment == ""longtable"":
                raise ValueError(
                    ""`position_float` cannot be used in 'longtable' `environment`""
                )
            if position_float not in [""raggedright"", ""raggedleft"", ""centering""]:
                raise ValueError(
                    f""`position_float` should be one of ""
                    f""'raggedright', 'raggedleft', 'centering', ""
                    f""got: '{position_float}'""
                )
            obj.set_table_styles(
                [{""selector"": ""position_float"", ""props"": f"":{position_float}""}],
                overwrite=False,
            )

        hrules = get_option(""styler.latex.hrules"") if hrules is None else hrules
        if hrules:
            obj.set_table_styles(
                [
                    {""selector"": ""toprule"", ""props"": "":toprule""},
                    {""selector"": ""midrule"", ""props"": "":midrule""},
                    {""selector"": ""bottomrule"", ""props"": "":bottomrule""},
                ],
                overwrite=False,
            )

        if label:
            obj.set_table_styles(
                [{""selector"": ""label"", ""props"": f"":{{{label.replace(':', 'ยง')}}}""}],
                overwrite=False,
            )

        if caption:
            obj.set_caption(caption)

        if sparse_index is None:
            sparse_index = get_option(""styler.sparse.index"")
        if sparse_columns is None:
            sparse_columns = get_option(""styler.sparse.columns"")
        environment = environment or get_option(""styler.latex.environment"")
        multicol_align = multicol_align or get_option(""styler.latex.multicol_align"")
        multirow_align = multirow_align or get_option(""styler.latex.multirow_align"")
        latex = obj._render_latex(
            sparse_index=sparse_index,
            sparse_columns=sparse_columns,
            multirow_align=multirow_align,
            multicol_align=multicol_align,
            environment=environment,
            convert_css=convert_css,
            siunitx=siunitx,
            clines=clines,
        )

        encoding = (
            (encoding or get_option(""styler.render.encoding""))
            if isinstance(buf, str)  # i.e. a filepath
            else encoding
        )
        return save_to_buffer(latex, buf=buf, encoding=encoding)",_208.py,539,"':', 'ยง'",*':ยง'
https://github.com/conan-io/conan-center-index/tree/master/recipes/protobuf/all/conanfile.py,"def _configure_cmake(self):
        cmake = CMake(self)
        cmake.definitions[""CMAKE_INSTALL_CMAKEDIR""] = self._cmake_install_base_path.replace(""\\"", ""/"")
        cmake.definitions[""protobuf_WITH_ZLIB""] = self.options.with_zlib
        cmake.definitions[""protobuf_BUILD_TESTS""] = False
        cmake.definitions[""protobuf_BUILD_PROTOC_BINARIES""] = True
        if not self.options.debug_suffix:
            cmake.definitions[""protobuf_DEBUG_POSTFIX""] = """"
        if Version(self.version) >= ""3.14.0"":
            cmake.definitions[""protobuf_BUILD_LIBPROTOC""] = True
        if self._can_disable_rtti:
            cmake.definitions[""protobuf_DISABLE_RTTI""] = not self.options.with_rtti
        if is_msvc(self) or self._is_clang_cl:
            runtime = msvc_runtime_flag(self)
            if not runtime:
                runtime = self.settings.get_safe(""compiler.runtime"")
            cmake.definitions[""protobuf_MSVC_STATIC_RUNTIME""] = ""MT"" in runtime
        if Version(self.version) < ""3.18.0"" and self._is_clang_cl:
            cmake.definitions[""CMAKE_RC_COMPILER""] = os.environ.get(""RC"", ""llvm-rc"")
        cmake.configure(build_folder=self._build_subfolder)
        return cmake",_752.py,3,"'\\', '/'",*'\\/'
https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/appservice/tests/latest/test_webapp_commands_thru_mock.py,"def test_webapp_github_actions_add(self, get_app_details_mock, web_client_factory_mock, site_availability_mock,  *args):
        runtime = ""python:3.9""
        rg = ""group""
        is_linux = True
        cmd = _get_test_cmd()
        get_app_details_mock.return_value = mock.Mock()
        get_app_details_mock.return_value.resource_group = rg
        web_client_factory_mock.return_value.app_service_plans.get.return_value.reserved = is_linux
        site_availability_mock.return_value.name_available = False

        with mock.patch('azure.cli.command_modules.appservice.custom._runtime_supports_github_actions', autospec=True) as m:
            add_github_actions(cmd, rg, ""name"", ""repo"", runtime, ""token"")
            m.assert_called_with(cmd, runtime.replace("":"", ""|""), is_linux)",_893.py,13,"':', '|'",*':|'
https://github.com/sympy/sympy/tree/master/sympy/printing/pretty/pretty.py,"def _print_SingularityFunction(self, e):
        if self._use_unicode:
            shift = self._print(e.args[0]-e.args[1])
            n = self._print(e.args[2])
            base = prettyForm(""<"")
            base = prettyForm(*base.right(shift))
            base = prettyForm(*base.right("">""))
            pform = base**n
            return pform
        else:
            n = self._print(e.args[2])
            shift = self._print(e.args[0]-e.args[1])
            base = self._print_seq(shift, ""<"", "">"", ' ')
            return base**n",_921.py,13,"'<', '>', ' '",*'<> '
https://github.com/sympy/sympy/tree/master/sympy/printing/pretty/pretty.py,"def _print_SingularityFunction(self, e):
        if self._use_unicode:
            shift = self._print(e.args[0]-e.args[1])
            n = self._print(e.args[2])
            base = prettyForm(""<"")
            base = prettyForm(*base.right(shift))
            base = prettyForm(*base.right("">""))
            pform = base**n
            return pform
        else:
            n = self._print(e.args[2])
            shift = self._print(e.args[0]-e.args[1])
            base = self._print_seq(shift, ""<"", "">"", ' ')
            return base**n",_921.py,13,"'<', '>', ' '",*'<> '
https://github.com/flairNLP/flair/tree/master/flair/datasets/entity_linking.py,"def __init__(
        self,
        base_path: Union[str, Path] = None,
        in_memory: bool = True,
        **corpusargs,
    ):
        """"""
        Initialize Tweeki Entity Linking corpus introduced in ""Tweeki: Linking Named Entities on Twitter to a Knowledge Graph"" Harandizadeh, Singh.
        The data consits of tweets with manually annotated wikipedia links.
        If you call the constructor the first time the dataset gets automatically downloaded and transformed in tab-separated column format.

        Parameters
        ----------
        base_path : Union[str, Path], optional
            Default is None, meaning that corpus gets auto-downloaded and loaded. You can override this
            to point to a different folder but typically this should not be necessary.
        in_memory: If True, keeps dataset in memory giving speedups in training.
        """"""
        if not base_path:
            base_path = flair.cache_root / ""datasets""
        else:
            base_path = Path(base_path)

        # this dataset name
        dataset_name = self.__class__.__name__.lower()

        data_folder = base_path / dataset_name

        tweeki_gold_el_path = ""https://raw.githubusercontent.com/ucinlp/tweeki/main/data/Tweeki_gold/Tweeki_gold""
        corpus_file_name = ""tweeki_gold.txt""
        parsed_dataset = data_folder / corpus_file_name

        # download and parse data if necessary
        if not parsed_dataset.exists():

            original_file_path = cached_path(f""{tweeki_gold_el_path}"", Path(""datasets"") / dataset_name)

            with open(original_file_path, ""r"", encoding=""utf-8"") as read, open(
                parsed_dataset, ""w"", encoding=""utf-8""
            ) as write:
                line = read.readline()
                while line:
                    if line.startswith(""#""):
                        out_line = """"
                    elif line == ""\n"":  # tweet ends
                        out_line = ""\n-DOCSTART-\n\n""
                    else:
                        line_list = line.split(""\t"")
                        out_line = line_list[1] + ""\t""
                        if line_list[3] == ""-\n"":  # no wiki name
                            out_line += ""O\n""
                        else:
                            out_line += line_list[2][:2] + line_list[3].split(""|"")[0].replace("" "", ""_"") + ""\n""
                    write.write(out_line)
                    line = read.readline()

            os.rename(original_file_path, str(original_file_path) + ""_original"")

        super(NEL_ENGLISH_TWEEKI, self).__init__(
            data_folder,
            column_format={0: ""text"", 1: ""nel""},
            train_file=corpus_file_name,
            in_memory=in_memory,
            **corpusargs,
        )",_1098.py,53,"' ', '_'",*' _'
https://github.com/3b1b/videos/tree/master/_2020/chess.py,"def construct(self):
        # Camera stuffs
        frame = self.camera.frame
        light = self.camera.light_source
        light.move_to([-25, -20, 20])

        # Setup cube
        colors = [RED, GREEN, BLUE_D, YELLOW]
        cube = self.get_hypercube()
        for n, vert in enumerate(cube.verts):
            code = boolian_linear_combo(int_to_bit_coords(n, 4))
            cube.verts[n].set_color(colors[code])

        # Create trees
        trees = Group()
        original_trees = Group()
        for vert in cube.verts:
            tree = Group(
                vert,
                vert.edges,
                vert.neighbors,
            ).copy()
            original = tree.copy()
            original[0].set_color(GREY)
            original[0].scale(0)
            original_trees.add(original)
            trees.add(tree)
        for tree in trees:
            tree[0].set_color(GREY)
            tree[0].rotate(90 * DEGREES, LEFT)
            sorted_verts = Group(*tree[2])
            sorted_verts.submobjects.sort(key=lambda m: m.get_color().hex)
            sorted_verts.arrange(DOWN, buff=SMALL_BUFF)
            sorted_verts.next_to(tree[0], RIGHT, buff=0.75)
            for edge, neighbor in zip(tree[1], tree[2]):
                edge.become(Line3D(
                    tree[0].get_center(),
                    neighbor.get_center(),
                    resolution=edge.resolution,
                ))
                neighbor.rotate(90 * DEGREES, LEFT)

        trees.arrange_in_grid(4, 4, buff=MED_LARGE_BUFF)
        for i in range(4):
            trees[i::4].shift(0.5 * i * RIGHT)
        trees.center()
        trees.set_height(6)
        trees.rotate(PI / 2, RIGHT)
        trees.move_to(10 * LEFT, LEFT)

        frame.set_phi(90 * DEGREES)
        frame.move_to(5 * LEFT)
        self.add(trees)
        self.wait()

        # Show transition
        anims = []
        for tree, original in zip(trees, original_trees):
            anims.append(Transform(tree, original))
        self.play(
            frame.set_euler_angles, 20 * DEGREES, 70 * DEGREES,
            frame.move_to, ORIGIN,
            LaggedStart(*anims, lag_ratio=0.2),
            run_time=8,
        )
        self.remove(trees)
        self.add(cube)
        frame.add_updater(lambda m, dt: m.increment_theta(2 * dt * DEGREES))
        self.wait(30)",_1137.py,35,"tree[1], tree[2]",*tree[1:3]
https://github.com/shadowsocksr-backup/shadowsocksr/tree/master/shadowsocks/udprelay.py,"def _handle_server(self):
        server = self._server_socket
        data, r_addr = server.recvfrom(BUF_SIZE)
        ogn_data = data
        if not data:
            logging.debug('UDP handle_server: data is empty')
        if self._stat_callback:
            self._stat_callback(self._listen_port, len(data))
        if self._is_local:
            frag = common.ord(data[2])
            if frag != 0:
                logging.warn('drop a message since frag is not 0')
                return
            else:
                data = data[3:]
        else:
            data = encrypt.encrypt_all(self._password, self._method, 0, data)
            # decrypt data
            if not data:
                logging.debug('UDP handle_server: data is empty after decrypt')
                return

        #logging.info(""UDP data %s"" % (binascii.hexlify(data),))
        if not self._is_local:
            data = pre_parse_header(data)

            data = self._pre_parse_udp_header(data)
            if data is None:
                return

            if type(data) is tuple:
                #(cmd, request_id, data)
                #logging.info(""UDP data %d %d %s"" % (data[0], data[1], binascii.hexlify(data[2])))
                try:
                    if data[0] == 0:
                        if len(data[2]) >= 4:
                            for i in range(64):
                                req_id = random.randint(1, 65535)
                                if req_id not in self._reqid_to_hd:
                                    break
                            if req_id in self._reqid_to_hd:
                                for i in range(64):
                                    req_id = random.randint(1, 65535)
                                    if type(self._reqid_to_hd[req_id]) is tuple:
                                        break
                            # return req id
                            self._reqid_to_hd[req_id] = (data[2][0:4], None)
                            rsp_data = self._pack_rsp_data(CMD_RSP_CONNECT, req_id, RSP_STATE_CONNECTED)
                            data_to_send = encrypt.encrypt_all(self._password, self._method, 1, rsp_data)
                            self.write_to_server_socket(data_to_send, r_addr)
                    elif data[0] == CMD_CONNECT_REMOTE:
                        if len(data[2]) > 4 and data[1] in self._reqid_to_hd:
                            # create
                            if type(self._reqid_to_hd[data[1]]) is tuple:
                                if data[2][0:4] == self._reqid_to_hd[data[1]][0]:
                                    handle = TCPRelayHandler(self, self._reqid_to_hd, self._fd_to_handlers,
                                        self._eventloop, self._server_socket,
                                        self._reqid_to_hd[data[1]][0], self._reqid_to_hd[data[1]][1],
                                        self._config, self._dns_resolver, self._is_local)
                                    self._reqid_to_hd[data[1]] = handle
                                    handle.handle_client(r_addr, CMD_CONNECT, data[1], data[2])
                                    handle.handle_client(r_addr, *data)
                                    self.update_activity(handle)
                                else:
                                    # disconnect
                                    rsp_data = self._pack_rsp_data(CMD_DISCONNECT, data[1], RSP_STATE_EMPTY)
                                    data_to_send = encrypt.encrypt_all(self._password, self._method, 1, rsp_data)
                                    self.write_to_server_socket(data_to_send, r_addr)
                            else:
                                self.update_activity(self._reqid_to_hd[data[1]])
                                self._reqid_to_hd[data[1]].handle_client(r_addr, *data)
                        else:
                            # disconnect
                            rsp_data = self._pack_rsp_data(CMD_DISCONNECT, data[1], RSP_STATE_EMPTY)
                            data_to_send = encrypt.encrypt_all(self._password, self._method, 1, rsp_data)
                            self.write_to_server_socket(data_to_send, r_addr)
                    elif data[0] > CMD_CONNECT_REMOTE and data[0] <= CMD_DISCONNECT:
                        if data[1] in self._reqid_to_hd:
                            if type(self._reqid_to_hd[data[1]]) is tuple:
                                pass
                            else:
                                self.update_activity(self._reqid_to_hd[data[1]])
                                self._reqid_to_hd[data[1]].handle_client(r_addr, *data)
                        else:
                            # disconnect
                            rsp_data = self._pack_rsp_data(CMD_DISCONNECT, data[1], RSP_STATE_EMPTY)
                            data_to_send = encrypt.encrypt_all(self._password, self._method, 1, rsp_data)
                            self.write_to_server_socket(data_to_send, r_addr)
                    return
                except Exception as e:
                    trace = traceback.format_exc()
                    logging.error(trace)
                    return

        try:
            header_result = parse_header(data)
        except:
            self._handel_protocol_error(r_addr, ogn_data)
            return

        if header_result is None:
            self._handel_protocol_error(r_addr, ogn_data)
            return
        connecttype, dest_addr, dest_port, header_length = header_result

        if self._is_local:
            server_addr, server_port = self._get_a_server()
        else:
            server_addr, server_port = dest_addr, dest_port

        addrs = self._dns_cache.get(server_addr, None)
        if addrs is None:
            addrs = socket.getaddrinfo(server_addr, server_port, 0,
                                       socket.SOCK_DGRAM, socket.SOL_UDP)
            if not addrs:
                # drop
                return
            else:
                self._dns_cache[server_addr] = addrs

        af, socktype, proto, canonname, sa = addrs[0]
        key = client_key(r_addr, af)
        client = self._cache.get(key, None)
        if not client:
            # TODO async getaddrinfo
            if self._forbidden_iplist:
                if common.to_str(sa[0]) in self._forbidden_iplist:
                    logging.debug('IP %s is in forbidden list, drop' %
                                  common.to_str(sa[0]))
                    # drop
                    return
            client = socket.socket(af, socktype, proto)
            client.setblocking(False)
            self._cache[key] = client
            self._client_fd_to_server_addr[client.fileno()] = r_addr

            self._sockets.add(client.fileno())
            self._eventloop.add(client, eventloop.POLL_IN, self)

            logging.debug('UDP port %5d sockets %d' % (self._listen_port, len(self._sockets)))

            logging.info('UDP data to %s:%d from %s:%d' %
                        (common.to_str(server_addr), server_port,
                            r_addr[0], r_addr[1]))

        if self._is_local:
            data = encrypt.encrypt_all(self._password, self._method, 1, data)
            if not data:
                return
        else:
            data = data[header_length:]
        if not data:
            return
        try:
            #logging.info('UDP handle_server sendto %s:%d %d bytes' % (common.to_str(server_addr), server_port, len(data)))
            client.sendto(data, (server_addr, server_port))
        except IOError as e:
            err = eventloop.errno_from_exception(e)
            if err in (errno.EINPROGRESS, errno.EAGAIN):
                pass
            else:
                shell.print_exception(e)",_1154.py,56,"self._reqid_to_hd[data[1]][0], self._reqid_to_hd[data[1]][1]",*self._reqid_to_hd[data[1]][:2]
https://github.com/shadowsocksr-backup/shadowsocksr/tree/master/shadowsocks/udprelay.py,"def _handle_server(self):
        server = self._server_socket
        data, r_addr = server.recvfrom(BUF_SIZE)
        ogn_data = data
        if not data:
            logging.debug('UDP handle_server: data is empty')
        if self._stat_callback:
            self._stat_callback(self._listen_port, len(data))
        if self._is_local:
            frag = common.ord(data[2])
            if frag != 0:
                logging.warn('drop a message since frag is not 0')
                return
            else:
                data = data[3:]
        else:
            data = encrypt.encrypt_all(self._password, self._method, 0, data)
            # decrypt data
            if not data:
                logging.debug('UDP handle_server: data is empty after decrypt')
                return

        #logging.info(""UDP data %s"" % (binascii.hexlify(data),))
        if not self._is_local:
            data = pre_parse_header(data)

            data = self._pre_parse_udp_header(data)
            if data is None:
                return

            if type(data) is tuple:
                #(cmd, request_id, data)
                #logging.info(""UDP data %d %d %s"" % (data[0], data[1], binascii.hexlify(data[2])))
                try:
                    if data[0] == 0:
                        if len(data[2]) >= 4:
                            for i in range(64):
                                req_id = random.randint(1, 65535)
                                if req_id not in self._reqid_to_hd:
                                    break
                            if req_id in self._reqid_to_hd:
                                for i in range(64):
                                    req_id = random.randint(1, 65535)
                                    if type(self._reqid_to_hd[req_id]) is tuple:
                                        break
                            # return req id
                            self._reqid_to_hd[req_id] = (data[2][0:4], None)
                            rsp_data = self._pack_rsp_data(CMD_RSP_CONNECT, req_id, RSP_STATE_CONNECTED)
                            data_to_send = encrypt.encrypt_all(self._password, self._method, 1, rsp_data)
                            self.write_to_server_socket(data_to_send, r_addr)
                    elif data[0] == CMD_CONNECT_REMOTE:
                        if len(data[2]) > 4 and data[1] in self._reqid_to_hd:
                            # create
                            if type(self._reqid_to_hd[data[1]]) is tuple:
                                if data[2][0:4] == self._reqid_to_hd[data[1]][0]:
                                    handle = TCPRelayHandler(self, self._reqid_to_hd, self._fd_to_handlers,
                                        self._eventloop, self._server_socket,
                                        self._reqid_to_hd[data[1]][0], self._reqid_to_hd[data[1]][1],
                                        self._config, self._dns_resolver, self._is_local)
                                    self._reqid_to_hd[data[1]] = handle
                                    handle.handle_client(r_addr, CMD_CONNECT, data[1], data[2])
                                    handle.handle_client(r_addr, *data)
                                    self.update_activity(handle)
                                else:
                                    # disconnect
                                    rsp_data = self._pack_rsp_data(CMD_DISCONNECT, data[1], RSP_STATE_EMPTY)
                                    data_to_send = encrypt.encrypt_all(self._password, self._method, 1, rsp_data)
                                    self.write_to_server_socket(data_to_send, r_addr)
                            else:
                                self.update_activity(self._reqid_to_hd[data[1]])
                                self._reqid_to_hd[data[1]].handle_client(r_addr, *data)
                        else:
                            # disconnect
                            rsp_data = self._pack_rsp_data(CMD_DISCONNECT, data[1], RSP_STATE_EMPTY)
                            data_to_send = encrypt.encrypt_all(self._password, self._method, 1, rsp_data)
                            self.write_to_server_socket(data_to_send, r_addr)
                    elif data[0] > CMD_CONNECT_REMOTE and data[0] <= CMD_DISCONNECT:
                        if data[1] in self._reqid_to_hd:
                            if type(self._reqid_to_hd[data[1]]) is tuple:
                                pass
                            else:
                                self.update_activity(self._reqid_to_hd[data[1]])
                                self._reqid_to_hd[data[1]].handle_client(r_addr, *data)
                        else:
                            # disconnect
                            rsp_data = self._pack_rsp_data(CMD_DISCONNECT, data[1], RSP_STATE_EMPTY)
                            data_to_send = encrypt.encrypt_all(self._password, self._method, 1, rsp_data)
                            self.write_to_server_socket(data_to_send, r_addr)
                    return
                except Exception as e:
                    trace = traceback.format_exc()
                    logging.error(trace)
                    return

        try:
            header_result = parse_header(data)
        except:
            self._handel_protocol_error(r_addr, ogn_data)
            return

        if header_result is None:
            self._handel_protocol_error(r_addr, ogn_data)
            return
        connecttype, dest_addr, dest_port, header_length = header_result

        if self._is_local:
            server_addr, server_port = self._get_a_server()
        else:
            server_addr, server_port = dest_addr, dest_port

        addrs = self._dns_cache.get(server_addr, None)
        if addrs is None:
            addrs = socket.getaddrinfo(server_addr, server_port, 0,
                                       socket.SOCK_DGRAM, socket.SOL_UDP)
            if not addrs:
                # drop
                return
            else:
                self._dns_cache[server_addr] = addrs

        af, socktype, proto, canonname, sa = addrs[0]
        key = client_key(r_addr, af)
        client = self._cache.get(key, None)
        if not client:
            # TODO async getaddrinfo
            if self._forbidden_iplist:
                if common.to_str(sa[0]) in self._forbidden_iplist:
                    logging.debug('IP %s is in forbidden list, drop' %
                                  common.to_str(sa[0]))
                    # drop
                    return
            client = socket.socket(af, socktype, proto)
            client.setblocking(False)
            self._cache[key] = client
            self._client_fd_to_server_addr[client.fileno()] = r_addr

            self._sockets.add(client.fileno())
            self._eventloop.add(client, eventloop.POLL_IN, self)

            logging.debug('UDP port %5d sockets %d' % (self._listen_port, len(self._sockets)))

            logging.info('UDP data to %s:%d from %s:%d' %
                        (common.to_str(server_addr), server_port,
                            r_addr[0], r_addr[1]))

        if self._is_local:
            data = encrypt.encrypt_all(self._password, self._method, 1, data)
            if not data:
                return
        else:
            data = data[header_length:]
        if not data:
            return
        try:
            #logging.info('UDP handle_server sendto %s:%d %d bytes' % (common.to_str(server_addr), server_port, len(data)))
            client.sendto(data, (server_addr, server_port))
        except IOError as e:
            err = eventloop.errno_from_exception(e)
            if err in (errno.EINPROGRESS, errno.EAGAIN):
                pass
            else:
                shell.print_exception(e)",_1154.py,61,"data[1], data[2]",*data[1:3]
https://github.com/sqlfluff/sqlfluff/tree/master/src/sqlfluff/core/templaters/placeholder.py,"def process(
        self, *, in_str: str, fname: str, config=None, formatter=None
    ) -> Tuple[Optional[TemplatedFile], list]:
        """"""Process a string and return a TemplatedFile.

        Note that the arguments are enforced as keywords
        because Templaters can have differences in their
        `process` method signature.
        A Templater that only supports reading from a file
        would need the following signature:
            process(*, fname, in_str=None, config=None)
        (arguments are swapped)

        Args:
            in_str (:obj:`str`): The input string.
            fname (:obj:`str`, optional): The filename of this string. This is
                mostly for loading config files at runtime.
            config (:obj:`FluffConfig`): A specific config to use for this
                templating operation. Only necessary for some templaters.
            formatter (:obj:`CallbackFormatter`): Optional object for output.

        """"""
        context = self.get_context(config)
        template_slices = []
        raw_slices = []
        last_pos_raw, last_pos_templated = 0, 0
        out_str = """"

        regex = context[""__bind_param_regex""]
        # when the param has no name, use a 1-based index
        param_counter = 1
        for found_param in regex.finditer(in_str):
            span = found_param.span()
            if ""param_name"" not in found_param.groupdict():
                param_name = str(param_counter)
                param_counter += 1
            else:
                param_name = found_param[""param_name""]
            last_literal_length = span[0] - last_pos_raw
            try:
                replacement = context[param_name]
            except KeyError as err:
                # TODO: Add a url here so people can get more help.
                raise SQLTemplaterError(
                    ""Failure in placeholder templating: {}. Have you configured your variables?"".format(
                        err
                    )
                )
            # add the literal to the slices
            template_slices.append(
                TemplatedFileSlice(
                    slice_type=""literal"",
                    source_slice=slice(last_pos_raw, span[0], None),
                    templated_slice=slice(
                        last_pos_templated,
                        last_pos_templated + last_literal_length,
                        None,
                    ),
                )
            )
            raw_slices.append(
                RawFileSlice(
                    raw=in_str[last_pos_raw : span[0]],
                    slice_type=""literal"",
                    source_idx=last_pos_raw,
                )
            )
            out_str += in_str[last_pos_raw : span[0]]
            # add the current replaced element
            start_template_pos = last_pos_templated + last_literal_length
            template_slices.append(
                TemplatedFileSlice(
                    slice_type=""block_start"",
                    source_slice=slice(span[0], span[1], None),
                    templated_slice=slice(
                        start_template_pos, start_template_pos + len(replacement), None
                    ),
                )
            )
            raw_slices.append(
                RawFileSlice(
                    raw=in_str[span[0] : span[1]],
                    slice_type=""block_start"",
                    source_idx=span[0],
                )
            )
            out_str += replacement
            # update the indexes
            last_pos_raw = span[1]
            last_pos_templated = start_template_pos + len(replacement)
        # add the last literal, if any
        if len(in_str) > last_pos_raw:
            template_slices.append(
                TemplatedFileSlice(
                    slice_type=""literal"",
                    source_slice=slice(last_pos_raw, len(in_str), None),
                    templated_slice=slice(
                        last_pos_templated,
                        last_pos_templated + (len(in_str) - last_pos_raw),
                        None,
                    ),
                )
            )
            raw_slices.append(
                RawFileSlice(
                    raw=in_str[last_pos_raw:],
                    slice_type=""literal"",
                    source_idx=last_pos_raw,
                )
            )
            out_str += in_str[last_pos_raw:]
        return (
            TemplatedFile(
                # original string
                source_str=in_str,
                # string after all replacements
                templated_str=out_str,
                # filename
                fname=fname,
                # list of TemplatedFileSlice
                sliced_file=template_slices,
                # list of RawFileSlice, same size
                raw_sliced=raw_slices,
            ),
            [],  # violations, always empty
        )",_1464.py,74,"span[0], span[1]",*span[:2]
https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_lookahead_swap.py,"def test_lookahead_swap_hang_in_min_case(self):
        """"""Verify LookaheadSwap does not stall in minimal case.""""""
        # ref: https://github.com/Qiskit/qiskit-terra/issues/2171

        qr = QuantumRegister(14, ""q"")
        qc = QuantumCircuit(qr)
        qc.cx(qr[0], qr[13])
        qc.cx(qr[1], qr[13])
        qc.cx(qr[1], qr[0])
        qc.cx(qr[13], qr[1])
        dag = circuit_to_dag(qc)

        cmap = CouplingMap(FakeMelbourne().configuration().coupling_map)

        out = LookaheadSwap(cmap, search_depth=4, search_width=4).run(dag)

        self.assertIsInstance(out, DAGCircuit)",_1606.py,7,"qr[0], qr[13]",*qr[:26:13]
https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_lookahead_swap.py,"def test_lookahead_swap_hang_in_min_case(self):
        """"""Verify LookaheadSwap does not stall in minimal case.""""""
        # ref: https://github.com/Qiskit/qiskit-terra/issues/2171

        qr = QuantumRegister(14, ""q"")
        qc = QuantumCircuit(qr)
        qc.cx(qr[0], qr[13])
        qc.cx(qr[1], qr[13])
        qc.cx(qr[1], qr[0])
        qc.cx(qr[13], qr[1])
        dag = circuit_to_dag(qc)

        cmap = CouplingMap(FakeMelbourne().configuration().coupling_map)

        out = LookaheadSwap(cmap, search_depth=4, search_width=4).run(dag)

        self.assertIsInstance(out, DAGCircuit)",_1606.py,8,"qr[1], qr[13]",*qr[1:25:12]
https://github.com/moloch--/RootTheBox/tree/master/libs/ChatManager.py,"def create_user(self, user, password):
        # Create the user's account on RocketChat
        if not self.rocket:
            return
        email = user.email
        if email is None:
            email = ""%s@rocketchat.com"" % user.uuid
        account = self.rocket.users_create(
            email, user.name, password, user.handle.replace("" "", ""_"")
        ).json()
        self.create_team(user.team, account)",_1980.py,9,"' ', '_'",*' _'
https://github.com/redis/redis-py/tree/master/tests/test_commands.py,"def test_lindex(self, r):
        r.rpush(""a"", ""1"", ""2"", ""3"")
        assert r.lindex(""a"", ""0"") == b""1""
        assert r.lindex(""a"", ""1"") == b""2""
        assert r.lindex(""a"", ""2"") == b""3""",_2215.py,2,"'a', '1', '2', '3'",*'a123'
https://github.com/redis/redis-py/tree/master/tests/test_commands.py,"def test_lindex(self, r):
        r.rpush(""a"", ""1"", ""2"", ""3"")
        assert r.lindex(""a"", ""0"") == b""1""
        assert r.lindex(""a"", ""1"") == b""2""
        assert r.lindex(""a"", ""2"") == b""3""",_2215.py,2,"'a', '1', '2', '3'",*'a123'
https://github.com/redis/redis-py/tree/master/tests/test_commands.py,"def test_lindex(self, r):
        r.rpush(""a"", ""1"", ""2"", ""3"")
        assert r.lindex(""a"", ""0"") == b""1""
        assert r.lindex(""a"", ""1"") == b""2""
        assert r.lindex(""a"", ""2"") == b""3""",_2215.py,2,"'a', '1', '2', '3'",*'a123'
https://github.com/redis/redis-py/tree/master/tests/test_commands.py,"def test_lindex(self, r):
        r.rpush(""a"", ""1"", ""2"", ""3"")
        assert r.lindex(""a"", ""0"") == b""1""
        assert r.lindex(""a"", ""1"") == b""2""
        assert r.lindex(""a"", ""2"") == b""3""",_2215.py,3,"'a', '0'",*'a0'
https://github.com/redis/redis-py/tree/master/tests/test_commands.py,"def test_lindex(self, r):
        r.rpush(""a"", ""1"", ""2"", ""3"")
        assert r.lindex(""a"", ""0"") == b""1""
        assert r.lindex(""a"", ""1"") == b""2""
        assert r.lindex(""a"", ""2"") == b""3""",_2215.py,4,"'a', '1'",*'a1'
https://github.com/redis/redis-py/tree/master/tests/test_commands.py,"def test_lindex(self, r):
        r.rpush(""a"", ""1"", ""2"", ""3"")
        assert r.lindex(""a"", ""0"") == b""1""
        assert r.lindex(""a"", ""1"") == b""2""
        assert r.lindex(""a"", ""2"") == b""3""",_2215.py,5,"'a', '2'",*'a2'
https://github.com/PeizeSun/OneNet/tree/master/detectron2/data/datasets/crowdhuman.py,"if __name__ == ""__main__"":
    """"""
    Test the COCO json dataset loader.

    Usage:
        python -m detectron2.data.datasets.coco \
            path/to/json path/to/image_root dataset_name

        ""dataset_name"" can be ""coco_2014_minival_100"", or other
        pre-registered ones
    """"""
    from detectron2.utils.logger import setup_logger
    from detectron2.utils.visualizer import Visualizer
    import detectron2.data.datasets  # noqa # add pre-defined metadata
    import sys

    #logger = setup_logger(name=__name__)
    #assert sys.argv[3] in DatasetCatalog.list()
    # meta = MetadataCatalog.get(sys.argv[3])

    dicts = load_crowdhuman_json(sys.argv[1], sys.argv[2], sys.argv[3])
    logger.info(""Done loading {} samples."".format(len(dicts)))

    dirname = ""coco-data-vis""
    os.makedirs(dirname, exist_ok=True)
    for d in dicts:
        img = np.array(Image.open(d[""file_name""]))
        visualizer = Visualizer(img, metadata=meta)
        vis = visualizer.draw_dataset_dict(d)
        fpath = os.path.join(dirname, os.path.basename(d[""file_name""]))
        vis.save(fpath)",_2574.py,21,"sys.argv[1], sys.argv[2], sys.argv[3]",*sys.argv[1:4]
https://github.com/google/clusterfuzz/tree/master/src/clusterfuzz/_internal/build_management/source_mapper.py,"def normalize_source_path(path):
  """"""Normalizes source path for comparison with component sources.""""""
  # Account for ../../ at start of path due to working directory
  # out/<build_dir>/ at time of build generation (chromium only).
  path = utils.remove_prefix(path, '../../')

  # Remove /proc/self/cwd prefix added by Bazel.
  path = utils.remove_prefix(path, '/proc/self/cwd/')

  # Cross-platform way to determine path absoluteness.
  is_path_absolute = path.startswith('/') or DRIVE_LETTER_REGEX.match(path)

  # Normalize backslashes into slashes.
  normalized_path = path.replace('\\', '/')

  if is_path_absolute:
    source_start_id_index = normalized_path.find(SOURCE_START_ID)
    if source_start_id_index == -1:
      # This absolute path does not have source start id, so we cannot
      # figure out a relative path. Bail out.
      return None

    return normalized_path[source_start_id_index + len(SOURCE_START_ID):]

  return normalized_path",_3377.py,14,"'\\', '/'",*'\\/'
https://github.com/apache/tvm/tree/master/tests/python/frontend/pytorch/test_forward.py,"def forward(self, *args):
            return torch.matmul(args[0], args[1])",_3422.py,2,"args[0], args[1]",*args[:2]
https://github.com/keras-team/keras/tree/master/keras/optimizer_v2/optimizer_v2_test.py,"def identify_redundant_ops(graph):
  """"""Implements basic common subexpression elimination.

  This is not intended to replicate the graph semantics of TensorFlow Graphs
  (for instance it does not handle stateful op ordering), nor is it intended to
  replace the common subexpression elimination Grappler pass. Rather, it
  provides a high level sanity check that clearly redundant ops are not being
  created.

  Args:
    graph: The graph to be analyzed.

  Returns:
    A count of the duplicate ops and a description of the structure of each.
  """"""
  sorted_ops = topological_sort(graph)
  duplicates = collections.defaultdict(list)
  unified_node_defs = {}
  name_map = {}

  for op in sorted_ops:
    input_names = []
    for op_input, name in zip(*get_inputs(op)):
      input_def = op_input.node_def

      # Operations can have multiple outputs. We track which is used to prevent
      # overzealous elimination.
      input_def.name = name

      input_def.input[:] = [name_map.get(i, i) for i in input_def.input]
      strip_name(input_def)

      # NodeDef.SerializeToString() does not provide identical serialized
      # representations for identical NodeDefs, so we instead use string
      # representation as a dict key.
      key = repr(input_def)

      if key in unified_node_defs:
        input_names.append(unified_node_defs[key])

      else:
        unified_node_defs[key] = op_input.name
        input_names.append(name)

    node_def = op.node_def
    node_def.input[:] = input_names
    strip_name(node_def)

    key = repr(node_def)
    duplicates[key].append(op)
    name_map[op.name] = duplicates[key][0].name

  num_duplicates = 0
  duplicate_types = []
  for standard_def, op_defs in duplicates.items():
    # We are only interested in testing the apply method of the optimizer
    op_defs = [i for i in op_defs if APPLY_SCOPE in i.name]

    # We only check for per-apply redundant ops.
    if len(op_defs) < _NUM_LEARNERS:
      continue

    # Certain ops are simply not worth eliminating, and are instead simply
    # ignored.
    name, op_type = op_defs[0].name, op_defs[0].type
    if any(allowlisted_scope in name and op_type == allowlisted_type
           for allowlisted_scope, allowlisted_type in ALLOWLIST):
      continue

    num_duplicates += len(op_defs)
    traceback = []
    for level in op_defs[0].traceback:
      traceback.append('  {} {}:{}'.format(level[0], level[2], level[1]))

    duplicate_types.append(
        '# Example name: {}\n# Op creation stack:\n{}\n{}'.format(
            op_defs[0].name,
            '\n'.join(traceback),
            standard_def))

  return num_duplicates, duplicate_types",_3432.py,73,"level[0], level[2]",*level[:4:2]
https://github.com/svip-lab/impersonator/tree/master/thirdparty/his_evaluators/his_evaluators/metrics/lpips/util/util.py,"def normalize_tensor(in_feat, eps=1e-10):
    # norm_factor = torch.sqrt(torch.sum(in_feat**2,dim=1)).view(in_feat.size()[0],1,in_feat.size()[2],in_feat.size()[3]).repeat(1,in_feat.size()[1],1,1)
    norm_factor = torch.sqrt(torch.sum(in_feat ** 2, dim=1)).view(in_feat.size()[0], 1, in_feat.size()[2],
                                                                  in_feat.size()[3])
    return in_feat / (norm_factor.expand_as(in_feat) + eps)",_3474.py,3,"in_feat.size()[2], in_feat.size()[3]",*in_feat.size()[2:4]
https://github.com/open-mmlab/OpenPCDet/tree/master/pcdet/ops/pointnet2/pointnet2_batch/pointnet2_modules.py,"def __init__(self, *, npoint: int, radii: List[float], nsamples: List[int], mlps: List[List[int]], bn: bool = True,
                 use_xyz: bool = True, pool_method='max_pool'):
        """"""
        :param npoint: int
        :param radii: list of float, list of radii to group with
        :param nsamples: list of int, number of samples in each ball query
        :param mlps: list of list of int, spec of the pointnet before the global pooling for each scale
        :param bn: whether to use batchnorm
        :param use_xyz:
        :param pool_method: max_pool / avg_pool
        """"""
        super().__init__()

        assert len(radii) == len(nsamples) == len(mlps)

        self.npoint = npoint
        self.groupers = nn.ModuleList()
        self.mlps = nn.ModuleList()
        for i in range(len(radii)):
            radius = radii[i]
            nsample = nsamples[i]
            self.groupers.append(
                pointnet2_utils.QueryAndGroup(radius, nsample, use_xyz=use_xyz)
                if npoint is not None else pointnet2_utils.GroupAll(use_xyz)
            )
            mlp_spec = mlps[i]
            if use_xyz:
                mlp_spec[0] += 3

            shared_mlps = []
            for k in range(len(mlp_spec) - 1):
                shared_mlps.extend([
                    nn.Conv2d(mlp_spec[k], mlp_spec[k + 1], kernel_size=1, bias=False),
                    nn.BatchNorm2d(mlp_spec[k + 1]),
                    nn.ReLU()
                ])
            self.mlps.append(nn.Sequential(*shared_mlps))

        self.pool_method = pool_method",_3511.py,33,"mlp_spec[k], mlp_spec[k + 1]",*mlp_spec[k:k + 2]
https://github.com/yashaka/selene/tree/master/tests/acceptance/helpers/givenpage.py,"def render_body(self, body, timeout=0):
        self._driver.execute_script(
            'setTimeout(function() { document.getElementsByTagName(""body"")[0].innerHTML = ""'
            + body.replace('\n', ' ').replace('""', '\\""')
            + '"";}, '
            + str(convert_sec_to_ms(timeout))
            + ');'
        )
        return self",_3523.py,4,"'\n', ' '",*'\n '
https://github.com/hanyazou/TelloPy/tree/master/tellopy/_internal/tello.py,"def __process_packet(self, data):
        if isinstance(data, str):
            data = bytearray([x for x in data])

        if str(data[0:9]) == 'conn_ack:' or data[0:9] == b'conn_ack:':
            log.info('connected. (port=%2x%2x)' % (data[9], data[10]))
            log.debug('    %s' % byte_to_hexstring(data))
            if self.video_enabled:
                self.__send_exposure()
                self.__send_video_encoder_rate()
                self.__send_start_video()
            self.__publish(self.__EVENT_CONN_ACK, data)

            return True

        if data[0] != START_OF_PACKET:
            log.info('start of packet != %02x (%02x) (ignored)' % (START_OF_PACKET, data[0]))
            log.info('    %s' % byte_to_hexstring(data))
            log.info('    %s' % str(map(chr, data))[1:-1])
            return False

        pkt = Packet(data)
        cmd = uint16(data[5], data[6])
        if cmd == LOG_HEADER_MSG:
            id = uint16(data[9], data[10])
            log.info(""recv: log_header: id=%04x, '%s'"" % (id, str(data[28:54])))
            log.debug(""recv: log_header: %s"" % byte_to_hexstring(data[9:]))
            self.__send_ack_log(id)
            self.__publish(event=self.EVENT_LOG_HEADER, data=data[9:])
            if self.log_data_file and not self.log_data_header_recorded:
                self.log_data_file.write(data[12:-2])
                self.log_data_header_recorded = True
        elif cmd == LOG_DATA_MSG:
            log.debug(""recv: log_data: length=%d, %s"" % (len(data[9:]), byte_to_hexstring(data[9:])))
            self.__publish(event=self.EVENT_LOG_RAWDATA, data=data[9:])
            try:
                self.log_data.update(data[10:])
                if self.log_data_file:
                    self.log_data_file.write(data[10:-2])
            except Exception as ex:
                log.error('%s' % str(ex))
            self.__publish(event=self.EVENT_LOG_DATA, data=self.log_data)

        elif cmd == LOG_CONFIG_MSG:
            log.debug(""recv: log_config: length=%d, %s"" % (len(data[9:]), byte_to_hexstring(data[9:])))
            self.__publish(event=self.EVENT_LOG_CONFIG, data=data[9:])
        elif cmd == WIFI_MSG:
            log.debug(""recv: wifi: %s"" % byte_to_hexstring(data[9:]))
            self.wifi_strength = data[9]
            self.__publish(event=self.EVENT_WIFI, data=data[9:])
        elif cmd == ALT_LIMIT_MSG:
            log.info(""recv: altitude limit: %s"" % byte_to_hexstring(data[9:-2]))
        elif cmd == ATT_LIMIT_MSG:
            log.info(""recv: attitude limit: %s"" % byte_to_hexstring(data[9:-2]))
        elif cmd == LOW_BAT_THRESHOLD_MSG:
            log.info(""recv: low battery threshold: %s"" % byte_to_hexstring(data[9:-2]))
        elif cmd == LIGHT_MSG:
            log.debug(""recv: light: %s"" % byte_to_hexstring(data[9:-2]))
            self.__publish(event=self.EVENT_LIGHT, data=data[9:])
        elif cmd == FLIGHT_MSG:
            flight_data = FlightData(data[9:])
            flight_data.wifi_strength = self.wifi_strength
            log.debug(""recv: flight data: %s"" % str(flight_data))
            self.__publish(event=self.EVENT_FLIGHT_DATA, data=flight_data)
        elif cmd == TIME_CMD:
            log.debug(""recv: time data: %s"" % byte_to_hexstring(data))
            self.__publish(event=self.EVENT_TIME, data=data[7:9])
        elif cmd in (SET_ALT_LIMIT_CMD, ATT_LIMIT_CMD, LOW_BAT_THRESHOLD_CMD, TAKEOFF_CMD, LAND_CMD, VIDEO_START_CMD, VIDEO_ENCODER_RATE_CMD, PALM_LAND_CMD,
                     EXPOSURE_CMD, THROW_AND_GO_CMD, EMERGENCY_CMD):
            log.debug(""recv: ack: cmd=0x%02x seq=0x%04x %s"" %
                     (uint16(data[5], data[6]), uint16(data[7], data[8]), byte_to_hexstring(data)))
        elif cmd == TELLO_CMD_FILE_SIZE:
            # Drone is about to send us a file. Get ready.
            # N.b. one of the fields in the packet is a file ID; by demuxing
            # based on file ID we can receive multiple files at once. This
            # code doesn't support that yet, though, so don't take one photo
            # while another is still being received.
            log.info(""recv: file size: %s"" % byte_to_hexstring(data))
            if len(pkt.get_data()) >= 7:
                (size, filenum) = struct.unpack('<xLH', pkt.get_data())
                log.info('      file size: num=%d bytes=%d' % (filenum, size))
                # Initialize file download state.
                self.file_recv[filenum] = DownloadedFile(filenum, size)
            else:
                # We always seem to get two files, one with most of the payload missing.
                # Not sure what the second one is for.
                log.warn('      file size: payload too small: %s' % byte_to_hexstring(pkt.get_data()))
            # Ack the packet.
            self.send_packet(pkt)
        elif cmd == TELLO_CMD_FILE_DATA:
            # log.info(""recv: file data: %s"" % byte_to_hexstring(data[9:21]))
            # Drone is sending us a fragment of a file it told us to prepare
            # for earlier.
            self.recv_file_data(pkt.get_data())
        else:
            log.info('unknown packet: %04x %s' % (cmd, byte_to_hexstring(data)))
            return False

        return True",_3650.py,23,"data[5], data[6]",*data[5:7]
https://github.com/hanyazou/TelloPy/tree/master/tellopy/_internal/tello.py,"def __process_packet(self, data):
        if isinstance(data, str):
            data = bytearray([x for x in data])

        if str(data[0:9]) == 'conn_ack:' or data[0:9] == b'conn_ack:':
            log.info('connected. (port=%2x%2x)' % (data[9], data[10]))
            log.debug('    %s' % byte_to_hexstring(data))
            if self.video_enabled:
                self.__send_exposure()
                self.__send_video_encoder_rate()
                self.__send_start_video()
            self.__publish(self.__EVENT_CONN_ACK, data)

            return True

        if data[0] != START_OF_PACKET:
            log.info('start of packet != %02x (%02x) (ignored)' % (START_OF_PACKET, data[0]))
            log.info('    %s' % byte_to_hexstring(data))
            log.info('    %s' % str(map(chr, data))[1:-1])
            return False

        pkt = Packet(data)
        cmd = uint16(data[5], data[6])
        if cmd == LOG_HEADER_MSG:
            id = uint16(data[9], data[10])
            log.info(""recv: log_header: id=%04x, '%s'"" % (id, str(data[28:54])))
            log.debug(""recv: log_header: %s"" % byte_to_hexstring(data[9:]))
            self.__send_ack_log(id)
            self.__publish(event=self.EVENT_LOG_HEADER, data=data[9:])
            if self.log_data_file and not self.log_data_header_recorded:
                self.log_data_file.write(data[12:-2])
                self.log_data_header_recorded = True
        elif cmd == LOG_DATA_MSG:
            log.debug(""recv: log_data: length=%d, %s"" % (len(data[9:]), byte_to_hexstring(data[9:])))
            self.__publish(event=self.EVENT_LOG_RAWDATA, data=data[9:])
            try:
                self.log_data.update(data[10:])
                if self.log_data_file:
                    self.log_data_file.write(data[10:-2])
            except Exception as ex:
                log.error('%s' % str(ex))
            self.__publish(event=self.EVENT_LOG_DATA, data=self.log_data)

        elif cmd == LOG_CONFIG_MSG:
            log.debug(""recv: log_config: length=%d, %s"" % (len(data[9:]), byte_to_hexstring(data[9:])))
            self.__publish(event=self.EVENT_LOG_CONFIG, data=data[9:])
        elif cmd == WIFI_MSG:
            log.debug(""recv: wifi: %s"" % byte_to_hexstring(data[9:]))
            self.wifi_strength = data[9]
            self.__publish(event=self.EVENT_WIFI, data=data[9:])
        elif cmd == ALT_LIMIT_MSG:
            log.info(""recv: altitude limit: %s"" % byte_to_hexstring(data[9:-2]))
        elif cmd == ATT_LIMIT_MSG:
            log.info(""recv: attitude limit: %s"" % byte_to_hexstring(data[9:-2]))
        elif cmd == LOW_BAT_THRESHOLD_MSG:
            log.info(""recv: low battery threshold: %s"" % byte_to_hexstring(data[9:-2]))
        elif cmd == LIGHT_MSG:
            log.debug(""recv: light: %s"" % byte_to_hexstring(data[9:-2]))
            self.__publish(event=self.EVENT_LIGHT, data=data[9:])
        elif cmd == FLIGHT_MSG:
            flight_data = FlightData(data[9:])
            flight_data.wifi_strength = self.wifi_strength
            log.debug(""recv: flight data: %s"" % str(flight_data))
            self.__publish(event=self.EVENT_FLIGHT_DATA, data=flight_data)
        elif cmd == TIME_CMD:
            log.debug(""recv: time data: %s"" % byte_to_hexstring(data))
            self.__publish(event=self.EVENT_TIME, data=data[7:9])
        elif cmd in (SET_ALT_LIMIT_CMD, ATT_LIMIT_CMD, LOW_BAT_THRESHOLD_CMD, TAKEOFF_CMD, LAND_CMD, VIDEO_START_CMD, VIDEO_ENCODER_RATE_CMD, PALM_LAND_CMD,
                     EXPOSURE_CMD, THROW_AND_GO_CMD, EMERGENCY_CMD):
            log.debug(""recv: ack: cmd=0x%02x seq=0x%04x %s"" %
                     (uint16(data[5], data[6]), uint16(data[7], data[8]), byte_to_hexstring(data)))
        elif cmd == TELLO_CMD_FILE_SIZE:
            # Drone is about to send us a file. Get ready.
            # N.b. one of the fields in the packet is a file ID; by demuxing
            # based on file ID we can receive multiple files at once. This
            # code doesn't support that yet, though, so don't take one photo
            # while another is still being received.
            log.info(""recv: file size: %s"" % byte_to_hexstring(data))
            if len(pkt.get_data()) >= 7:
                (size, filenum) = struct.unpack('<xLH', pkt.get_data())
                log.info('      file size: num=%d bytes=%d' % (filenum, size))
                # Initialize file download state.
                self.file_recv[filenum] = DownloadedFile(filenum, size)
            else:
                # We always seem to get two files, one with most of the payload missing.
                # Not sure what the second one is for.
                log.warn('      file size: payload too small: %s' % byte_to_hexstring(pkt.get_data()))
            # Ack the packet.
            self.send_packet(pkt)
        elif cmd == TELLO_CMD_FILE_DATA:
            # log.info(""recv: file data: %s"" % byte_to_hexstring(data[9:21]))
            # Drone is sending us a fragment of a file it told us to prepare
            # for earlier.
            self.recv_file_data(pkt.get_data())
        else:
            log.info('unknown packet: %04x %s' % (cmd, byte_to_hexstring(data)))
            return False

        return True",_3650.py,25,"data[9], data[10]",*data[9:11]
https://github.com/hanyazou/TelloPy/tree/master/tellopy/_internal/tello.py,"def __process_packet(self, data):
        if isinstance(data, str):
            data = bytearray([x for x in data])

        if str(data[0:9]) == 'conn_ack:' or data[0:9] == b'conn_ack:':
            log.info('connected. (port=%2x%2x)' % (data[9], data[10]))
            log.debug('    %s' % byte_to_hexstring(data))
            if self.video_enabled:
                self.__send_exposure()
                self.__send_video_encoder_rate()
                self.__send_start_video()
            self.__publish(self.__EVENT_CONN_ACK, data)

            return True

        if data[0] != START_OF_PACKET:
            log.info('start of packet != %02x (%02x) (ignored)' % (START_OF_PACKET, data[0]))
            log.info('    %s' % byte_to_hexstring(data))
            log.info('    %s' % str(map(chr, data))[1:-1])
            return False

        pkt = Packet(data)
        cmd = uint16(data[5], data[6])
        if cmd == LOG_HEADER_MSG:
            id = uint16(data[9], data[10])
            log.info(""recv: log_header: id=%04x, '%s'"" % (id, str(data[28:54])))
            log.debug(""recv: log_header: %s"" % byte_to_hexstring(data[9:]))
            self.__send_ack_log(id)
            self.__publish(event=self.EVENT_LOG_HEADER, data=data[9:])
            if self.log_data_file and not self.log_data_header_recorded:
                self.log_data_file.write(data[12:-2])
                self.log_data_header_recorded = True
        elif cmd == LOG_DATA_MSG:
            log.debug(""recv: log_data: length=%d, %s"" % (len(data[9:]), byte_to_hexstring(data[9:])))
            self.__publish(event=self.EVENT_LOG_RAWDATA, data=data[9:])
            try:
                self.log_data.update(data[10:])
                if self.log_data_file:
                    self.log_data_file.write(data[10:-2])
            except Exception as ex:
                log.error('%s' % str(ex))
            self.__publish(event=self.EVENT_LOG_DATA, data=self.log_data)

        elif cmd == LOG_CONFIG_MSG:
            log.debug(""recv: log_config: length=%d, %s"" % (len(data[9:]), byte_to_hexstring(data[9:])))
            self.__publish(event=self.EVENT_LOG_CONFIG, data=data[9:])
        elif cmd == WIFI_MSG:
            log.debug(""recv: wifi: %s"" % byte_to_hexstring(data[9:]))
            self.wifi_strength = data[9]
            self.__publish(event=self.EVENT_WIFI, data=data[9:])
        elif cmd == ALT_LIMIT_MSG:
            log.info(""recv: altitude limit: %s"" % byte_to_hexstring(data[9:-2]))
        elif cmd == ATT_LIMIT_MSG:
            log.info(""recv: attitude limit: %s"" % byte_to_hexstring(data[9:-2]))
        elif cmd == LOW_BAT_THRESHOLD_MSG:
            log.info(""recv: low battery threshold: %s"" % byte_to_hexstring(data[9:-2]))
        elif cmd == LIGHT_MSG:
            log.debug(""recv: light: %s"" % byte_to_hexstring(data[9:-2]))
            self.__publish(event=self.EVENT_LIGHT, data=data[9:])
        elif cmd == FLIGHT_MSG:
            flight_data = FlightData(data[9:])
            flight_data.wifi_strength = self.wifi_strength
            log.debug(""recv: flight data: %s"" % str(flight_data))
            self.__publish(event=self.EVENT_FLIGHT_DATA, data=flight_data)
        elif cmd == TIME_CMD:
            log.debug(""recv: time data: %s"" % byte_to_hexstring(data))
            self.__publish(event=self.EVENT_TIME, data=data[7:9])
        elif cmd in (SET_ALT_LIMIT_CMD, ATT_LIMIT_CMD, LOW_BAT_THRESHOLD_CMD, TAKEOFF_CMD, LAND_CMD, VIDEO_START_CMD, VIDEO_ENCODER_RATE_CMD, PALM_LAND_CMD,
                     EXPOSURE_CMD, THROW_AND_GO_CMD, EMERGENCY_CMD):
            log.debug(""recv: ack: cmd=0x%02x seq=0x%04x %s"" %
                     (uint16(data[5], data[6]), uint16(data[7], data[8]), byte_to_hexstring(data)))
        elif cmd == TELLO_CMD_FILE_SIZE:
            # Drone is about to send us a file. Get ready.
            # N.b. one of the fields in the packet is a file ID; by demuxing
            # based on file ID we can receive multiple files at once. This
            # code doesn't support that yet, though, so don't take one photo
            # while another is still being received.
            log.info(""recv: file size: %s"" % byte_to_hexstring(data))
            if len(pkt.get_data()) >= 7:
                (size, filenum) = struct.unpack('<xLH', pkt.get_data())
                log.info('      file size: num=%d bytes=%d' % (filenum, size))
                # Initialize file download state.
                self.file_recv[filenum] = DownloadedFile(filenum, size)
            else:
                # We always seem to get two files, one with most of the payload missing.
                # Not sure what the second one is for.
                log.warn('      file size: payload too small: %s' % byte_to_hexstring(pkt.get_data()))
            # Ack the packet.
            self.send_packet(pkt)
        elif cmd == TELLO_CMD_FILE_DATA:
            # log.info(""recv: file data: %s"" % byte_to_hexstring(data[9:21]))
            # Drone is sending us a fragment of a file it told us to prepare
            # for earlier.
            self.recv_file_data(pkt.get_data())
        else:
            log.info('unknown packet: %04x %s' % (cmd, byte_to_hexstring(data)))
            return False

        return True",_3650.py,71,"data[5], data[6]",*data[5:7]
https://github.com/hanyazou/TelloPy/tree/master/tellopy/_internal/tello.py,"def __process_packet(self, data):
        if isinstance(data, str):
            data = bytearray([x for x in data])

        if str(data[0:9]) == 'conn_ack:' or data[0:9] == b'conn_ack:':
            log.info('connected. (port=%2x%2x)' % (data[9], data[10]))
            log.debug('    %s' % byte_to_hexstring(data))
            if self.video_enabled:
                self.__send_exposure()
                self.__send_video_encoder_rate()
                self.__send_start_video()
            self.__publish(self.__EVENT_CONN_ACK, data)

            return True

        if data[0] != START_OF_PACKET:
            log.info('start of packet != %02x (%02x) (ignored)' % (START_OF_PACKET, data[0]))
            log.info('    %s' % byte_to_hexstring(data))
            log.info('    %s' % str(map(chr, data))[1:-1])
            return False

        pkt = Packet(data)
        cmd = uint16(data[5], data[6])
        if cmd == LOG_HEADER_MSG:
            id = uint16(data[9], data[10])
            log.info(""recv: log_header: id=%04x, '%s'"" % (id, str(data[28:54])))
            log.debug(""recv: log_header: %s"" % byte_to_hexstring(data[9:]))
            self.__send_ack_log(id)
            self.__publish(event=self.EVENT_LOG_HEADER, data=data[9:])
            if self.log_data_file and not self.log_data_header_recorded:
                self.log_data_file.write(data[12:-2])
                self.log_data_header_recorded = True
        elif cmd == LOG_DATA_MSG:
            log.debug(""recv: log_data: length=%d, %s"" % (len(data[9:]), byte_to_hexstring(data[9:])))
            self.__publish(event=self.EVENT_LOG_RAWDATA, data=data[9:])
            try:
                self.log_data.update(data[10:])
                if self.log_data_file:
                    self.log_data_file.write(data[10:-2])
            except Exception as ex:
                log.error('%s' % str(ex))
            self.__publish(event=self.EVENT_LOG_DATA, data=self.log_data)

        elif cmd == LOG_CONFIG_MSG:
            log.debug(""recv: log_config: length=%d, %s"" % (len(data[9:]), byte_to_hexstring(data[9:])))
            self.__publish(event=self.EVENT_LOG_CONFIG, data=data[9:])
        elif cmd == WIFI_MSG:
            log.debug(""recv: wifi: %s"" % byte_to_hexstring(data[9:]))
            self.wifi_strength = data[9]
            self.__publish(event=self.EVENT_WIFI, data=data[9:])
        elif cmd == ALT_LIMIT_MSG:
            log.info(""recv: altitude limit: %s"" % byte_to_hexstring(data[9:-2]))
        elif cmd == ATT_LIMIT_MSG:
            log.info(""recv: attitude limit: %s"" % byte_to_hexstring(data[9:-2]))
        elif cmd == LOW_BAT_THRESHOLD_MSG:
            log.info(""recv: low battery threshold: %s"" % byte_to_hexstring(data[9:-2]))
        elif cmd == LIGHT_MSG:
            log.debug(""recv: light: %s"" % byte_to_hexstring(data[9:-2]))
            self.__publish(event=self.EVENT_LIGHT, data=data[9:])
        elif cmd == FLIGHT_MSG:
            flight_data = FlightData(data[9:])
            flight_data.wifi_strength = self.wifi_strength
            log.debug(""recv: flight data: %s"" % str(flight_data))
            self.__publish(event=self.EVENT_FLIGHT_DATA, data=flight_data)
        elif cmd == TIME_CMD:
            log.debug(""recv: time data: %s"" % byte_to_hexstring(data))
            self.__publish(event=self.EVENT_TIME, data=data[7:9])
        elif cmd in (SET_ALT_LIMIT_CMD, ATT_LIMIT_CMD, LOW_BAT_THRESHOLD_CMD, TAKEOFF_CMD, LAND_CMD, VIDEO_START_CMD, VIDEO_ENCODER_RATE_CMD, PALM_LAND_CMD,
                     EXPOSURE_CMD, THROW_AND_GO_CMD, EMERGENCY_CMD):
            log.debug(""recv: ack: cmd=0x%02x seq=0x%04x %s"" %
                     (uint16(data[5], data[6]), uint16(data[7], data[8]), byte_to_hexstring(data)))
        elif cmd == TELLO_CMD_FILE_SIZE:
            # Drone is about to send us a file. Get ready.
            # N.b. one of the fields in the packet is a file ID; by demuxing
            # based on file ID we can receive multiple files at once. This
            # code doesn't support that yet, though, so don't take one photo
            # while another is still being received.
            log.info(""recv: file size: %s"" % byte_to_hexstring(data))
            if len(pkt.get_data()) >= 7:
                (size, filenum) = struct.unpack('<xLH', pkt.get_data())
                log.info('      file size: num=%d bytes=%d' % (filenum, size))
                # Initialize file download state.
                self.file_recv[filenum] = DownloadedFile(filenum, size)
            else:
                # We always seem to get two files, one with most of the payload missing.
                # Not sure what the second one is for.
                log.warn('      file size: payload too small: %s' % byte_to_hexstring(pkt.get_data()))
            # Ack the packet.
            self.send_packet(pkt)
        elif cmd == TELLO_CMD_FILE_DATA:
            # log.info(""recv: file data: %s"" % byte_to_hexstring(data[9:21]))
            # Drone is sending us a fragment of a file it told us to prepare
            # for earlier.
            self.recv_file_data(pkt.get_data())
        else:
            log.info('unknown packet: %04x %s' % (cmd, byte_to_hexstring(data)))
            return False

        return True",_3650.py,71,"data[7], data[8]",*data[7:9]
https://github.com/nodejs/node-gyp/tree/master/gyp/pylib/gyp/win_tool.py,"def ExecLinkWrapper(self, arch, use_separate_mspdbsrv, *args):
        """"""Filter diagnostic output from link that looks like:
    '   Creating library ui.dll.lib and object ui.dll.exp'
    This happens when there are exports from the dll or exe.
    """"""
        env = self._GetEnv(arch)
        if use_separate_mspdbsrv == ""True"":
            self._UseSeparateMspdbsrv(env, args)
        if sys.platform == ""win32"":
            args = list(args)  # *args is a tuple by default, which is read-only.
            args[0] = args[0].replace(""/"", ""\\"")
        # https://docs.python.org/2/library/subprocess.html:
        # ""On Unix with shell=True [...] if args is a sequence, the first item
        # specifies the command string, and any additional items will be treated as
        # additional arguments to the shell itself.  That is to say, Popen does the
        # equivalent of:
        #   Popen(['/bin/sh', '-c', args[0], args[1], ...])""
        # For that reason, since going through the shell doesn't seem necessary on
        # non-Windows don't do that there.
        link = subprocess.Popen(
            args,
            shell=sys.platform == ""win32"",
            env=env,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
        )
        out = link.communicate()[0].decode(""utf-8"")
        for line in out.splitlines():
            if (
                not line.startswith(""   Creating library "")
                and not line.startswith(""Generating code"")
                and not line.startswith(""Finished generating code"")
            ):
                print(line)
        return link.returncode",_3823.py,11,"'/', '\\'",*'/\\'
https://github.com/qiucheng025/zao-/tree/master/lib/gui/display_analysis.py,"def opts_loss_keys(self, frame):
        """""" Add loss key selections """"""
        logger.debug(""Building Loss Key Check Buttons"")
        loss_keys = self.session.loss_keys
        lk_vars = dict()
        section_added = False
        for loss_key in sorted(loss_keys):
            text = loss_key.replace(""_"", "" "").title()
            helptext = ""Display {}"".format(text)
            var = tk.BooleanVar()
            if loss_key.startswith(""total""):
                var.set(True)
            lk_vars[loss_key] = var

            if len(loss_keys) == 1:
                # Don't display if there's only one item
                var.set(True)
                break

            if not section_added:
                self.add_section(frame, ""Keys"")
                section_added = True

            ctl = ttk.Checkbutton(frame, variable=var, text=text)
            ctl.pack(side=tk.TOP, padx=5, pady=5, anchor=tk.W)
            Tooltip(ctl, text=helptext, wraplength=200)

        self.vars[""loss_keys""] = lk_vars
        logger.debug(""Built Loss Key Check Buttons"")",_3947.py,8,"'_', ' '",*'_ '
https://github.com/cantools/cantools/tree/master/cantools/database/can/formats/dbc.py,"def _load_choices(tokens):
    choices = defaultdict(dict)

    for choice in tokens.get('VAL_', []):
        if len(choice[1]) == 0:
            continue

        od = odict((int(v[0]), NamedSignalValue(v[0], v[1])) for v in choice[3])

        if len(od) == 0:
            continue

        frame_id = int(choice[1][0])
        choices[frame_id][choice[2]] = od

    return choices",_3970.py,8,"v[0], v[1]",*v[:2]
https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_check_map.py,"def test_swap_mapped_false(self):
        """"""Needs [0]-[1] in a [0]--[2]--[1]
        qr0:--(+)--
               |
        qr1:---.---

        CouplingMap map: [0]--[2]--[1]
        """"""
        qr = QuantumRegister(2, ""qr"")
        circuit = QuantumCircuit(qr)
        circuit.cx(qr[0], qr[1])
        coupling = CouplingMap([[0, 2], [2, 1]])
        dag = circuit_to_dag(circuit)

        pass_ = CheckMap(coupling)
        pass_.run(dag)

        self.assertFalse(pass_.property_set[""is_swap_mapped""])",_4070.py,11,"qr[0], qr[1]",*qr[:2]
https://github.com/conan-io/conan-center-index/tree/master/recipes/cqrlib/all/conanfile.py,"def generate(self):
        tc = CMakeToolchain(self)
        tc.variables[""CQRLIB_SRC_DIR""] = self.source_folder.replace(""\\"", ""/"")
        tc.generate()",_4094.py,3,"'\\', '/'",*'\\/'
https://github.com/mushorg/conpot/tree/master/conpot/protocols/bacnet/bacnet_server.py,"def handle(self, data, address):
        session = conpot_core.get_session(
            ""bacnet"",
            address[0],
            address[1],
            get_interface_ip(address[0]),
            self.server.server_port,
        )
        logger.info(
            ""New Bacnet connection from %s:%d. (%s)"", address[0], address[1], session.id
        )
        session.add_event({""type"": ""NEW_CONNECTION""})
        # I'm not sure if gevent DatagramServer handles issues where the
        # received data is over the MTU -> fragmentation
        if data:
            pdu = PDU()
            pdu.pduData = bytearray(data)
            apdu = APDU()
            try:
                apdu.decode(pdu)
            except DecodingError:
                logger.warning(""DecodingError - PDU: {}"".format(pdu))
                return
            self.bacnet_app.indication(apdu, address, self.thisDevice)
            # send an appropriate response from BACnet app to the attacker
            self.bacnet_app.response(self.bacnet_app._response, address)
        logger.info(
            ""Bacnet client disconnected %s:%d. (%s)"", address[0], address[1], session.id
        )",_4135.py,2,"address[0], address[1]",*address[:2]
https://github.com/mushorg/conpot/tree/master/conpot/protocols/bacnet/bacnet_server.py,"def handle(self, data, address):
        session = conpot_core.get_session(
            ""bacnet"",
            address[0],
            address[1],
            get_interface_ip(address[0]),
            self.server.server_port,
        )
        logger.info(
            ""New Bacnet connection from %s:%d. (%s)"", address[0], address[1], session.id
        )
        session.add_event({""type"": ""NEW_CONNECTION""})
        # I'm not sure if gevent DatagramServer handles issues where the
        # received data is over the MTU -> fragmentation
        if data:
            pdu = PDU()
            pdu.pduData = bytearray(data)
            apdu = APDU()
            try:
                apdu.decode(pdu)
            except DecodingError:
                logger.warning(""DecodingError - PDU: {}"".format(pdu))
                return
            self.bacnet_app.indication(apdu, address, self.thisDevice)
            # send an appropriate response from BACnet app to the attacker
            self.bacnet_app.response(self.bacnet_app._response, address)
        logger.info(
            ""Bacnet client disconnected %s:%d. (%s)"", address[0], address[1], session.id
        )",_4135.py,9,"address[0], address[1]",*address[:2]
https://github.com/mushorg/conpot/tree/master/conpot/protocols/bacnet/bacnet_server.py,"def handle(self, data, address):
        session = conpot_core.get_session(
            ""bacnet"",
            address[0],
            address[1],
            get_interface_ip(address[0]),
            self.server.server_port,
        )
        logger.info(
            ""New Bacnet connection from %s:%d. (%s)"", address[0], address[1], session.id
        )
        session.add_event({""type"": ""NEW_CONNECTION""})
        # I'm not sure if gevent DatagramServer handles issues where the
        # received data is over the MTU -> fragmentation
        if data:
            pdu = PDU()
            pdu.pduData = bytearray(data)
            apdu = APDU()
            try:
                apdu.decode(pdu)
            except DecodingError:
                logger.warning(""DecodingError - PDU: {}"".format(pdu))
                return
            self.bacnet_app.indication(apdu, address, self.thisDevice)
            # send an appropriate response from BACnet app to the attacker
            self.bacnet_app.response(self.bacnet_app._response, address)
        logger.info(
            ""Bacnet client disconnected %s:%d. (%s)"", address[0], address[1], session.id
        )",_4135.py,27,"address[0], address[1]",*address[:2]
https://github.com/jyapayne/Web2Executable/tree/master//utils.py,"def move(src, dest, **kwargs):
    if is_windows():
        if os.path.isabs(src):
            src = '\\\\?\\'+src.replace('/', '\\')
        if os.path.isabs(dest):
            dest = '\\\\?\\'+dest.replace('/', '\\')
    shutil.move(src, dest, **kwargs)",_4419.py,4,"'/', '\\'",*'/\\'
https://github.com/jyapayne/Web2Executable/tree/master//utils.py,"def move(src, dest, **kwargs):
    if is_windows():
        if os.path.isabs(src):
            src = '\\\\?\\'+src.replace('/', '\\')
        if os.path.isabs(dest):
            dest = '\\\\?\\'+dest.replace('/', '\\')
    shutil.move(src, dest, **kwargs)",_4419.py,6,"'/', '\\'",*'/\\'
https://github.com/cool-RR/PySnooper/tree/master/tests/test_pysnooper.py,"def test_watch(normalize):
    class Foo(object):
        def __init__(self):
            self.x = 2

        def square(self):
            self.x **= 2

    @pysnooper.snoop(watch=(
            'foo.x',
            'io.__name__',
            'len(foo.__dict__[""x""] * ""abc"")',
    ), normalize=normalize, color=False)
    def my_function():
        foo = Foo()
        for i in range(2):
            foo.square()

    with mini_toolbox.OutputCapturer(stdout=False,
                                     stderr=True) as output_capturer:
        result = my_function()
    assert result is None
    output = output_capturer.string_io.getvalue()
    assert_output(
        output,
        (
            SourcePathEntry(),
            VariableEntry('Foo'),
            VariableEntry('io.__name__', ""'io'""),
            CallEntry('def my_function():'),
            LineEntry('foo = Foo()'),
            VariableEntry('foo'),
            VariableEntry('foo.x', '2'),
            VariableEntry('len(foo.__dict__[""x""] * ""abc"")', '6'),
            LineEntry(),
            VariableEntry('i', '0'),
            LineEntry(),
            VariableEntry('foo.x', '4'),
            VariableEntry('len(foo.__dict__[""x""] * ""abc"")', '12'),
            LineEntry(),
            VariableEntry('i', '1'),
            LineEntry(),
            VariableEntry('foo.x', '16'),
            VariableEntry('len(foo.__dict__[""x""] * ""abc"")', '48'),
            LineEntry(),
            ReturnEntry(),
            ReturnValueEntry('None'),
            ElapsedTimeEntry(),
        ),
        normalize=normalize,
    )",_4609.py,36,"'i', '0'",*'i0'
https://github.com/cool-RR/PySnooper/tree/master/tests/test_pysnooper.py,"def test_watch(normalize):
    class Foo(object):
        def __init__(self):
            self.x = 2

        def square(self):
            self.x **= 2

    @pysnooper.snoop(watch=(
            'foo.x',
            'io.__name__',
            'len(foo.__dict__[""x""] * ""abc"")',
    ), normalize=normalize, color=False)
    def my_function():
        foo = Foo()
        for i in range(2):
            foo.square()

    with mini_toolbox.OutputCapturer(stdout=False,
                                     stderr=True) as output_capturer:
        result = my_function()
    assert result is None
    output = output_capturer.string_io.getvalue()
    assert_output(
        output,
        (
            SourcePathEntry(),
            VariableEntry('Foo'),
            VariableEntry('io.__name__', ""'io'""),
            CallEntry('def my_function():'),
            LineEntry('foo = Foo()'),
            VariableEntry('foo'),
            VariableEntry('foo.x', '2'),
            VariableEntry('len(foo.__dict__[""x""] * ""abc"")', '6'),
            LineEntry(),
            VariableEntry('i', '0'),
            LineEntry(),
            VariableEntry('foo.x', '4'),
            VariableEntry('len(foo.__dict__[""x""] * ""abc"")', '12'),
            LineEntry(),
            VariableEntry('i', '1'),
            LineEntry(),
            VariableEntry('foo.x', '16'),
            VariableEntry('len(foo.__dict__[""x""] * ""abc"")', '48'),
            LineEntry(),
            ReturnEntry(),
            ReturnValueEntry('None'),
            ElapsedTimeEntry(),
        ),
        normalize=normalize,
    )",_4609.py,41,"'i', '1'",*'i1'
https://github.com/10se1ucgo/DisableWinTracking/tree/master//dwt_util.py,"def set_registry(keys):
    mask = winreg.KEY_WOW64_64KEY | winreg.KEY_ALL_ACCESS if is_64bit() else winreg.KEY_ALL_ACCESS

    for key_name, values in keys.items():
        try:
            key = winreg.CreateKeyEx(values[0], values[1], 0, mask)
            winreg.SetValueEx(key, values[2], 0, values[3], values[4])
            winreg.CloseKey(key)
            logger.info(""Registry: Successfully modified {key} key."".format(key=key_name))
        except OSError:
            logger.exception(""Registry: Unable to modify {key} key."".format(key=key_name))",_4902.py,6,"values[0], values[1]",*values[:2]
https://github.com/10se1ucgo/DisableWinTracking/tree/master//dwt_util.py,"def set_registry(keys):
    mask = winreg.KEY_WOW64_64KEY | winreg.KEY_ALL_ACCESS if is_64bit() else winreg.KEY_ALL_ACCESS

    for key_name, values in keys.items():
        try:
            key = winreg.CreateKeyEx(values[0], values[1], 0, mask)
            winreg.SetValueEx(key, values[2], 0, values[3], values[4])
            winreg.CloseKey(key)
            logger.info(""Registry: Successfully modified {key} key."".format(key=key_name))
        except OSError:
            logger.exception(""Registry: Unable to modify {key} key."".format(key=key_name))",_4902.py,7,"values[3], values[4]",*values[3:5]
https://github.com/genforce/interfacegan/tree/master/models/pggan_generator_model.py,"def forward(self, x):
    if len(x.shape) != 2:
      raise ValueError(f'The input tensor should be with shape [batch_size, '
                       f'noise_dim], but {x.shape} received!')
    x = x.view(x.shape[0], x.shape[1], 1, 1)

    lod = self.lod.cpu().tolist()
    for block_idx in range(1, len(self.channels)):
      if block_idx + lod < len(self.channels):
        x = self.__getattr__(f'layer{2 * block_idx - 2}')(x)
        x = self.__getattr__(f'layer{2 * block_idx - 1}')(x)
        image = self.__getattr__(f'output{block_idx - 1}')(x)
      else:
        image = self.upsample(image)
    return image",_5086.py,5,"x.shape[0], x.shape[1]",*x.shape[:2]
https://github.com/ZJULearning/ttfnet/tree/master/mmdet/ops/dcn/deform_pool.py,"def forward(self, data, rois):
        assert data.size(1) == self.out_channels
        if self.no_trans:
            offset = data.new_empty(0)
            return deform_roi_pooling(data, rois, offset, self.spatial_scale,
                                      self.out_size, self.out_channels,
                                      self.no_trans, self.group_size,
                                      self.part_size, self.sample_per_part,
                                      self.trans_std)
        else:
            n = rois.shape[0]
            offset = data.new_empty(0)
            x = deform_roi_pooling(data, rois, offset, self.spatial_scale,
                                   self.out_size, self.out_channels, True,
                                   self.group_size, self.part_size,
                                   self.sample_per_part, self.trans_std)
            offset = self.offset_fc(x.view(n, -1))
            offset = offset.view(n, 2, self.out_size[0], self.out_size[1])
            return deform_roi_pooling(data, rois, offset, self.spatial_scale,
                                      self.out_size, self.out_channels,
                                      self.no_trans, self.group_size,
                                      self.part_size, self.sample_per_part,
                                      self.trans_std)",_5563.py,18,"self.out_size[0], self.out_size[1]",*self.out_size[:2]
https://github.com/pyglet/pyglet/tree/master/tools/wraptypes/preprocessor.py,"def p_equality_expression(self, p):
        '''equality_expression : relational_expression
                               | equality_expression EQ_OP relational_expression
                               | equality_expression NE_OP relational_expression
        '''
        if len(p) == 2:
            p[0] = p[1]
        else:
            p[0] = BinaryExpressionNode({
                '==': operator.eq,
                '!=': operator.ne}[p[2]], p[2], p[1], p[3])",_5632.py,9,"p[1], p[3]",*p[1:5:2]
https://github.com/boston-dynamics/spot-sdk/tree/master/python/bosdyn-client/src/bosdyn/client/math_helpers.py,"def slerp(a, b, fraction):
        v0 = numpy.array([a.w, a.x, a.y, a.z])
        v1 = numpy.array([b.w, b.x, b.y, b.z])
        dot = numpy.dot(v0.transpose(), v1)
        # If the dot product is negative, slerp will not take
        # the shorter path. Note that v1 and -v1 are equivalent when
        # the negation is applied to all four components. Fix by
        # reversing one quaternion.
        if dot < 0.0:
            v0 *= -1.0
            dot = -dot

        DOT_THRESHOLD = 1.0 - 1e-4
        if dot > DOT_THRESHOLD:
            # If the inputs are too close for comfort, linearly interpolate
            # and normalize the result.
            result = v0 + fraction * (v1 - v0)
            result /= numpy.sqrt(numpy.dot(result.transpose(), result))
        else:
            # Since dot is in range [0, DOT_THRESHOLD], acos is safe
            theta_0 = math.acos(dot)  # theta_0 = angle between input vectors
            theta = theta_0 * fraction  # theta = angle between v0 and result
            sin_theta = math.sin(theta)  # compute this value only once
            sin_theta_0 = math.sin(theta_0)  # compute this value only once

            s0 = math.cos(
                theta) - dot * sin_theta / sin_theta_0  # == sin(theta_0 - theta) / sin(theta_0)
            s1 = sin_theta / sin_theta_0

            result = (s0 * v0) + (s1 * v1)
        return Quat(result[0], result[1], result[2], result[3])",_5787.py,31,"result[0], result[1], result[2], result[3]",*result[:4]
https://github.com/sebp/scikit-survival/tree/master/sksurv/io/arffwrite.py,"def _check_str_value(x):
    """"""If string has a space, wrap it in double quotes and remove/escape illegal characters""""""
    if isinstance(x, str):
        # remove commas, and single quotation marks since loadarff cannot deal with it
        x = x.replace("","", ""."").replace(chr(0x2018), ""'"").replace(chr(0x2019), ""'"")

        # put string in double quotes
        if "" "" in x:
            if x[0] in ('""', ""'""):
                x = x[1:]
            if x[-1] in ('""', ""'""):
                x = x[:len(x) - 1]
            x = '""' + x.replace('""', ""\\\"""") + '""'
    return str(x)",_5853.py,5,"',', '.'","*',.'"
https://github.com/sfepy/sfepy/tree/master/sfepy/homogenization/utils.py,"def get_lattice_volume(axes):
    r""""""
    Volume of a periodic cell in a rectangular 3D (or 2D) lattice.

    Parameters
    ----------
    axes : array
        The array with the periodic cell axes :math:`a_1, \dots, a_3` as rows.

    Returns
    -------
    volume : float
        The periodic cell volume :math:`V = (a_1 \times a_2) \cdot a_3`. In 2D
        :math:`V = |(a_1 \times a_2)|` with zeros as the third components of
        vectors :math:`a_1`, :math:`a_2`.
    """"""
    axes = nm.asarray(axes)

    dim = axes.shape[0]

    if dim == 2:
        volume = nm.abs(nm.cross(axes[0], axes[1]))

    elif dim == 3:
        volume = nm.dot(nm.cross(axes[0], axes[1]), axes[2])

    else:
        raise ValueError('wrong axes shape! (%s)' % axes.shape)

    return volume",_5954.py,22,"axes[0], axes[1]",*axes[:2]
https://github.com/sfepy/sfepy/tree/master/sfepy/homogenization/utils.py,"def get_lattice_volume(axes):
    r""""""
    Volume of a periodic cell in a rectangular 3D (or 2D) lattice.

    Parameters
    ----------
    axes : array
        The array with the periodic cell axes :math:`a_1, \dots, a_3` as rows.

    Returns
    -------
    volume : float
        The periodic cell volume :math:`V = (a_1 \times a_2) \cdot a_3`. In 2D
        :math:`V = |(a_1 \times a_2)|` with zeros as the third components of
        vectors :math:`a_1`, :math:`a_2`.
    """"""
    axes = nm.asarray(axes)

    dim = axes.shape[0]

    if dim == 2:
        volume = nm.abs(nm.cross(axes[0], axes[1]))

    elif dim == 3:
        volume = nm.dot(nm.cross(axes[0], axes[1]), axes[2])

    else:
        raise ValueError('wrong axes shape! (%s)' % axes.shape)

    return volume",_5954.py,25,"axes[0], axes[1]",*axes[:2]
https://github.com/Zulko/moviepy/tree/master/moviepy/tools.py,"def convert_to_seconds(time):
    """"""Will convert any time into seconds.

    If the type of `time` is not valid,
    it's returned as is.

    Here are the accepted formats:

    >>> convert_to_seconds(15.4)   # seconds
    15.4
    >>> convert_to_seconds((1, 21.5))   # (min,sec)
    81.5
    >>> convert_to_seconds((1, 1, 2))   # (hr, min, sec)
    3662
    >>> convert_to_seconds('01:01:33.045')
    3693.045
    >>> convert_to_seconds('01:01:33,5')    # coma works too
    3693.5
    >>> convert_to_seconds('1:33,5')    # only minutes and secs
    99.5
    >>> convert_to_seconds('33.5')      # only secs
    33.5
    """"""
    factors = (1, 60, 3600)

    if isinstance(time, str):
        time = [float(part.replace("","", ""."")) for part in time.split("":"")]

    if not isinstance(time, (tuple, list)):
        return time

    return sum(mult * part for mult, part in zip(factors, reversed(time)))",_6248.py,27,"',', '.'","*',.'"
https://github.com/chainer/chainer/tree/master/chainer/training/extensions/snapshot_writers.py,"def consume(self, q):
        while True:
            task = q.get()
            if task is None:
                q.task_done()
                return
            else:
                task[0](task[1], task[2], task[3])
                q.task_done()",_6463.py,8,"task[1], task[2], task[3]",*task[1:4]
https://github.com/Alexey-T/CudaText/tree/master/app/cudatext.app/Contents/Resources/py/cuda_options_editor/cd_plug_lib.py,"def _take_val(self, name, liv_val, defv=None):
        tp      = self.ctrls[name]['type']
        old_val = self.ctrls[name].get('val', defv)
        new_val = liv_val
        if False:pass
        elif tp=='memo':
            # For memo: ""\t""-separated lines (in lines ""\t"" must be replaced to chr(2)) 
            if isinstance(old_val, list):
                new_val = [v.replace(chr(2), '\t') for v in liv_val.split('\t')]
               #liv_val = '\t'.join([v.replace('\t', chr(2)) for v in old_val])
            else:
                new_val = liv_val.replace('\t','\n').replace(chr(2), '\t')
               #liv_val = old_val.replace('\t', chr(2)).replace('\r\n','\n').replace('\r','\n').replace('\n','\t')
        elif tp=='checkgroup' and isinstance(old_val, list):
            # For checkgroup: "",""-separated checks (values ""0""/""1"") 
            new_val = liv_val.split(',')
           #in_val = ','.join(in_val)
        elif tp in ['checklistbox', 'checklistview'] and isinstance(old_val, tuple):
            new_val = liv_val.split(';')
            new_val = (new_val[0], new_val[1].split(','))
           #liv_val = ';'.join(old_val[0], ','.join(old_val[1]))
        elif isinstance(old_val, bool): 
            new_val = liv_val=='1'
        elif tp=='listview':
            new_val = -1 if liv_val=='' else int(liv_val)
        elif old_val is not None: 
            new_val = type(old_val)(liv_val)
        return new_val",_6687.py,12,"'\t', '\n'",*'\t\n'
https://github.com/searx/searx/tree/master/searx/engines/imdb.py,"def request(query, params):
    query = query.replace("" "", ""_"").lower()
    params['url'] = suggestion_url.format(letter=query[0], query=query)
    return params",_6765.py,2,"' ', '_'",*' _'
https://github.com/facebookarchive/WEASEL/tree/master/server/server.py,"def log_reply(self, handler, reply):
        if DEBUG_DNS:
            self.logger.debug(""Reply: [%s:%d] (%s) / '%s' (%s) / RRs: %s"",
                              handler.client_address[0],
                              handler.client_address[1],
                              handler.protocol,
                              reply.q.qname,
                              QTYPE[reply.q.qtype],
                              "","".join([QTYPE[a.rtype] for a in reply.rr]))
            self.log_data(reply)",_7174.py,3,"handler.client_address[0], handler.client_address[1]",*handler.client_address[:2]
https://github.com/bayespy/bayespy/tree/master/bayespy/inference/vmp/nodes/gaussian.py,"def random(self, *phi, plates=None):
        r""""""
        Draw a random sample from the distribution.
        """"""
        # TODO/FIXME: This is incorrect, I think. Gamma distribution parameters
        # aren't directly those, because phi has some parts from the Gaussian
        # distribution.
        alpha = GammaDistribution().random(
            phi[2],
            phi[3],
            plates=plates
        )
        mu = GaussianARDDistribution(self.shape).random(
            misc.add_trailing_axes(alpha, self.ndim) * phi[0],
            misc.add_trailing_axes(alpha, 2*self.ndim) * phi[1],
            plates=plates
        )
        return (mu, alpha)",_7196.py,8,"phi[2], phi[3]",*phi[2:4]
https://github.com/openstack/swift/tree/master/test/unit/obj/test_server.py,"def test_delete_at_update_cleans_old_entries(self):
        # Test how delete_at_update works with a request to overwrite an object
        # with delete-at metadata
        policy = random.choice(list(POLICIES))

        def do_test(method, headers, expected_args):
            given_args = []

            def fake_async_update(*args):
                given_args.extend(args)

            headers.update({'X-Timestamp': 1,
                            'X-Trans-Id': '123',
                            'X-Backend-Storage-Policy-Index': int(policy)})
            req = Request.blank(
                '/v1/a/c/o',
                environ={'REQUEST_METHOD': method},
                headers=headers)
            with mock.patch.object(self.object_controller, 'async_update',
                                   fake_async_update):
                self.object_controller.delete_at_update(
                    'DELETE', 2, 'a', 'c', 'o', req, 'sda1', policy)
            self.assertEqual(expected_args, given_args)

        for method in ('PUT', 'POST', 'DELETE'):
            expected_args = [
                'DELETE', '.expiring_objects', '0000000000',
                '0000000002-a/c/o', None, None,
                None, HeaderKeyDict({
                    'X-Backend-Storage-Policy-Index': 0,
                    'x-timestamp': utils.Timestamp('1').internal,
                    'x-trans-id': '123',
                    'referer': '%s http://localhost/v1/a/c/o' % method}),
                'sda1', policy]
            # async_update should be called by default...
            do_test(method, {}, expected_args)
            do_test(method, {'X-Backend-Clean-Expiring-Object-Queue': 'true'},
                    expected_args)
            do_test(method, {'X-Backend-Clean-Expiring-Object-Queue': 't'},
                    expected_args)
            # ...unless header has a false value
            do_test(method, {'X-Backend-Clean-Expiring-Object-Queue': 'false'},
                    [])
            do_test(method, {'X-Backend-Clean-Expiring-Object-Queue': 'f'}, [])",_7310.py,21,"'a', 'c', 'o'",*'aco'
https://github.com/openstack/swift/tree/master/test/unit/obj/test_server.py,"def test_delete_at_update_cleans_old_entries(self):
        # Test how delete_at_update works with a request to overwrite an object
        # with delete-at metadata
        policy = random.choice(list(POLICIES))

        def do_test(method, headers, expected_args):
            given_args = []

            def fake_async_update(*args):
                given_args.extend(args)

            headers.update({'X-Timestamp': 1,
                            'X-Trans-Id': '123',
                            'X-Backend-Storage-Policy-Index': int(policy)})
            req = Request.blank(
                '/v1/a/c/o',
                environ={'REQUEST_METHOD': method},
                headers=headers)
            with mock.patch.object(self.object_controller, 'async_update',
                                   fake_async_update):
                self.object_controller.delete_at_update(
                    'DELETE', 2, 'a', 'c', 'o', req, 'sda1', policy)
            self.assertEqual(expected_args, given_args)

        for method in ('PUT', 'POST', 'DELETE'):
            expected_args = [
                'DELETE', '.expiring_objects', '0000000000',
                '0000000002-a/c/o', None, None,
                None, HeaderKeyDict({
                    'X-Backend-Storage-Policy-Index': 0,
                    'x-timestamp': utils.Timestamp('1').internal,
                    'x-trans-id': '123',
                    'referer': '%s http://localhost/v1/a/c/o' % method}),
                'sda1', policy]
            # async_update should be called by default...
            do_test(method, {}, expected_args)
            do_test(method, {'X-Backend-Clean-Expiring-Object-Queue': 'true'},
                    expected_args)
            do_test(method, {'X-Backend-Clean-Expiring-Object-Queue': 't'},
                    expected_args)
            # ...unless header has a false value
            do_test(method, {'X-Backend-Clean-Expiring-Object-Queue': 'false'},
                    [])
            do_test(method, {'X-Backend-Clean-Expiring-Object-Queue': 'f'}, [])",_7310.py,21,"'a', 'c', 'o'",*'aco'
https://github.com/openstack/swift/tree/master/test/unit/obj/test_server.py,"def test_delete_at_update_cleans_old_entries(self):
        # Test how delete_at_update works with a request to overwrite an object
        # with delete-at metadata
        policy = random.choice(list(POLICIES))

        def do_test(method, headers, expected_args):
            given_args = []

            def fake_async_update(*args):
                given_args.extend(args)

            headers.update({'X-Timestamp': 1,
                            'X-Trans-Id': '123',
                            'X-Backend-Storage-Policy-Index': int(policy)})
            req = Request.blank(
                '/v1/a/c/o',
                environ={'REQUEST_METHOD': method},
                headers=headers)
            with mock.patch.object(self.object_controller, 'async_update',
                                   fake_async_update):
                self.object_controller.delete_at_update(
                    'DELETE', 2, 'a', 'c', 'o', req, 'sda1', policy)
            self.assertEqual(expected_args, given_args)

        for method in ('PUT', 'POST', 'DELETE'):
            expected_args = [
                'DELETE', '.expiring_objects', '0000000000',
                '0000000002-a/c/o', None, None,
                None, HeaderKeyDict({
                    'X-Backend-Storage-Policy-Index': 0,
                    'x-timestamp': utils.Timestamp('1').internal,
                    'x-trans-id': '123',
                    'referer': '%s http://localhost/v1/a/c/o' % method}),
                'sda1', policy]
            # async_update should be called by default...
            do_test(method, {}, expected_args)
            do_test(method, {'X-Backend-Clean-Expiring-Object-Queue': 'true'},
                    expected_args)
            do_test(method, {'X-Backend-Clean-Expiring-Object-Queue': 't'},
                    expected_args)
            # ...unless header has a false value
            do_test(method, {'X-Backend-Clean-Expiring-Object-Queue': 'false'},
                    [])
            do_test(method, {'X-Backend-Clean-Expiring-Object-Queue': 'f'}, [])",_7310.py,21,"'a', 'c', 'o'",*'aco'
https://github.com/openstack/swift/tree/master/test/unit/obj/test_server.py,"def test_delete_at_update_cleans_old_entries(self):
        # Test how delete_at_update works with a request to overwrite an object
        # with delete-at metadata
        policy = random.choice(list(POLICIES))

        def do_test(method, headers, expected_args):
            given_args = []

            def fake_async_update(*args):
                given_args.extend(args)

            headers.update({'X-Timestamp': 1,
                            'X-Trans-Id': '123',
                            'X-Backend-Storage-Policy-Index': int(policy)})
            req = Request.blank(
                '/v1/a/c/o',
                environ={'REQUEST_METHOD': method},
                headers=headers)
            with mock.patch.object(self.object_controller, 'async_update',
                                   fake_async_update):
                self.object_controller.delete_at_update(
                    'DELETE', 2, 'a', 'c', 'o', req, 'sda1', policy)
            self.assertEqual(expected_args, given_args)

        for method in ('PUT', 'POST', 'DELETE'):
            expected_args = [
                'DELETE', '.expiring_objects', '0000000000',
                '0000000002-a/c/o', None, None,
                None, HeaderKeyDict({
                    'X-Backend-Storage-Policy-Index': 0,
                    'x-timestamp': utils.Timestamp('1').internal,
                    'x-trans-id': '123',
                    'referer': '%s http://localhost/v1/a/c/o' % method}),
                'sda1', policy]
            # async_update should be called by default...
            do_test(method, {}, expected_args)
            do_test(method, {'X-Backend-Clean-Expiring-Object-Queue': 'true'},
                    expected_args)
            do_test(method, {'X-Backend-Clean-Expiring-Object-Queue': 't'},
                    expected_args)
            # ...unless header has a false value
            do_test(method, {'X-Backend-Clean-Expiring-Object-Queue': 'false'},
                    [])
            do_test(method, {'X-Backend-Clean-Expiring-Object-Queue': 'f'}, [])",_7310.py,21,"'a', 'c', 'o'",*'aco'
https://github.com/openstack/swift/tree/master/test/unit/obj/test_server.py,"def test_delete_at_update_cleans_old_entries(self):
        # Test how delete_at_update works with a request to overwrite an object
        # with delete-at metadata
        policy = random.choice(list(POLICIES))

        def do_test(method, headers, expected_args):
            given_args = []

            def fake_async_update(*args):
                given_args.extend(args)

            headers.update({'X-Timestamp': 1,
                            'X-Trans-Id': '123',
                            'X-Backend-Storage-Policy-Index': int(policy)})
            req = Request.blank(
                '/v1/a/c/o',
                environ={'REQUEST_METHOD': method},
                headers=headers)
            with mock.patch.object(self.object_controller, 'async_update',
                                   fake_async_update):
                self.object_controller.delete_at_update(
                    'DELETE', 2, 'a', 'c', 'o', req, 'sda1', policy)
            self.assertEqual(expected_args, given_args)

        for method in ('PUT', 'POST', 'DELETE'):
            expected_args = [
                'DELETE', '.expiring_objects', '0000000000',
                '0000000002-a/c/o', None, None,
                None, HeaderKeyDict({
                    'X-Backend-Storage-Policy-Index': 0,
                    'x-timestamp': utils.Timestamp('1').internal,
                    'x-trans-id': '123',
                    'referer': '%s http://localhost/v1/a/c/o' % method}),
                'sda1', policy]
            # async_update should be called by default...
            do_test(method, {}, expected_args)
            do_test(method, {'X-Backend-Clean-Expiring-Object-Queue': 'true'},
                    expected_args)
            do_test(method, {'X-Backend-Clean-Expiring-Object-Queue': 't'},
                    expected_args)
            # ...unless header has a false value
            do_test(method, {'X-Backend-Clean-Expiring-Object-Queue': 'false'},
                    [])
            do_test(method, {'X-Backend-Clean-Expiring-Object-Queue': 'f'}, [])",_7310.py,21,"'a', 'c', 'o'",*'aco'
https://github.com/openstack/swift/tree/master/test/unit/obj/test_server.py,"def test_delete_at_update_cleans_old_entries(self):
        # Test how delete_at_update works with a request to overwrite an object
        # with delete-at metadata
        policy = random.choice(list(POLICIES))

        def do_test(method, headers, expected_args):
            given_args = []

            def fake_async_update(*args):
                given_args.extend(args)

            headers.update({'X-Timestamp': 1,
                            'X-Trans-Id': '123',
                            'X-Backend-Storage-Policy-Index': int(policy)})
            req = Request.blank(
                '/v1/a/c/o',
                environ={'REQUEST_METHOD': method},
                headers=headers)
            with mock.patch.object(self.object_controller, 'async_update',
                                   fake_async_update):
                self.object_controller.delete_at_update(
                    'DELETE', 2, 'a', 'c', 'o', req, 'sda1', policy)
            self.assertEqual(expected_args, given_args)

        for method in ('PUT', 'POST', 'DELETE'):
            expected_args = [
                'DELETE', '.expiring_objects', '0000000000',
                '0000000002-a/c/o', None, None,
                None, HeaderKeyDict({
                    'X-Backend-Storage-Policy-Index': 0,
                    'x-timestamp': utils.Timestamp('1').internal,
                    'x-trans-id': '123',
                    'referer': '%s http://localhost/v1/a/c/o' % method}),
                'sda1', policy]
            # async_update should be called by default...
            do_test(method, {}, expected_args)
            do_test(method, {'X-Backend-Clean-Expiring-Object-Queue': 'true'},
                    expected_args)
            do_test(method, {'X-Backend-Clean-Expiring-Object-Queue': 't'},
                    expected_args)
            # ...unless header has a false value
            do_test(method, {'X-Backend-Clean-Expiring-Object-Queue': 'false'},
                    [])
            do_test(method, {'X-Backend-Clean-Expiring-Object-Queue': 'f'}, [])",_7310.py,21,"'a', 'c', 'o'",*'aco'
https://github.com/InQuest/ThreatIngestor/tree/master/tests/test_operators_mysql.py,"def test_filter_string_and_allowed_sources_are_set_if_passed_in(self, connect):
        self.assertEqual(threatingestor.operators.mysql.Plugin('a', 'b', 'c', filter_string='test').filter_string, 'test')
        self.assertEqual(threatingestor.operators.mysql.Plugin('a', 'b', 'c', allowed_sources=['test-one']).allowed_sources, ['test-one'])",_7743.py,2,"'a', 'b', 'c'",*'abc'
https://github.com/InQuest/ThreatIngestor/tree/master/tests/test_operators_mysql.py,"def test_filter_string_and_allowed_sources_are_set_if_passed_in(self, connect):
        self.assertEqual(threatingestor.operators.mysql.Plugin('a', 'b', 'c', filter_string='test').filter_string, 'test')
        self.assertEqual(threatingestor.operators.mysql.Plugin('a', 'b', 'c', allowed_sources=['test-one']).allowed_sources, ['test-one'])",_7743.py,2,"'a', 'b', 'c'",*'abc'
https://github.com/InQuest/ThreatIngestor/tree/master/tests/test_operators_mysql.py,"def test_filter_string_and_allowed_sources_are_set_if_passed_in(self, connect):
        self.assertEqual(threatingestor.operators.mysql.Plugin('a', 'b', 'c', filter_string='test').filter_string, 'test')
        self.assertEqual(threatingestor.operators.mysql.Plugin('a', 'b', 'c', allowed_sources=['test-one']).allowed_sources, ['test-one'])",_7743.py,3,"'a', 'b', 'c'",*'abc'
https://github.com/InQuest/ThreatIngestor/tree/master/tests/test_operators_mysql.py,"def test_filter_string_and_allowed_sources_are_set_if_passed_in(self, connect):
        self.assertEqual(threatingestor.operators.mysql.Plugin('a', 'b', 'c', filter_string='test').filter_string, 'test')
        self.assertEqual(threatingestor.operators.mysql.Plugin('a', 'b', 'c', allowed_sources=['test-one']).allowed_sources, ['test-one'])",_7743.py,3,"'a', 'b', 'c'",*'abc'
https://github.com/FateScript/CenterNet-better/tree/master/dl_lib/layers/deformable/deform_conv.py,"def forward(
        ctx,
        input,
        offset,
        mask,
        weight,
        bias=None,
        stride=1,
        padding=0,
        dilation=1,
        groups=1,
        deformable_groups=1,
    ):
        ctx.stride = stride
        ctx.padding = padding
        ctx.dilation = dilation
        ctx.groups = groups
        ctx.deformable_groups = deformable_groups
        ctx.with_bias = bias is not None
        if not ctx.with_bias:
            bias = input.new_empty(1)  # fake tensor
        if not input.is_cuda:
            raise NotImplementedError
        if (
            weight.requires_grad
            or mask.requires_grad
            or offset.requires_grad
            or input.requires_grad
        ):
            ctx.save_for_backward(input, offset, mask, weight, bias)
        output = input.new_empty(_ModulatedDeformConv._infer_shape(ctx, input, weight))
        ctx._bufs = [input.new_empty(0), input.new_empty(0)]
        _C.modulated_deform_conv_forward(
            input,
            weight,
            bias,
            ctx._bufs[0],
            offset,
            mask,
            output,
            ctx._bufs[1],
            weight.shape[2],
            weight.shape[3],
            ctx.stride,
            ctx.stride,
            ctx.padding,
            ctx.padding,
            ctx.dilation,
            ctx.dilation,
            ctx.groups,
            ctx.deformable_groups,
            ctx.with_bias,
        )
        return output",_7750.py,33,"weight.shape[2], weight.shape[3]",*weight.shape[2:4]
https://github.com/microsoft/playwright-python/tree/master/playwright/_impl/_wait_helper.py,"def _cleanup(self) -> None:
        for task in self._pending_tasks:
            if not task.done():
                task.cancel()
        for listener in self._registered_listeners:
            listener[0].remove_listener(listener[1], listener[2])",_7780.py,6,"listener[1], listener[2]",*listener[1:3]
https://github.com/DLLXW/data-science-competition/tree/master/ๅคฉๆฑ/2021ๅนฟไธๅทฅไธๆบ้ๅๆฐๅคง่ต/yolov5/utils/utils.py,"def plot_labels(labels):
    # plot dataset labels
    c, b = labels[:, 0], labels[:, 1:].transpose()  # classees, boxes

    def hist2d(x, y, n=100):
        xedges, yedges = np.linspace(x.min(), x.max(), n), np.linspace(y.min(), y.max(), n)
        hist, xedges, yedges = np.histogram2d(x, y, (xedges, yedges))
        xidx = np.clip(np.digitize(x, xedges) - 1, 0, hist.shape[0] - 1)
        yidx = np.clip(np.digitize(y, yedges) - 1, 0, hist.shape[1] - 1)
        return np.log(hist[xidx, yidx])

    fig, ax = plt.subplots(2, 2, figsize=(8, 8), tight_layout=True)
    ax = ax.ravel()
    ax[0].hist(c, bins=int(c.max() + 1))
    ax[0].set_xlabel('classes')
    ax[1].scatter(b[0], b[1], c=hist2d(b[0], b[1], 90), cmap='jet')
    ax[1].set_xlabel('x')
    ax[1].set_ylabel('y')
    ax[2].scatter(b[2], b[3], c=hist2d(b[2], b[3], 90), cmap='jet')
    ax[2].set_xlabel('width')
    ax[2].set_ylabel('height')
    plt.savefig('labels.png', dpi=200)
    plt.close()",_7899.py,16,"b[0], b[1]",*b[:2]
https://github.com/DLLXW/data-science-competition/tree/master/ๅคฉๆฑ/2021ๅนฟไธๅทฅไธๆบ้ๅๆฐๅคง่ต/yolov5/utils/utils.py,"def plot_labels(labels):
    # plot dataset labels
    c, b = labels[:, 0], labels[:, 1:].transpose()  # classees, boxes

    def hist2d(x, y, n=100):
        xedges, yedges = np.linspace(x.min(), x.max(), n), np.linspace(y.min(), y.max(), n)
        hist, xedges, yedges = np.histogram2d(x, y, (xedges, yedges))
        xidx = np.clip(np.digitize(x, xedges) - 1, 0, hist.shape[0] - 1)
        yidx = np.clip(np.digitize(y, yedges) - 1, 0, hist.shape[1] - 1)
        return np.log(hist[xidx, yidx])

    fig, ax = plt.subplots(2, 2, figsize=(8, 8), tight_layout=True)
    ax = ax.ravel()
    ax[0].hist(c, bins=int(c.max() + 1))
    ax[0].set_xlabel('classes')
    ax[1].scatter(b[0], b[1], c=hist2d(b[0], b[1], 90), cmap='jet')
    ax[1].set_xlabel('x')
    ax[1].set_ylabel('y')
    ax[2].scatter(b[2], b[3], c=hist2d(b[2], b[3], 90), cmap='jet')
    ax[2].set_xlabel('width')
    ax[2].set_ylabel('height')
    plt.savefig('labels.png', dpi=200)
    plt.close()",_7899.py,19,"b[2], b[3]",*b[2:4]
https://github.com/DLLXW/data-science-competition/tree/master/ๅคฉๆฑ/2021ๅนฟไธๅทฅไธๆบ้ๅๆฐๅคง่ต/yolov5/utils/utils.py,"def plot_labels(labels):
    # plot dataset labels
    c, b = labels[:, 0], labels[:, 1:].transpose()  # classees, boxes

    def hist2d(x, y, n=100):
        xedges, yedges = np.linspace(x.min(), x.max(), n), np.linspace(y.min(), y.max(), n)
        hist, xedges, yedges = np.histogram2d(x, y, (xedges, yedges))
        xidx = np.clip(np.digitize(x, xedges) - 1, 0, hist.shape[0] - 1)
        yidx = np.clip(np.digitize(y, yedges) - 1, 0, hist.shape[1] - 1)
        return np.log(hist[xidx, yidx])

    fig, ax = plt.subplots(2, 2, figsize=(8, 8), tight_layout=True)
    ax = ax.ravel()
    ax[0].hist(c, bins=int(c.max() + 1))
    ax[0].set_xlabel('classes')
    ax[1].scatter(b[0], b[1], c=hist2d(b[0], b[1], 90), cmap='jet')
    ax[1].set_xlabel('x')
    ax[1].set_ylabel('y')
    ax[2].scatter(b[2], b[3], c=hist2d(b[2], b[3], 90), cmap='jet')
    ax[2].set_xlabel('width')
    ax[2].set_ylabel('height')
    plt.savefig('labels.png', dpi=200)
    plt.close()",_7899.py,16,"b[0], b[1]",*b[:2]
https://github.com/DLLXW/data-science-competition/tree/master/ๅคฉๆฑ/2021ๅนฟไธๅทฅไธๆบ้ๅๆฐๅคง่ต/yolov5/utils/utils.py,"def plot_labels(labels):
    # plot dataset labels
    c, b = labels[:, 0], labels[:, 1:].transpose()  # classees, boxes

    def hist2d(x, y, n=100):
        xedges, yedges = np.linspace(x.min(), x.max(), n), np.linspace(y.min(), y.max(), n)
        hist, xedges, yedges = np.histogram2d(x, y, (xedges, yedges))
        xidx = np.clip(np.digitize(x, xedges) - 1, 0, hist.shape[0] - 1)
        yidx = np.clip(np.digitize(y, yedges) - 1, 0, hist.shape[1] - 1)
        return np.log(hist[xidx, yidx])

    fig, ax = plt.subplots(2, 2, figsize=(8, 8), tight_layout=True)
    ax = ax.ravel()
    ax[0].hist(c, bins=int(c.max() + 1))
    ax[0].set_xlabel('classes')
    ax[1].scatter(b[0], b[1], c=hist2d(b[0], b[1], 90), cmap='jet')
    ax[1].set_xlabel('x')
    ax[1].set_ylabel('y')
    ax[2].scatter(b[2], b[3], c=hist2d(b[2], b[3], 90), cmap='jet')
    ax[2].set_xlabel('width')
    ax[2].set_ylabel('height')
    plt.savefig('labels.png', dpi=200)
    plt.close()",_7899.py,19,"b[2], b[3]",*b[2:4]
https://github.com/tkarras/progressive_growing_of_gans/tree/master//tfutil.py,"def _init_graph(self):
        # Collect inputs.
        self.input_names = []
        for param in inspect.signature(self._build_func).parameters.values():
            if param.kind == param.POSITIONAL_OR_KEYWORD and param.default is param.empty:
                self.input_names.append(param.name)
        self.num_inputs = len(self.input_names)
        assert self.num_inputs >= 1

        # Choose name and scope.
        if self.name is None:
            self.name = self._build_func_name
        self.scope = tf.get_default_graph().unique_name(self.name.replace('/', '_'), mark_as_used=False)
        
        # Build template graph.
        with tf.variable_scope(self.scope, reuse=tf.AUTO_REUSE):
            assert tf.get_variable_scope().name == self.scope
            with absolute_name_scope(self.scope): # ignore surrounding name_scope
                with tf.control_dependencies(None): # ignore surrounding control_dependencies
                    self.input_templates = [tf.placeholder(tf.float32, name=name) for name in self.input_names]
                    out_expr = self._build_func(*self.input_templates, is_template_graph=True, **self.static_kwargs)
            
        # Collect outputs.
        assert is_tf_expression(out_expr) or isinstance(out_expr, tuple)
        self.output_templates = [out_expr] if is_tf_expression(out_expr) else list(out_expr)
        self.output_names = [t.name.split('/')[-1].split(':')[0] for t in self.output_templates]
        self.num_outputs = len(self.output_templates)
        assert self.num_outputs >= 1
        
        # Populate remaining fields.
        self.input_shapes   = [shape_to_list(t.shape) for t in self.input_templates]
        self.output_shapes  = [shape_to_list(t.shape) for t in self.output_templates]
        self.input_shape    = self.input_shapes[0]
        self.output_shape   = self.output_shapes[0]
        self.vars           = OrderedDict([(self.get_var_localname(var), var) for var in tf.global_variables(self.scope + '/')])
        self.trainables     = OrderedDict([(self.get_var_localname(var), var) for var in tf.trainable_variables(self.scope + '/')])",_8178.py,13,"'/', '_'",*'/_'
https://github.com/enthought/mayavi/tree/master/tvtk/tools/mlab.py,"def __init__(self, x, y, z, scalars, **traits):
        """"""
        Parameters
        ----------

        - x : array
          A list of x coordinate values formed using numpy.mgrid.
        - y : array
          A list of y coordinate values formed using numpy.mgrid.
        - z : array
          A list of z coordinate values formed using numpy.mgrid.
        - scalars : array
          Scalars to associate with the points.
        """"""
        super(Contour3, self).__init__(**traits)
        triangles, points = make_triangles_points(x, y, z, scalars)
        self.pd = make_triangle_polydata(triangles, points, scalars)

        dr = self.pd.point_data.scalars.range
        self.lut.table_range = dr

        cf = self.contour_filter
        configure_input_data(cf, self.pd)
        cf.generate_values(self.number_of_contours, dr[0], dr[1])
        mapper = tvtk.PolyDataMapper(input=cf.output, lookup_table=self.lut,
                                     scalar_range=dr)
        cont_actor = _make_actor(mapper=mapper)

        self.actors.append(cont_actor)",_8181.py,24,"dr[0], dr[1]",*dr[:2]
https://github.com/EONRaider/violent-python3/tree/master/chapter06/image_mirror.py,"def mirror_images(tgt_url, dest_dir):
    ab = AnonBrowser()
    ab.anonymize()
    ab.open(tgt_url)
    html = str(ab.get_current_page())
    soup = BeautifulSoup(html, 'html.parser')
    image_tags = soup.find_all('img')

    for image in image_tags:
        filename = image['src'].lstrip('http://')
        filename = os.path.join(dest_dir, filename.replace('/', '_'))
        data = ab.open(image['src']).read()
        with open(filename, 'wb') as save:
            print(f'[+] Saving {str(filename)}')
            save.write(data)",_8220.py,11,"'/', '_'",*'/_'
https://github.com/ahmedkhlief/APT-Hunter/tree/master/lib/EvtxDetection.py,"def detect_events_powershell_log(file_name,input_timzone):

    for file in file_name:
        parser = PyEvtxParser(file)
        for record in parser.records():
            EventID = EventID_rex.findall(record['data'])
            Computer = Computer_rex.findall(record['data'])
            Channel = Channel_rex.findall(record['data'])

            if len(EventID) > 0:
                Host_Application = HostApplication_rex.findall(record['data'])
                User =UserId_rex.findall(record['data'])
                Engine_Version = EngineVersion_rex.findall(record['data'])
                ScriptName = ScriptName_rex.findall(record['data'])
                CommandLine= CommandLine_rex.findall(record['data'])
                Error_Message = ErrorMessage_rex.findall(record['data'])
                Suspicious=[]
                #Powershell Pipeline Execution details
                host_app=""""

                if record['data'].strip().find(""\\temp\\"") > -1 or record['data'].strip().find(
                        ""\\tmp\\"") > -1:
                    Event_desc=""Powershell Operation including TEMP Folder""
                    Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                    Powershell_events[0]['Computer Name'].append(Computer[0])
                    Powershell_events[0]['Channel'].append(Channel[0])
                    Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                    Powershell_events[0]['Detection Rule'].append(
                        ""Powershell Executing Pipeline - Operation including TEMP folder "")
                    Powershell_events[0]['Detection Domain'].append(""Threat"")
                    Powershell_events[0]['Severity'].append(""High"")
                    Powershell_events[0]['Event Description'].append(Event_desc)
                    Powershell_events[0]['Event ID'].append(EventID[0])
                    Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"", "" ""))


                #Detect any log that contain suspicious process name or argument
                for i in Suspicious_executables:

                    if record['data'].lower().find(i.lower())>-1:

                        #print(""##### "" + record[""timestamp""] + "" ####  "", end='')
                        #print(""## Found Suspicios Process "", end='')
                        #print(""User Name : ( %s ) "" % Account_Name[0][0].strip(), end='')
                        #print(""with Command Line : ( "" + Process_Command_Line[0][0].strip() + "" )"")
                        # print(""###########"")

                        Event_desc =""Found a log contain suspicious powershell command ( %s)""%i
                        Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                        Powershell_events[0]['Computer Name'].append(Computer[0])
                        Powershell_events[0]['Channel'].append(Channel[0])
                        Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                        Powershell_events[0]['Detection Rule'].append(""Suspicious Command or process found in the log"")
                        Powershell_events[0]['Detection Domain'].append(""Threat"")
                        Powershell_events[0]['Severity'].append(""Critical"")
                        Powershell_events[0]['Event Description'].append(Event_desc)
                        Powershell_events[0]['Event ID'].append(EventID[0])
                        Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"", "" ""))
                        break

                if EventID[0]==""800"" :
                    if len(Host_Application) == 0:
                        host_app = """"
                    else:
                        host_app = Host_Application[0].strip()
                    for i in Suspicious_powershell_commands:
                        if i in record['data']:
                            Suspicious.append(i)

                    if len(Suspicious)>0:
                        #print(""##### "" + record[""timestamp""] + "" #### EventID=800 ### Powershell Pipeline Execution details #### "", end='')
                        #print(""Found User (""+User[0].strip()+"") run Suspicious PowerShell commands that include (""+"","".join(Suspicious)+"") in event with Command Line (""+CommandLine[0].strip()+"") and full command (""+Host_Application[0].strip()+"") "", end='')#, check event details ""+record['data'])
                        Event_desc =""Found User (""+User[0].strip()+"") run Suspicious PowerShell commands that include (""+"","".join(Suspicious)+"") in event with Command Line (""+CommandLine[0].strip()+"") and full command (""+host_app+"") ""
                        if len(Error_Message)>0:
                            Event_desc = Event_desc +""Error Message (""+Error_Message[0].strip()+"")""
                            #print(""Error Message (""+Error_Message[0].strip()+"")"")
                        #else:
                        #    print("""")

                        Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                        Powershell_events[0]['Computer Name'].append(Computer[0])
                        Powershell_events[0]['Channel'].append(Channel[0])
                        Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                        Powershell_events[0]['Detection Rule'].append(""Powershell Executing Pipeline - Suspicious Powershell Commands detected"")
                        Powershell_events[0]['Detection Domain'].append(""Threat"")
                        Powershell_events[0]['Severity'].append(""Critical"")
                        Powershell_events[0]['Event Description'].append(Event_desc)
                        Powershell_events[0]['Event ID'].append(EventID[0])
                        Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"", "" ""))

                Suspicious = []

                if EventID[0]==""600"" or EventID[0]==""400"" or EventID[0]==""403"" :
                    if len(Host_Application) == 0:
                        host_app = """"
                    else:
                        host_app = Host_Application[0].strip()
                    for i in Suspicious_powershell_commands:
                        if i in record['data']:
                            Suspicious.append(i)

                    if len(Suspicious)>0:
                        #print(""##### "" + record[""timestamp""] + "" #### EventID=""+EventID[0].strip()+"" ### Engine state is changed #### "", end='')
                        #print(""Found  Suspicious PowerShell commands that include (""+"","".join(Suspicious)+"") in event with Command Line (""+CommandLine[0].strip()+"") and full command (""+Host_Application[0].strip()+"") "", end='')#, check event details ""+record['data'])
                        Event_desc =""Found  Suspicious PowerShell commands that include ("" + "","".join(
                            Suspicious) + "") in event with Command Line ("" + CommandLine[
                            0].strip() + "") and full command ("" + host_app + "") ""

                        if len(Error_Message)>0:
                            Event_desc = Event_desc + ""Error Message ("" + Error_Message[0].strip() + "")""
                            #print(""Error Message (""+Error_Message[0].strip()+"")"")
                        #else:
                        #    print("""")
                        Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                        Powershell_events[0]['Computer Name'].append(Computer[0])
                        Powershell_events[0]['Channel'].append(Channel[0])
                        Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                        Powershell_events[0]['Detection Rule'].append(""Suspicious PowerShell commands Detected"")
                        Powershell_events[0]['Detection Domain'].append(""Threat"")
                        Powershell_events[0]['Severity'].append(""Critical"")
                        Powershell_events[0]['Event Description'].append(Event_desc)
                        Powershell_events[0]['Event ID'].append(EventID[0])
                        Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"","" ""))


                Suspicious = []
                if EventID[0]!=""600"" and EventID[0]!=""400"" or EventID[0]!=""403"" or EventID[0]!=""800"":
                    for i in Suspicious_powershell_commands:
                        if i in record['data']:
                            Suspicious.append(i)

                    if len(Suspicious)>0:
                        Event_desc =""Found  Suspicious PowerShell commands that include ("" + "","".join(Suspicious) + "") in event ""
                        Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                        Powershell_events[0]['Computer Name'].append(Computer[0])
                        Powershell_events[0]['Channel'].append(Channel[0])
                        Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                        Powershell_events[0]['Detection Rule'].append(""Suspicious PowerShell commands Detected"")
                        Powershell_events[0]['Detection Domain'].append(""Threat"")
                        Powershell_events[0]['Severity'].append(""Critical"")
                        Powershell_events[0]['Event Description'].append(Event_desc)
                        Powershell_events[0]['Event ID'].append(EventID[0])
                        Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"","" ""))
                Suspicious = []
            else:
                print(record['data'])",_8471.py,34,"'\r', ' '",*'\r '
https://github.com/ahmedkhlief/APT-Hunter/tree/master/lib/EvtxDetection.py,"def detect_events_powershell_log(file_name,input_timzone):

    for file in file_name:
        parser = PyEvtxParser(file)
        for record in parser.records():
            EventID = EventID_rex.findall(record['data'])
            Computer = Computer_rex.findall(record['data'])
            Channel = Channel_rex.findall(record['data'])

            if len(EventID) > 0:
                Host_Application = HostApplication_rex.findall(record['data'])
                User =UserId_rex.findall(record['data'])
                Engine_Version = EngineVersion_rex.findall(record['data'])
                ScriptName = ScriptName_rex.findall(record['data'])
                CommandLine= CommandLine_rex.findall(record['data'])
                Error_Message = ErrorMessage_rex.findall(record['data'])
                Suspicious=[]
                #Powershell Pipeline Execution details
                host_app=""""

                if record['data'].strip().find(""\\temp\\"") > -1 or record['data'].strip().find(
                        ""\\tmp\\"") > -1:
                    Event_desc=""Powershell Operation including TEMP Folder""
                    Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                    Powershell_events[0]['Computer Name'].append(Computer[0])
                    Powershell_events[0]['Channel'].append(Channel[0])
                    Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                    Powershell_events[0]['Detection Rule'].append(
                        ""Powershell Executing Pipeline - Operation including TEMP folder "")
                    Powershell_events[0]['Detection Domain'].append(""Threat"")
                    Powershell_events[0]['Severity'].append(""High"")
                    Powershell_events[0]['Event Description'].append(Event_desc)
                    Powershell_events[0]['Event ID'].append(EventID[0])
                    Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"", "" ""))


                #Detect any log that contain suspicious process name or argument
                for i in Suspicious_executables:

                    if record['data'].lower().find(i.lower())>-1:

                        #print(""##### "" + record[""timestamp""] + "" ####  "", end='')
                        #print(""## Found Suspicios Process "", end='')
                        #print(""User Name : ( %s ) "" % Account_Name[0][0].strip(), end='')
                        #print(""with Command Line : ( "" + Process_Command_Line[0][0].strip() + "" )"")
                        # print(""###########"")

                        Event_desc =""Found a log contain suspicious powershell command ( %s)""%i
                        Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                        Powershell_events[0]['Computer Name'].append(Computer[0])
                        Powershell_events[0]['Channel'].append(Channel[0])
                        Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                        Powershell_events[0]['Detection Rule'].append(""Suspicious Command or process found in the log"")
                        Powershell_events[0]['Detection Domain'].append(""Threat"")
                        Powershell_events[0]['Severity'].append(""Critical"")
                        Powershell_events[0]['Event Description'].append(Event_desc)
                        Powershell_events[0]['Event ID'].append(EventID[0])
                        Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"", "" ""))
                        break

                if EventID[0]==""800"" :
                    if len(Host_Application) == 0:
                        host_app = """"
                    else:
                        host_app = Host_Application[0].strip()
                    for i in Suspicious_powershell_commands:
                        if i in record['data']:
                            Suspicious.append(i)

                    if len(Suspicious)>0:
                        #print(""##### "" + record[""timestamp""] + "" #### EventID=800 ### Powershell Pipeline Execution details #### "", end='')
                        #print(""Found User (""+User[0].strip()+"") run Suspicious PowerShell commands that include (""+"","".join(Suspicious)+"") in event with Command Line (""+CommandLine[0].strip()+"") and full command (""+Host_Application[0].strip()+"") "", end='')#, check event details ""+record['data'])
                        Event_desc =""Found User (""+User[0].strip()+"") run Suspicious PowerShell commands that include (""+"","".join(Suspicious)+"") in event with Command Line (""+CommandLine[0].strip()+"") and full command (""+host_app+"") ""
                        if len(Error_Message)>0:
                            Event_desc = Event_desc +""Error Message (""+Error_Message[0].strip()+"")""
                            #print(""Error Message (""+Error_Message[0].strip()+"")"")
                        #else:
                        #    print("""")

                        Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                        Powershell_events[0]['Computer Name'].append(Computer[0])
                        Powershell_events[0]['Channel'].append(Channel[0])
                        Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                        Powershell_events[0]['Detection Rule'].append(""Powershell Executing Pipeline - Suspicious Powershell Commands detected"")
                        Powershell_events[0]['Detection Domain'].append(""Threat"")
                        Powershell_events[0]['Severity'].append(""Critical"")
                        Powershell_events[0]['Event Description'].append(Event_desc)
                        Powershell_events[0]['Event ID'].append(EventID[0])
                        Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"", "" ""))

                Suspicious = []

                if EventID[0]==""600"" or EventID[0]==""400"" or EventID[0]==""403"" :
                    if len(Host_Application) == 0:
                        host_app = """"
                    else:
                        host_app = Host_Application[0].strip()
                    for i in Suspicious_powershell_commands:
                        if i in record['data']:
                            Suspicious.append(i)

                    if len(Suspicious)>0:
                        #print(""##### "" + record[""timestamp""] + "" #### EventID=""+EventID[0].strip()+"" ### Engine state is changed #### "", end='')
                        #print(""Found  Suspicious PowerShell commands that include (""+"","".join(Suspicious)+"") in event with Command Line (""+CommandLine[0].strip()+"") and full command (""+Host_Application[0].strip()+"") "", end='')#, check event details ""+record['data'])
                        Event_desc =""Found  Suspicious PowerShell commands that include ("" + "","".join(
                            Suspicious) + "") in event with Command Line ("" + CommandLine[
                            0].strip() + "") and full command ("" + host_app + "") ""

                        if len(Error_Message)>0:
                            Event_desc = Event_desc + ""Error Message ("" + Error_Message[0].strip() + "")""
                            #print(""Error Message (""+Error_Message[0].strip()+"")"")
                        #else:
                        #    print("""")
                        Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                        Powershell_events[0]['Computer Name'].append(Computer[0])
                        Powershell_events[0]['Channel'].append(Channel[0])
                        Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                        Powershell_events[0]['Detection Rule'].append(""Suspicious PowerShell commands Detected"")
                        Powershell_events[0]['Detection Domain'].append(""Threat"")
                        Powershell_events[0]['Severity'].append(""Critical"")
                        Powershell_events[0]['Event Description'].append(Event_desc)
                        Powershell_events[0]['Event ID'].append(EventID[0])
                        Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"","" ""))


                Suspicious = []
                if EventID[0]!=""600"" and EventID[0]!=""400"" or EventID[0]!=""403"" or EventID[0]!=""800"":
                    for i in Suspicious_powershell_commands:
                        if i in record['data']:
                            Suspicious.append(i)

                    if len(Suspicious)>0:
                        Event_desc =""Found  Suspicious PowerShell commands that include ("" + "","".join(Suspicious) + "") in event ""
                        Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                        Powershell_events[0]['Computer Name'].append(Computer[0])
                        Powershell_events[0]['Channel'].append(Channel[0])
                        Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                        Powershell_events[0]['Detection Rule'].append(""Suspicious PowerShell commands Detected"")
                        Powershell_events[0]['Detection Domain'].append(""Threat"")
                        Powershell_events[0]['Severity'].append(""Critical"")
                        Powershell_events[0]['Event Description'].append(Event_desc)
                        Powershell_events[0]['Event ID'].append(EventID[0])
                        Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"","" ""))
                Suspicious = []
            else:
                print(record['data'])",_8471.py,58,"'\r', ' '",*'\r '
https://github.com/ahmedkhlief/APT-Hunter/tree/master/lib/EvtxDetection.py,"def detect_events_powershell_log(file_name,input_timzone):

    for file in file_name:
        parser = PyEvtxParser(file)
        for record in parser.records():
            EventID = EventID_rex.findall(record['data'])
            Computer = Computer_rex.findall(record['data'])
            Channel = Channel_rex.findall(record['data'])

            if len(EventID) > 0:
                Host_Application = HostApplication_rex.findall(record['data'])
                User =UserId_rex.findall(record['data'])
                Engine_Version = EngineVersion_rex.findall(record['data'])
                ScriptName = ScriptName_rex.findall(record['data'])
                CommandLine= CommandLine_rex.findall(record['data'])
                Error_Message = ErrorMessage_rex.findall(record['data'])
                Suspicious=[]
                #Powershell Pipeline Execution details
                host_app=""""

                if record['data'].strip().find(""\\temp\\"") > -1 or record['data'].strip().find(
                        ""\\tmp\\"") > -1:
                    Event_desc=""Powershell Operation including TEMP Folder""
                    Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                    Powershell_events[0]['Computer Name'].append(Computer[0])
                    Powershell_events[0]['Channel'].append(Channel[0])
                    Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                    Powershell_events[0]['Detection Rule'].append(
                        ""Powershell Executing Pipeline - Operation including TEMP folder "")
                    Powershell_events[0]['Detection Domain'].append(""Threat"")
                    Powershell_events[0]['Severity'].append(""High"")
                    Powershell_events[0]['Event Description'].append(Event_desc)
                    Powershell_events[0]['Event ID'].append(EventID[0])
                    Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"", "" ""))


                #Detect any log that contain suspicious process name or argument
                for i in Suspicious_executables:

                    if record['data'].lower().find(i.lower())>-1:

                        #print(""##### "" + record[""timestamp""] + "" ####  "", end='')
                        #print(""## Found Suspicios Process "", end='')
                        #print(""User Name : ( %s ) "" % Account_Name[0][0].strip(), end='')
                        #print(""with Command Line : ( "" + Process_Command_Line[0][0].strip() + "" )"")
                        # print(""###########"")

                        Event_desc =""Found a log contain suspicious powershell command ( %s)""%i
                        Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                        Powershell_events[0]['Computer Name'].append(Computer[0])
                        Powershell_events[0]['Channel'].append(Channel[0])
                        Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                        Powershell_events[0]['Detection Rule'].append(""Suspicious Command or process found in the log"")
                        Powershell_events[0]['Detection Domain'].append(""Threat"")
                        Powershell_events[0]['Severity'].append(""Critical"")
                        Powershell_events[0]['Event Description'].append(Event_desc)
                        Powershell_events[0]['Event ID'].append(EventID[0])
                        Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"", "" ""))
                        break

                if EventID[0]==""800"" :
                    if len(Host_Application) == 0:
                        host_app = """"
                    else:
                        host_app = Host_Application[0].strip()
                    for i in Suspicious_powershell_commands:
                        if i in record['data']:
                            Suspicious.append(i)

                    if len(Suspicious)>0:
                        #print(""##### "" + record[""timestamp""] + "" #### EventID=800 ### Powershell Pipeline Execution details #### "", end='')
                        #print(""Found User (""+User[0].strip()+"") run Suspicious PowerShell commands that include (""+"","".join(Suspicious)+"") in event with Command Line (""+CommandLine[0].strip()+"") and full command (""+Host_Application[0].strip()+"") "", end='')#, check event details ""+record['data'])
                        Event_desc =""Found User (""+User[0].strip()+"") run Suspicious PowerShell commands that include (""+"","".join(Suspicious)+"") in event with Command Line (""+CommandLine[0].strip()+"") and full command (""+host_app+"") ""
                        if len(Error_Message)>0:
                            Event_desc = Event_desc +""Error Message (""+Error_Message[0].strip()+"")""
                            #print(""Error Message (""+Error_Message[0].strip()+"")"")
                        #else:
                        #    print("""")

                        Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                        Powershell_events[0]['Computer Name'].append(Computer[0])
                        Powershell_events[0]['Channel'].append(Channel[0])
                        Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                        Powershell_events[0]['Detection Rule'].append(""Powershell Executing Pipeline - Suspicious Powershell Commands detected"")
                        Powershell_events[0]['Detection Domain'].append(""Threat"")
                        Powershell_events[0]['Severity'].append(""Critical"")
                        Powershell_events[0]['Event Description'].append(Event_desc)
                        Powershell_events[0]['Event ID'].append(EventID[0])
                        Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"", "" ""))

                Suspicious = []

                if EventID[0]==""600"" or EventID[0]==""400"" or EventID[0]==""403"" :
                    if len(Host_Application) == 0:
                        host_app = """"
                    else:
                        host_app = Host_Application[0].strip()
                    for i in Suspicious_powershell_commands:
                        if i in record['data']:
                            Suspicious.append(i)

                    if len(Suspicious)>0:
                        #print(""##### "" + record[""timestamp""] + "" #### EventID=""+EventID[0].strip()+"" ### Engine state is changed #### "", end='')
                        #print(""Found  Suspicious PowerShell commands that include (""+"","".join(Suspicious)+"") in event with Command Line (""+CommandLine[0].strip()+"") and full command (""+Host_Application[0].strip()+"") "", end='')#, check event details ""+record['data'])
                        Event_desc =""Found  Suspicious PowerShell commands that include ("" + "","".join(
                            Suspicious) + "") in event with Command Line ("" + CommandLine[
                            0].strip() + "") and full command ("" + host_app + "") ""

                        if len(Error_Message)>0:
                            Event_desc = Event_desc + ""Error Message ("" + Error_Message[0].strip() + "")""
                            #print(""Error Message (""+Error_Message[0].strip()+"")"")
                        #else:
                        #    print("""")
                        Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                        Powershell_events[0]['Computer Name'].append(Computer[0])
                        Powershell_events[0]['Channel'].append(Channel[0])
                        Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                        Powershell_events[0]['Detection Rule'].append(""Suspicious PowerShell commands Detected"")
                        Powershell_events[0]['Detection Domain'].append(""Threat"")
                        Powershell_events[0]['Severity'].append(""Critical"")
                        Powershell_events[0]['Event Description'].append(Event_desc)
                        Powershell_events[0]['Event ID'].append(EventID[0])
                        Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"","" ""))


                Suspicious = []
                if EventID[0]!=""600"" and EventID[0]!=""400"" or EventID[0]!=""403"" or EventID[0]!=""800"":
                    for i in Suspicious_powershell_commands:
                        if i in record['data']:
                            Suspicious.append(i)

                    if len(Suspicious)>0:
                        Event_desc =""Found  Suspicious PowerShell commands that include ("" + "","".join(Suspicious) + "") in event ""
                        Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                        Powershell_events[0]['Computer Name'].append(Computer[0])
                        Powershell_events[0]['Channel'].append(Channel[0])
                        Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                        Powershell_events[0]['Detection Rule'].append(""Suspicious PowerShell commands Detected"")
                        Powershell_events[0]['Detection Domain'].append(""Threat"")
                        Powershell_events[0]['Severity'].append(""Critical"")
                        Powershell_events[0]['Event Description'].append(Event_desc)
                        Powershell_events[0]['Event ID'].append(EventID[0])
                        Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"","" ""))
                Suspicious = []
            else:
                print(record['data'])",_8471.py,89,"'\r', ' '",*'\r '
https://github.com/ahmedkhlief/APT-Hunter/tree/master/lib/EvtxDetection.py,"def detect_events_powershell_log(file_name,input_timzone):

    for file in file_name:
        parser = PyEvtxParser(file)
        for record in parser.records():
            EventID = EventID_rex.findall(record['data'])
            Computer = Computer_rex.findall(record['data'])
            Channel = Channel_rex.findall(record['data'])

            if len(EventID) > 0:
                Host_Application = HostApplication_rex.findall(record['data'])
                User =UserId_rex.findall(record['data'])
                Engine_Version = EngineVersion_rex.findall(record['data'])
                ScriptName = ScriptName_rex.findall(record['data'])
                CommandLine= CommandLine_rex.findall(record['data'])
                Error_Message = ErrorMessage_rex.findall(record['data'])
                Suspicious=[]
                #Powershell Pipeline Execution details
                host_app=""""

                if record['data'].strip().find(""\\temp\\"") > -1 or record['data'].strip().find(
                        ""\\tmp\\"") > -1:
                    Event_desc=""Powershell Operation including TEMP Folder""
                    Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                    Powershell_events[0]['Computer Name'].append(Computer[0])
                    Powershell_events[0]['Channel'].append(Channel[0])
                    Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                    Powershell_events[0]['Detection Rule'].append(
                        ""Powershell Executing Pipeline - Operation including TEMP folder "")
                    Powershell_events[0]['Detection Domain'].append(""Threat"")
                    Powershell_events[0]['Severity'].append(""High"")
                    Powershell_events[0]['Event Description'].append(Event_desc)
                    Powershell_events[0]['Event ID'].append(EventID[0])
                    Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"", "" ""))


                #Detect any log that contain suspicious process name or argument
                for i in Suspicious_executables:

                    if record['data'].lower().find(i.lower())>-1:

                        #print(""##### "" + record[""timestamp""] + "" ####  "", end='')
                        #print(""## Found Suspicios Process "", end='')
                        #print(""User Name : ( %s ) "" % Account_Name[0][0].strip(), end='')
                        #print(""with Command Line : ( "" + Process_Command_Line[0][0].strip() + "" )"")
                        # print(""###########"")

                        Event_desc =""Found a log contain suspicious powershell command ( %s)""%i
                        Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                        Powershell_events[0]['Computer Name'].append(Computer[0])
                        Powershell_events[0]['Channel'].append(Channel[0])
                        Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                        Powershell_events[0]['Detection Rule'].append(""Suspicious Command or process found in the log"")
                        Powershell_events[0]['Detection Domain'].append(""Threat"")
                        Powershell_events[0]['Severity'].append(""Critical"")
                        Powershell_events[0]['Event Description'].append(Event_desc)
                        Powershell_events[0]['Event ID'].append(EventID[0])
                        Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"", "" ""))
                        break

                if EventID[0]==""800"" :
                    if len(Host_Application) == 0:
                        host_app = """"
                    else:
                        host_app = Host_Application[0].strip()
                    for i in Suspicious_powershell_commands:
                        if i in record['data']:
                            Suspicious.append(i)

                    if len(Suspicious)>0:
                        #print(""##### "" + record[""timestamp""] + "" #### EventID=800 ### Powershell Pipeline Execution details #### "", end='')
                        #print(""Found User (""+User[0].strip()+"") run Suspicious PowerShell commands that include (""+"","".join(Suspicious)+"") in event with Command Line (""+CommandLine[0].strip()+"") and full command (""+Host_Application[0].strip()+"") "", end='')#, check event details ""+record['data'])
                        Event_desc =""Found User (""+User[0].strip()+"") run Suspicious PowerShell commands that include (""+"","".join(Suspicious)+"") in event with Command Line (""+CommandLine[0].strip()+"") and full command (""+host_app+"") ""
                        if len(Error_Message)>0:
                            Event_desc = Event_desc +""Error Message (""+Error_Message[0].strip()+"")""
                            #print(""Error Message (""+Error_Message[0].strip()+"")"")
                        #else:
                        #    print("""")

                        Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                        Powershell_events[0]['Computer Name'].append(Computer[0])
                        Powershell_events[0]['Channel'].append(Channel[0])
                        Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                        Powershell_events[0]['Detection Rule'].append(""Powershell Executing Pipeline - Suspicious Powershell Commands detected"")
                        Powershell_events[0]['Detection Domain'].append(""Threat"")
                        Powershell_events[0]['Severity'].append(""Critical"")
                        Powershell_events[0]['Event Description'].append(Event_desc)
                        Powershell_events[0]['Event ID'].append(EventID[0])
                        Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"", "" ""))

                Suspicious = []

                if EventID[0]==""600"" or EventID[0]==""400"" or EventID[0]==""403"" :
                    if len(Host_Application) == 0:
                        host_app = """"
                    else:
                        host_app = Host_Application[0].strip()
                    for i in Suspicious_powershell_commands:
                        if i in record['data']:
                            Suspicious.append(i)

                    if len(Suspicious)>0:
                        #print(""##### "" + record[""timestamp""] + "" #### EventID=""+EventID[0].strip()+"" ### Engine state is changed #### "", end='')
                        #print(""Found  Suspicious PowerShell commands that include (""+"","".join(Suspicious)+"") in event with Command Line (""+CommandLine[0].strip()+"") and full command (""+Host_Application[0].strip()+"") "", end='')#, check event details ""+record['data'])
                        Event_desc =""Found  Suspicious PowerShell commands that include ("" + "","".join(
                            Suspicious) + "") in event with Command Line ("" + CommandLine[
                            0].strip() + "") and full command ("" + host_app + "") ""

                        if len(Error_Message)>0:
                            Event_desc = Event_desc + ""Error Message ("" + Error_Message[0].strip() + "")""
                            #print(""Error Message (""+Error_Message[0].strip()+"")"")
                        #else:
                        #    print("""")
                        Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                        Powershell_events[0]['Computer Name'].append(Computer[0])
                        Powershell_events[0]['Channel'].append(Channel[0])
                        Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                        Powershell_events[0]['Detection Rule'].append(""Suspicious PowerShell commands Detected"")
                        Powershell_events[0]['Detection Domain'].append(""Threat"")
                        Powershell_events[0]['Severity'].append(""Critical"")
                        Powershell_events[0]['Event Description'].append(Event_desc)
                        Powershell_events[0]['Event ID'].append(EventID[0])
                        Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"","" ""))


                Suspicious = []
                if EventID[0]!=""600"" and EventID[0]!=""400"" or EventID[0]!=""403"" or EventID[0]!=""800"":
                    for i in Suspicious_powershell_commands:
                        if i in record['data']:
                            Suspicious.append(i)

                    if len(Suspicious)>0:
                        Event_desc =""Found  Suspicious PowerShell commands that include ("" + "","".join(Suspicious) + "") in event ""
                        Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                        Powershell_events[0]['Computer Name'].append(Computer[0])
                        Powershell_events[0]['Channel'].append(Channel[0])
                        Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                        Powershell_events[0]['Detection Rule'].append(""Suspicious PowerShell commands Detected"")
                        Powershell_events[0]['Detection Domain'].append(""Threat"")
                        Powershell_events[0]['Severity'].append(""Critical"")
                        Powershell_events[0]['Event Description'].append(Event_desc)
                        Powershell_events[0]['Event ID'].append(EventID[0])
                        Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"","" ""))
                Suspicious = []
            else:
                print(record['data'])",_8471.py,123,"'\r', ' '",*'\r '
https://github.com/ahmedkhlief/APT-Hunter/tree/master/lib/EvtxDetection.py,"def detect_events_powershell_log(file_name,input_timzone):

    for file in file_name:
        parser = PyEvtxParser(file)
        for record in parser.records():
            EventID = EventID_rex.findall(record['data'])
            Computer = Computer_rex.findall(record['data'])
            Channel = Channel_rex.findall(record['data'])

            if len(EventID) > 0:
                Host_Application = HostApplication_rex.findall(record['data'])
                User =UserId_rex.findall(record['data'])
                Engine_Version = EngineVersion_rex.findall(record['data'])
                ScriptName = ScriptName_rex.findall(record['data'])
                CommandLine= CommandLine_rex.findall(record['data'])
                Error_Message = ErrorMessage_rex.findall(record['data'])
                Suspicious=[]
                #Powershell Pipeline Execution details
                host_app=""""

                if record['data'].strip().find(""\\temp\\"") > -1 or record['data'].strip().find(
                        ""\\tmp\\"") > -1:
                    Event_desc=""Powershell Operation including TEMP Folder""
                    Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                    Powershell_events[0]['Computer Name'].append(Computer[0])
                    Powershell_events[0]['Channel'].append(Channel[0])
                    Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                    Powershell_events[0]['Detection Rule'].append(
                        ""Powershell Executing Pipeline - Operation including TEMP folder "")
                    Powershell_events[0]['Detection Domain'].append(""Threat"")
                    Powershell_events[0]['Severity'].append(""High"")
                    Powershell_events[0]['Event Description'].append(Event_desc)
                    Powershell_events[0]['Event ID'].append(EventID[0])
                    Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"", "" ""))


                #Detect any log that contain suspicious process name or argument
                for i in Suspicious_executables:

                    if record['data'].lower().find(i.lower())>-1:

                        #print(""##### "" + record[""timestamp""] + "" ####  "", end='')
                        #print(""## Found Suspicios Process "", end='')
                        #print(""User Name : ( %s ) "" % Account_Name[0][0].strip(), end='')
                        #print(""with Command Line : ( "" + Process_Command_Line[0][0].strip() + "" )"")
                        # print(""###########"")

                        Event_desc =""Found a log contain suspicious powershell command ( %s)""%i
                        Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                        Powershell_events[0]['Computer Name'].append(Computer[0])
                        Powershell_events[0]['Channel'].append(Channel[0])
                        Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                        Powershell_events[0]['Detection Rule'].append(""Suspicious Command or process found in the log"")
                        Powershell_events[0]['Detection Domain'].append(""Threat"")
                        Powershell_events[0]['Severity'].append(""Critical"")
                        Powershell_events[0]['Event Description'].append(Event_desc)
                        Powershell_events[0]['Event ID'].append(EventID[0])
                        Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"", "" ""))
                        break

                if EventID[0]==""800"" :
                    if len(Host_Application) == 0:
                        host_app = """"
                    else:
                        host_app = Host_Application[0].strip()
                    for i in Suspicious_powershell_commands:
                        if i in record['data']:
                            Suspicious.append(i)

                    if len(Suspicious)>0:
                        #print(""##### "" + record[""timestamp""] + "" #### EventID=800 ### Powershell Pipeline Execution details #### "", end='')
                        #print(""Found User (""+User[0].strip()+"") run Suspicious PowerShell commands that include (""+"","".join(Suspicious)+"") in event with Command Line (""+CommandLine[0].strip()+"") and full command (""+Host_Application[0].strip()+"") "", end='')#, check event details ""+record['data'])
                        Event_desc =""Found User (""+User[0].strip()+"") run Suspicious PowerShell commands that include (""+"","".join(Suspicious)+"") in event with Command Line (""+CommandLine[0].strip()+"") and full command (""+host_app+"") ""
                        if len(Error_Message)>0:
                            Event_desc = Event_desc +""Error Message (""+Error_Message[0].strip()+"")""
                            #print(""Error Message (""+Error_Message[0].strip()+"")"")
                        #else:
                        #    print("""")

                        Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                        Powershell_events[0]['Computer Name'].append(Computer[0])
                        Powershell_events[0]['Channel'].append(Channel[0])
                        Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                        Powershell_events[0]['Detection Rule'].append(""Powershell Executing Pipeline - Suspicious Powershell Commands detected"")
                        Powershell_events[0]['Detection Domain'].append(""Threat"")
                        Powershell_events[0]['Severity'].append(""Critical"")
                        Powershell_events[0]['Event Description'].append(Event_desc)
                        Powershell_events[0]['Event ID'].append(EventID[0])
                        Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"", "" ""))

                Suspicious = []

                if EventID[0]==""600"" or EventID[0]==""400"" or EventID[0]==""403"" :
                    if len(Host_Application) == 0:
                        host_app = """"
                    else:
                        host_app = Host_Application[0].strip()
                    for i in Suspicious_powershell_commands:
                        if i in record['data']:
                            Suspicious.append(i)

                    if len(Suspicious)>0:
                        #print(""##### "" + record[""timestamp""] + "" #### EventID=""+EventID[0].strip()+"" ### Engine state is changed #### "", end='')
                        #print(""Found  Suspicious PowerShell commands that include (""+"","".join(Suspicious)+"") in event with Command Line (""+CommandLine[0].strip()+"") and full command (""+Host_Application[0].strip()+"") "", end='')#, check event details ""+record['data'])
                        Event_desc =""Found  Suspicious PowerShell commands that include ("" + "","".join(
                            Suspicious) + "") in event with Command Line ("" + CommandLine[
                            0].strip() + "") and full command ("" + host_app + "") ""

                        if len(Error_Message)>0:
                            Event_desc = Event_desc + ""Error Message ("" + Error_Message[0].strip() + "")""
                            #print(""Error Message (""+Error_Message[0].strip()+"")"")
                        #else:
                        #    print("""")
                        Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                        Powershell_events[0]['Computer Name'].append(Computer[0])
                        Powershell_events[0]['Channel'].append(Channel[0])
                        Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                        Powershell_events[0]['Detection Rule'].append(""Suspicious PowerShell commands Detected"")
                        Powershell_events[0]['Detection Domain'].append(""Threat"")
                        Powershell_events[0]['Severity'].append(""Critical"")
                        Powershell_events[0]['Event Description'].append(Event_desc)
                        Powershell_events[0]['Event ID'].append(EventID[0])
                        Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"","" ""))


                Suspicious = []
                if EventID[0]!=""600"" and EventID[0]!=""400"" or EventID[0]!=""403"" or EventID[0]!=""800"":
                    for i in Suspicious_powershell_commands:
                        if i in record['data']:
                            Suspicious.append(i)

                    if len(Suspicious)>0:
                        Event_desc =""Found  Suspicious PowerShell commands that include ("" + "","".join(Suspicious) + "") in event ""
                        Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                        Powershell_events[0]['Computer Name'].append(Computer[0])
                        Powershell_events[0]['Channel'].append(Channel[0])
                        Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                        Powershell_events[0]['Detection Rule'].append(""Suspicious PowerShell commands Detected"")
                        Powershell_events[0]['Detection Domain'].append(""Threat"")
                        Powershell_events[0]['Severity'].append(""Critical"")
                        Powershell_events[0]['Event Description'].append(Event_desc)
                        Powershell_events[0]['Event ID'].append(EventID[0])
                        Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"","" ""))
                Suspicious = []
            else:
                print(record['data'])",_8471.py,143,"'\r', ' '",*'\r '
https://github.com/duckietown/gym-duckietown/tree/master/src/gym_duckietown/distortion.py,"def norm(_):
            return np.hypot(_[0], _[1])",_8612.py,2,"_[0], _[1]",*_[:2]
https://github.com/blueman-project/blueman/tree/master/test/gui/manager/test_imports.py,"def load_tests(*_args):
    test_cases = TestSuite()
    home, subpath = os.path.dirname(__file__).rsplit(""/test/"", 1)
    for package in pkgutil.iter_modules([f""{home}/blueman/{subpath}""], f""blueman.{subpath.replace('/', '.')}.""):
        test_cases.addTest(TestImports(package.name))

    assert test_cases.countTestCases() > 0

    return test_cases",_8620.py,4,"'/', '.'",*'/.'
https://github.com/david-cortes/contextualbandits/tree/master/contextualbandits/online.py,"def fit(self, X, a, r, warm_start=False, continue_from_last=False):
        """"""
        Fits the base algorithm (one per class [and per sample if bootstrapped]) to partially labeled data.

        Parameters
        ----------
        X : array(n_samples, n_features) or CSR(n_samples, n_features)
            Matrix of covariates for the available data.
        a : array(n_samples, ), int type
            Arms or actions that were chosen for each observations.
        r : array(n_samples, ), {0,1}
            Rewards that were observed for the chosen actions. Must be binary rewards 0/1.
        warm_start : bool
            Whether to use the results of previous calls to 'fit' as a start
            for fitting to the 'X' data passed here. This will only be available
            if the base classifier has a property ``warm_start`` too and that
            property is also set to 'True'. You can double-check that it's
            recognized as such by checking this object's property
            ``has_warm_start``. Passing 'True' when the classifier doesn't
            support warm start despite having the property might slow down
            things.
            Dropping arms will make this functionality unavailable.
            This options is not available for 'BootstrappedUCB',
            nor for 'BootstrappedTS'.
        continue_from_last : bool
            If the policy was previously fit to data, whether to assume that
            this new call to 'fit' will continue from the exact same dataset as before
            plus new rows appended at the end of 'X', 'a', 'r'. In this case,
            will only refit the models that have new data according to 'a'.
            Note that the bootstrapped policies will still benefit from extra refits.
            This option should not be used when there are calls to 'partial_fit' between
            calls to fit.
            Ignored if using ``assume_unique_reward=True``.

        Returns
        -------
        self : obj
            This object
        """"""
        X, a, r = _check_fit_input(X, a, r, self.choice_names)
        if X.shape[0] == 0:
            return self

        use_warm = warm_start and self.has_warm_start and self.is_fitted
        continue_from_last = False if not self.is_fitted else continue_from_last
        continue_from_last = False if self.assume_unique_reward else continue_from_last
        if continue_from_last:
            if X.shape[0] <= self._obs_to_fit:
                raise ValueError(""X contains less rows than in the last call to 'fit'."")
            arms_to_update = np.unique(a[self._obs_to_fit:])
        else:
            arms_to_update = None

        self._oracles = _OneVsRest(self.base_algorithm,
                                   X, a, r,
                                   self.nchoices,
                                   self._beta_prior_by_arm[0],
                                   self._beta_prior_by_arm[1],
                                   self._beta_prior_by_arm[2],
                                   self.random_state,
                                   self.smoothing, self.noise_to_smooth,
                                   self.assume_unique_reward,
                                   self.batch_train,
                                   refit_buffer = self.refit_buffer,
                                   deep_copy = self.deep_copy_buffer,
                                   force_fit = self._force_fit,
                                   force_counters = self._force_counters,
                                   prev_ovr = self._oracles if self.is_fitted else None,
                                   warm = use_warm,
                                   force_unfit_predict = self.force_unfit_predict,
                                   arms_to_update = arms_to_update,
                                   njobs = self.njobs)
        self._obs_to_fit = X.shape[0]
        self.is_fitted = True
        return self",_8994.py,54,"self._beta_prior_by_arm[0], self._beta_prior_by_arm[1], self._beta_prior_by_arm[2]",*self._beta_prior_by_arm[:3]
https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/squad/agents.py,"def get(self, episode_idx, entry_idx=None):
        action = {}
        episode = self.episodes[episode_idx][entry_idx]
        context = ' '.join(episode['text'].split('\n')[:-1]).replace(
            '\xa0', ' '
        )  # get rid of non breaking space characters
        question = episode['text'].split('\n')[-1]
        label_field = 'labels' if 'labels' in episode else 'eval_labels'
        answers = []
        for answer in episode[label_field]:
            new_answer = answer.replace('.', '').replace('?', '').replace('!', '')
            context = context.replace(answer, new_answer)
            answers.append(new_answer)
        sentences = self.sent_tok.tokenize(context)
        labels = []
        label_starts = []
        for sentence in sentences:
            for answer in answers:
                if answer in sentence and sentence not in labels:
                    labels.append(sentence)
                    label_starts.append(context.index(sentence))

        action = {
            'context': context,
            'text': question,
            label_field: labels,
            'answer_starts': label_starts,
            'label_candidates': sentences,
            'episode_done': episode['episode_done'],
        }

        if self.include_context:
            action['text'] = action['context'] + '\n' + action['text']
            del action['context']

        return action",_9032.py,4,"'\xa0', ' '",*'\xa0 '
https://github.com/RDFLib/rdflib/tree/master/rdflib/plugins/parsers/notation3.py,"def newBlankNode(self, uri=None, why=None):
        if uri is None:
            self.counter += 1
            bn = BNode(""f%sb%s"" % (self.uuid, self.counter))
        else:
            bn = BNode(uri.split(""#"").pop().replace(""_"", ""b""))
        return bn",_9161.py,6,"'_', 'b'",*'_b'
https://github.com/kliment/Printrun/tree/master/printrun/gui/xybuttons.py,"def OnMotion(self, event):
        if not self.enabled:
            return

        oldcorner = self.corner
        oldq, oldc = self.quadrant, self.concentric
        old_hovered_keypad = self.hovered_keypad

        mpos = event.GetPosition()
        self.hovered_keypad = self.mouseOverKeypad(mpos)
        self.quadrant = None
        self.concentric = None
        if self.hovered_keypad is None:
            center = wx.Point(self.center[0], self.center[1])
            riseDist = self.distanceToLine(mpos, center.x - 1, center.y - 1, center.x + 1, center.y + 1)
            fallDist = self.distanceToLine(mpos, center.x - 1, center.y + 1, center.x + 1, center.y - 1)
            self.quadrant, self.concentric = self.getQuadrantConcentricFromPosition(mpos)

            # If mouse hovers in space between quadrants, don't commit to a quadrant
            if riseDist <= self.spacer or fallDist <= self.spacer:
                self.quadrant = None

        cx, cy = self.center
        if mpos.x < cx and mpos.y < cy:
            self.corner = 0
        if mpos.x >= cx and mpos.y < cy:
            self.corner = 1
        if mpos.x >= cx and mpos.y >= cy:
            self.corner = 2
        if mpos.x < cx and mpos.y >= cy:
            self.corner = 3

        if oldq != self.quadrant or oldc != self.concentric or oldcorner != self.corner \
            or old_hovered_keypad != self.hovered_keypad:
            self.update()",_9365.py,14,"self.center[0], self.center[1]",*self.center[:2]
https://github.com/malwaredllc/byob/tree/master/web-gui/buildyourownbotnet/modules/outlook.py,"def _get_emails():
    pythoncom.CoInitialize()
    outlook = win32com.client.Dispatch('Outlook.Application').GetNameSpace('MAPI')
    inbox   = outlook.GetDefaultFolder(6)
    unread  = inbox.Items
    while True:
        email = None
        try:
            email = unread.GetNext()
        except:
            break
        if email:
            sender   = email.SenderEmailAddress.encode('ascii','ignore')
            message  = email.Body.encode('ascii','ignore')[:100] + '...'
            subject  = email.Subject.encode('ascii','ignore')
            received = str(email.ReceivedTime).replace('/','-').replace('\\','')
            globals()['results'][received] = {'from': sender, 'subject': subject, 'message': message}
        else:
            break",_9381.py,16,"'/', '-'",*'/-'
https://github.com/olivierkes/manuskript/tree/master/manuskript/ui/views/corkView.py,"def updateBackground(self):
        if settings.corkBackground[""image""] != """":
            img = findBackground(settings.corkBackground[""image""])
            if img == None:
                img = """"
        else:
            # No background image
            img = """"
        self.setStyleSheet(""""""QListView {{
            background:{color};
            background-image: url({url});
            background-attachment: fixed;
            }}"""""".format(
                color=settings.corkBackground[""color""],
                url=img.replace(""\\"", ""/"")
        ))",_9441.py,15,"'\\', '/'",*'\\/'
https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/vision/models/densenet.py,"def _densenet(arch, layers, pretrained, **kwargs):
    model = DenseNet(layers=layers, **kwargs)
    if pretrained:
        assert (
            arch in model_urls
        ), ""{} model do not have a pretrained model now, you should set pretrained=False"".format(
            arch
        )
        weight_path = get_weights_path_from_url(
            model_urls[arch][0], model_urls[arch][1]
        )

        param = paddle.load(weight_path)
        model.set_dict(param)

    return model",_9530.py,9,"model_urls[arch][0], model_urls[arch][1]",*model_urls[arch][:2]
https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/4_Sarsa_lambda_maze/maze_env.py,"def step(self, action):
        s = self.canvas.coords(self.rect)
        base_action = np.array([0, 0])
        if action == 0:   # up
            if s[1] > UNIT:
                base_action[1] -= UNIT
        elif action == 1:   # down
            if s[1] < (MAZE_H - 1) * UNIT:
                base_action[1] += UNIT
        elif action == 2:   # right
            if s[0] < (MAZE_W - 1) * UNIT:
                base_action[0] += UNIT
        elif action == 3:   # left
            if s[0] > UNIT:
                base_action[0] -= UNIT

        self.canvas.move(self.rect, base_action[0], base_action[1])  # move agent

        s_ = self.canvas.coords(self.rect)  # next state

        # reward function
        if s_ == self.canvas.coords(self.oval):
            reward = 1
            done = True
            s_ = 'terminal'
        elif s_ in [self.canvas.coords(self.hell1), self.canvas.coords(self.hell2)]:
            reward = -1
            done = True
            s_ = 'terminal'
        else:
            reward = 0
            done = False

        return s_, reward, done",_9704.py,17,"base_action[0], base_action[1]",*base_action[:2]
https://github.com/douban/dpark/tree/master/dpark/tabular.py,"def __init__(self, fields, values):
        if isinstance(fields, (str,)):
            fields = fields.replace(',', ' ').split()

        fields = list(fields)
        values = tuple(values)
        if len(set(fields)) != len(fields):
            raise ValueError('Duplicated field names!')

        if len(fields) != len(values):
            raise ValueError('Key/value length not match!')

        self._fields = fields
        self._values = values",_9736.py,3,"',', ' '","*', '"
https://github.com/gugarosa/opytimizer/tree/master/tests/opytimizer/optimizers/population/test_aeo.py,"def test_aeo_carnivore_consumption():
    new_aeo = aeo.AEO()

    search_space = search.SearchSpace(n_agents=10, n_variables=2,
                                      lower_bound=[0, 0], upper_bound=[10, 10])

    a = new_aeo._carnivore_consumption(
        search_space.agents[0], search_space.agents[1], 0.5)

    assert type(a).__name__ == 'Agent'",_10167.py,7,"search_space.agents[0], search_space.agents[1]",*search_space.agents[:2]
https://github.com/HypothesisWorks/hypothesis/tree/master/whole-repo-tests/test_release_management.py,"def test_update_in_middle():
    assert replace(""a = 1\nb=2\nc = 3"", ""b"", ""4"") == ""a = 1\nb=4\nc = 3""",_10250.py,2,"'b', '4'",*'b4'
https://github.com/pydata/xarray/tree/master/xarray/tests/test_plot.py,"def test_empty_cell(self):
        g = xplt.FacetGrid(self.darray, col=""z"", col_wrap=2)
        g.map_dataarray(xplt.imshow, ""x"", ""y"")

        bottomright = g.axes[-1, -1]
        assert not bottomright.has_data()
        assert not bottomright.get_visible()",_10683.py,3,"'x', 'y'",*'xy'
https://github.com/openstates/openstates-scrapers/tree/master/scrapers/ca/bills.py,"def scrape_bill_type(
        self,
        chamber,
        session,
        bill_type,
        type_abbr,
        committee_abbr_regex=get_committee_name_regex(),
    ):
        bills = (
            self.session.query(CABill)
            .filter_by(session_year=session)
            .filter_by(measure_type=type_abbr)
        )

        archive_year = int(session[0:4])
        not_archive_year = archive_year >= 2009

        for bill in bills:
            bill_session = session
            if bill.session_num != ""0"":
                bill_session += "" Special Session %s"" % bill.session_num

            bill_id = bill.short_bill_id
            if bill_id.strip() == ""SB77"" and session == ""20052006"":
                continue

            fsbill = Bill(bill_id, bill_session, title="""", chamber=chamber)
            if (bill_id.startswith(""S"") and chamber == ""lower"") or (
                bill_id.startswith(""A"") and chamber == ""upper""
            ):
                print(""!!!! BAD ID/CHAMBER PAIR !!!!"", bill)
                continue

            # Construct a fake source url
            source_url = (
                ""http://leginfo.legislature.ca.gov/faces/""
                ""billNavClient.xhtml?bill_id=%s""
            ) % bill.bill_id

            fsbill.add_source(source_url)
            fsbill.add_version_link(bill_id, source_url, media_type=""text/html"")

            title = """"
            type_ = [""bill""]
            subject = """"
            all_titles = set()
            summary = """"

            # Get digest test (aka ""summary"") from latest version.
            if bill.versions and not_archive_year:
                version = bill.versions[-1]
                nsmap = version.xml.nsmap
                xpath = ""//caml:DigestText/xhtml:p""
                els = version.xml.xpath(xpath, namespaces=nsmap)
                chunks = []
                for el in els:
                    t = etree_text_content(el)
                    t = re.sub(r""\s+"", "" "", t)
                    t = re.sub(r""\)(\S)"", lambda m: "") %s"" % m.group(1), t)
                    chunks.append(t)
                summary = ""\n\n"".join(chunks)

            for version in bill.versions:
                if not version.bill_xml:
                    continue

                version_date = self._tz.localize(version.bill_version_action_date)

                # create a version name to match the state's format
                # 02/06/17 - Enrolled
                version_date_human = version_date.strftime(""%m/%d/%y"")
                version_name = ""{} - {}"".format(
                    version_date_human, version.bill_version_action
                )

                version_base = ""https://leginfo.legislature.ca.gov/faces""

                version_url_pdf = ""{}/billPdf.xhtml?bill_id={}&version={}"".format(
                    version_base, version.bill_id, version.bill_version_id
                )

                fsbill.add_version_link(
                    version_name,
                    version_url_pdf,
                    media_type=""application/pdf"",
                    date=version_date.date(),
                )

                # CA is inconsistent in that some bills have a short title
                # that is longer, more descriptive than title.
                if bill.measure_type in (""AB"", ""SB""):
                    impact_clause = clean_title(version.title)
                    title = clean_title(version.short_title)
                else:
                    impact_clause = None
                    if len(version.title) < len(
                        version.short_title
                    ) and not version.title.lower().startswith(""an act""):
                        title = clean_title(version.short_title)
                    else:
                        title = clean_title(version.title)

                if title:
                    all_titles.add(title)

                type_ = [bill_type]

                if version.appropriation == ""Yes"":
                    type_.append(""appropriation"")

                tags = []
                if version.fiscal_committee == ""Yes"":
                    tags.append(""fiscal committee"")
                if version.local_program == ""Yes"":
                    tags.append(""local program"")
                if version.urgency == ""Yes"":
                    tags.append(""urgency"")
                if version.taxlevy == ""Yes"":
                    tags.append(""tax levy"")

                if version.subject:
                    subject = clean_title(version.subject)

            if not title:
                self.warning(""Couldn't find title for %s, skipping"" % bill_id)
                continue

            fsbill.title = title
            if summary:
                fsbill.add_abstract(summary, note=""summary"")
            fsbill.classification = type_
            fsbill.subject = [subject] if subject else []
            fsbill.extras[""impact_clause""] = impact_clause
            fsbill.extras[""tags""] = tags

            # We don't want the current title in alternate_titles
            all_titles.remove(title)

            for title in all_titles:
                fsbill.add_title(title)

            for author in version.authors:
                fsbill.add_sponsorship(
                    author.name,
                    classification=SPONSOR_TYPES[author.contribution],
                    primary=author.primary_author_flg == ""Y"",
                    entity_type=""person"",
                )
                # fsbill.sponsorships[-1]['extras'] = {'official_type': author.contribution}

            seen_actions = set()
            for action in bill.actions:
                if not action.action:
                    # NULL action text seems to be an error on CA's part,
                    # unless it has some meaning I'm missing
                    continue
                actor = action.actor or chamber
                actor = actor.strip()
                match = re.match(r""(Assembly|Senate)($| \(Floor)"", actor)
                if match:
                    actor = {""Assembly"": ""lower"", ""Senate"": ""upper""}[match.group(1)]
                elif actor.startswith(""Governor""):
                    actor = ""executive""
                else:

                    def replacer(matchobj):
                        if matchobj:
                            return {""Assembly"": ""lower"", ""Senate"": ""upper""}[
                                matchobj.group()
                            ]
                        else:
                            return matchobj.group()

                    actor = re.sub(r""^(Assembly|Senate)"", replacer, actor)

                type_ = []

                act_str = action.action
                act_str = re.sub(r""\s+"", "" "", act_str)

                attrs = self.categorizer.categorize(act_str)

                # Add in the committee strings of the related committees, if any.
                kwargs = attrs
                matched_abbrs = committee_abbr_regex.findall(action.action)

                if re.search(r""Com[s]?. on"", action.action) and not matched_abbrs:
                    msg = ""Failed to extract committee abbr from %r.""
                    self.logger.warning(msg % action.action)

                if matched_abbrs:
                    committees = []
                    for abbr in matched_abbrs:
                        try:
                            name = self.committee_abbr_to_name(chamber, abbr)
                            committees.append(name)
                        except KeyError:
                            msg = (
                                ""Mapping contains no committee name for ""
                                ""abbreviation %r. Action text was %r.""
                            )
                            args = (abbr, action.action)
                            self.warning(msg % args)

                    committees = filter(None, committees)
                    kwargs[""committees""] = committees

                    code = re.search(r""C[SXZ]\d+"", actor)
                    if code is not None:
                        code = code.group()
                        kwargs[""actor_info""] = {""committee_code"": code}
                    if not_archive_year:
                        assert len(list(committees)) == len(matched_abbrs)
                    for committee, abbr in zip(committees, matched_abbrs):
                        act_str = act_str.replace(""Coms. on "", """")
                        act_str = act_str.replace(""Com. on "" + abbr, committee)
                        act_str = act_str.replace(abbr, committee)
                        if not act_str.endswith("".""):
                            act_str = act_str + "".""

                # Determine which chamber the action originated from.
                changed = False
                for committee_chamber in [""upper"", ""lower"", ""legislature""]:
                    if actor.startswith(committee_chamber):
                        actor = committee_chamber
                        changed = True
                        break
                if not changed:
                    actor = ""legislature""

                if actor != action.actor:
                    actor_info = kwargs.get(""actor_info"", {})
                    actor_info[""details""] = action.actor
                    kwargs[""actor_info""] = actor_info

                # Add strings for related legislators, if any.
                rgx = r""(?:senator|assembly[mwp][^ .,:;]+)\s+[^ .,:;]+""
                legislators = re.findall(rgx, action.action, re.I)
                if legislators:
                    kwargs[""legislators""] = legislators

                date = action.action_date
                date = self._tz.localize(date)
                date = date.date()
                if (actor, act_str, date) in seen_actions:
                    continue

                kwargs.update(self.categorizer.categorize(act_str))

                action = fsbill.add_action(
                    act_str,
                    date.strftime(""%Y-%m-%d""),
                    chamber=actor,
                    classification=kwargs[""classification""],
                )
                for committee in kwargs.get(""committees"", []):
                    action.add_related_entity(committee, entity_type=""organization"")
                seen_actions.add((actor, act_str, date))

            source_url = (
                ""http://leginfo.legislature.ca.gov/faces/billVotesClient.xhtml?""
            )
            source_url += f""bill_id={session}{bill.session_num}{fsbill.identifier}""

            # Votes for non archived years
            if archive_year > 2009:
                for vote_num, vote in enumerate(bill.votes):
                    if vote.vote_result == ""(PASS)"":
                        result = True
                    else:
                        result = False

                    if not vote.location:
                        continue

                    full_loc = vote.location.description
                    first_part = full_loc.split("" "")[0].lower()
                    if first_part in [""asm"", ""assembly""]:
                        vote_chamber = ""lower""
                        # vote_location = ' '.join(full_loc.split(' ')[1:])
                    elif first_part.startswith(""sen""):
                        vote_chamber = ""upper""
                        # vote_location = ' '.join(full_loc.split(' ')[1:])
                    else:
                        # raise ScrapeError(""Bad location: %s"" % full_loc) # To uncomment
                        continue

                    if vote.motion:
                        motion = vote.motion.motion_text or """"
                    else:
                        motion = """"

                    if ""Third Reading"" in motion or ""3rd Reading"" in motion:
                        vtype = ""passage""
                    elif ""Do Pass"" in motion:
                        vtype = ""passage""
                    else:
                        vtype = []

                    motion = motion.strip()
                    motion = re.compile(
                        r""(\w+)( Extraordinary)? Session$"", re.IGNORECASE
                    ).sub("""", motion)
                    motion = re.compile(r""^(Senate|Assembly) "", re.IGNORECASE).sub(
                        """", motion
                    )
                    motion = re.sub(
                        r""^(SCR|SJR|SB|AB|AJR|ACR)\s?\d+ \w+\.?  "", """", motion
                    )
                    motion = re.sub(r"" \(\w+\)$"", """", motion)
                    motion = re.sub(r""(SCR|SB|AB|AJR|ACR)\s?\d+ \w+\.?$"", """", motion)
                    motion = re.sub(
                        r""(SCR|SJR|SB|AB|AJR|ACR)\s?\d+ \w+\.? "" r""Urgency Clause$"",
                        ""(Urgency Clause)"",
                        motion,
                    )
                    motion = re.sub(r""\s+"", "" "", motion)

                    if not motion:
                        self.warning(""Got blank motion on vote for %s"" % bill_id)
                        continue

                    # XXX this is responsible for all the CA 'committee' votes, not
                    # sure if that's a feature or bug, so I'm leaving it as is...
                    # vote_classification = chamber if (vote_location == 'Floor') else 'committee'
                    # org = {
                    # 'name': vote_location,
                    # 'classification': vote_classification
                    # }

                    fsvote = VoteEvent(
                        motion_text=motion,
                        start_date=self._tz.localize(vote.vote_date_time),
                        result=""pass"" if result else ""fail"",
                        classification=vtype,
                        # organization=org,
                        chamber=vote_chamber,
                        bill=fsbill,
                    )
                    fsvote.extras = {""threshold"": vote.threshold}

                    fsvote.add_source(source_url)
                    fsvote.dedupe_key = source_url + ""#"" + str(vote_num)

                    rc = {""yes"": [], ""no"": [], ""other"": []}
                    for record in vote.votes:
                        if record.vote_code == ""AYE"":
                            rc[""yes""].append(record.legislator_name)
                        elif record.vote_code.startswith(""NO""):
                            rc[""no""].append(record.legislator_name)
                        else:
                            rc[""other""].append(record.legislator_name)

                    # Handle duplicate votes
                    for key in rc:
                        rc[key] = list(set(rc[key]))

                    for key, voters in rc.items():
                        for voter in voters:
                            fsvote.vote(key, voter)
                        # Set counts by summed votes for accuracy
                        fsvote.set_count(key, len(voters))

                    yield fsvote
            if len(bill.votes) > 0 and archive_year <= 2009:
                vote_page_url = (
                    ""http://leginfo.legislature.ca.gov/faces/billVotesClient.xhtml?""
                )
                vote_page_url += (
                    f""bill_id={session}{bill.session_num}{fsbill.identifier}""
                )

                # parse the bill data page, finding the latest html text
                data = self.get(vote_page_url).content
                doc = html.fromstring(data)
                doc.make_links_absolute(vote_page_url)
                num_of_votes = len(doc.xpath(""//div[@class='status']""))
                for vote_section in range(1, num_of_votes + 1):
                    lines = doc.xpath(
                        f""//div[@class='status'][{vote_section}]//div[@class='statusRow']""
                    )
                    date, result, motion, vtype, location = """", """", """", """", """"
                    votes = {}
                    for line in lines:
                        line = line.text_content().split()
                        if line[0] == ""Date"":
                            date = line[1]
                            date = datetime.datetime.strptime(date, ""%m/%d/%y"")
                            date = self._tz.localize(date)
                        elif line[0] == ""Result"":
                            result = ""pass"" if ""PASS"" in line[1] else ""fail""
                        elif line[0] == ""Motion"":
                            motion = "" "".join(line[1:])
                        elif line[0] == ""Location"":
                            location = "" "".join(line[1:])
                        elif len(line) > 1:
                            if line[0] == ""Ayes"" and line[1] != ""Count"":
                                votes[""yes""] = line[1:]
                            elif line[0] == ""Noes"" and line[1] != ""Count"":
                                votes[""no""] = line[1:]
                            elif line[0] == ""NVR"" and line[1] != ""Count"":
                                votes[""not voting""] = line[1:]
                    # Determine chamber based on location
                    first_part = location.split("" "")[0].lower()
                    vote_chamber = """"
                    if first_part in [""asm"", ""assembly""]:
                        vote_chamber = ""lower""
                    elif first_part.startswith(""sen""):
                        vote_chamber = ""upper""

                    if ""Third Reading"" in motion or ""3rd Reading"" in motion:
                        vtype = ""passage""
                    elif ""Do Pass"" in motion:
                        vtype = ""passage""
                    else:
                        vtype = ""other""
                    if len(motion) > 0:
                        fsvote = VoteEvent(
                            motion_text=motion,
                            start_date=date,
                            result=result,
                            classification=vtype,
                            chamber=vote_chamber,
                            bill=fsbill,
                        )
                        fsvote.add_source(vote_page_url)
                        fsvote.dedupe_key = vote_page_url + ""#"" + str(vote_section)

                        for how_voted, voters in votes.items():
                            for voter in voters:
                                voter = voter.replace("","", """")
                                fsvote.vote(how_voted, voter)
                        yield fsvote

            for analysis in bill.analyses:
                analysis_date = self._tz.localize(analysis.analysis_date)
                # create an analysis name to match the state's format
                # 05/31/20- Assembly Appropriations
                analysis_date_human = analysis_date.strftime(""%m/%d/%y"")
                analysis_name = ""{}- {}"".format(
                    analysis_date_human, analysis.committee_name
                )

                analysis_base = ""https://leginfo.legislature.ca.gov/faces""

                # unfortunately this just brings you to the analysis list
                # storing analysisId and analyzingOffice for a future POST request?
                analysis_url_pdf = ""{}/billAnalysisClient.xhtml?bill_id={}&analysisId={}&analyzingOffice={}"".format(
                    analysis_base,
                    analysis.bill_id,
                    analysis.analysis_id,
                    analysis.committee_name.replace("" "", ""+""),
                )

                fsbill.add_document_link(
                    analysis_name,
                    analysis_url_pdf,
                    classification=""analysis"",
                    date=analysis_date.date(),
                    media_type=""application/pdf"",
                    on_duplicate=""ignore"",
                )

            yield fsbill
            self.session.expire_all()",_10831.py,452,"' ', '+'",*' +'
https://github.com/google/jax/tree/master/tests/lax_test.py,"def _transpose_conv_kernel(data, kernel, dimension_numbers):
    dn = lax.conv_dimension_numbers(data.shape, kernel.shape,
                                    dimension_numbers)
    spatial_axes = np.array(dn.rhs_spec)[2:]
    for axis in spatial_axes:
      kernel = np.flip(kernel, axis)
    kernel = np.swapaxes(kernel, dn.rhs_spec[0], dn.rhs_spec[1])
    return kernel",_11041.py,7,"dn.rhs_spec[0], dn.rhs_spec[1]",*dn.rhs_spec[:2]
https://github.com/rafaelpadilla/Object-Detection-Metrics/tree/master/lib/Evaluator.py,"def CalculateAveragePrecision(rec, prec):
        mrec = []
        mrec.append(0)
        [mrec.append(e) for e in rec]
        mrec.append(1)
        mpre = []
        mpre.append(0)
        [mpre.append(e) for e in prec]
        mpre.append(0)
        for i in range(len(mpre) - 1, 0, -1):
            mpre[i - 1] = max(mpre[i - 1], mpre[i])
        ii = []
        for i in range(len(mrec) - 1):
            if mrec[1+i] != mrec[i]:
                ii.append(i + 1)
        ap = 0
        for i in ii:
            ap = ap + np.sum((mrec[i] - mrec[i - 1]) * mpre[i])
        # return [ap, mpre[1:len(mpre)-1], mrec[1:len(mpre)-1], ii]
        return [ap, mpre[0:len(mpre) - 1], mrec[0:len(mpre) - 1], ii]",_11073.py,11,"mpre[i - 1], mpre[i]",*mpre[i - 1:i + 1]
https://github.com/apache/airflow/tree/master/dev/provider_packages/prepare_provider_packages.py,"def get_all_changes_for_package(
    versions: List[str],
    provider_package_id: str,
    source_provider_package_path: str,
    verbose: bool,
) -> Tuple[bool, Optional[List[List[Change]]], str]:
    """"""
    Retrieves all changes for the package.
    :param versions: list of versions
    :param provider_package_id: provider package id
    :param source_provider_package_path: path where package is located
    :param verbose: whether to print verbose messages

    """"""
    current_version = versions[0]
    current_tag_no_suffix = get_version_tag(current_version, provider_package_id)
    if verbose:
        console.print(f""Checking if tag '{current_tag_no_suffix}' exist."")
    if not subprocess.call(
        get_git_tag_check_command(current_tag_no_suffix),
        cwd=source_provider_package_path,
        stderr=subprocess.DEVNULL,
    ):
        if verbose:
            console.print(f""The tag {current_tag_no_suffix} exists."")
        # The tag already exists
        changes = subprocess.check_output(
            get_git_log_command(verbose, HEAD_OF_HTTPS_REMOTE, current_tag_no_suffix),
            cwd=source_provider_package_path,
            universal_newlines=True,
        )
        if changes:
            provider_details = get_provider_details(provider_package_id)
            doc_only_change_file = os.path.join(
                provider_details.source_provider_package_path, "".latest-doc-only-change.txt""
            )
            if os.path.exists(doc_only_change_file):
                with open(doc_only_change_file) as f:
                    last_doc_only_hash = f.read().strip()
                try:
                    changes_since_last_doc_only_check = subprocess.check_output(
                        get_git_log_command(verbose, HEAD_OF_HTTPS_REMOTE, last_doc_only_hash),
                        cwd=source_provider_package_path,
                        universal_newlines=True,
                    )
                    if not changes_since_last_doc_only_check:
                        console.print()
                        console.print(
                            ""[yellow]The provider has doc-only changes since the last release. Skipping[/]""
                        )
                        # Returns 66 in case of doc-only changes
                        sys.exit(66)
                except subprocess.CalledProcessError:
                    # ignore when the commit mentioned as last doc-only change is obsolete
                    pass
            console.print(f""[yellow]The provider {provider_package_id} has changes since last release[/]"")
            console.print()
            console.print(
                ""[yellow]Please update version in ""
                f""'airflow/providers/{provider_package_id.replace('-','/')}/'""
                ""provider.yaml'[/]\n""
            )
            console.print(""[yellow]Or mark the changes as doc-only[/]"")
            changes_table, array_of_changes = convert_git_changes_to_table(
                ""UNKNOWN"",
                changes,
                base_url=""https://github.com/apache/airflow/commit/"",
                markdown=False,
            )
            print_changes_table(changes_table)
            return False, array_of_changes[0], changes_table
        else:
            console.print(f""No changes for {provider_package_id}"")
            return False, None, """"
    if verbose:
        console.print(""The tag does not exist. "")
    if len(versions) == 1:
        console.print(
            f""The provider '{provider_package_id}' has never been released but it is ready to release!\n""
        )
    else:
        console.print(f""New version of the '{provider_package_id}' package is ready to be released!\n"")
    next_version_tag = HEAD_OF_HTTPS_REMOTE
    changes_table = ''
    current_version = versions[0]
    list_of_list_of_changes: List[List[Change]] = []
    for version in versions[1:]:
        version_tag = get_version_tag(version, provider_package_id)
        changes = subprocess.check_output(
            get_git_log_command(verbose, next_version_tag, version_tag),
            cwd=source_provider_package_path,
            universal_newlines=True,
        )
        changes_table_for_version, array_of_changes_for_version = convert_git_changes_to_table(
            current_version, changes, base_url=""https://github.com/apache/airflow/commit/"", markdown=False
        )
        changes_table += changes_table_for_version
        list_of_list_of_changes.append(array_of_changes_for_version)
        next_version_tag = version_tag
        current_version = version
    changes = subprocess.check_output(
        get_git_log_command(verbose, next_version_tag),
        cwd=source_provider_package_path,
        universal_newlines=True,
    )
    changes_table_for_version, array_of_changes_for_version = convert_git_changes_to_table(
        current_version, changes, base_url=""https://github.com/apache/airflow/commit/"", markdown=False
    )
    changes_table += changes_table_for_version
    if verbose:
        print_changes_table(changes_table)
    return True, list_of_list_of_changes if len(list_of_list_of_changes) > 0 else None, changes_table",_11208.py,60,"'-', '/'",*'-/'
https://github.com/librosa/librosa/tree/master/librosa/core/pitch.py,"def pyin(
    y,
    *,
    fmin,
    fmax,
    sr=22050,
    frame_length=2048,
    win_length=None,
    hop_length=None,
    n_thresholds=100,
    beta_parameters=(2, 18),
    boltzmann_parameter=2,
    resolution=0.1,
    max_transition_rate=35.92,
    switch_prob=0.01,
    no_trough_prob=0.01,
    fill_na=np.nan,
    center=True,
    pad_mode=""constant"",
):
    """"""Fundamental frequency (F0) estimation using probabilistic YIN (pYIN).

    pYIN [#]_ is a modificatin of the YIN algorithm [#]_ for fundamental frequency (F0) estimation.
    In the first step of pYIN, F0 candidates and their probabilities are computed using the YIN algorithm.
    In the second step, Viterbi decoding is used to estimate the most likely F0 sequence and voicing flags.

    .. [#] Mauch, Matthias, and Simon Dixon.
        ""pYIN: A fundamental frequency estimator using probabilistic threshold distributions.""
        2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014.

    .. [#] De Cheveignรฉ, Alain, and Hideki Kawahara.
        ""YIN, a fundamental frequency estimator for speech and music.""
        The Journal of the Acoustical Society of America 111.4 (2002): 1917-1930.

    Parameters
    ----------
    y : np.ndarray [shape=(..., n)]
        audio time series. Multi-channel is supported.
    fmin : number > 0 [scalar]
        minimum frequency in Hertz.
        The recommended minimum is ``librosa.note_to_hz('C2')`` (~65 Hz)
        though lower values may be feasible.
    fmax : number > 0 [scalar]
        maximum frequency in Hertz.
        The recommended maximum is ``librosa.note_to_hz('C7')`` (~2093 Hz)
        though higher values may be feasible.
    sr : number > 0 [scalar]
        sampling rate of ``y`` in Hertz.
    frame_length : int > 0 [scalar]
        length of the frames in samples.
        By default, ``frame_length=2048`` corresponds to a time scale of about 93 ms at
        a sampling rate of 22050 Hz.
    win_length : None or int > 0 [scalar]
        length of the window for calculating autocorrelation in samples.
        If ``None``, defaults to ``frame_length // 2``
    hop_length : None or int > 0 [scalar]
        number of audio samples between adjacent pYIN predictions.
        If ``None``, defaults to ``frame_length // 4``.
    n_thresholds : int > 0 [scalar]
        number of thresholds for peak estimation.
    beta_parameters : tuple
        shape parameters for the beta distribution prior over thresholds.
    boltzmann_parameter : number > 0 [scalar]
        shape parameter for the Boltzmann distribution prior over troughs.
        Larger values will assign more mass to smaller periods.
    resolution : float in `(0, 1)`
        Resolution of the pitch bins.
        0.01 corresponds to cents.
    max_transition_rate : float > 0
        maximum pitch transition rate in octaves per second.
    switch_prob : float in ``(0, 1)``
        probability of switching from voiced to unvoiced or vice versa.
    no_trough_prob : float in ``(0, 1)``
        maximum probability to add to global minimum if no trough is below threshold.
    fill_na : None, float, or ``np.nan``
        default value for unvoiced frames of ``f0``.
        If ``None``, the unvoiced frames will contain a best guess value.
    center : boolean
        If ``True``, the signal ``y`` is padded so that frame
        ``D[:, t]`` is centered at ``y[t * hop_length]``.
        If ``False``, then ``D[:, t]`` begins at ``y[t * hop_length]``.
        Defaults to ``True``,  which simplifies the alignment of ``D`` onto a
        time grid by means of ``librosa.core.frames_to_samples``.
    pad_mode : string or function
        If ``center=True``, this argument is passed to ``np.pad`` for padding
        the edges of the signal ``y``. By default (``pad_mode=""constant""``),
        ``y`` is padded on both sides with zeros.
        If ``center=False``,  this argument is ignored.
        .. see also:: `np.pad`

    Returns
    -------
    f0: np.ndarray [shape=(..., n_frames)]
        time series of fundamental frequencies in Hertz.
    voiced_flag: np.ndarray [shape=(..., n_frames)]
        time series containing boolean flags indicating whether a frame is voiced or not.
    voiced_prob: np.ndarray [shape=(..., n_frames)]
        time series containing the probability that a frame is voiced.
    .. note:: If multi-channel input is provided, f0 and voicing are estimated separately for each channel.

    See Also
    --------
    librosa.yin :
        Fundamental frequency (F0) estimation using the YIN algorithm.

    Examples
    --------
    Computing a fundamental frequency (F0) curve from an audio input

    >>> y, sr = librosa.load(librosa.ex('trumpet'))
    >>> f0, voiced_flag, voiced_probs = librosa.pyin(y,
    ...                                              fmin=librosa.note_to_hz('C2'),
    ...                                              fmax=librosa.note_to_hz('C7'))
    >>> times = librosa.times_like(f0)

    Overlay F0 over a spectrogram

    >>> import matplotlib.pyplot as plt
    >>> D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)
    >>> fig, ax = plt.subplots()
    >>> img = librosa.display.specshow(D, x_axis='time', y_axis='log', ax=ax)
    >>> ax.set(title='pYIN fundamental frequency estimation')
    >>> fig.colorbar(img, ax=ax, format=""%+2.f dB"")
    >>> ax.plot(times, f0, label='f0', color='cyan', linewidth=3)
    >>> ax.legend(loc='upper right')
    """"""

    if fmin is None or fmax is None:
        raise ParameterError('both ""fmin"" and ""fmax"" must be provided')

    # Set the default window length if it is not already specified.
    if win_length is None:
        win_length = frame_length // 2

    if win_length >= frame_length:
        raise ParameterError(
            ""win_length={} cannot exceed given frame_length={}"".format(
                win_length, frame_length
            )
        )

    # Set the default hop if it is not already specified.
    if hop_length is None:
        hop_length = frame_length // 4

    # Check that audio is valid.
    util.valid_audio(y, mono=False)

    # Pad the time series so that frames are centered
    if center:
        padding = [(0, 0) for _ in y.shape]
        padding[-1] = (frame_length // 2, frame_length // 2)
        y = np.pad(y, padding, mode=pad_mode)

    # Frame audio.
    y_frames = util.frame(y, frame_length=frame_length, hop_length=hop_length)

    # Calculate minimum and maximum periods
    min_period = max(int(np.floor(sr / fmax)), 1)
    max_period = min(int(np.ceil(sr / fmin)), frame_length - win_length - 1)

    # Calculate cumulative mean normalized difference function.
    yin_frames = _cumulative_mean_normalized_difference(
        y_frames, frame_length, win_length, min_period, max_period
    )

    # Parabolic interpolation.
    parabolic_shifts = _parabolic_interpolation(yin_frames)

    # Find Yin candidates and probabilities.
    # The implementation here follows the official pYIN software which
    # differs from the method described in the paper.
    # 1. Define the prior over the thresholds.
    thresholds = np.linspace(0, 1, n_thresholds + 1)
    beta_cdf = scipy.stats.beta.cdf(thresholds, beta_parameters[0], beta_parameters[1])
    beta_probs = np.diff(beta_cdf)

    n_bins_per_semitone = int(np.ceil(1.0 / resolution))
    n_pitch_bins = int(np.floor(12 * n_bins_per_semitone * np.log2(fmax / fmin))) + 1

    def _helper(a, b):
        return __pyin_helper(
            a,
            b,
            sr,
            thresholds,
            boltzmann_parameter,
            beta_probs,
            no_trough_prob,
            min_period,
            fmin,
            n_pitch_bins,
            n_bins_per_semitone,
        )

    helper = np.vectorize(_helper, signature=""(f,t),(k,t)->(1,d,t),(j,t)"")
    observation_probs, voiced_prob = helper(yin_frames, parabolic_shifts)

    # Construct transition matrix.
    max_semitones_per_frame = round(max_transition_rate * 12 * hop_length / sr)
    transition_width = max_semitones_per_frame * n_bins_per_semitone + 1
    # Construct the within voicing transition probabilities
    transition = sequence.transition_local(
        n_pitch_bins, transition_width, window=""triangle"", wrap=False
    )

    # Include across voicing transition probabilities
    t_switch = sequence.transition_loop(2, 1 - switch_prob)
    transition = np.kron(t_switch, transition)

    p_init = np.zeros(2 * n_pitch_bins)
    p_init[n_pitch_bins:] = 1 / n_pitch_bins

    states = sequence.viterbi(observation_probs, transition, p_init=p_init)

    # Find f0 corresponding to each decoded pitch bin.
    freqs = fmin * 2 ** (np.arange(n_pitch_bins) / (12 * n_bins_per_semitone))
    f0 = freqs[states % n_pitch_bins]
    voiced_flag = states < n_pitch_bins

    if fill_na is not None:
        f0[~voiced_flag] = fill_na

    return f0[..., 0, :], voiced_flag[..., 0, :], voiced_prob[..., 0, :]",_11334.py,175,"beta_parameters[0], beta_parameters[1]",*beta_parameters[:2]
https://github.com/apache/tvm/tree/master/python/tvm/relay/op/nn/nn.py,"def sparse_add(dense_mat, sparse_mat):
    r""""""
    Computes the matrix addition of `dense_mat` and `sparse_mat`, where `dense_mat` is
    a dense matrix and `sparse_mat` is a sparse (CSR) namedtuple with
    fields `data`, `indices`, and `indptr`.

    .. math::

        \mbox{sparse_add}(dense_mat, sparse_mat)[m, n] = \mbox{add}(\mbox{as_dense}(S), (D))[m, n]

    where `as_dense` returns dense equivalent of the given S(sparse matrix)
    while performing addition with given D(dense matrix).

    Parameters
    ----------
    dense_mat : tvm.relay.Expr
        The input dense matrix for the matrix addition

    sparse_mat : Union[namedtuple, Tuple[ndarray, ndarray, ndarray]].
        The input sparse matrix(CSR) for the matrix addition.

    Returns
    -------
    result: tvm.relay.Expr
        The computed result.

    Examples
    -------
    .. code-block:: python

        dense_data = [[ 3.,   4.,   4. ]
                      [ 4.,  2.,  5. ]]
        sparse_data = [4., 8.]
        sparse_indices =[0, 2]
        sparse_indptr =[0, 1, 2]

        output = relay.sparse_add(dense_data, sparse_data, sparse_indices, sparse_indptr)

        output = [[ 7.,   4.,   4. ]
                  [ 4.,  2.,  13. ]]
    """"""
    if hasattr(sparse_mat, ""indices""):
        return _make.sparse_add(dense_mat, sparse_mat.data, sparse_mat.indices, sparse_mat.indptr)
    else:
        return _make.sparse_add(dense_mat, sparse_mat[0], sparse_mat[1], sparse_mat[2])",_11531.py,45,"sparse_mat[0], sparse_mat[1], sparse_mat[2]",*sparse_mat[:3]
https://github.com/bpython/bpython/tree/master/bpython/test/test_preprocess.py,"def assertShowWhitespaceEqual(self, a, b):
        self.assertEqual(
            a,
            b,
            """".join(
                difflib.context_diff(
                    a.replace("" "", ""~"").splitlines(True),
                    b.replace("" "", ""~"").splitlines(True),
                    fromfile=""actual"",
                    tofile=""expected"",
                    n=5,
                )
            ),
        )",_11719.py,7,"' ', '~'",*' ~'
https://github.com/bpython/bpython/tree/master/bpython/test/test_preprocess.py,"def assertShowWhitespaceEqual(self, a, b):
        self.assertEqual(
            a,
            b,
            """".join(
                difflib.context_diff(
                    a.replace("" "", ""~"").splitlines(True),
                    b.replace("" "", ""~"").splitlines(True),
                    fromfile=""actual"",
                    tofile=""expected"",
                    n=5,
                )
            ),
        )",_11719.py,8,"' ', '~'",*' ~'
https://github.com/pytorch/fairseq/tree/master/tests/distributed/test_bmuf.py,"def test_bmuf_sync(self):
        # Train model for 1 iteration and do bmuf sync without doing warmup
        cfg, args = setup_args()
        iterations = 1
        results = self.bmuf_process(cfg, args, iterations)
        # Make sure params in both machines are same
        assert len(results) == 2
        self.assertAlmostEqual(results[0], results[1])",_11738.py,8,"results[0], results[1]",*results[:2]
https://github.com/yaqwsx/KiKit/tree/master/kikit/common.py,"def shpBBoxRight(bbox):
    """"""
    Given a shapely bounding box, return right edge as (pos, interval)
    """"""
    return AxialLine(bbox[2], bbox[1], bbox[3])",_11867.py,5,"bbox[1], bbox[3]",*bbox[1:5:2]
https://github.com/geopandas/geopandas/tree/master/geopandas/tools/_show_versions.py,"def _get_sys_info():
    """"""System information

    Returns
    -------
    sys_info : dict
        system and Python version information
    """"""
    python = sys.version.replace(""\n"", "" "")

    blob = [
        (""python"", python),
        (""executable"", sys.executable),
        (""machine"", platform.platform()),
    ]

    return dict(blob)",_12017.py,9,"'\n', ' '",*'\n '
https://github.com/datitran/object_detector_app/tree/master/object_detection/core/box_list_ops_test.py,"def test_to_absolute_coordinates_already_abolute(self):
    coordinates = tf.constant([[0, 0, 100, 100],
                               [25, 25, 75, 75]], tf.float32)
    img = tf.ones((128, 100, 100, 3))
    boxlist = box_list.BoxList(coordinates)
    absolute_boxlist = box_list_ops.to_absolute_coordinates(boxlist,
                                                            tf.shape(img)[1],
                                                            tf.shape(img)[2])

    with self.test_session() as sess:
      with self.assertRaisesOpError('assertion failed'):
        sess.run(absolute_boxlist.get())",_12186.py,6,"tf.shape(img)[1], tf.shape(img)[2]",*tf.shape(img)[1:3]
https://github.com/openstack/nova/tree/master/nova/tests/unit/virt/libvirt/test_driver.py,"def test_raw_with_rbd_clone_eperm(self, mock_rbd, mock_driver,
                                      mock_convert_image):
        self.flags(images_type='rbd', group='libvirt')
        rbd = mock_driver.return_value
        rbd.parent_info = mock.Mock(return_value=['test-pool', '', ''])
        rbd.parse_url = mock.Mock(return_value=['a', 'b', 'c', 'd'])
        rbd.clone = mock.Mock(side_effect=exception.Forbidden(
                image_id='fake_id', reason='rbd testing'))
        self._test_snapshot(disk_format='raw')
        # Ensure that the direct_snapshot attempt was cleaned up
        rbd.remove_snap.assert_called_with('c', 'd', ignore_errors=False,
                                           pool='b', force=True)",_12407.py,11,"'c', 'd'",*'cd'
https://github.com/neuralchen/SimSwap/tree/master//test_video_swap_multispecific.py,"if __name__ == '__main__':
    opt = TestOptions().parse()
    pic_specific = opt.pic_specific_path
    start_epoch, epoch_iter = 1, 0
    crop_size = opt.crop_size

    multisepcific_dir = opt.multisepcific_dir
    torch.nn.Module.dump_patches = True
    if crop_size == 512:
        opt.which_epoch = 550000
        opt.name = '512'
        mode = 'ffhq'
    else:
        mode = 'None'
    model = create_model(opt)
    model.eval()


    app = Face_detect_crop(name='antelope', root='./insightface_func/models')
    app.prepare(ctx_id= 0, det_thresh=0.6, det_size=(640,640),mode=mode)

    # The specific person to be swapped(source)

    source_specific_id_nonorm_list = []
    source_path = os.path.join(multisepcific_dir,'SRC_*')
    source_specific_images_path = sorted(glob.glob(source_path))
    with torch.no_grad():
        for source_specific_image_path in source_specific_images_path:
            specific_person_whole = cv2.imread(source_specific_image_path)
            specific_person_align_crop, _ = app.get(specific_person_whole,crop_size)
            specific_person_align_crop_pil = Image.fromarray(cv2.cvtColor(specific_person_align_crop[0],cv2.COLOR_BGR2RGB)) 
            specific_person = transformer_Arcface(specific_person_align_crop_pil)
            specific_person = specific_person.view(-1, specific_person.shape[0], specific_person.shape[1], specific_person.shape[2])
            # convert numpy to tensor
            specific_person = specific_person.cuda()
            #create latent id
            specific_person_downsample = F.interpolate(specific_person, size=(112,112))
            specific_person_id_nonorm = model.netArc(specific_person_downsample)
            source_specific_id_nonorm_list.append(specific_person_id_nonorm.clone())


        # The person who provides id information (list)
        target_id_norm_list = []
        target_path = os.path.join(multisepcific_dir,'DST_*')
        target_images_path = sorted(glob.glob(target_path))

        for target_image_path in target_images_path:
            img_a_whole = cv2.imread(target_image_path)
            img_a_align_crop, _ = app.get(img_a_whole,crop_size)
            img_a_align_crop_pil = Image.fromarray(cv2.cvtColor(img_a_align_crop[0],cv2.COLOR_BGR2RGB)) 
            img_a = transformer_Arcface(img_a_align_crop_pil)
            img_id = img_a.view(-1, img_a.shape[0], img_a.shape[1], img_a.shape[2])
            # convert numpy to tensor
            img_id = img_id.cuda()
            #create latent id
            img_id_downsample = F.interpolate(img_id, size=(112,112))
            latend_id = model.netArc(img_id_downsample)
            latend_id = F.normalize(latend_id, p=2, dim=1)
            target_id_norm_list.append(latend_id.clone())

        assert len(target_id_norm_list) == len(source_specific_id_nonorm_list), ""The number of images in source and target directory must be same !!!""



        video_swap(opt.video_path, target_id_norm_list,source_specific_id_nonorm_list, opt.id_thres, \
            model, app, opt.output_path,temp_results_dir=opt.temp_path,no_simswaplogo=opt.no_simswaplogo,use_mask=opt.use_mask,crop_size=crop_size)",_12476.py,33,"specific_person.shape[0], specific_person.shape[1], specific_person.shape[2]",*specific_person.shape[:3]
https://github.com/neuralchen/SimSwap/tree/master//test_video_swap_multispecific.py,"if __name__ == '__main__':
    opt = TestOptions().parse()
    pic_specific = opt.pic_specific_path
    start_epoch, epoch_iter = 1, 0
    crop_size = opt.crop_size

    multisepcific_dir = opt.multisepcific_dir
    torch.nn.Module.dump_patches = True
    if crop_size == 512:
        opt.which_epoch = 550000
        opt.name = '512'
        mode = 'ffhq'
    else:
        mode = 'None'
    model = create_model(opt)
    model.eval()


    app = Face_detect_crop(name='antelope', root='./insightface_func/models')
    app.prepare(ctx_id= 0, det_thresh=0.6, det_size=(640,640),mode=mode)

    # The specific person to be swapped(source)

    source_specific_id_nonorm_list = []
    source_path = os.path.join(multisepcific_dir,'SRC_*')
    source_specific_images_path = sorted(glob.glob(source_path))
    with torch.no_grad():
        for source_specific_image_path in source_specific_images_path:
            specific_person_whole = cv2.imread(source_specific_image_path)
            specific_person_align_crop, _ = app.get(specific_person_whole,crop_size)
            specific_person_align_crop_pil = Image.fromarray(cv2.cvtColor(specific_person_align_crop[0],cv2.COLOR_BGR2RGB)) 
            specific_person = transformer_Arcface(specific_person_align_crop_pil)
            specific_person = specific_person.view(-1, specific_person.shape[0], specific_person.shape[1], specific_person.shape[2])
            # convert numpy to tensor
            specific_person = specific_person.cuda()
            #create latent id
            specific_person_downsample = F.interpolate(specific_person, size=(112,112))
            specific_person_id_nonorm = model.netArc(specific_person_downsample)
            source_specific_id_nonorm_list.append(specific_person_id_nonorm.clone())


        # The person who provides id information (list)
        target_id_norm_list = []
        target_path = os.path.join(multisepcific_dir,'DST_*')
        target_images_path = sorted(glob.glob(target_path))

        for target_image_path in target_images_path:
            img_a_whole = cv2.imread(target_image_path)
            img_a_align_crop, _ = app.get(img_a_whole,crop_size)
            img_a_align_crop_pil = Image.fromarray(cv2.cvtColor(img_a_align_crop[0],cv2.COLOR_BGR2RGB)) 
            img_a = transformer_Arcface(img_a_align_crop_pil)
            img_id = img_a.view(-1, img_a.shape[0], img_a.shape[1], img_a.shape[2])
            # convert numpy to tensor
            img_id = img_id.cuda()
            #create latent id
            img_id_downsample = F.interpolate(img_id, size=(112,112))
            latend_id = model.netArc(img_id_downsample)
            latend_id = F.normalize(latend_id, p=2, dim=1)
            target_id_norm_list.append(latend_id.clone())

        assert len(target_id_norm_list) == len(source_specific_id_nonorm_list), ""The number of images in source and target directory must be same !!!""



        video_swap(opt.video_path, target_id_norm_list,source_specific_id_nonorm_list, opt.id_thres, \
            model, app, opt.output_path,temp_results_dir=opt.temp_path,no_simswaplogo=opt.no_simswaplogo,use_mask=opt.use_mask,crop_size=crop_size)",_12476.py,52,"img_a.shape[0], img_a.shape[1], img_a.shape[2]",*img_a.shape[:3]
https://github.com/dropreg/R-Drop/tree/master/huggingface_transformer_src/src/transformers/models/tapas/tokenization_tapas.py,"def format_text(text):
    """"""Lowercases and strips punctuation.""""""
    text = text.lower().strip()
    if text == ""n/a"" or text == ""?"" or text == ""nan"":
        text = EMPTY_TEXT

    text = re.sub(r""[^\w\d]+"", "" "", text).replace(""_"", "" "")
    text = "" "".join(text.split())
    text = text.strip()
    if text:
        return text
    return EMPTY_TEXT",_12538.py,7,"'_', ' '",*'_ '
https://github.com/pythonarcade/arcade/tree/master/arcade/context.py,"def projection_2d(self, value: Tuple[float, float, float, float]):
        if not isinstance(value, tuple) or len(value) != 4:
            raise ValueError(
                f""projection must be a 4-component tuple, not {type(value)}: {value}""
            )

        self._projection_2d = value
        self._projection_2d_matrix = Mat4.orthogonal_projection(
            value[0], value[1], value[2], value[3], -100, 100,
        )
        self._projection_2d_buffer.write(self._projection_2d_matrix)",_12594.py,8,"value[0], value[1], value[2], value[3]",*value[:4]
https://github.com/pupil-labs/pupil/tree/master/pupil_src/shared_modules/math_helper/transformations.py,"def compose_matrix(
    scale=None, shear=None, angles=None, translate=None, perspective=None
):
    """"""Return transformation matrix from sequence of transformations.

    This is the inverse of the decompose_matrix function.

    Sequence of transformations:
        scale : vector of 3 scaling factors
        shear : list of shear factors for x-y, x-z, y-z axes
        angles : list of Euler angles about static x, y, z axes
        translate : translation vector along x, y, z axes
        perspective : perspective partition of matrix

    >>> scale = numpy.random.random(3) - 0.5
    >>> shear = numpy.random.random(3) - 0.5
    >>> angles = (numpy.random.random(3) - 0.5) * (2*math.pi)
    >>> trans = numpy.random.random(3) - 0.5
    >>> persp = numpy.random.random(4) - 0.5
    >>> M0 = compose_matrix(scale, shear, angles, trans, persp)
    >>> result = decompose_matrix(M0)
    >>> M1 = compose_matrix(*result)
    >>> is_same_transform(M0, M1)
    True

    """"""
    M = numpy.identity(4)
    if perspective is not None:
        P = numpy.identity(4)
        P[3, :] = perspective[:4]
        M = numpy.dot(M, P)
    if translate is not None:
        T = numpy.identity(4)
        T[:3, 3] = translate[:3]
        M = numpy.dot(M, T)
    if angles is not None:
        R = euler_matrix(angles[0], angles[1], angles[2], ""sxyz"")
        M = numpy.dot(M, R)
    if shear is not None:
        Z = numpy.identity(4)
        Z[1, 2] = shear[2]
        Z[0, 2] = shear[1]
        Z[0, 1] = shear[0]
        M = numpy.dot(M, Z)
    if scale is not None:
        S = numpy.identity(4)
        S[0, 0] = scale[0]
        S[1, 1] = scale[1]
        S[2, 2] = scale[2]
        M = numpy.dot(M, S)
    M /= M[3, 3]
    return M",_12615.py,37,"angles[0], angles[1], angles[2]",*angles[:3]
https://github.com/open-mmlab/OpenPCDet/tree/master/pcdet/datasets/augmentor/augmentor_utils.py,"def local_frustum_dropout_right(gt_boxes, points, intensity_range):
    """"""
    Args:
        gt_boxes: (N, 7), [x, y, z, dx, dy, dz, heading, [vx], [vy]],
        points: (M, 3 + C),
        intensity: [min, max]
    Returns:
    """"""
    for idx, box in enumerate(gt_boxes):
        x, y, z, dx, dy, dz = box[0], box[1], box[2], box[3], box[4], box[5]
        
        intensity = np.random.uniform(intensity_range[0], intensity_range[1])
        points_in_box, mask = get_points_in_box(points, box)
        threshold = (y - dy / 2) + intensity * dy
        
        points = points[np.logical_not(np.logical_and(mask, points[:, 1] <= threshold))]
    
    return gt_boxes, points",_12778.py,12,"intensity_range[0], intensity_range[1]",*intensity_range[:2]
https://github.com/wee-slack/wee-slack/tree/master//wee_slack.py,"def linkify_text(message, team, only_users=False, escape_characters=True):
    # The get_username_map function is a bit heavy, but this whole
    # function is only called on message send..
    usernames = team.get_username_map()
    channels = team.get_channel_map()
    usergroups = team.generate_usergroup_map()
    if escape_characters:
        message = (
            message
            # Replace IRC formatting chars with Slack formatting chars.
            .replace(""\x02"", ""*"")
            .replace(""\x1D"", ""_"")
            .replace(""\x1F"", config.map_underline_to)
            # Escape chars that have special meaning to Slack. Note that we do not
            # (and should not) perform full HTML entity-encoding here.
            # See https://api.slack.com/docs/message-formatting for details.
            .replace(""&"", ""&amp;"")
            .replace(""<"", ""&lt;"")
            .replace("">"", ""&gt;"")
        )

    def linkify_word(match):
        word = match.group(0)
        prefix, name = match.groups()
        if prefix == ""@"":
            if name in [""channel"", ""everyone"", ""group"", ""here""]:
                return ""<!{}>"".format(name)
            elif name in usernames:
                return ""<@{}>"".format(usernames[name])
            elif word in usergroups.keys():
                return ""<!subteam^{}|{}>"".format(usergroups[word], word)
        elif prefix == ""#"" and not only_users:
            if word in channels:
                return ""<#{}|{}>"".format(channels[word], name)
        return word

    linkify_regex = r""(?:^|(?<=\s))([@#])([\w\(\)\'.-]+)""
    return re.sub(linkify_regex, linkify_word, message, flags=re.UNICODE)",_12941.py,9,"'\x1d', '_'",*'\x1d_'
https://github.com/wee-slack/wee-slack/tree/master//wee_slack.py,"def linkify_text(message, team, only_users=False, escape_characters=True):
    # The get_username_map function is a bit heavy, but this whole
    # function is only called on message send..
    usernames = team.get_username_map()
    channels = team.get_channel_map()
    usergroups = team.generate_usergroup_map()
    if escape_characters:
        message = (
            message
            # Replace IRC formatting chars with Slack formatting chars.
            .replace(""\x02"", ""*"")
            .replace(""\x1D"", ""_"")
            .replace(""\x1F"", config.map_underline_to)
            # Escape chars that have special meaning to Slack. Note that we do not
            # (and should not) perform full HTML entity-encoding here.
            # See https://api.slack.com/docs/message-formatting for details.
            .replace(""&"", ""&amp;"")
            .replace(""<"", ""&lt;"")
            .replace("">"", ""&gt;"")
        )

    def linkify_word(match):
        word = match.group(0)
        prefix, name = match.groups()
        if prefix == ""@"":
            if name in [""channel"", ""everyone"", ""group"", ""here""]:
                return ""<!{}>"".format(name)
            elif name in usernames:
                return ""<@{}>"".format(usernames[name])
            elif word in usergroups.keys():
                return ""<!subteam^{}|{}>"".format(usergroups[word], word)
        elif prefix == ""#"" and not only_users:
            if word in channels:
                return ""<#{}|{}>"".format(channels[word], name)
        return word

    linkify_regex = r""(?:^|(?<=\s))([@#])([\w\(\)\'.-]+)""
    return re.sub(linkify_regex, linkify_word, message, flags=re.UNICODE)",_12941.py,9,"'\x02', '*'",*'\x02*'
https://github.com/kivy/buildozer/tree/master/buildozer/target.py,"def path_or_git_url(self, repo, owner='kivy', branch='master',
                        url_format='https://github.com/{owner}/{repo}.git',
                        platform=None,
                        squash_hyphen=True):
        """"""Get source location for a git checkout

        This method will check the `buildozer.spec` for the keys:
            {repo}_dir
            {repo}_url
            {repo}_branch

        and use them to determine the source location for a git checkout.

        If a `platform` is specified, {platform}.{repo} will be used
        as the base for the buildozer key

        `{repo}_dir` specifies a custom checkout location
        (relative to `buildozer.root_dir`). If present, `path` will be
        set to this value and `url`, `branch` will be set to None,
        None. Otherwise, `{repo}_url` and `{repo}_branch` will be
        examined.

        If no keys are present, the kwargs will be used to create
        a sensible default URL and branch.

        :Parameters:
            `repo`: str (required)
                name of repository to fetch. Used both for buildozer
                keys ({platform}.{repo}_dir|_url|_branch) and in building
                default git URL
            `branch`: str (default 'master')
                Specific branch to retrieve if none specified in
                buildozer.spec.
            `owner`: str
                owner of repo.
            `platform`: str or None
                platform prefix to use when retrieving `buildozer.spec`
                keys. If specified, key names will be {platform}.{repo}
                instead of just {repo}
            `squash_hyphen`: boolean
                if True, change '-' to '_' when looking for
                keys in buildozer.spec. This lets us keep backwards
                compatibility with old buildozer.spec files
            `url_format`: format string
                Used to construct default git URL.
                can use {repo} {owner} and {branch} if needed.

        :Returns:
            A Tuple (path, url, branch) where
                `path`
                    Path to a custom git checkout. If specified,
                    both `url` and `branch` will be None
                `url`
                    URL of git repository from where code should be
                    checked-out
                `branch`
                    branch name (or tag) that should be used for the
                    check-out.

        """"""
        if squash_hyphen:
            key = repo.replace('-', '_')
        else:
            key = repo
        if platform:
            key = ""{}.{}"".format(platform, key)
        config = self.buildozer.config
        path = config.getdefault('app', '{}_dir'.format(key), None)

        if path is not None:
            path = join(self.buildozer.root_dir, path)
            url = None
            branch = None
        else:
            branch = config.getdefault('app', '{}_branch'.format(key), branch)
            default_url = url_format.format(owner=owner, repo=repo, branch=branch)
            url = config.getdefault('app', '{}_url'.format(key), default_url)
        return path, url, branch",_13000.py,62,"'-', '_'",*'-_'
https://github.com/freewym/espresso/tree/master/examples/MMPT/mmpt/processors/dedupprocessor.py,"def _dedup(self, caption):
        def random_merge(end_idx, start, end, text, starts, ends, texts):
            if random.random() > 0.5:
                # print(clip_idx, ""[PARTIAL INTO PREV]"", end_idx)
                # overlapped part goes to the end of previous.
                ends[-1] = max(ends[-1], start)  # ?
                rest_text = text[end_idx:].strip()
                if rest_text:
                    starts.append(max(ends[-1], start))
                    ends.append(max(end, starts[-1]))
                    texts.append(rest_text)
            else:  # goes to the beginning of the current.
                # strip the previous.
                left_text = texts[-1][:-end_idx].strip()
                if left_text:
                    # print(clip_idx, ""[PREV PARTIAL INTO CUR]"", end_idx)
                    ends[-1] = min(ends[-1], start)
                    texts[-1] = left_text
                else:
                    # print(clip_idx, ""[PREV LEFT NOTHING ALL INTO CUR]"", end_idx)
                    starts.pop(-1)
                    ends.pop(-1)
                    texts.pop(-1)
                starts.append(start)
                ends.append(end)
                texts.append(text)

        starts, ends, texts = [], [], []
        for clip_idx, (start, end, text) in enumerate(
            zip(caption[""start""], caption[""end""], caption[""text""])
        ):
            if not isinstance(text, str):
                continue
            text = text.replace(""\n"", "" "").strip()
            if len(text) == 0:
                continue
            starts.append(start)
            ends.append(end)
            texts.append(text)
            break

        for clip_idx, (start, end, text) in enumerate(
            zip(
                caption[""start""][clip_idx + 1:],
                caption[""end""][clip_idx + 1:],
                caption[""text""][clip_idx + 1:],
            )
        ):
            if not isinstance(text, str):
                continue
            text = text.replace(""\n"", "" "").strip()
            if len(text) == 0:
                continue

            # print(clip_idx, texts[-5:])
            # print(clip_idx, start, end, text)
            if texts[-1].endswith(text):  # subset of prev caption -> merge
                # print(clip_idx, ""[MERGE INTO PREV]"")
                ends[-1] = max(ends[-1], end)
            elif text.startswith(texts[-1]):  # superset of prev caption -> merge
                # print(clip_idx, ""[PREV MERGE INTO CUR]"")
                texts[-1] = text
                starts[-1] = min(starts[-1], start)
                ends[-1] = max(ends[-1], end)
            else:  # overlapping or non-overlapping.
                for end_idx in range(1, len(text) + 1):
                    if texts[-1].endswith(text[:end_idx]):
                        random_merge(end_idx, start, end, text, starts, ends, texts)
                        break
                else:
                    starts.append(start)
                    ends.append(end)
                    texts.append(text)

            assert (ends[-1] + 0.001) >= starts[-1] and len(
                texts[-1]
            ) > 0, ""{} {} {} <- {} {} {}, {} {} {}"".format(
                str(starts[-1]),
                str(ends[-1]),
                texts[-1],
                caption[""start""][clip_idx - 1],
                caption[""end""][clip_idx - 1],
                caption[""text""][clip_idx - 1],
                str(start),
                str(end),
                text,
            )

        return {""start"": starts, ""end"": ends, ""text"": texts}",_13462.py,34,"'\n', ' '",*'\n '
https://github.com/freewym/espresso/tree/master/examples/MMPT/mmpt/processors/dedupprocessor.py,"def _dedup(self, caption):
        def random_merge(end_idx, start, end, text, starts, ends, texts):
            if random.random() > 0.5:
                # print(clip_idx, ""[PARTIAL INTO PREV]"", end_idx)
                # overlapped part goes to the end of previous.
                ends[-1] = max(ends[-1], start)  # ?
                rest_text = text[end_idx:].strip()
                if rest_text:
                    starts.append(max(ends[-1], start))
                    ends.append(max(end, starts[-1]))
                    texts.append(rest_text)
            else:  # goes to the beginning of the current.
                # strip the previous.
                left_text = texts[-1][:-end_idx].strip()
                if left_text:
                    # print(clip_idx, ""[PREV PARTIAL INTO CUR]"", end_idx)
                    ends[-1] = min(ends[-1], start)
                    texts[-1] = left_text
                else:
                    # print(clip_idx, ""[PREV LEFT NOTHING ALL INTO CUR]"", end_idx)
                    starts.pop(-1)
                    ends.pop(-1)
                    texts.pop(-1)
                starts.append(start)
                ends.append(end)
                texts.append(text)

        starts, ends, texts = [], [], []
        for clip_idx, (start, end, text) in enumerate(
            zip(caption[""start""], caption[""end""], caption[""text""])
        ):
            if not isinstance(text, str):
                continue
            text = text.replace(""\n"", "" "").strip()
            if len(text) == 0:
                continue
            starts.append(start)
            ends.append(end)
            texts.append(text)
            break

        for clip_idx, (start, end, text) in enumerate(
            zip(
                caption[""start""][clip_idx + 1:],
                caption[""end""][clip_idx + 1:],
                caption[""text""][clip_idx + 1:],
            )
        ):
            if not isinstance(text, str):
                continue
            text = text.replace(""\n"", "" "").strip()
            if len(text) == 0:
                continue

            # print(clip_idx, texts[-5:])
            # print(clip_idx, start, end, text)
            if texts[-1].endswith(text):  # subset of prev caption -> merge
                # print(clip_idx, ""[MERGE INTO PREV]"")
                ends[-1] = max(ends[-1], end)
            elif text.startswith(texts[-1]):  # superset of prev caption -> merge
                # print(clip_idx, ""[PREV MERGE INTO CUR]"")
                texts[-1] = text
                starts[-1] = min(starts[-1], start)
                ends[-1] = max(ends[-1], end)
            else:  # overlapping or non-overlapping.
                for end_idx in range(1, len(text) + 1):
                    if texts[-1].endswith(text[:end_idx]):
                        random_merge(end_idx, start, end, text, starts, ends, texts)
                        break
                else:
                    starts.append(start)
                    ends.append(end)
                    texts.append(text)

            assert (ends[-1] + 0.001) >= starts[-1] and len(
                texts[-1]
            ) > 0, ""{} {} {} <- {} {} {}, {} {} {}"".format(
                str(starts[-1]),
                str(ends[-1]),
                texts[-1],
                caption[""start""][clip_idx - 1],
                caption[""end""][clip_idx - 1],
                caption[""text""][clip_idx - 1],
                str(start),
                str(end),
                text,
            )

        return {""start"": starts, ""end"": ends, ""text"": texts}",_13462.py,51,"'\n', ' '",*'\n '
https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/cdn/tests/latest/test_endpoint_scenarios.py,"def _create_profile(self, resource_group, profile_sku):
        profile_name = (profile_sku + ""_profile"").replace(""_"", ""-"")  # profile names must match ^[a-zA-Z0-9]+(-*[a-zA-Z0-9])*$
        check = JMESPathCheck('sku.name', profile_sku)
        self.profile_create_cmd(resource_group, profile_name, sku=profile_sku, checks=check)
        return profile_name",_13552.py,2,"'_', '-'",*'_-'
https://github.com/cltk/cltk/tree/master/src/cltk/tokenizers/word.py,"def tokenize(self, text: str):
        """"""
        :rtype: list
        :param text: text to be tokenized into sentences
        :type text: str
        :param model: tokenizer object to used # Should be in init?
        :type model: object
        """"""
        for pattern in self.patterns:
            text = re.sub(pattern[0], pattern[1], text)
        return text.split()",_13820.py,10,"pattern[0], pattern[1]",*pattern[:2]
https://github.com/mushorg/conpot/tree/master/conpot/protocols/IEC104/IEC104_server.py,"def handle(self, sock, address):
        sock.settimeout(self.timeout)
        session = conpot_core.get_session(
            ""IEC104"",
            address[0],
            address[1],
            sock.getsockname()[0],
            sock.getsockname()[1],
        )
        logger.info(
            ""New IEC 104 connection from %s:%s. (%s)"",
            address[0],
            address[1],
            session.id,
        )
        session.add_event({""type"": ""NEW_CONNECTION""})
        iec104_handler = IEC104(self.device_data_controller, sock, address, session.id)
        try:
            while True:
                timeout_t3 = gevent.Timeout(
                    conpot_core.get_databus().get_value(""T_3""), Timeout_t3
                )
                timeout_t3.start()
                try:
                    try:
                        request = sock.recv(6)
                        if not request:
                            logger.info(""IEC104 Station disconnected. (%s)"", session.id)
                            session.add_event({""type"": ""CONNECTION_LOST""})
                            iec104_handler.disconnect()
                            break
                        while request and len(request) < 2:
                            new_byte = sock.recv(1)
                            request += new_byte

                        _, length = struct.unpack("">BB"", request[:2])
                        while len(request) < (length + 2):
                            new_byte = sock.recv(1)
                            if not new_byte:
                                break
                            request += new_byte

                        # check if IEC 104 packet or for the first occurrence of the indication 0x68 for IEC 104
                        for elem in list(request):
                            if 0x68 == elem:
                                index = request.index(elem)

                                iec_request = request[index:]
                                timeout_t3.cancel()
                                response = None
                                # check which frame type
                                if not (iec_request[2] & 0x01):  # i_frame
                                    response = iec104_handler.handle_i_frame(
                                        iec_request
                                    )
                                elif iec_request[2] & 0x01 and not (
                                    iec_request[2] & 0x02
                                ):  # s_frame
                                    iec104_handler.handle_s_frame(iec_request)
                                elif iec_request[2] & 0x03:  # u_frame
                                    response = iec104_handler.handle_u_frame(
                                        iec_request
                                    )
                                else:
                                    logger.warning(
                                        ""%s ---> No valid IEC104 type (%s)"",
                                        address,
                                        session.id,
                                    )

                                if response:
                                    for resp_packet in response:
                                        if resp_packet:
                                            sock.send(resp_packet)
                                break

                    except Timeout_t3:
                        pkt = iec104_handler.send_104frame(TESTFR_act)
                        if pkt:
                            sock.send(pkt)
                    finally:
                        timeout_t3.cancel()
                except gevent.Timeout:
                    logger.warning(""T1 timed out. (%s)"", session.id)
                    logger.info(""IEC104 Station disconnected. (%s)"", session.id)
                    session.add_event({""type"": ""CONNECTION_LOST""})
                    iec104_handler.disconnect()
                    break
        except socket.timeout:
            logger.debug(""Socket timeout, remote: %s. (%s)"", address[0], session.id)
            session.add_event({""type"": ""CONNECTION_LOST""})
        except socket.error as err:
            if isinstance(err.args, tuple):
                if err.errno == errno.EPIPE:
                    # remote peer disconnected
                    logger.info(""IEC104 Station disconnected. (%s)"", session.id)
                    session.add_event({""type"": ""CONNECTION_LOST""})
                else:
                    # determine and handle different error
                    pass
            else:
                print((""socket error "", err))
            iec104_handler.disconnect()",_13838.py,3,"address[0], address[1]",*address[:2]
https://github.com/mushorg/conpot/tree/master/conpot/protocols/IEC104/IEC104_server.py,"def handle(self, sock, address):
        sock.settimeout(self.timeout)
        session = conpot_core.get_session(
            ""IEC104"",
            address[0],
            address[1],
            sock.getsockname()[0],
            sock.getsockname()[1],
        )
        logger.info(
            ""New IEC 104 connection from %s:%s. (%s)"",
            address[0],
            address[1],
            session.id,
        )
        session.add_event({""type"": ""NEW_CONNECTION""})
        iec104_handler = IEC104(self.device_data_controller, sock, address, session.id)
        try:
            while True:
                timeout_t3 = gevent.Timeout(
                    conpot_core.get_databus().get_value(""T_3""), Timeout_t3
                )
                timeout_t3.start()
                try:
                    try:
                        request = sock.recv(6)
                        if not request:
                            logger.info(""IEC104 Station disconnected. (%s)"", session.id)
                            session.add_event({""type"": ""CONNECTION_LOST""})
                            iec104_handler.disconnect()
                            break
                        while request and len(request) < 2:
                            new_byte = sock.recv(1)
                            request += new_byte

                        _, length = struct.unpack("">BB"", request[:2])
                        while len(request) < (length + 2):
                            new_byte = sock.recv(1)
                            if not new_byte:
                                break
                            request += new_byte

                        # check if IEC 104 packet or for the first occurrence of the indication 0x68 for IEC 104
                        for elem in list(request):
                            if 0x68 == elem:
                                index = request.index(elem)

                                iec_request = request[index:]
                                timeout_t3.cancel()
                                response = None
                                # check which frame type
                                if not (iec_request[2] & 0x01):  # i_frame
                                    response = iec104_handler.handle_i_frame(
                                        iec_request
                                    )
                                elif iec_request[2] & 0x01 and not (
                                    iec_request[2] & 0x02
                                ):  # s_frame
                                    iec104_handler.handle_s_frame(iec_request)
                                elif iec_request[2] & 0x03:  # u_frame
                                    response = iec104_handler.handle_u_frame(
                                        iec_request
                                    )
                                else:
                                    logger.warning(
                                        ""%s ---> No valid IEC104 type (%s)"",
                                        address,
                                        session.id,
                                    )

                                if response:
                                    for resp_packet in response:
                                        if resp_packet:
                                            sock.send(resp_packet)
                                break

                    except Timeout_t3:
                        pkt = iec104_handler.send_104frame(TESTFR_act)
                        if pkt:
                            sock.send(pkt)
                    finally:
                        timeout_t3.cancel()
                except gevent.Timeout:
                    logger.warning(""T1 timed out. (%s)"", session.id)
                    logger.info(""IEC104 Station disconnected. (%s)"", session.id)
                    session.add_event({""type"": ""CONNECTION_LOST""})
                    iec104_handler.disconnect()
                    break
        except socket.timeout:
            logger.debug(""Socket timeout, remote: %s. (%s)"", address[0], session.id)
            session.add_event({""type"": ""CONNECTION_LOST""})
        except socket.error as err:
            if isinstance(err.args, tuple):
                if err.errno == errno.EPIPE:
                    # remote peer disconnected
                    logger.info(""IEC104 Station disconnected. (%s)"", session.id)
                    session.add_event({""type"": ""CONNECTION_LOST""})
                else:
                    # determine and handle different error
                    pass
            else:
                print((""socket error "", err))
            iec104_handler.disconnect()",_13838.py,3,"sock.getsockname()[0], sock.getsockname()[1]",*sock.getsockname()[:2]
https://github.com/mushorg/conpot/tree/master/conpot/protocols/IEC104/IEC104_server.py,"def handle(self, sock, address):
        sock.settimeout(self.timeout)
        session = conpot_core.get_session(
            ""IEC104"",
            address[0],
            address[1],
            sock.getsockname()[0],
            sock.getsockname()[1],
        )
        logger.info(
            ""New IEC 104 connection from %s:%s. (%s)"",
            address[0],
            address[1],
            session.id,
        )
        session.add_event({""type"": ""NEW_CONNECTION""})
        iec104_handler = IEC104(self.device_data_controller, sock, address, session.id)
        try:
            while True:
                timeout_t3 = gevent.Timeout(
                    conpot_core.get_databus().get_value(""T_3""), Timeout_t3
                )
                timeout_t3.start()
                try:
                    try:
                        request = sock.recv(6)
                        if not request:
                            logger.info(""IEC104 Station disconnected. (%s)"", session.id)
                            session.add_event({""type"": ""CONNECTION_LOST""})
                            iec104_handler.disconnect()
                            break
                        while request and len(request) < 2:
                            new_byte = sock.recv(1)
                            request += new_byte

                        _, length = struct.unpack("">BB"", request[:2])
                        while len(request) < (length + 2):
                            new_byte = sock.recv(1)
                            if not new_byte:
                                break
                            request += new_byte

                        # check if IEC 104 packet or for the first occurrence of the indication 0x68 for IEC 104
                        for elem in list(request):
                            if 0x68 == elem:
                                index = request.index(elem)

                                iec_request = request[index:]
                                timeout_t3.cancel()
                                response = None
                                # check which frame type
                                if not (iec_request[2] & 0x01):  # i_frame
                                    response = iec104_handler.handle_i_frame(
                                        iec_request
                                    )
                                elif iec_request[2] & 0x01 and not (
                                    iec_request[2] & 0x02
                                ):  # s_frame
                                    iec104_handler.handle_s_frame(iec_request)
                                elif iec_request[2] & 0x03:  # u_frame
                                    response = iec104_handler.handle_u_frame(
                                        iec_request
                                    )
                                else:
                                    logger.warning(
                                        ""%s ---> No valid IEC104 type (%s)"",
                                        address,
                                        session.id,
                                    )

                                if response:
                                    for resp_packet in response:
                                        if resp_packet:
                                            sock.send(resp_packet)
                                break

                    except Timeout_t3:
                        pkt = iec104_handler.send_104frame(TESTFR_act)
                        if pkt:
                            sock.send(pkt)
                    finally:
                        timeout_t3.cancel()
                except gevent.Timeout:
                    logger.warning(""T1 timed out. (%s)"", session.id)
                    logger.info(""IEC104 Station disconnected. (%s)"", session.id)
                    session.add_event({""type"": ""CONNECTION_LOST""})
                    iec104_handler.disconnect()
                    break
        except socket.timeout:
            logger.debug(""Socket timeout, remote: %s. (%s)"", address[0], session.id)
            session.add_event({""type"": ""CONNECTION_LOST""})
        except socket.error as err:
            if isinstance(err.args, tuple):
                if err.errno == errno.EPIPE:
                    # remote peer disconnected
                    logger.info(""IEC104 Station disconnected. (%s)"", session.id)
                    session.add_event({""type"": ""CONNECTION_LOST""})
                else:
                    # determine and handle different error
                    pass
            else:
                print((""socket error "", err))
            iec104_handler.disconnect()",_13838.py,10,"address[0], address[1]",*address[:2]
https://github.com/holoviz/hvplot/tree/master/hvplot/tests/testoptions.py,"def test_holoviews_defined_default_opts(self, df, backend):
        hv.opts.defaults(hv.opts.Scatter(height=400, width=900, show_grid=True))
        plot = df.hvplot.scatter('x', 'y', c='category')
        opts = Store.lookup_options(backend, plot, 'plot')
        # legend_position shouldn't apply only to bokeh
        if backend == 'bokeh':
            assert opts.kwargs['legend_position'] == 'right'
        assert opts.kwargs['show_grid'] is True
        assert opts.kwargs['height'] == 400
        assert opts.kwargs['width'] == 900",_14340.py,3,"'x', 'y'",*'xy'
https://github.com/kivy/kivy/tree/master/kivy/tools/pep8checker/pep8.py,"def read_config(options, args, arglist, parser):
    """"""Read and parse configurations.

    If a config file is specified on the command line with the ""--config""
    option, then only it is used for configuration.

    Otherwise, the user configuration (~/.config/pycodestyle) and any local
    configurations in the current directory or above will be merged together
    (in that order) using the read method of ConfigParser.
    """"""
    config = RawConfigParser()

    cli_conf = options.config

    local_dir = os.curdir

    if USER_CONFIG and os.path.isfile(USER_CONFIG):
        if options.verbose:
            print('user configuration: %s' % USER_CONFIG)
        config.read(USER_CONFIG)

    parent = tail = args and os.path.abspath(os.path.commonprefix(args))
    while tail:
        if config.read(os.path.join(parent, fn) for fn in PROJECT_CONFIG):
            local_dir = parent
            if options.verbose:
                print('local configuration: in %s' % parent)
            break
        (parent, tail) = os.path.split(parent)

    if cli_conf and os.path.isfile(cli_conf):
        if options.verbose:
            print('cli configuration: %s' % cli_conf)
        config.read(cli_conf)

    pycodestyle_section = None
    if config.has_section(parser.prog):
        pycodestyle_section = parser.prog
    elif config.has_section('pep8'):
        pycodestyle_section = 'pep8'  # Deprecated
        warnings.warn('[pep8] section is deprecated. Use [pycodestyle].')

    if pycodestyle_section:
        option_list = dict([(o.dest, o.type or o.action)
                            for o in parser.option_list])

        # First, read the default values
        (new_options, __) = parser.parse_args([])

        # Second, parse the configuration
        for opt in config.options(pycodestyle_section):
            if opt.replace('_', '-') not in parser.config_options:
                print(""  unknown option '%s' ignored"" % opt)
                continue
            if options.verbose > 1:
                print(""  %s = %s"" % (opt,
                                     config.get(pycodestyle_section, opt)))
            normalized_opt = opt.replace('-', '_')
            opt_type = option_list[normalized_opt]
            if opt_type in ('int', 'count'):
                value = config.getint(pycodestyle_section, opt)
            elif opt_type in ('store_true', 'store_false'):
                value = config.getboolean(pycodestyle_section, opt)
            else:
                value = config.get(pycodestyle_section, opt)
                if normalized_opt == 'exclude':
                    value = normalize_paths(value, local_dir)
            setattr(new_options, normalized_opt, value)

        # Third, overwrite with the command-line options
        (options, __) = parser.parse_args(arglist, values=new_options)
    options.doctest = options.testsuite = False
    return options",_14446.py,58,"'-', '_'",*'-_'
https://github.com/kivy/kivy/tree/master/kivy/tools/pep8checker/pep8.py,"def read_config(options, args, arglist, parser):
    """"""Read and parse configurations.

    If a config file is specified on the command line with the ""--config""
    option, then only it is used for configuration.

    Otherwise, the user configuration (~/.config/pycodestyle) and any local
    configurations in the current directory or above will be merged together
    (in that order) using the read method of ConfigParser.
    """"""
    config = RawConfigParser()

    cli_conf = options.config

    local_dir = os.curdir

    if USER_CONFIG and os.path.isfile(USER_CONFIG):
        if options.verbose:
            print('user configuration: %s' % USER_CONFIG)
        config.read(USER_CONFIG)

    parent = tail = args and os.path.abspath(os.path.commonprefix(args))
    while tail:
        if config.read(os.path.join(parent, fn) for fn in PROJECT_CONFIG):
            local_dir = parent
            if options.verbose:
                print('local configuration: in %s' % parent)
            break
        (parent, tail) = os.path.split(parent)

    if cli_conf and os.path.isfile(cli_conf):
        if options.verbose:
            print('cli configuration: %s' % cli_conf)
        config.read(cli_conf)

    pycodestyle_section = None
    if config.has_section(parser.prog):
        pycodestyle_section = parser.prog
    elif config.has_section('pep8'):
        pycodestyle_section = 'pep8'  # Deprecated
        warnings.warn('[pep8] section is deprecated. Use [pycodestyle].')

    if pycodestyle_section:
        option_list = dict([(o.dest, o.type or o.action)
                            for o in parser.option_list])

        # First, read the default values
        (new_options, __) = parser.parse_args([])

        # Second, parse the configuration
        for opt in config.options(pycodestyle_section):
            if opt.replace('_', '-') not in parser.config_options:
                print(""  unknown option '%s' ignored"" % opt)
                continue
            if options.verbose > 1:
                print(""  %s = %s"" % (opt,
                                     config.get(pycodestyle_section, opt)))
            normalized_opt = opt.replace('-', '_')
            opt_type = option_list[normalized_opt]
            if opt_type in ('int', 'count'):
                value = config.getint(pycodestyle_section, opt)
            elif opt_type in ('store_true', 'store_false'):
                value = config.getboolean(pycodestyle_section, opt)
            else:
                value = config.get(pycodestyle_section, opt)
                if normalized_opt == 'exclude':
                    value = normalize_paths(value, local_dir)
            setattr(new_options, normalized_opt, value)

        # Third, overwrite with the command-line options
        (options, __) = parser.parse_args(arglist, values=new_options)
    options.doctest = options.testsuite = False
    return options",_14446.py,52,"'_', '-'",*'_-'
https://github.com/postgres/pgadmin4/tree/master/web/pgadmin/utils/paths.py,"def _preprocess_username(un):
        ret_un = un
        if len(ret_un) == 0 or ret_un[0].isdigit():
            ret_un = 'pga_user_' + un

        ret_un = ret_un.replace('@', '_')\
            .replace('/', 'slash')\
            .replace('\\', 'slash')

        return ret_un",_15052.py,6,"'@', '_'",*'@_'
https://github.com/openstack/swift/tree/master/test/unit/proxy/controllers/test_container.py,"def do_test(method, resp_status, num_resp):
            self.assertGreater(num_resp, 0)  # sanity check
            memcache = FakeMemcache()
            cont_key = get_cache_key('a', 'c')
            shard_key = get_cache_key('a', 'c', shard='listing')
            memcache.set(cont_key, 'container info', 60)
            memcache.set(shard_key, 'shard ranges', 600)
            req = Request.blank('/v1/a/c', method=method)
            req.environ['swift.cache'] = memcache
            self.assertIn(cont_key, req.environ['swift.cache'].store)
            self.assertIn(shard_key, req.environ['swift.cache'].store)
            resp_status = [resp_status] * num_resp
            with mocked_http_conn(
                    *resp_status, body_iter=[b''] * num_resp,
                    headers=[{}] * num_resp):
                resp = req.get_response(self.app)
            self.assertEqual(resp_status[0], resp.status_int)
            self.assertNotIn(cont_key, req.environ['swift.cache'].store)
            self.assertNotIn(shard_key, req.environ['swift.cache'].store)",_15168.py,4,"'a', 'c'",*'ac'
https://github.com/openstack/swift/tree/master/test/unit/proxy/controllers/test_container.py,"def do_test(method, resp_status, num_resp):
            self.assertGreater(num_resp, 0)  # sanity check
            memcache = FakeMemcache()
            cont_key = get_cache_key('a', 'c')
            shard_key = get_cache_key('a', 'c', shard='listing')
            memcache.set(cont_key, 'container info', 60)
            memcache.set(shard_key, 'shard ranges', 600)
            req = Request.blank('/v1/a/c', method=method)
            req.environ['swift.cache'] = memcache
            self.assertIn(cont_key, req.environ['swift.cache'].store)
            self.assertIn(shard_key, req.environ['swift.cache'].store)
            resp_status = [resp_status] * num_resp
            with mocked_http_conn(
                    *resp_status, body_iter=[b''] * num_resp,
                    headers=[{}] * num_resp):
                resp = req.get_response(self.app)
            self.assertEqual(resp_status[0], resp.status_int)
            self.assertNotIn(cont_key, req.environ['swift.cache'].store)
            self.assertNotIn(shard_key, req.environ['swift.cache'].store)",_15168.py,5,"'a', 'c'",*'ac'
https://github.com/microsoft/playwright-python/tree/master/tests/conftest.py,"def assert_to_be_golden(browser_name: str) -> Callable[[bytes, str], None]:
    def compare(received_raw: bytes, golden_name: str) -> None:
        golden_file_path = _dirname / f""golden-{browser_name}"" / golden_name
        try:
            golden_file = golden_file_path.read_bytes()
            received_image = Image.open(io.BytesIO(received_raw))
            golden_image = Image.open(io.BytesIO(golden_file))

            if golden_image.size != received_image.size:
                pytest.fail(""Image size differs to golden image"")
                return
            diff_pixels = pixelmatch(
                from_PIL_to_raw_data(received_image),
                from_PIL_to_raw_data(golden_image),
                golden_image.size[0],
                golden_image.size[1],
                threshold=0.2,
            )
            assert diff_pixels == 0
        except Exception:
            if os.getenv(""PW_WRITE_SCREENSHOT""):
                golden_file_path.parent.mkdir(parents=True, exist_ok=True)
                golden_file_path.write_bytes(received_raw)
                print(f""Wrote {golden_file_path}"")
            raise

    return compare",_15624.py,12,"golden_image.size[0], golden_image.size[1]",*golden_image.size[:2]
https://github.com/microsoft/playwright-python/tree/master/tests/conftest.py,"def assert_to_be_golden(browser_name: str) -> Callable[[bytes, str], None]:
    def compare(received_raw: bytes, golden_name: str) -> None:
        golden_file_path = _dirname / f""golden-{browser_name}"" / golden_name
        try:
            golden_file = golden_file_path.read_bytes()
            received_image = Image.open(io.BytesIO(received_raw))
            golden_image = Image.open(io.BytesIO(golden_file))

            if golden_image.size != received_image.size:
                pytest.fail(""Image size differs to golden image"")
                return
            diff_pixels = pixelmatch(
                from_PIL_to_raw_data(received_image),
                from_PIL_to_raw_data(golden_image),
                golden_image.size[0],
                golden_image.size[1],
                threshold=0.2,
            )
            assert diff_pixels == 0
        except Exception:
            if os.getenv(""PW_WRITE_SCREENSHOT""):
                golden_file_path.parent.mkdir(parents=True, exist_ok=True)
                golden_file_path.write_bytes(received_raw)
                print(f""Wrote {golden_file_path}"")
            raise

    return compare",_15624.py,12,"golden_image.size[0], golden_image.size[1]",*golden_image.size[:2]
https://github.com/3b1b/videos/tree/master/_2017/eoc/chapter1.py,"def describe_sliver(self):
        dx_brace = Brace(self.right_v_lines, DOWN, buff = 0)
        dx_label = dx_brace.get_text(""$dx$"")
        dx_group = VGroup(dx_brace, dx_label)

        dA_rect = Rectangle(
            width = self.right_v_lines.get_width(),
            height = self.shadow_rects[-1].get_height(),
            stroke_width = 0,
            fill_color = YELLOW,
            fill_opacity = 0.5,
        ).move_to(self.right_v_lines, DOWN)
        dA_label = Tex(""d"", ""A"")
        dA_label.next_to(dA_rect, RIGHT, MED_LARGE_BUFF, UP)
        dA_label.set_color(GREEN)
        dA_arrow = Arrow(
            dA_label.get_bottom()+MED_SMALL_BUFF*DOWN, 
            dA_rect.get_center(),
            buff = 0,
            color = WHITE
        )

        difference_in_area = TexText(
            ""d"", ""ifference in "", ""A"", ""rea"",
            arg_separator = """"
        )
        difference_in_area.set_color_by_tex(""d"", GREEN)
        difference_in_area.set_color_by_tex(""A"", GREEN)
        difference_in_area.scale(0.7)
        difference_in_area.next_to(dA_label, UP, MED_SMALL_BUFF, LEFT)

        side_brace = Brace(dA_rect, LEFT, buff = 0)
        graph_label_copy = self.graph_label.copy()

        self.play(
            FadeOut(self.right_point_slider),
            FadeIn(dx_group)
        )
        self.play(Indicate(dx_label))
        self.wait()
        self.play(ShowCreation(dA_arrow))
        self.wait()
        self.play(Write(dA_label, run_time = 2))
        self.wait()
        self.play(
            ReplacementTransform(dA_label[0].copy(), difference_in_area[0]),
            ReplacementTransform(dA_label[1].copy(), difference_in_area[2]),
            *list(map(FadeIn, [difference_in_area[1], difference_in_area[3]]))
        )
        self.wait(2)
        self.play(FadeIn(dA_rect), Animation(dA_arrow))
        self.play(GrowFromCenter(side_brace))
        self.play(
            graph_label_copy.set_color, WHITE,
            graph_label_copy.next_to, side_brace, LEFT, SMALL_BUFF
        )
        self.wait()
        self.play(Indicate(dx_group))
        self.wait()
        self.play(FadeOut(difference_in_area))

        self.dx_group = dx_group
        self.dA_rect = dA_rect
        self.dA_label = dA_label
        self.graph_label_copy = graph_label_copy",_15686.py,13,"'d', 'A'",*'dA'
https://github.com/EFForg/crocodilehunter/tree/master/src/webui.py,"def enodeb_sightings(self):
        trilat_pts = []
        enodebs = []
        known_towers = [kt.to_dict() for kt in self.watchdog.get_known_towers().all()]
        towers = self.watchdog.get_unique_enodebs()
        for t in towers:
            self.watchdog.get_max_column_by_enodeb
            sightings = self.watchdog.get_sightings_for_enodeb(t)

            trilat = self.watchdog.trilaterate_enodeb_location(sightings)
            enodebs.append({
                ""trilat"": list(trilat),
                ""enodeb_id"": t.enodeb_id,
                ""plmn"": t.plmn(),
                ""closest_tower"": self.watchdog.closest_known_tower(trilat[0], trilat[1]),
                ""unique_cells"": self.watchdog.get_cells_count_for_enodebid(t),
                ""sightings"": sightings.count(),
                ""max_suspiciousness"": self.watchdog.get_suspicious_percentage_by_enodeb(t),
                ""first_seen"": str(self.watchdog.get_min_column_by_enodeb(t, 'timestamp')),
                ""last_seen"": str(self.watchdog.get_max_column_by_enodeb(t, 'timestamp'))

            })
        return render_template('index.html', name=self.watchdog.project_name,
                                known_towers = json.dumps(known_towers),
                                key = 'enodeb_id',
                                enodebs=json.dumps(enodebs))",_15916.py,15,"trilat[0], trilat[1]",*trilat[:2]
https://github.com/sheepzh/poetry/tree/master/tests/installation/test_installer.py,"def test_run_install_duplicate_dependencies_different_constraints_with_lock(
    installer: Installer, locker: Locker, repo: Repository, package: ProjectPackage
):
    locker.locked(True)
    locker.mock_lock_data(
        {
            ""package"": [
                {
                    ""name"": ""A"",
                    ""version"": ""1.0"",
                    ""category"": ""main"",
                    ""optional"": False,
                    ""platform"": ""*"",
                    ""python-versions"": ""*"",
                    ""checksum"": [],
                    ""dependencies"": {
                        ""B"": [
                            {""version"": ""^1.0"", ""python"": ""<4.0""},
                            {""version"": ""^2.0"", ""python"": "">=4.0""},
                        ]
                    },
                },
                {
                    ""name"": ""B"",
                    ""version"": ""1.0"",
                    ""category"": ""dev"",
                    ""optional"": False,
                    ""platform"": ""*"",
                    ""python-versions"": ""*"",
                    ""checksum"": [],
                    ""dependencies"": {""C"": ""1.2""},
                    ""requirements"": {""python"": ""<4.0""},
                },
                {
                    ""name"": ""B"",
                    ""version"": ""2.0"",
                    ""category"": ""dev"",
                    ""optional"": False,
                    ""platform"": ""*"",
                    ""python-versions"": ""*"",
                    ""checksum"": [],
                    ""dependencies"": {""C"": ""1.5""},
                    ""requirements"": {""python"": "">=4.0""},
                },
                {
                    ""name"": ""C"",
                    ""version"": ""1.2"",
                    ""category"": ""dev"",
                    ""optional"": False,
                    ""platform"": ""*"",
                    ""python-versions"": ""*"",
                    ""checksum"": [],
                },
                {
                    ""name"": ""C"",
                    ""version"": ""1.5"",
                    ""category"": ""dev"",
                    ""optional"": False,
                    ""platform"": ""*"",
                    ""python-versions"": ""*"",
                    ""checksum"": [],
                },
            ],
            ""metadata"": {
                ""python-versions"": ""*"",
                ""platform"": ""*"",
                ""content-hash"": ""123456789"",
                ""hashes"": {""A"": [], ""B"": [], ""C"": []},
            },
        }
    )
    package.add_dependency(Factory.create_dependency(""A"", ""*""))

    package_a = get_package(""A"", ""1.0"")
    package_a.add_dependency(
        Factory.create_dependency(""B"", {""version"": ""^1.0"", ""python"": ""<4.0""})
    )
    package_a.add_dependency(
        Factory.create_dependency(""B"", {""version"": ""^2.0"", ""python"": "">=4.0""})
    )

    package_b10 = get_package(""B"", ""1.0"")
    package_b20 = get_package(""B"", ""2.0"")
    package_b10.add_dependency(Factory.create_dependency(""C"", ""1.2""))
    package_b20.add_dependency(Factory.create_dependency(""C"", ""1.5""))

    package_c12 = get_package(""C"", ""1.2"")
    package_c15 = get_package(""C"", ""1.5"")

    repo.add_package(package_a)
    repo.add_package(package_b10)
    repo.add_package(package_b20)
    repo.add_package(package_c12)
    repo.add_package(package_c15)

    installer.update(True)
    installer.run()

    expected = fixture(""with-duplicate-dependencies"")

    assert locker.written_data == expected

    assert installer.executor.installations_count == 3
    assert installer.executor.updates_count == 0
    assert installer.executor.removals_count == 0",_16068.py,72,"'A', '*'",*'A*'
https://github.com/flennerhag/mlens/tree/master/mlens/externals/joblib/_memory_helpers.py,"def _get_normal_name(orig_enc):
        """"""Imitates get_normal_name in tokenizer.c.""""""
        # Only care about the first 12 characters.
        enc = orig_enc[:12].lower().replace(""_"", ""-"")
        if enc == ""utf-8"" or enc.startswith(""utf-8-""):
            return ""utf-8""
        if enc in (""latin-1"", ""iso-8859-1"", ""iso-latin-1"") or \
           enc.startswith((""latin-1-"", ""iso-8859-1-"", ""iso-latin-1-"")):
            return ""iso-8859-1""
        return orig_enc",_16122.py,4,"_', '-'",*'_-'