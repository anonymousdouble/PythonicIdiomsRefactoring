file_html,method_content,file_name,lineno,old_code,new_code,
https://github.com/crdoconnor/strictyaml/tree/master/strictyaml/ruamel/comments.py,"def yaml_set_start_comment(self, comment, indent=0):
        # type: (Any, Any) -> None
        """"""overwrites any preceding comment lines on an object
        expects comment to be without `#` and possible have multiple lines
        """"""
        from .error import CommentMark
        from .tokens import CommentToken

        pre_comments = self._yaml_get_pre_comment()
        if comment[-1] == ""\n"":
            comment = comment[:-1]  # strip final newline if there
        start_mark = CommentMark(indent)
        for com in comment.split(""\n""):
            c = com.strip()
            if len(c) > 0 and c[0] != ""#"":
                com = ""# "" + com
            pre_comments.append(CommentToken(com + ""\n"", start_mark, None))",_80.py,13,"for com in comment.split('\n'):
    c = com.strip()
    if len(c) > 0 and c[0] != '#':
        com = '# ' + com
    pre_comments.append(CommentToken(com + '\n', start_mark, None))","pre_comments += [CommentToken('# ' + com + '\n', start_mark, None) if len(com.strip()) > 0 and c[0] != ‘#' else CommentToken(com + '\n', start_mark, None) for com in comment.split('\n')]",nan
https://github.com/Arthur151/ROMP/tree/master/romp/lib/evaluation/pw3d_eval/evaluate.py,"def joint_angle_error(pred_mat, gt_mat):
    """"""
    Compute the geodesic distance between the two input matrices.
    :param pred_mat: predicted rotation matrices. Shape: ( Seq, 9g, 3, 3)
    :param gt_mat: ground truth rotation matrices. Shape: ( Seq, 24, 3, 3)
    :return: Mean geodesic distance between input matrices.
    """"""

    gt_mat = gt_mat[:, SMPL_OR_JOINTS, :, :]

    # Reshape the matrices into B x 3 x 3 arrays
    r1 = np.reshape(pred_mat, [-1, 3, 3])
    r2 = np.reshape(gt_mat, [-1, 3, 3])

    # Transpose gt matrices
    r2t = np.transpose(r2, [0, 2, 1])

    # compute R1 * R2.T, if prediction and target match, this will be the identity matrix
    r = np.matmul(r1, r2t)

    angles = []
    # Convert rotation matrix to axis angle representation and find the angle
    for i in range(r1.shape[0]):
        aa, _ = cv2.Rodrigues(r[i])
        angles.append(np.linalg.norm(aa))

    return np.mean(np.array(angles))",_221.py,23,"for i in range(r1.shape[0]):
    (aa, _) = cv2.Rodrigues(r[i])
    angles.append(np.linalg.norm(aa))",Angles = [np.linalg.norm(cv2.Rodrigues(r[I])[0]) for i in range(r1.shape[0])],nan
https://github.com/holoviz/holoviews/tree/master/holoviews/plotting/util.py,"def get_directed_graph_paths(element, arrow_length):
    """"""
    Computes paths for a directed path which include an arrow to
    indicate the directionality of each edge.
    """"""
    edgepaths = element._split_edgepaths
    edges = edgepaths.split(datatype='array', dimensions=edgepaths.kdims)
    arrows = []
    for e in edges:
        sx, sy = e[0]
        ex, ey = e[1]
        rad = np.arctan2(ey-sy, ex-sx)
        xa0 = ex - np.cos(rad+np.pi/8)*arrow_length
        ya0 = ey - np.sin(rad+np.pi/8)*arrow_length
        xa1 = ex - np.cos(rad-np.pi/8)*arrow_length
        ya1 = ey - np.sin(rad-np.pi/8)*arrow_length
        arrow = np.array([(sx, sy), (ex, ey), (np.nan, np.nan),
                          (xa0, ya0), (ex, ey), (xa1, ya1)])
        arrows.append(arrow)
    return arrows",_460.py,9,"for e in edges:
    (sx, sy) = e[0]
    (ex, ey) = e[1]
    rad = np.arctan2(ey - sy, ex - sx)
    xa0 = ex - np.cos(rad + np.pi / 8) * arrow_length
    ya0 = ey - np.sin(rad + np.pi / 8) * arrow_length
    xa1 = ex - np.cos(rad - np.pi / 8) * arrow_length
    ya1 = ey - np.sin(rad - np.pi / 8) * arrow_length
    arrow = np.array([(sx, sy), (ex, ey), (np.nan, np.nan), (xa0, ya0), (ex, ey), (xa1, ya1)])
    arrows.append(arrow)","arrows = [np.array([(e[0][0], e[0][1]), (e[1][0], e[1][1]), (np.nan, np.nan),
                          (e[1][0] - np.cos(np.arctan2(e[1][1]-e[0][1], e[1][0]-e[0][0])+np.pi/8)*arrow_length, e[1][1] - np.sin(np.arctan2(e[1][1]-e[0][1], e[1][0]-e[0][0])+np.pi/8)*arrow_length), (e[1][0], e[1][1]), (e[1][0] - np.cos(np.arctan2(e[1][1]-e[0][1], e[1][0]-e[0][0])-np.pi/8)*arrow_length, e[1][1] - np.sin(np.arctan2(e[1][1]-e[0][1], e[1][0]-e[0][0])-np.pi/8)*arrow_length)])
for e in edges]",nan
https://github.com/napalm-automation/napalm/tree/master/napalm/eos/eos.py,"def _load_config(self, filename=None, config=None, replace=True):
        if self.config_session is None:
            self.config_session = ""napalm_{}"".format(datetime.now().microsecond)

        commands = []
        commands.append(""configure session {}"".format(self.config_session))
        if replace:
            commands.append(""rollback clean-config"")

        if filename is not None:
            with open(filename, ""r"") as f:
                lines = f.readlines()
        else:
            if isinstance(config, list):
                lines = config
            else:
                lines = config.splitlines()

        for line in lines:
            line = line.strip()
            if line == """":
                continue
            if line.startswith(""!"") and not line.startswith(""!!""):
                continue
            commands.append(line)

        for start, depth in [
            (s, d) for (s, d) in self.HEREDOC_COMMANDS if s in commands
        ]:
            commands = self._multiline_convert(commands, start=start, depth=depth)

        commands = self._mode_comment_convert(commands)

        try:
            if self.eos_autoComplete is not None:
                self.device.run_commands(
                    commands,
                    autoComplete=self.eos_autoComplete,
                    fn0039_transform=self.fn0039_config,
                )
            else:
                self.device.run_commands(commands, fn0039_transform=self.fn0039_config)
        except pyeapi.eapilib.CommandError as e:
            self.discard_config()
            msg = str(e)
            if replace:
                raise ReplaceConfigException(msg)
            else:
                raise MergeConfigException(msg)",_557.py,19,"for line in lines:
    line = line.strip()
    if line == '':
        continue
    if line.startswith('!') and (not line.startswith('!!')):
        continue
    commands.append(line)",commands += [line.strip() for line in lines if line.strip()!='' and (not line.strip().startswith('!') or line.strip().startswith('!!')],nan
https://github.com/bhoov/exbert/tree/master/server/transformers/src/transformers/modeling_tf_transfo_xl.py,"def __init__(self, n_token, d_embed, d_proj, cutoffs, div_val=1, init_std=0.02, sample_softmax=False, **kwargs):
        super().__init__(**kwargs)

        self.n_token = n_token
        self.d_embed = d_embed
        self.init_std = init_std

        self.cutoffs = cutoffs + [n_token]
        self.div_val = div_val
        self.d_proj = d_proj

        self.emb_scale = d_proj ** 0.5

        self.cutoff_ends = [0] + self.cutoffs

        self.emb_layers = []
        self.emb_projs = []
        if div_val == 1:
            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint
        else:
            for i in range(len(self.cutoffs)):
                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]
                d_emb_i = d_embed // (div_val ** i)
                self.emb_layers.append(
                    tf.keras.layers.Embedding(
                        r_idx - l_idx,
                        d_emb_i,
                        embeddings_initializer=get_initializer(init_std),
                        name=""emb_layers_._{}"".format(i),
                    )
                )",_798.py,21,"for i in range(len(self.cutoffs)):
    (l_idx, r_idx) = (self.cutoff_ends[i], self.cutoff_ends[i + 1])
    d_emb_i = d_embed // div_val ** i
    self.emb_layers.append(tf.keras.layers.Embedding(r_idx - l_idx, d_emb_i, embeddings_initializer=get_initializer(init_std), name='emb_layers_._{}'.format(i)))","self.emb_layers = [tf.keras.layers.Embedding(
            self.cutoff_ends[i], self.cutoff_ends[i + 1][1] - self.cutoff_ends[i], self.cutoff_ends[i + 1][0],
            d_embed // (div_val ** i),
            embeddings_initializer=get_initializer(init_std),
            name=""emb_layers_._{}"".format(i),
        ) for i in range(len(self.cutoffs))]",nan
https://github.com/MhLiao/MaskTextSpotterV3/tree/master/maskrcnn_benchmark/utils/comm.py,"def reduce_dict(input_dict, average=True):
    """"""
    Args:
        input_dict (dict): all the values will be reduced
        average (bool): whether to do average or sum
    Reduce the values in the dictionary from all processes so that process with rank
    0 has the averaged results. Returns a dict with the same fields as
    input_dict, after reduction.
    """"""
    world_size = get_world_size()
    if world_size < 2:
        return input_dict
    with torch.no_grad():
        names = []
        values = []
        # sort the keys so that they are consistent across processes
        for k in sorted(input_dict.keys()):
            names.append(k)
            values.append(input_dict[k])
        values = torch.stack(values, dim=0)
        dist.reduce(values, dst=0)
        if dist.get_rank() == 0 and average:
            # only main process gets accumulated, so only divide by
            # world_size in this case
            values /= world_size
        reduced_dict = {k: v for k, v in zip(names, values)}
    return reduced_dict",_886.py,17,"for k in sorted(input_dict.keys()):
    names.append(k)
    values.append(input_dict[k])","names = [k for k in sorted(input_dict.keys())]
values = [input_dict[k] for k in sorted(input_dict.keys())]",nan
https://github.com/tensorlayer/tensorlayer/tree/master/examples/reinforcement_learning/tutorial_C51.py,"def _encode_sample(self, idxes):
        b_o, b_a, b_r, b_o_, b_d = [], [], [], [], []
        for i in idxes:
            o, a, r, o_, d = self._storage[i]
            b_o.append(o)
            b_a.append(a)
            b_r.append(r)
            b_o_.append(o_)
            b_d.append(d)
        return (
            np.stack(b_o).astype('float32') * ob_scale,
            np.stack(b_a).astype('int32'),
            np.stack(b_r).astype('float32'),
            np.stack(b_o_).astype('float32') * ob_scale,
            np.stack(b_d).astype('float32'),
        )",_1072.py,3,"for i in idxes:
    (o, a, r, o_, d) = self._storage[i]
    b_o.append(o)
    b_a.append(a)
    b_r.append(r)
    b_o_.append(o_)
    b_d.append(d)","b_o = [self._storage[I][0] for i in idxes]
b_a = [self._storage[I][1] for i in idxes]
b_r = [self._storage[I][2] for i in idxes] 
b_o_ = [self._storage[I][3] for i in idxes] 
b_d = [self._storage[I][4] for i in idxes]",同时对4个list相加
https://github.com/socialpoint-labs/sheetfu/tree/master/sheetfu/model.py,"def set_background(self, background_color, batch_to=None):
        backgrounds = list()
        for row in range(0, self.coordinates.number_of_rows):
            backgrounds.append(list())
            for column in range(0, self.coordinates.number_of_columns):
                backgrounds[row].append(background_color)

        return self.set_backgrounds(backgrounds, batch_to)",_1133.py,3,"for row in range(0, self.coordinates.number_of_rows):
    backgrounds.append(list())
    for column in range(0, self.coordinates.number_of_columns):
        backgrounds[row].append(background_color)","backgrounds = [[background_color for column in range(0, self.coordinates.number_of_columns)] for row in range(0, self.coordinates.number_of_rows)]","数据依赖
加本身判断
有点困难"
https://github.com/ansible/awx/tree/master/awx/conf/registry.py,"def get_registered_settings(self, category_slug=None, read_only=None, slugs_to_ignore=set()):
        setting_names = []
        if category_slug == 'user-defaults':
            category_slug = 'user'
        if category_slug == 'changed':
            category_slug = 'all'
        for setting, kwargs in self._registry.items():
            if category_slug not in {None, 'all', kwargs.get('category_slug', None)}:
                continue
            if kwargs.get('category_slug', None) in slugs_to_ignore:
                continue
            if read_only in {True, False} and kwargs.get('read_only', False) != read_only and setting != 'INSTALL_UUID':
                # Note: Doesn't catch fields that set read_only via __init__;
                # read-only field kwargs should always include read_only=True.
                continue
            setting_names.append(setting)
        return setting_names",_1195.py,7,"for (setting, kwargs) in self._registry.items():
    if category_slug not in {None, 'all', kwargs.get('category_slug', None)}:
        continue
    if kwargs.get('category_slug', None) in slugs_to_ignore:
        continue
    if read_only in {True, False} and kwargs.get('read_only', False) != read_only and (setting != 'INSTALL_UUID'):
        continue
    setting_names.append(setting)","setting_names =[setting
    for setting, kwargs in self._registry.items() 
    if category_slug in {None, 'all', kwargs.get('category_slug', None)} and kwargs.get('category_slug', None) not in slugs_to_ignore and (read_only not in {True, False} or  kwargs.get('read_only', False) == read_only or setting == ‘INSTALL_UUID')]",nan
https://github.com/Megvii-BaseDetection/cvpods/tree/master/cvpods/evaluation/lvis_evaluation.py,"def process(self, inputs, outputs):
        """"""
        Args:
            inputs: the inputs to a LVIS model (e.g., GeneralizedRCNN).
                It is a list of dict. Each dict corresponds to an image and
                contains keys like ""height"", ""width"", ""file_name"", ""image_id"".
            outputs: the outputs of a LVIS model. It is a list of dicts with key
                ""instances"" that contains :class:`Instances`.
        """"""
        for input, output in zip(inputs, outputs):
            prediction = {""image_id"": input[""image_id""]}

            if ""instances"" in output:
                instances = output[""instances""].to(self._cpu_device)
                prediction[""instances""] = instances_to_coco_json(instances, input[""image_id""])
            if ""proposals"" in output:
                prediction[""proposals""] = output[""proposals""].to(self._cpu_device)
            self._predictions.append(prediction)",_1219.py,10,"for (input, output) in zip(inputs, outputs):
    prediction = {'image_id': input['image_id']}
    if 'instances' in output:
        instances = output['instances'].to(self._cpu_device)
        prediction['instances'] = instances_to_coco_json(instances, input['image_id'])
    if 'proposals' in output:
        prediction['proposals'] = output['proposals'].to(self._cpu_device)
    self._predictions.append(prediction)","for (input, output) in zip(inputs, outputs):
    prediction = {'image_id': input['image_id']}
    if 'instances' in output:
        instances = output['instances'].to(self._cpu_device)
        prediction['instances'] = instances_to_coco_json(instances, input['image_id'])
    if 'proposals' in output:
        prediction['proposals'] = output['proposals'].to(self._cpu_device)
    self._predictions.append(prediction)",nan
https://github.com/tianweiy/CenterPoint/tree/master/det3d/utils/buildtools/command.py,"def __call__(self, *nodes):
        for node in nodes:
            self.prev.append(node)
            node.next.append(self)
        return self",_1257.py,2,"for node in nodes:
    self.prev.append(node)
    node.next.append(self)","self.prev += [node for node in nodes]
node.next += [self for node in nodes]",这个不确定数据类型
https://github.com/tensorflow/tfx/tree/master/tfx/orchestration/experimental/core/pipeline_state_test.py,"def test_initiate_node_start_stop(self, mock_time):
    mock_time.time.return_value = time.time()
    events = []

    def recorder(event):
      events.append(event)

    with event_observer.init(), self._mlmd_connection as m:
      event_observer.register_observer(recorder)

      pipeline = _test_pipeline('pipeline1', pipeline_nodes=['Trainer'])
      pipeline_uid = task_lib.PipelineUid.from_pipeline(pipeline)
      node_uid = task_lib.NodeUid(node_id='Trainer', pipeline_uid=pipeline_uid)
      with pstate.PipelineState.new(m, pipeline) as pipeline_state:
        with pipeline_state.node_state_update_context(node_uid) as node_state:
          node_state.update(pstate.NodeState.STARTING)
        node_state = pipeline_state.get_node_state(node_uid)
        self.assertEqual(pstate.NodeState.STARTING, node_state.state)

      # Reload from MLMD and verify node is started.
      with pstate.PipelineState.load(
          m, task_lib.PipelineUid.from_pipeline(pipeline)) as pipeline_state:
        node_state = pipeline_state.get_node_state(node_uid)
        self.assertEqual(pstate.NodeState.STARTING, node_state.state)

        # Set node state to STOPPING.
        status = status_lib.Status(
            code=status_lib.Code.ABORTED, message='foo bar')
        with pipeline_state.node_state_update_context(node_uid) as node_state:
          node_state.update(pstate.NodeState.STOPPING, status)
        node_state = pipeline_state.get_node_state(node_uid)
        self.assertEqual(pstate.NodeState.STOPPING, node_state.state)
        self.assertEqual(status, node_state.status)

      # Reload from MLMD and verify node is stopped.
      with pstate.PipelineState.load(
          m, task_lib.PipelineUid.from_pipeline(pipeline)) as pipeline_state:
        node_state = pipeline_state.get_node_state(node_uid)
        self.assertEqual(pstate.NodeState.STOPPING, node_state.state)
        self.assertEqual(status, node_state.status)

        # Set node state to STARTED.
        with pipeline_state.node_state_update_context(node_uid) as node_state:
          node_state.update(pstate.NodeState.STARTED)
        node_state = pipeline_state.get_node_state(node_uid)
        self.assertEqual(pstate.NodeState.STARTED, node_state.state)

      # Reload from MLMD and verify node is started.
      with pstate.PipelineState.load(
          m, task_lib.PipelineUid.from_pipeline(pipeline)) as pipeline_state:
        node_state = pipeline_state.get_node_state(node_uid)
        self.assertEqual(pstate.NodeState.STARTED, node_state.state)

      event_observer.testonly_wait()

      want = [
          event_observer.PipelineStarted(
              pipeline_state=None, pipeline_uid=pipeline_uid),
          event_observer.NodeStateChange(
              execution=None,
              pipeline_uid=pipeline_uid,
              pipeline_run=None,
              node_id='Trainer',
              old_state=pstate.NodeState(state='started'),
              new_state=pstate.NodeState(
                  state='starting',
                  state_history=[
                      pstate.StateRecord(
                          state=pstate.NodeState.STARTED,
                          status_code=None,
                          update_time=mock_time.time.return_value)
                  ])),
          event_observer.NodeStateChange(
              execution=None,
              pipeline_uid=pipeline_uid,
              pipeline_run=None,
              node_id='Trainer',
              old_state=pstate.NodeState(
                  state='starting',
                  state_history=[
                      pstate.StateRecord(
                          state=pstate.NodeState.STARTED,
                          status_code=None,
                          update_time=mock_time.time.return_value)
                  ]),
              new_state=pstate.NodeState(
                  state='stopping',
                  status_code=status_lib.Code.ABORTED,
                  status_msg='foo bar',
                  state_history=[
                      pstate.StateRecord(
                          state=pstate.NodeState.STARTED,
                          status_code=None,
                          update_time=mock_time.time.return_value),
                      pstate.StateRecord(
                          state=pstate.NodeState.STARTING,
                          status_code=None,
                          update_time=mock_time.time.return_value)
                  ])),
          event_observer.NodeStateChange(
              execution=None,
              pipeline_uid=pipeline_uid,
              pipeline_run=None,
              node_id='Trainer',
              old_state=pstate.NodeState(
                  state='stopping',
                  status_code=status_lib.Code.ABORTED,
                  status_msg='foo bar',
                  state_history=[
                      pstate.StateRecord(
                          state=pstate.NodeState.STARTED,
                          status_code=None,
                          update_time=mock_time.time.return_value),
                      pstate.StateRecord(
                          state=pstate.NodeState.STARTING,
                          status_code=None,
                          update_time=mock_time.time.return_value)
                  ]),
              new_state=pstate.NodeState(
                  state='started',
                  state_history=[
                      pstate.StateRecord(
                          state=pstate.NodeState.STARTED,
                          status_code=None,
                          update_time=mock_time.time.return_value),
                      pstate.StateRecord(
                          state=pstate.NodeState.STARTING,
                          status_code=None,
                          update_time=mock_time.time.return_value),
                      pstate.StateRecord(
                          state=pstate.NodeState.STOPPING,
                          status_code=status_lib.Code.ABORTED,
                          update_time=mock_time.time.return_value)
                  ])),
      ]
      # Set execution / pipeline_state to None, so we don't compare those fields
      got = []
      for x in events:
        r = x
        if hasattr(x, 'execution'):
          r = dataclasses.replace(r, execution=None)
        if hasattr(x, 'pipeline_state'):
          r = dataclasses.replace(r, pipeline_state=None)
        got.append(r)

      self.assertListEqual(want, got)",_1346.py,138,"for x in events:
    r = x
    if hasattr(x, 'execution'):
        r = dataclasses.replace(r, execution=None)
    if hasattr(x, 'pipeline_state'):
        r = dataclasses.replace(r, pipeline_state=None)
    got.append(r)","got = [dataclasses.replace(dataclasses.replace(x, execution=None), pipeline_state=None) if hasattr(x, 'execution') and hasattr(x, 'pipeline_state')
       else dataclasses.replace(x, pipeline_state=None) if not hasattr(x, 'execution') and hasattr(x, 'pipeline_state')
       else dataclasses.replace(x, execution=None) if hasattr(x, 'execution') and not hasattr(x, 'pipeline_state')
       else x
    for x in events]","数据依赖
加本身判断
有点困难"
https://github.com/facebookresearch/ReAgent/tree/master/reagent/training/gradient_free/evolution_pool.py,"def __init__(
        self,
        seed: int,
        es_params: EvolutionParameters,
        tensor_sizes: Dict[str, List[int]],
    ) -> None:
        self.es_params = es_params
        self.tensor_sizes = tensor_sizes
        self.seed = seed
        assert self.seed < MAX_RNG_SEED, ""The random seed must be less than "" + str(
            MAX_RNG_SEED
        )
        logger.info(""Starting pool with RNG seed: "" + str(self.seed))

        # Fill the population with empty values: will populate later
        self.population_tensors: List[Dict[str, torch.Tensor]] = []
        for _ in range(es_params.population_size):
            individual = {}
            for tensor_name, tensor_size in self.tensor_sizes.items():
                individual[tensor_name] = torch.zeros(tensor_size, dtype=torch.float)
            self.population_tensors.append(individual)

        torch.manual_seed(self.seed)
        self.parent_tensors: Dict[str, torch.Tensor] = {}
        for tensor_name, tensor_size in self.tensor_sizes.items():
            self.parent_tensors[tensor_name] = torch.randn(
                tensor_size, dtype=torch.float
            )
            self.parent_tensors[tensor_name].grad = torch.randn(
                tensor_size, dtype=torch.float
            )

        self.optimizer = torch.optim.Adam(
            self.parent_tensors.values(), lr=self.es_params.learning_rate
        )

        self.populate_children(0)",_1484.py,17,"for _ in range(es_params.population_size):
    individual = {}
    for (tensor_name, tensor_size) in self.tensor_sizes.items():
        individual[tensor_name] = torch.zeros(tensor_size, dtype=torch.float)
    self.population_tensors.append(individual)","self.population_tensors= [ {tensor_name: torch.zeros(tensor_size, dtype=torch.float) for tensor_name, tensor_size in self.tensor_sizes.items()}",nan
https://github.com/lyft/cartography/tree/master/cartography/intel/pagerduty/escalation_policies.py,"def get_escalation_policies(pd_session: APISession) -> List[Dict[str, Any]]:
    all_escalation_policies: List[Dict[str, Any]] = []
    params = {""include[]"": [""services"", ""teams"", ""targets""]}
    for escalation_policy in pd_session.iter_all(""escalation_policies"", params=params):
        all_escalation_policies.append(escalation_policy)
    return all_escalation_policies",_1825.py,4,"for escalation_policy in pd_session.iter_all('escalation_policies', params=params):
    all_escalation_policies.append(escalation_policy)","all_escalation_policies= [escalation_policy for escalation_policy in pd_session.iter_all(""escalation_policies"", params=params)]

","List[Dict[str, Any]] = []"
https://github.com/emmett-framework/emmett/tree/master/emmett/orm/migrations/generation.py,"def tables(self):
        db_table_names = OrderedSet([t._tablename for t in self.db])
        meta_table_names = OrderedSet(list(self.meta.tables))
        #: new tables
        for table_name in db_table_names.difference(meta_table_names):
            meta_table = self._build_metatable(self.db[table_name])
            self.ops.append(CreateTableOp.from_table(meta_table))
            self.indexes_and_uniques(self.db[table_name], meta_table)
            self.foreign_keys(self.db[table_name], meta_table)
        #: removed tables
        for table_name in meta_table_names.difference(db_table_names):
            #: remove table indexes too
            metatable = self.meta.tables[table_name]
            for idx in metatable.indexes.values():
                self.ops.append(DropIndexOp.from_index(idx))
            #: remove table
            self.ops.append(DropTableOp.from_table(self.meta.tables[table_name]))
        #: existing tables
        for table_name in meta_table_names.intersection(db_table_names):
            self.columns(self.db[table_name], self.meta.tables[table_name])
            self.table(self.db[table_name], self.meta.tables[table_name])",_1896.py,14,"for idx in metatable.indexes.values():
    self.ops.append(DropIndexOp.from_index(idx))",self.ops += [DropIndexOp.from_index(idx) for idx in metatable.indexes.values()],"extend
不确定是否为list
需要结合上下问"
https://github.com/fonttools/fonttools/tree/master/Lib/fontTools/ttLib/tables/O_S_2f_2.py,"def _getUnicodeRanges():
	# build the ranges of codepoints for each unicode range bit, and cache result
	if not _unicodeStarts:
		unicodeRanges = [
			(start, (stop, bit)) for bit, blocks in enumerate(OS2_UNICODE_RANGES)
			for _, (start, stop) in blocks]
		for start, (stop, bit) in sorted(unicodeRanges):
			_unicodeStarts.append(start)
			_unicodeValues.append((stop, bit))
	return _unicodeStarts, _unicodeValues",_2116.py,7,"for (start, (stop, bit)) in sorted(unicodeRanges):
    _unicodeStarts.append(start)
    _unicodeValues.append((stop, bit))","_unicodeStarts = [start for (start, (stop, bit)) in sorted(unicodeRanges)]
_unicodeValues = [ (stop, bit) for (start, (stop, bit)) in sorted(unicodeRanges)]",multiple list need to have
https://github.com/scipy/scipy/tree/master/scipy/special/_generate_pyx.py,"def unique(lst):
    """"""
    Return a list without repeated entries (first occurrence is kept),
    preserving order.
    """"""
    seen = set()
    new_lst = []
    for item in lst:
        if item in seen:
            continue
        seen.add(item)
        new_lst.append(item)
    return new_lst",_2436.py,8,"for item in lst:
    if item in seen:
        continue
    seen.add(item)
    new_lst.append(item)","seen = {item for item in lst if item not in seen}
new_lst = [item for item in lst if item not in seen]",multiple list/set need
https://github.com/qq547276542/Agriculture_KnowledgeGraph/tree/master/demo/Model/neo_models.py,"def findRelationByEntities(self,entity1,entity2):
		answer = self.graph.run(""MATCH (p1:HudongItem {title:\"""" + str(entity1) + ""\""}),(p2:HudongItem{title:\""""+str(entity2)+""\""}),p=shortestpath((p1)-[rel:RELATION*]-(p2)) RETURN rel"").evaluate()
		#answer = self.graph.run(""MATCH (p1:HudongItem {title:\"""" + entity1 + ""\""})-[rel:RELATION]-(p2:HudongItem{title:\""""+entity2+""\""}) RETURN p1,p2"").data()
		
		if(answer is None):	
			answer = self.graph.run(""MATCH (p1:HudongItem {title:\"""" + str(entity1) + ""\""}),(p2:NewNode {title:\""""+str(entity2)+""\""}),p=shortestpath((p1)-[rel:RELATION*]-(p2)) RETURN p"").evaluate()
		if(answer is None):
			answer = self.graph.run(""MATCH (p1:NewNode {title:\"""" + str(entity1) + ""\""}),(p2:HudongItem{title:\""""+str(entity2)+""\""}),p=shortestpath((p1)-[rel:RELATION*]-(p2)) RETURN p"").evaluate()
		if(answer is None):
			answer = self.graph.run(""MATCH (p1:NewNode {title:\"""" + str(entity1) + ""\""}),(p2:NewNode {title:\""""+str(entity2)+""\""}),p=shortestpath((p1)-[rel:RELATION*]-(p2)) RETURN p"").evaluate()
		#answer = self.graph.data(""MATCH (n1:HudongItem {title:\"""" + entity1 + ""\""})- [rel] -> (n2:HudongItem{title:\""""+entity2+""\""}) RETURN n1,rel,n2"" )
		#if(answer is None):
		#	answer = self.graph.data(""MATCH (n1:HudongItem {title:\"""" + entity1 + ""\""})- [rel] -> (n2:NewNode{title:\""""+entity2+""\""}) RETURN n1,rel,n2"" )
		#if(answer is None):
		#	answer = self.graph.data(""MATCH (n1:NewNode {title:\"""" + entity1 + ""\""})- [rel] -> (n2:HudongItem{title:\""""+entity2+""\""}) RETURN n1,rel,n2"" )
		#if(answer is None):
		#	answer = self.graph.data(""MATCH (n1:NewNode {title:\"""" + entity1 + ""\""})- [rel] -> (n2:NewNode{title:\""""+entity2+""\""}) RETURN n1,rel,n2"" )
		relationDict = []
		if(answer is not None):
			for x in answer:
				tmp = {}
				start_node = x.start_node
				end_node = x.end_node
				tmp['n1'] = start_node
				tmp['n2'] = end_node
				tmp['rel'] = x
				relationDict.append(tmp)		
		return relationDict",_2736.py,20,"for x in answer:
    tmp = {}
    start_node = x.start_node
    end_node = x.end_node
    tmp['n1'] = start_node
    tmp['n2'] = end_node
    tmp['rel'] = x
    relationDict.append(tmp)","relationDict = [{'n1': x.start_node,'n2':x.end_node,'rel':x } for x in answer]",nan
https://github.com/3b1b/videos/tree/master/_2020/beta/beta2.py,"def construct(self):
        # Titles
        titles = VGroup(
            TexText(""Discrete context""),
            TexText(""Continuous context""),
        )
        titles.set_height(0.5)
        for title, vect in zip(titles, [LEFT, RIGHT]):
            title.move_to(vect * FRAME_WIDTH / 4)
            title.to_edge(UP, buff=MED_SMALL_BUFF)

        v_line = Line(UP, DOWN).set_height(FRAME_HEIGHT)
        h_line = Line(LEFT, RIGHT).set_width(FRAME_WIDTH)
        h_line.next_to(titles, DOWN)
        h_line.set_x(0)
        v_line.center()

        self.play(
            ShowCreation(VGroup(h_line, v_line)),
            LaggedStartMap(
                FadeInFrom, titles,
                lambda m: (m, -0.2 * m.get_center()[0] * RIGHT),
                run_time=1,
                lag_ratio=0.1,
            ),
        )
        self.wait()

        # Sum and int
        kw = {""tex_to_color_map"": {""S"": BLUE}}
        s_sym = Tex(""\\sum"", ""_{x \\in S} P(x)"", **kw)
        i_sym = Tex(""\\int_{S} p(x)"", ""\\text{d}x"", **kw)
        syms = VGroup(s_sym, i_sym)
        syms.scale(2)
        for sym, title in zip(syms, titles):
            sym.shift(-sym[-1].get_center())
            sym.match_x(title)

        arrow = Arrow(
            s_sym[0].get_corner(UP),
            i_sym[0].get_corner(UP),
            path_arc=-90 * DEGREES,
        )
        arrow.set_color(YELLOW)

        self.play(Write(s_sym, run_time=1))
        anims = [ShowCreation(arrow)]
        for i, j in [(0, 0), (2, 1), (3, 2)]:
            source = s_sym[i].deepcopy()
            target = i_sym[j]
            target.save_state()
            source.generate_target()
            target.replace(source, stretch=True)
            source.target.replace(target, stretch=True)
            target.set_opacity(0)
            source.target.set_opacity(0)
            anims += [
                Restore(target, path_arc=-60 * DEGREES),
                MoveToTarget(source, path_arc=-60 * DEGREES),
            ]
        self.play(LaggedStart(*anims))
        self.play(FadeInFromDown(i_sym[3]))
        self.add(i_sym)
        self.wait()
        self.play(
            FadeOut(arrow, UP),
            syms.next_to, h_line, DOWN, {""buff"": MED_LARGE_BUFF},
            syms.match_x, syms,
        )

        # Add curve area in editing
        # Add bar chart
        axes = Axes(
            x_min=0,
            x_max=10,
            y_min=0,
            y_max=7,
            y_axis_config={
                ""unit_size"": 0.75,
            }
        )
        axes.set_width(0.5 * FRAME_WIDTH - 1)
        axes.next_to(s_sym, DOWN)
        axes.y_axis.add_numbers(2, 4, 6)

        bars = VGroup()
        for x, y in [(1, 1), (4, 3), (7, 2)]:
            bar = Rectangle()
            bar.set_stroke(WHITE, 1)
            bar.set_fill(BLUE_D, 1)
            line = Line(axes.c2p(x, 0), axes.c2p(x + 2, y))
            bar.replace(line, stretch=True)
            bars.add(bar)

        addition_formula = Tex(*""1+3+2"")
        addition_formula.space_out_submobjects(2.1)
        addition_formula.next_to(bars, UP)

        for bar in bars:
            bar.save_state()
            bar.stretch(0, 1, about_edge=DOWN)

        self.play(
            Write(axes),
            LaggedStartMap(Restore, bars),
            LaggedStartMap(FadeInFromDown, addition_formula),
        )
        self.wait()

        # Confusion
        morty = Mortimer()
        morty.to_corner(DR)
        morty.look_at(i_sym)
        self.play(
            *map(FadeOut, [axes, bars, addition_formula]),
            FadeIn(morty)
        )
        self.play(morty.change, ""maybe"")
        self.play(Blink(morty))
        self.play(morty.change, ""confused"", i_sym.get_right())
        self.play(Blink(morty))
        self.wait()

        # Focus on integral
        self.play(
            Uncreate(VGroup(v_line, h_line)),
            FadeOut(titles, UP),
            FadeOut(morty, RIGHT),
            FadeOut(s_sym, LEFT),
            i_sym.center,
            i_sym.to_edge, LEFT
        )

        arrows = VGroup()
        for vect in [UP, DOWN]:
            corner = i_sym[-1].get_corner(RIGHT + vect)
            arrows.add(Arrow(
                corner,
                corner + 2 * RIGHT + 2 * vect,
                path_arc=-np.sign(vect[1]) * 60 * DEGREES,
            ))

        self.play(*map(ShowCreation, arrows))

        # Types of integration
        dist = scipy.stats.beta(7 + 1, 3 + 1)
        axes_pair = VGroup()
        graph_pair = VGroup()
        for arrow in arrows:
            axes = get_beta_dist_axes(y_max=5, y_unit=1)
            axes.set_width(4)
            axes.next_to(arrow.get_end(), RIGHT)
            graph = axes.get_graph(dist.pdf)
            graph.set_stroke(BLUE, 2)
            graph.set_fill(BLUE_E, 0)
            graph.make_smooth()
            axes_pair.add(axes)
            graph_pair.add(graph)

        r_axes, l_axes = axes_pair
        r_graph, l_graph = graph_pair
        r_name = TexText(""Riemann\\\\Integration"")
        r_name.next_to(r_axes, RIGHT)
        l_name = TexText(""Lebesgue\\\\Integration$^*$"")
        l_name.next_to(l_axes, RIGHT)
        footnote = TexText(""*a bit more complicated than\\\\these bars make it look"")
        footnote.match_width(l_name)
        footnote.next_to(l_name, DOWN)

        self.play(LaggedStart(
            FadeIn(r_axes),
            FadeIn(r_graph),
            FadeIn(r_name),
            FadeIn(l_axes),
            FadeIn(l_graph),
            FadeIn(l_name),
            run_time=1,
        ))

        # Approximation bars
        def get_riemann_rects(dx, axes=r_axes, func=dist.pdf):
            bars = VGroup()
            for x in np.arange(0, 1, dx):
                bar = Rectangle()
                line = Line(
                    axes.c2p(x, 0),
                    axes.c2p(x + dx, func(x)),
                )
                bar.replace(line, stretch=True)
                bar.set_stroke(BLUE_E, width=10 * dx, opacity=1)
                bar.set_fill(BLUE, 0.5)
                bars.add(bar)
            return bars

        def get_lebesgue_bars(dy, axes=l_axes, func=dist.pdf, mx=0.7, y_max=dist.pdf(0.7)):
            bars = VGroup()
            for y in np.arange(dy, y_max + dy, dy):
                x0 = binary_search(func, y, 0, mx) or mx
                x1 = binary_search(func, y, mx, 1) or mx
                line = Line(axes.c2p(x0, y - dy), axes.c2p(x1, y))
                bar = Rectangle()
                bar.set_stroke(RED_E, 0)
                bar.set_fill(RED_E, 0.5)
                bar.replace(line, stretch=True)
                bars.add(bar)
            return bars

        r_bar_groups = []
        l_bar_groups = []
        Ns = [10, 20, 40, 80, 160]
        Ms = [2, 4, 8, 16, 32]
        for N, M in zip(Ns, Ms):
            r_bar_groups.append(get_riemann_rects(dx=1 / N))
            l_bar_groups.append(get_lebesgue_bars(dy=1 / M))
        self.play(
            FadeIn(r_bar_groups[0], lag_ratio=0.1),
            FadeIn(l_bar_groups[0], lag_ratio=0.1),
            FadeIn(footnote),
        )
        self.wait()
        for rbg0, rbg1, lbg0, lbg1 in zip(r_bar_groups, r_bar_groups[1:], l_bar_groups, l_bar_groups[1:]):
            self.play(
                ReplacementTransform(
                    rbg0, rbg1,
                    lag_ratio=1 / len(rbg0),
                    run_time=2,
                ),
                ReplacementTransform(
                    lbg0, lbg1,
                    lag_ratio=1 / len(lbg0),
                    run_time=2,
                ),
            )
            self.wait()
        self.play(
            FadeOut(r_bar_groups[-1]),
            FadeOut(l_bar_groups[-1]),
            r_graph.set_fill, BLUE_E, 1,
            l_graph.set_fill, RED_E, 1,
        )",_2782.py,212,"for (N, M) in zip(Ns, Ms):
    r_bar_groups.append(get_riemann_rects(dx=1 / N))
    l_bar_groups.append(get_lebesgue_bars(dy=1 / M))","r_bar_groups =[get_riemann_rects(dx=1 / N) for N, M in zip(Ns, Ms)]
l_bar_groups =[get_lebesgue_bars(dy=1 / M) for N, M in zip(Ns, Ms)]",nan
https://github.com/neuralchen/SimSwap/tree/master/util/videoswap_multispecific.py,"def video_swap(video_path, target_id_norm_list,source_specific_id_nonorm_list,id_thres, swap_model, detect_model, save_path, temp_results_dir='./temp_results', crop_size=224, no_simswaplogo = False,use_mask =False):
    video_forcheck = VideoFileClip(video_path)
    if video_forcheck.audio is None:
        no_audio = True
    else:
        no_audio = False

    del video_forcheck

    if not no_audio:
        video_audio_clip = AudioFileClip(video_path)

    video = cv2.VideoCapture(video_path)
    logoclass = watermark_image('./simswaplogo/simswaplogo.png')
    ret = True
    frame_index = 0

    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))

    # video_WIDTH = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))

    # video_HEIGHT = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    fps = video.get(cv2.CAP_PROP_FPS)
    if  os.path.exists(temp_results_dir):
            shutil.rmtree(temp_results_dir)

    spNorm =SpecificNorm()
    mse = torch.nn.MSELoss().cuda()

    if use_mask:
        n_classes = 19
        net = BiSeNet(n_classes=n_classes)
        net.cuda()
        save_pth = os.path.join('./parsing_model/checkpoint', '79999_iter.pth')
        net.load_state_dict(torch.load(save_pth))
        net.eval()
    else:
        net =None

    # while ret:
    for frame_index in tqdm(range(frame_count)): 
        ret, frame = video.read()
        if  ret:
            detect_results = detect_model.get(frame,crop_size)

            if detect_results is not None:
                # print(frame_index)
                if not os.path.exists(temp_results_dir):
                        os.mkdir(temp_results_dir)
                frame_align_crop_list = detect_results[0]
                frame_mat_list = detect_results[1]

                id_compare_values = [] 
                frame_align_crop_tenor_list = []
                for frame_align_crop in frame_align_crop_list:

                    # BGR TO RGB
                    # frame_align_crop_RGB = frame_align_crop[...,::-1]

                    frame_align_crop_tenor = _totensor(cv2.cvtColor(frame_align_crop,cv2.COLOR_BGR2RGB))[None,...].cuda()

                    frame_align_crop_tenor_arcnorm = spNorm(frame_align_crop_tenor)
                    frame_align_crop_tenor_arcnorm_downsample = F.interpolate(frame_align_crop_tenor_arcnorm, size=(112,112))
                    frame_align_crop_crop_id_nonorm = swap_model.netArc(frame_align_crop_tenor_arcnorm_downsample)
                    id_compare_values.append([])
                    for source_specific_id_nonorm_tmp in source_specific_id_nonorm_list:
                        id_compare_values[-1].append(mse(frame_align_crop_crop_id_nonorm,source_specific_id_nonorm_tmp).detach().cpu().numpy())
                    frame_align_crop_tenor_list.append(frame_align_crop_tenor)

                id_compare_values_array = np.array(id_compare_values).transpose(1,0)
                min_indexs = np.argmin(id_compare_values_array,axis=0)
                min_value = np.min(id_compare_values_array,axis=0)

                swap_result_list = [] 
                swap_result_matrix_list = []
                swap_result_ori_pic_list = []
                for tmp_index, min_index in enumerate(min_indexs):
                    if min_value[tmp_index] < id_thres:
                        swap_result = swap_model(None, frame_align_crop_tenor_list[tmp_index], target_id_norm_list[min_index], None, True)[0]
                        swap_result_list.append(swap_result)
                        swap_result_matrix_list.append(frame_mat_list[tmp_index])
                        swap_result_ori_pic_list.append(frame_align_crop_tenor_list[tmp_index])
                    else:
                        pass



                if len(swap_result_list) !=0:
                    
                    reverse2wholeimage(swap_result_ori_pic_list,swap_result_list, swap_result_matrix_list, crop_size, frame, logoclass,\
                        os.path.join(temp_results_dir, 'frame_{:0>7d}.jpg'.format(frame_index)),no_simswaplogo,pasring_model =net,use_mask=use_mask, norm = spNorm)
                else:
                    if not os.path.exists(temp_results_dir):
                        os.mkdir(temp_results_dir)
                    frame = frame.astype(np.uint8)
                    if not no_simswaplogo:
                        frame = logoclass.apply_frames(frame)
                    cv2.imwrite(os.path.join(temp_results_dir, 'frame_{:0>7d}.jpg'.format(frame_index)), frame)

            else:
                if not os.path.exists(temp_results_dir):
                    os.mkdir(temp_results_dir)
                frame = frame.astype(np.uint8)
                if not no_simswaplogo:
                    frame = logoclass.apply_frames(frame)
                cv2.imwrite(os.path.join(temp_results_dir, 'frame_{:0>7d}.jpg'.format(frame_index)), frame)
        else:
            break

    video.release()

    # image_filename_list = []
    path = os.path.join(temp_results_dir,'*.jpg')
    image_filenames = sorted(glob.glob(path))

    clips = ImageSequenceClip(image_filenames,fps = fps)

    if not no_audio:
        clips = clips.set_audio(video_audio_clip)


    clips.write_videofile(save_path,audio_codec='aac')",_3580.py,67,"for source_specific_id_nonorm_tmp in source_specific_id_nonorm_list:
    id_compare_values[-1].append(mse(frame_align_crop_crop_id_nonorm, source_specific_id_nonorm_tmp).detach().cpu().numpy())","id_compare_values +=[mse(frame_align_crop_crop_id_nonorm, source_specific_id_nonorm_tmp).detach().cpu().numpy() for source_specific_id_nonorm_tmp in source_specific_id_nonorm_list]",这个虽然没有声明，但是数据类型是可以有的
https://github.com/sympy/sympy/tree/master/sympy/assumptions/facts.py,"def get_known_facts_keys():
    """"""
    Return every unary predicates registered to ``Q``.

    This function is used to generate the keys for
    ``generate_known_facts_dict``.

    """"""
    exclude = set()
    for pred in [Q.eq, Q.ne, Q.gt, Q.lt, Q.ge, Q.le]:
        # exclude polyadic predicates
        exclude.add(pred)

    result = []
    for attr in Q.__class__.__dict__:
        if attr.startswith('__'):
            continue
        pred = getattr(Q, attr)
        if pred in exclude:
            continue
        result.append(pred)
    return result",_3986.py,15,"for attr in Q.__class__.__dict__:
    if attr.startswith('__'):
        continue
    pred = getattr(Q, attr)
    if pred in exclude:
        continue
    result.append(pred)","result = [getattr(Q, attr) for attr in Q.__class__.__dict__ if not attr.startswith('__') and if getattr(Q, attr) not in exclude]",continue
https://github.com/scipy/scipy/tree/master/scipy/special/_generate_pyx.py,"def _get_codes(self):
        inarg_num, outarg_num = None, None
        all_inp, all_outp = [], []
        for _, inarg, outarg, ret, _ in self.signatures:
            outp = re.sub(r'\*.*', '', ret) + outarg
            if inarg_num is None:
                inarg_num = len(inarg)
                outarg_num = len(outp)
            inp, outp = list(iter_variants(inarg, outp))[0]
            all_inp.append(inp)
            all_outp.append(outp)

        incodes = []
        for n in range(inarg_num):
            codes = unique([x[n] for x in all_inp])
            codes.sort()
            incodes.append(''.join(codes))
        outcodes = []
        for n in range(outarg_num):
            codes = unique([x[n] for x in all_outp])
            codes.sort()
            outcodes.append(''.join(codes))

        return tuple(incodes), tuple(outcodes)",_4074.py,4,"for (_, inarg, outarg, ret, _) in self.signatures:
    outp = re.sub('\\*.*', '', ret) + outarg
    if inarg_num is None:
        inarg_num = len(inarg)
        outarg_num = len(outp)
    (inp, outp) = list(iter_variants(inarg, outp))[0]
    all_inp.append(inp)
    all_outp.append(outp)",incodes = [''.join(sorted(unique([x[n] for x in all_inp]))) for n in range(inarg_num)],multiple list need to have
https://github.com/shibing624/pycorrector/tree/master/pycorrector/utils/tokenizer.py,"def segment(sentence, cut_type='word', pos=False):
    """"""
    切词
    :param sentence:
    :param cut_type: 'word' use jieba.lcut; 'char' use list(sentence)
    :param pos: enable POS
    :return: list
    """"""
    if pos:
        if cut_type == 'word':
            word_pos_seq = posseg.lcut(sentence)
            word_seq, pos_seq = [], []
            for w, p in word_pos_seq:
                word_seq.append(w)
                pos_seq.append(p)
            return word_seq, pos_seq
        elif cut_type == 'char':
            word_seq = list(sentence)
            pos_seq = []
            for w in word_seq:
                w_p = posseg.lcut(w)
                pos_seq.append(w_p[0].flag)
            return word_seq, pos_seq
    else:
        if cut_type == 'word':
            return jieba.lcut(sentence)
        elif cut_type == 'char':
            return list(sentence)",_4177.py,13,"for (w, p) in word_pos_seq:
    word_seq.append(w)
    pos_seq.append(p)","word_seq= [w for (w, p) in word_pos_seq]
pos_seq = [p for (w, p) in word_pos_seq]",multiple list need to have
https://github.com/FormerLurker/Octolapse/tree/master/octoprint_octolapse/settings.py,"def get_profiles_dict(self):
        profiles_dict = {
            'current_printer_profile_guid': self.current_printer_profile_guid,
            'current_stabilization_profile_guid': self.current_stabilization_profile_guid,
            'current_trigger_profile_guid': self.current_trigger_profile_guid,
            'current_rendering_profile_guid': self.current_rendering_profile_guid,
            'current_camera_profile_guid': self.current_camera_profile_guid,
            'current_logging_profile_guid': self.current_logging_profile_guid,
            'printers': [],
            'stabilizations': [],
            'triggers': [],
            'renderings': [],
            'cameras': [],
            'logging': []
        }

        for key, printer in self.printers.items():
            profiles_dict[""printers""].append({
                ""name"": printer.name,
                ""guid"": printer.guid,
                ""description"": printer.description,
                ""has_been_saved_by_user"": printer.has_been_saved_by_user,
                ""slicer_type"": printer.slicer_type
            })

        for key, stabilization in self.stabilizations.items():
            profiles_dict[""stabilizations""].append({
                ""name"": stabilization.name,
                ""guid"": stabilization.guid,
                ""description"": stabilization.description,
                ""wait_for_moves_to_finish"": stabilization.wait_for_moves_to_finish
            })

        for key, trigger in self.triggers.items():
            profiles_dict[""triggers""].append({
                ""name"": trigger.name,
                ""guid"": trigger.guid,
                ""description"": trigger.description,
                ""trigger_type"": trigger.trigger_type
            })

        for key, rendering in self.renderings.items():
            profiles_dict[""renderings""].append({
                ""name"": rendering.name,
                ""guid"": rendering.guid,
                ""description"": rendering.description,
            })

        for key, camera in self.cameras.items():
            profiles_dict[""cameras""].append({
                ""name"": camera.name,
                ""guid"": camera.guid,
                ""description"": camera.description,
                ""enabled"": camera.enabled,
                ""enable_custom_image_preferences"": camera.enable_custom_image_preferences
            })

        for key, loggingProfile in self.logging.items():
            profiles_dict[""logging""].append({
                ""name"": loggingProfile.name,
                ""guid"": loggingProfile.guid,
                ""description"": loggingProfile.description
            })
        return profiles_dict",_4267.py,17,"for (key, printer) in self.printers.items():
    profiles_dict['printers'].append({'name': printer.name, 'guid': printer.guid, 'description': printer.description, 'has_been_saved_by_user': printer.has_been_saved_by_user, 'slicer_type': printer.slicer_type})","profiles_dict[‘printers'] = [{'name': printer.name, 'guid': printer.guid, 'description': printer.description, 'has_been_saved_by_user': printer.has_been_saved_by_user, 'slicer_type': printer.slicer_type} for (key, printer) in self.printers.items()]","函数定义在这里：profiles_dict = {'printers': [],
            'stabilizations': [],}"
https://github.com/FormerLurker/Octolapse/tree/master/octoprint_octolapse/settings.py,"def get_profiles_dict(self):
        profiles_dict = {
            'current_printer_profile_guid': self.current_printer_profile_guid,
            'current_stabilization_profile_guid': self.current_stabilization_profile_guid,
            'current_trigger_profile_guid': self.current_trigger_profile_guid,
            'current_rendering_profile_guid': self.current_rendering_profile_guid,
            'current_camera_profile_guid': self.current_camera_profile_guid,
            'current_logging_profile_guid': self.current_logging_profile_guid,
            'printers': [],
            'stabilizations': [],
            'triggers': [],
            'renderings': [],
            'cameras': [],
            'logging': []
        }

        for key, printer in self.printers.items():
            profiles_dict[""printers""].append({
                ""name"": printer.name,
                ""guid"": printer.guid,
                ""description"": printer.description,
                ""has_been_saved_by_user"": printer.has_been_saved_by_user,
                ""slicer_type"": printer.slicer_type
            })

        for key, stabilization in self.stabilizations.items():
            profiles_dict[""stabilizations""].append({
                ""name"": stabilization.name,
                ""guid"": stabilization.guid,
                ""description"": stabilization.description,
                ""wait_for_moves_to_finish"": stabilization.wait_for_moves_to_finish
            })

        for key, trigger in self.triggers.items():
            profiles_dict[""triggers""].append({
                ""name"": trigger.name,
                ""guid"": trigger.guid,
                ""description"": trigger.description,
                ""trigger_type"": trigger.trigger_type
            })

        for key, rendering in self.renderings.items():
            profiles_dict[""renderings""].append({
                ""name"": rendering.name,
                ""guid"": rendering.guid,
                ""description"": rendering.description,
            })

        for key, camera in self.cameras.items():
            profiles_dict[""cameras""].append({
                ""name"": camera.name,
                ""guid"": camera.guid,
                ""description"": camera.description,
                ""enabled"": camera.enabled,
                ""enable_custom_image_preferences"": camera.enable_custom_image_preferences
            })

        for key, loggingProfile in self.logging.items():
            profiles_dict[""logging""].append({
                ""name"": loggingProfile.name,
                ""guid"": loggingProfile.guid,
                ""description"": loggingProfile.description
            })
        return profiles_dict",_4267.py,26,"for (key, stabilization) in self.stabilizations.items():
    profiles_dict['stabilizations'].append({'name': stabilization.name, 'guid': stabilization.guid, 'description': stabilization.description, 'wait_for_moves_to_finish': stabilization.wait_for_moves_to_finish})","profiles_dict[‘stabilizations'] = [ {'name': stabilization.name, 'guid': stabilization.guid, 'description': stabilization.description, 'wait_for_moves_to_finish': stabilization.wait_for_moves_to_finish} for (key, stabilization) in self.stabilizations.items()]",nan
https://github.com/FormerLurker/Octolapse/tree/master/octoprint_octolapse/settings.py,"def get_profiles_dict(self):
        profiles_dict = {
            'current_printer_profile_guid': self.current_printer_profile_guid,
            'current_stabilization_profile_guid': self.current_stabilization_profile_guid,
            'current_trigger_profile_guid': self.current_trigger_profile_guid,
            'current_rendering_profile_guid': self.current_rendering_profile_guid,
            'current_camera_profile_guid': self.current_camera_profile_guid,
            'current_logging_profile_guid': self.current_logging_profile_guid,
            'printers': [],
            'stabilizations': [],
            'triggers': [],
            'renderings': [],
            'cameras': [],
            'logging': []
        }

        for key, printer in self.printers.items():
            profiles_dict[""printers""].append({
                ""name"": printer.name,
                ""guid"": printer.guid,
                ""description"": printer.description,
                ""has_been_saved_by_user"": printer.has_been_saved_by_user,
                ""slicer_type"": printer.slicer_type
            })

        for key, stabilization in self.stabilizations.items():
            profiles_dict[""stabilizations""].append({
                ""name"": stabilization.name,
                ""guid"": stabilization.guid,
                ""description"": stabilization.description,
                ""wait_for_moves_to_finish"": stabilization.wait_for_moves_to_finish
            })

        for key, trigger in self.triggers.items():
            profiles_dict[""triggers""].append({
                ""name"": trigger.name,
                ""guid"": trigger.guid,
                ""description"": trigger.description,
                ""trigger_type"": trigger.trigger_type
            })

        for key, rendering in self.renderings.items():
            profiles_dict[""renderings""].append({
                ""name"": rendering.name,
                ""guid"": rendering.guid,
                ""description"": rendering.description,
            })

        for key, camera in self.cameras.items():
            profiles_dict[""cameras""].append({
                ""name"": camera.name,
                ""guid"": camera.guid,
                ""description"": camera.description,
                ""enabled"": camera.enabled,
                ""enable_custom_image_preferences"": camera.enable_custom_image_preferences
            })

        for key, loggingProfile in self.logging.items():
            profiles_dict[""logging""].append({
                ""name"": loggingProfile.name,
                ""guid"": loggingProfile.guid,
                ""description"": loggingProfile.description
            })
        return profiles_dict",_4267.py,34,"for (key, trigger) in self.triggers.items():
    profiles_dict['triggers'].append({'name': trigger.name, 'guid': trigger.guid, 'description': trigger.description, 'trigger_type': trigger.trigger_type})","profiles_dict[‘triggers'] = [{'name': trigger.name, 'guid': trigger.guid, 'description': trigger.description, 'trigger_type': trigger.trigger_type} for (key, trigger) in self.triggers.items()]",nan
https://github.com/FormerLurker/Octolapse/tree/master/octoprint_octolapse/settings.py,"def get_profiles_dict(self):
        profiles_dict = {
            'current_printer_profile_guid': self.current_printer_profile_guid,
            'current_stabilization_profile_guid': self.current_stabilization_profile_guid,
            'current_trigger_profile_guid': self.current_trigger_profile_guid,
            'current_rendering_profile_guid': self.current_rendering_profile_guid,
            'current_camera_profile_guid': self.current_camera_profile_guid,
            'current_logging_profile_guid': self.current_logging_profile_guid,
            'printers': [],
            'stabilizations': [],
            'triggers': [],
            'renderings': [],
            'cameras': [],
            'logging': []
        }

        for key, printer in self.printers.items():
            profiles_dict[""printers""].append({
                ""name"": printer.name,
                ""guid"": printer.guid,
                ""description"": printer.description,
                ""has_been_saved_by_user"": printer.has_been_saved_by_user,
                ""slicer_type"": printer.slicer_type
            })

        for key, stabilization in self.stabilizations.items():
            profiles_dict[""stabilizations""].append({
                ""name"": stabilization.name,
                ""guid"": stabilization.guid,
                ""description"": stabilization.description,
                ""wait_for_moves_to_finish"": stabilization.wait_for_moves_to_finish
            })

        for key, trigger in self.triggers.items():
            profiles_dict[""triggers""].append({
                ""name"": trigger.name,
                ""guid"": trigger.guid,
                ""description"": trigger.description,
                ""trigger_type"": trigger.trigger_type
            })

        for key, rendering in self.renderings.items():
            profiles_dict[""renderings""].append({
                ""name"": rendering.name,
                ""guid"": rendering.guid,
                ""description"": rendering.description,
            })

        for key, camera in self.cameras.items():
            profiles_dict[""cameras""].append({
                ""name"": camera.name,
                ""guid"": camera.guid,
                ""description"": camera.description,
                ""enabled"": camera.enabled,
                ""enable_custom_image_preferences"": camera.enable_custom_image_preferences
            })

        for key, loggingProfile in self.logging.items():
            profiles_dict[""logging""].append({
                ""name"": loggingProfile.name,
                ""guid"": loggingProfile.guid,
                ""description"": loggingProfile.description
            })
        return profiles_dict",_4267.py,42,"for (key, rendering) in self.renderings.items():
    profiles_dict['renderings'].append({'name': rendering.name, 'guid': rendering.guid, 'description': rendering.description})","profiles_dict[‘renderings'] = [{'name': rendering.name, 'guid': rendering.guid, 'description': rendering.description} for (key, rendering) in self.renderings.items()]",nan
https://github.com/FormerLurker/Octolapse/tree/master/octoprint_octolapse/settings.py,"def get_profiles_dict(self):
        profiles_dict = {
            'current_printer_profile_guid': self.current_printer_profile_guid,
            'current_stabilization_profile_guid': self.current_stabilization_profile_guid,
            'current_trigger_profile_guid': self.current_trigger_profile_guid,
            'current_rendering_profile_guid': self.current_rendering_profile_guid,
            'current_camera_profile_guid': self.current_camera_profile_guid,
            'current_logging_profile_guid': self.current_logging_profile_guid,
            'printers': [],
            'stabilizations': [],
            'triggers': [],
            'renderings': [],
            'cameras': [],
            'logging': []
        }

        for key, printer in self.printers.items():
            profiles_dict[""printers""].append({
                ""name"": printer.name,
                ""guid"": printer.guid,
                ""description"": printer.description,
                ""has_been_saved_by_user"": printer.has_been_saved_by_user,
                ""slicer_type"": printer.slicer_type
            })

        for key, stabilization in self.stabilizations.items():
            profiles_dict[""stabilizations""].append({
                ""name"": stabilization.name,
                ""guid"": stabilization.guid,
                ""description"": stabilization.description,
                ""wait_for_moves_to_finish"": stabilization.wait_for_moves_to_finish
            })

        for key, trigger in self.triggers.items():
            profiles_dict[""triggers""].append({
                ""name"": trigger.name,
                ""guid"": trigger.guid,
                ""description"": trigger.description,
                ""trigger_type"": trigger.trigger_type
            })

        for key, rendering in self.renderings.items():
            profiles_dict[""renderings""].append({
                ""name"": rendering.name,
                ""guid"": rendering.guid,
                ""description"": rendering.description,
            })

        for key, camera in self.cameras.items():
            profiles_dict[""cameras""].append({
                ""name"": camera.name,
                ""guid"": camera.guid,
                ""description"": camera.description,
                ""enabled"": camera.enabled,
                ""enable_custom_image_preferences"": camera.enable_custom_image_preferences
            })

        for key, loggingProfile in self.logging.items():
            profiles_dict[""logging""].append({
                ""name"": loggingProfile.name,
                ""guid"": loggingProfile.guid,
                ""description"": loggingProfile.description
            })
        return profiles_dict",_4267.py,49,"for (key, camera) in self.cameras.items():
    profiles_dict['cameras'].append({'name': camera.name, 'guid': camera.guid, 'description': camera.description, 'enabled': camera.enabled, 'enable_custom_image_preferences': camera.enable_custom_image_preferences})","profiles_dict[‘cameras'] = [{'name': camera.name, 'guid': camera.guid, 'description': camera.description, 'enabled': camera.enabled, 'enable_custom_image_preferences': camera.enable_custom_image_preferences} for (key, camera) in self.cameras.items()]",nan
https://github.com/FormerLurker/Octolapse/tree/master/octoprint_octolapse/settings.py,"def get_profiles_dict(self):
        profiles_dict = {
            'current_printer_profile_guid': self.current_printer_profile_guid,
            'current_stabilization_profile_guid': self.current_stabilization_profile_guid,
            'current_trigger_profile_guid': self.current_trigger_profile_guid,
            'current_rendering_profile_guid': self.current_rendering_profile_guid,
            'current_camera_profile_guid': self.current_camera_profile_guid,
            'current_logging_profile_guid': self.current_logging_profile_guid,
            'printers': [],
            'stabilizations': [],
            'triggers': [],
            'renderings': [],
            'cameras': [],
            'logging': []
        }

        for key, printer in self.printers.items():
            profiles_dict[""printers""].append({
                ""name"": printer.name,
                ""guid"": printer.guid,
                ""description"": printer.description,
                ""has_been_saved_by_user"": printer.has_been_saved_by_user,
                ""slicer_type"": printer.slicer_type
            })

        for key, stabilization in self.stabilizations.items():
            profiles_dict[""stabilizations""].append({
                ""name"": stabilization.name,
                ""guid"": stabilization.guid,
                ""description"": stabilization.description,
                ""wait_for_moves_to_finish"": stabilization.wait_for_moves_to_finish
            })

        for key, trigger in self.triggers.items():
            profiles_dict[""triggers""].append({
                ""name"": trigger.name,
                ""guid"": trigger.guid,
                ""description"": trigger.description,
                ""trigger_type"": trigger.trigger_type
            })

        for key, rendering in self.renderings.items():
            profiles_dict[""renderings""].append({
                ""name"": rendering.name,
                ""guid"": rendering.guid,
                ""description"": rendering.description,
            })

        for key, camera in self.cameras.items():
            profiles_dict[""cameras""].append({
                ""name"": camera.name,
                ""guid"": camera.guid,
                ""description"": camera.description,
                ""enabled"": camera.enabled,
                ""enable_custom_image_preferences"": camera.enable_custom_image_preferences
            })

        for key, loggingProfile in self.logging.items():
            profiles_dict[""logging""].append({
                ""name"": loggingProfile.name,
                ""guid"": loggingProfile.guid,
                ""description"": loggingProfile.description
            })
        return profiles_dict",_4267.py,58,"for (key, loggingProfile) in self.logging.items():
    profiles_dict['logging'].append({'name': loggingProfile.name, 'guid': loggingProfile.guid, 'description': loggingProfile.description})","profiles_dict[‘logging’] = [{'name': loggingProfile.name, 'guid': loggingProfile.guid, 'description': loggingProfile.description} for (key, loggingProfile) in self.logging.items()]",nan
https://github.com/kizniche/Mycodo/tree/master/mycodo/mycodo_flask/utils/utils_general.py,"def custom_options_return_string(error, dict_options, mod_dev, request_form):
    # Custom options
    list_options = []

    # TODO: name same name in next major release
    if hasattr(mod_dev, 'device'):
        device = mod_dev.device
    elif hasattr(mod_dev, 'output_type'):
        device = mod_dev.output_type
    else:
        logger.error(""Unknown device"")
        return

    if 'custom_options' in dict_options[device]:
        for each_option in dict_options[device]['custom_options']:
            if 'id' not in each_option:
                continue

            null_value = True

            for key in request_form.keys():
                if each_option['id'] == key:
                    constraints_pass = True
                    constraints_errors = []
                    value = None

                    if each_option['type'] == 'float':
                        if str_is_float(request_form.get(key)):
                            if 'constraints_pass' in each_option:
                                (constraints_pass,
                                 constraints_errors,
                                 mod_dev) = each_option['constraints_pass'](
                                    mod_dev, float(request_form.get(key)))
                            if constraints_pass:
                                value = float(request_form.get(key))
                        elif 'required' in each_option and not each_option['required']:
                            value = None
                        else:
                            error.append(
                                ""{name} must represent a float/decimal value ""
                                ""(submitted '{value}')"".format(
                                    name=each_option['name'],
                                    value=request_form.get(key)))

                    elif each_option['type'] == 'integer':
                        if is_int(request_form.get(key)):
                            if 'constraints_pass' in each_option:
                                (constraints_pass,
                                 constraints_errors,
                                 mod_dev) = each_option['constraints_pass'](
                                    mod_dev, int(request_form.get(key)))
                            if constraints_pass:
                                value = int(request_form.get(key))
                        elif 'required' in each_option and not each_option['required']:
                            value = None
                        else:
                            error.append(
                                ""{name} must represent an integer value ""
                                ""(submitted '{value}')"".format(
                                    name=each_option['name'],
                                    value=request_form.get(key)))

                    elif each_option['type'] in [
                            'text',
                            'select',
                            'select_custom_choices',
                            'select_measurement',
                            'select_channel',
                            'select_measurement_channel',
                            'select_type_measurement',
                            'select_type_unit',
                            'select_device']:
                        if 'constraints_pass' in each_option:
                            (constraints_pass,
                             constraints_errors,
                             mod_dev) = each_option['constraints_pass'](
                                mod_dev, request_form.get(key))
                        if constraints_pass:
                            value = request_form.get(key)

                    elif each_option['type'] == 'select_multi_measurement':
                        if 'constraints_pass' in each_option:
                            (constraints_pass,
                             constraints_errors,
                             mod_dev) = each_option['constraints_pass'](
                                mod_dev, request_form.get(key))
                        if constraints_pass:
                            value = "","".join(request_form.getlist(key))

                    elif each_option['type'] == 'bool':
                        value = bool(request_form.get(key))

                    for each_error in constraints_errors:
                        error.append(
                            ""Error: {name}: {error}"".format(
                                name=each_option['name'],
                                error=each_error))

                    if value is not None:
                        null_value = False
                        option = '{id},{value}'.format(
                            id=key,
                            value=value)
                        list_options.append(option)

            if (request_form and
                    each_option['type'] == 'bool' and
                    each_option['id'] not in request_form.keys()):
                option = '{id},{value}'.format(id=each_option['id'], value=False)
                list_options.append(option)

            elif null_value:
                option = '{id},'.format(id=each_option['id'])
                list_options.append(option)

    return error, ';'.join(list_options)",_4308.py,93,"for each_error in constraints_errors:
    error.append('Error: {name}: {error}'.format(name=each_option['name'], error=each_error))","error +=['Error: {name}: {error}'.format(name=each_option['name'], error=each_error) for each_error in constraints_errors]","缺少数据类型的定义，不知道是否为list
extend"
https://github.com/alan-turing-institute/sktime/tree/master/sktime/transformations/panel/summarize/_extract.py,"def transform(self, X, y=None):
        """"""Transform X.
        Parameters
        ----------
        X : nested pandas DataFrame of shape [n_samples, n_columns]
            Nested dataframe with time-series in cells.
        Returns
        -------
        Xt : pandas DataFrame
          Transformed pandas DataFrame
        """"""

        # input checks
        self.check_is_fitted()
        X = check_X(X, enforce_univariate=True, coerce_to_pandas=True)

        # get column name
        column_name = X.columns[0]

        self._starts = []
        self._lengths = []

        # find plateaus (segments of the same value)
        for x in X.iloc[:, 0]:
            x = np.asarray(x)

            # find indices of transition
            if np.isnan(self.value):
                i = np.where(np.isnan(x), 1, 0)

            elif np.isinf(self.value):
                i = np.where(np.isinf(x), 1, 0)

            else:
                i = np.where(x == self.value, 1, 0)

            # pad and find where segments transition
            transitions = np.diff(np.hstack([0, i, 0]))

            # compute starts, ends and lengths of the segments
            starts = np.where(transitions == 1)[0]
            ends = np.where(transitions == -1)[0]
            lengths = ends - starts

            # filter out single points
            starts = starts[lengths >= self.min_length]
            lengths = lengths[lengths >= self.min_length]

            self._starts.append(starts)
            self._lengths.append(lengths)

        # put into dataframe
        Xt = pd.DataFrame()
        column_prefix = ""%s_%s"" % (
            column_name,
            ""nan"" if np.isnan(self.value) else str(self.value),
        )
        Xt[""%s_starts"" % column_prefix] = pd.Series(self._starts)
        Xt[""%s_lengths"" % column_prefix] = pd.Series(self._lengths)
        return Xt",_4650.py,24,"for x in X.iloc[:, 0]:
    x = np.asarray(x)
    if np.isnan(self.value):
        i = np.where(np.isnan(x), 1, 0)
    elif np.isinf(self.value):
        i = np.where(np.isinf(x), 1, 0)
    else:
        i = np.where(x == self.value, 1, 0)
    transitions = np.diff(np.hstack([0, i, 0]))
    starts = np.where(transitions == 1)[0]
    ends = np.where(transitions == -1)[0]
    lengths = ends - starts
    starts = starts[lengths >= self.min_length]
    lengths = lengths[lengths >= self.min_length]
    self._starts.append(starts)
    self._lengths.append(lengths)","self._starts =[starts[np.where(np.diff(np.hstack([0, np.where(np.isnan(np.asarray(x)), 1, 0), 0])) == -1)[0] - np.where(np.diff(np.hstack([0, np.where(np.isnan(np.asarray(x)), 1, 0), 0])) == 1)[0] >= self.min_length] if np.isnan(self.value)
               else starts[np.where(np.diff(np.hstack([0, np.where(np.isinf(np.asarray(x)), 1, 0), 0])) == -1)[0] - np.where(np.diff(np.hstack([0, np.where(np.isinf(np.asarray(x)), 1, 0), 0])) == 1)[0] >= self.min_length] if np.isinf(self.value)
               else starts[np.where( np.diff(np.hstack([0, np.where(np.asarray(x) == self.value, 1, 0), 0])) == -1)[0] - np.where( np.diff(np.hstack([0, np.where(np.asarray(x) == self.value, 1, 0), 0])) == 1)[0] >= self.min_length] for x in X.iloc[:, 0]]

self._lengths =[lengths[np.where(np.diff(np.hstack([0, np.where(np.isnan(np.asarray(x)), 1, 0), 0])) == -1)[0] - np.where(np.diff(np.hstack([0, np.where(np.isnan(np.asarray(x)), 1, 0), 0])) == 1)[0] >= self.min_length] if np.isnan(self.value)
                else lengths[np.where(np.diff(np.hstack([0, np.where(np.isinf(np.asarray(x)), 1, 0), 0])) == -1)[0] - np.where(np.diff(np.hstack([0, np.where(np.isinf(np.asarray(x)), 1, 0), 0])) == 1)[0] >= self.min_length] if np.isinf(self.value)
    else lengths[np.where(np.diff(np.hstack([0, np.where(np.asarray(x) == self.value, 1, 0), 0])) == -1)[0] - np.where(np.diff(np.hstack([0, np.where(np.asarray(x) == self.value, 1, 0), 0])) == 1)[0] >= self.min_length]
    for x in X.iloc[:, 0]])",multiple list need to have
https://github.com/biubug6/Face-Detector-1MB-with-landmark/tree/master/models/net_rfb.py,"def forward(self,inputs):
        detections = list()
        loc = list()
        conf = list()
        landm = list()

        x1 = self.conv1(inputs)
        x2 = self.conv2(x1)
        x3 = self.conv3(x2)
        x4 = self.conv4(x3)
        x5 = self.conv5(x4)
        x6 = self.conv6(x5)
        x7 = self.conv7(x6)
        x8 = self.conv8(x7)
        detections.append(x8)

        x9 = self.conv9(x8)
        x10 = self.conv10(x9)
        x11 = self.conv11(x10)
        detections.append(x11)

        x12 = self.conv12(x11)
        x13 = self.conv13(x12)
        detections.append(x13)

        x14= self.conv14(x13)
        detections.append(x14)

        for (x, l, c, lam) in zip(detections, self.loc, self.conf, self.landm):
            loc.append(l(x).permute(0, 2, 3, 1).contiguous())
            conf.append(c(x).permute(0, 2, 3, 1).contiguous())
            landm.append(lam(x).permute(0, 2, 3, 1).contiguous())

        bbox_regressions = torch.cat([o.view(o.size(0), -1, 4) for o in loc], 1)
        classifications = torch.cat([o.view(o.size(0), -1, 2) for o in conf], 1)
        ldm_regressions = torch.cat([o.view(o.size(0), -1, 10) for o in landm], 1)



        if self.phase == 'train':
            output = (bbox_regressions, classifications, ldm_regressions)
        else:
            output = (bbox_regressions, F.softmax(classifications, dim=-1), ldm_regressions)
        return output",_4869.py,29,"for (x, l, c, lam) in zip(detections, self.loc, self.conf, self.landm):
    loc.append(l(x).permute(0, 2, 3, 1).contiguous())
    conf.append(c(x).permute(0, 2, 3, 1).contiguous())
    landm.append(lam(x).permute(0, 2, 3, 1).contiguous())","loc= [l(x).permute(0, 2, 3, 1).contiguous() for (x, l, c, lam) in zip(detections, self.loc, self.conf, self.landm)]
conf= [c(x).permute(0, 2, 3, 1).contiguous() for (x, l, c, lam) in zip(detections, self.loc, self.conf, self.landm)]
landm= [lam(x).permute(0, 2, 3, 1).contiguous() for (x, l, c, lam) in zip(detections, self.loc, self.conf, self.landm)]","entries = list()
multiple list need to have"
https://github.com/pytorch/fairseq/tree/master/examples/speech_to_text/prep_mtedx_data.py,"def __init__(self, root: str, lang: str, split: str) -> None:
        assert split in self.SPLITS and lang in self.LANGPAIRS
        _root = Path(root) / f""{lang}"" / ""data"" / split
        wav_root, txt_root = _root / ""wav"", _root / ""txt""
        assert _root.is_dir() and wav_root.is_dir() and txt_root.is_dir()
        # Load audio segments
        try:
            import yaml
        except ImportError:
            print(
                ""Please install PyYAML to load the Multilingual TEDx YAML files""
            )
        with open(txt_root / f""{split}.yaml"") as f:
            segments = yaml.load(f, Loader=yaml.BaseLoader)
        # Load source and target utterances
        src, tgt = lang.split(""-"")
        for _lang in [src, tgt]:
            with open(txt_root / f""{split}.{_lang}"") as f:
                utterances = [r.strip() for r in f]
            assert len(segments) == len(utterances)
            for i, u in enumerate(utterances):
                segments[i][_lang] = u
        # Gather info
        self.data = []
        for wav_filename, _seg_group in groupby(segments, lambda x: x[""wav""]):
            wav_filename = wav_filename.replace("".wav"", "".flac"")
            wav_path = wav_root / wav_filename
            sample_rate = sf.info(wav_path.as_posix()).samplerate
            seg_group = sorted(_seg_group, key=lambda x: float(x[""offset""]))
            for i, segment in enumerate(seg_group):
                offset = int(float(segment[""offset""]) * sample_rate)
                n_frames = int(float(segment[""duration""]) * sample_rate)
                _id = f""{wav_path.stem}_{i}""
                self.data.append(
                    (
                        wav_path.as_posix(),
                        offset,
                        n_frames,
                        sample_rate,
                        segment[src],
                        segment[tgt],
                        segment[""speaker_id""],
                        tgt,
                        _id,
                    )
                )",_4943.py,30,"for (i, segment) in enumerate(seg_group):
    offset = int(float(segment['offset']) * sample_rate)
    n_frames = int(float(segment['duration']) * sample_rate)
    _id = f'{wav_path.stem}_{i}'
    self.data.append((wav_path.as_posix(), offset, n_frames, sample_rate, segment[src], segment[tgt], segment['speaker_id'], tgt, _id))","self.data += [(
            wav_path.as_posix(),
            int(float(segment[""offset""]) * sample_rate),
            int(float(segment[""duration""]) * sample_rate),
            sample_rate,
            segment[src],
            segment[tgt],
            segment[""speaker_id""],
            tgt,
            f""{wav_path.stem}_{i}"",
        ) for i, segment in enumerate(seg_group)]",数据依赖
https://github.com/RiotGames/leaguedirector/tree/master/leaguedirector/enable.py,"def findMacInstalled(paths):
    """"""
    Ask the mac system profiler to list all installed apps.
    """"""
    query = 'kMDItemCFBundleIdentifier==com.riotgames.leagueoflegends'
    for line in subprocess.check_output(['mdfind', query]).splitlines():
        paths.append(line.decode())",_4964.py,6,"for line in subprocess.check_output(['mdfind', query]).splitlines():
    paths.append(line.decode())","paths += [line.decode() for line in subprocess.check_output(['mdfind', query]).splitlines()]",缺少数据类型的定义，不知道是否为list
https://github.com/openstack/keystone/tree/master/keystone/tests/unit/assignment/test_backends.py,"def _handle_domain_spec(self, test_data, domain_spec):
        """"""Handle the creation of domains and their contents.

        domain_spec may either be a count of the number of empty domains to
        create, a dict describing the domain contents, or a list of
        domain_specs.

        In the case when a list is provided, this method calls itself
        recursively to handle the list elements.

        This method will insert any entities created into test_data

        """"""
        def _create_domain(domain_id=None):
            if domain_id is None:
                new_domain = unit.new_domain_ref()
                PROVIDERS.resource_api.create_domain(
                    new_domain['id'], new_domain
                )
                return new_domain
            else:
                # The test plan specified an existing domain to use
                return PROVIDERS.resource_api.get_domain(domain_id)

        def _create_entity_in_domain(entity_type, domain_id):
            """"""Create a user or group entity in the domain.""""""
            if entity_type == 'users':
                new_entity = unit.new_user_ref(domain_id=domain_id)
                new_entity = PROVIDERS.identity_api.create_user(new_entity)
            elif entity_type == 'groups':
                new_entity = unit.new_group_ref(domain_id=domain_id)
                new_entity = PROVIDERS.identity_api.create_group(new_entity)
            elif entity_type == 'roles':
                new_entity = self._create_role(domain_id=domain_id)
            else:
                # Must be a bad test plan
                raise exception.NotImplemented()
            return new_entity

        if isinstance(domain_spec, list):
            for x in domain_spec:
                self._handle_domain_spec(test_data, x)
        elif isinstance(domain_spec, dict):
            # If there is a domain ID specified, then use it
            the_domain = _create_domain(domain_spec.get('id'))
            test_data['domains'].append(the_domain)
            for entity_type, value in domain_spec.items():
                if entity_type == 'id':
                    # We already used this above to determine whether to
                    # use and existing domain
                    continue
                if entity_type == 'projects':
                    # If it's projects, we need to handle the potential
                    # specification of a project hierarchy
                    self._handle_project_spec(
                        test_data, the_domain['id'], value)
                else:
                    # It's a count of number of entities
                    for _ in range(value):
                        test_data[entity_type].append(
                            _create_entity_in_domain(
                                entity_type, the_domain['id']))
        else:
            for _ in range(domain_spec):
                test_data['domains'].append(_create_domain())",_5000.py,64,"for _ in range(domain_spec):
    test_data['domains'].append(_create_domain())",test_data[‘domains'] += [_create_domain() for _ in range(domain_spec)],缺少数据类型的定义，不知道是否为list
https://github.com/openstack/keystone/tree/master/keystone/tests/unit/assignment/test_backends.py,"def _handle_domain_spec(self, test_data, domain_spec):
        """"""Handle the creation of domains and their contents.

        domain_spec may either be a count of the number of empty domains to
        create, a dict describing the domain contents, or a list of
        domain_specs.

        In the case when a list is provided, this method calls itself
        recursively to handle the list elements.

        This method will insert any entities created into test_data

        """"""
        def _create_domain(domain_id=None):
            if domain_id is None:
                new_domain = unit.new_domain_ref()
                PROVIDERS.resource_api.create_domain(
                    new_domain['id'], new_domain
                )
                return new_domain
            else:
                # The test plan specified an existing domain to use
                return PROVIDERS.resource_api.get_domain(domain_id)

        def _create_entity_in_domain(entity_type, domain_id):
            """"""Create a user or group entity in the domain.""""""
            if entity_type == 'users':
                new_entity = unit.new_user_ref(domain_id=domain_id)
                new_entity = PROVIDERS.identity_api.create_user(new_entity)
            elif entity_type == 'groups':
                new_entity = unit.new_group_ref(domain_id=domain_id)
                new_entity = PROVIDERS.identity_api.create_group(new_entity)
            elif entity_type == 'roles':
                new_entity = self._create_role(domain_id=domain_id)
            else:
                # Must be a bad test plan
                raise exception.NotImplemented()
            return new_entity

        if isinstance(domain_spec, list):
            for x in domain_spec:
                self._handle_domain_spec(test_data, x)
        elif isinstance(domain_spec, dict):
            # If there is a domain ID specified, then use it
            the_domain = _create_domain(domain_spec.get('id'))
            test_data['domains'].append(the_domain)
            for entity_type, value in domain_spec.items():
                if entity_type == 'id':
                    # We already used this above to determine whether to
                    # use and existing domain
                    continue
                if entity_type == 'projects':
                    # If it's projects, we need to handle the potential
                    # specification of a project hierarchy
                    self._handle_project_spec(
                        test_data, the_domain['id'], value)
                else:
                    # It's a count of number of entities
                    for _ in range(value):
                        test_data[entity_type].append(
                            _create_entity_in_domain(
                                entity_type, the_domain['id']))
        else:
            for _ in range(domain_spec):
                test_data['domains'].append(_create_domain())",_5000.py,59,"for _ in range(value):
    test_data[entity_type].append(_create_entity_in_domain(entity_type, the_domain['id']))","test_data[entity_type] += [_create_entity_in_domain(entity_type, the_domain[‘id']) for _ in range(value)]",缺少数据类型的定义，不知道是否为list
https://github.com/tum-pbs/PhiFlow/tree/master/phi/math/backend/_backend.py,"def unstack(self, tensor, axis=0, keepdims=False) -> tuple:
        if axis < 0:
            axis += len(tensor.shape)
        if axis >= len(tensor.shape) or axis < 0:
            raise ValueError(""Illegal axis value"")
        result = []
        for slice_idx in range(tensor.shape[axis]):
            if keepdims:
                component = tensor[tuple([slice(slice_idx, slice_idx + 1) if d == axis else slice(None) for d in range(len(tensor.shape))])]
            else:
                component = tensor[tuple([slice_idx if d == axis else slice(None) for d in range(len(tensor.shape))])]
            result.append(component)
        return tuple(result)",_5027.py,7,"for slice_idx in range(tensor.shape[axis]):
    if keepdims:
        component = tensor[tuple([slice(slice_idx, slice_idx + 1) if d == axis else slice(None) for d in range(len(tensor.shape))])]
    else:
        component = tensor[tuple([slice_idx if d == axis else slice(None) for d in range(len(tensor.shape))])]
    result.append(component)","result = [tensor[tuple(
                [slice(slice_idx, slice_idx + 1) if d == axis else slice(None) for d in range(len(tensor.shape))])]
if keepdims else tensor[tuple([slice_idx if d == axis else slice(None) for d in range(len(tensor.shape))])]
for slice_idx in range(tensor.shape[axis])]",数据依赖
https://github.com/searx/searx/tree/master/searx/engines/google_scholar.py,"def response(resp):
    """"""Get response from google's search request""""""
    results = []

    detect_google_sorry(resp)

    # which subdomain ?
    # subdomain = resp.search_params.get('google_subdomain')

    # convert the text to dom
    dom = html.fromstring(resp.text)

    # parse results
    for result in eval_xpath_list(dom, '//div[@class=""gs_ri""]'):

        title = extract_text(eval_xpath(result, './h3[1]//a'))

        if not title:
            # this is a [ZITATION] block
            continue

        url = eval_xpath(result, './h3[1]//a/@href')[0]
        content = extract_text(eval_xpath(result, './div[@class=""gs_rs""]')) or ''

        pub_info = extract_text(eval_xpath(result, './div[@class=""gs_a""]'))
        if pub_info:
            content += ""[%s]"" % pub_info

        pub_type = extract_text(eval_xpath(result, './/span[@class=""gs_ct1""]'))
        if pub_type:
            title = title + "" "" + pub_type

        results.append({
            'url':      url,
            'title':    title,
            'content':  content,
        })

    # parse suggestion
    for suggestion in eval_xpath(dom, '//div[contains(@class, ""gs_qsuggest_wrap"")]//li//a'):
        # append suggestion
        results.append({'suggestion': extract_text(suggestion)})

    for correction in eval_xpath(dom, '//div[@class=""gs_r gs_pda""]/a'):
        results.append({'correction': extract_text(correction)})

    return results",_5422.py,14,"for result in eval_xpath_list(dom, '//div[@class=""gs_ri""]'):
    title = extract_text(eval_xpath(result, './h3[1]//a'))
    if not title:
        continue
    url = eval_xpath(result, './h3[1]//a/@href')[0]
    content = extract_text(eval_xpath(result, './div[@class=""gs_rs""]')) or ''
    pub_info = extract_text(eval_xpath(result, './div[@class=""gs_a""]'))
    if pub_info:
        content += '[%s]' % pub_info
    pub_type = extract_text(eval_xpath(result, './/span[@class=""gs_ct1""]'))
    if pub_type:
        title = title + ' ' + pub_type
    results.append({'url': url, 'title': title, 'content': content})","results = [
        'url':      eval_xpath(result, './h3[1]//a/@href')[0],
        'title':    extract_text(eval_xpath(result, './h3[1]//a')) + "" "" + pub_type if extract_text(eval_xpath(result, './/span[@class=""gs_ct1""]')) else title,
        'content':  content+ ""[%s]"" % pub_info if extract_text(eval_xpath(result, './div[@class=""gs_a""]')) else content,
    } for result in eval_xpath_list(dom, '//div[@class=""gs_ri""]') if extract_text(eval_xpath(result, './h3[1]//a'))]","continue
数据依赖"
https://github.com/PAIR-code/lit/tree/master/lit_nlp/components/word_replacer.py,"def _get_replacement_pattern(self,
                               replacements: Dict[Text, List[Text]],
                               ignore_casing: bool = True) -> Pattern[str]:
    r""""""Generate replacement pattern for whole word match.

    If the source word does not end or begin with non-word characters
    (e.g. punctuation) then do a whole word match by using the special word
    boundary character ""\b"". This allows us to replace whole words ignoring
    punctuation around them (e.g. ""cat."" becomes ""dog."" with rule ""cat""->""dog"").
    However, if the source word has a non-word character at its edges this
    fails. For example for the rule "".""-> "","" it would not find ""cat. "" as there
    is no boundary between ""."" and "" "". Therefore, for patterns with punctuation
    at the word boundaries, we ignore the whole word match and replace all
    instances. So ""cat.dog"" will become ""dogdog"" for ""cat.""->""dog"" instead of
    being ignored. Also ""."" -> "","" will replace all instances of ""."" with "","".

    Args:
      replacements: A dict of word replacements
      ignore_casing: Ignore casing for source words if True.

    Returns:
      regexp_pattern: Compiled regexp pattern used to find source words in
                      replacements.
    """"""
    re_strings = []
    for s in replacements:
      pattern_str = r'\b%s\b' % re.escape(s)
      # If the source word ends or begins with a non-word character (see above.)
      if not re.search(pattern_str, s):
        pattern_str = r'%s' % re.escape(s)
      re_strings.append(pattern_str)

    casing_flag = re.IGNORECASE if ignore_casing else 0

    return re.compile('|'.join(re_strings), casing_flag)",_5447.py,26,"for s in replacements:
    pattern_str = '\\b%s\\b' % re.escape(s)
    if not re.search(pattern_str, s):
        pattern_str = '%s' % re.escape(s)
    re_strings.append(pattern_str)","re_strings = [ r'%s' % re.escape(s) if not re.search(r'\b%s\b' % re.escape(s), s) else r'\b%s\b' % re.escape(s) for s in replacements]",数据依赖
https://github.com/lrvick/youtube-dl/tree/master/youtube_dl/extractor/googlesearch.py,"def _get_n_results(self, query, n):
        """"""Get a specified number of results for a query""""""

        entries = []
        res = {
            '_type': 'playlist',
            'id': query,
            'title': query,
        }

        for pagenum in itertools.count():
            webpage = self._download_webpage(
                'http://www.google.com/search',
                'gvsearch:' + query,
                note='Downloading result page %s' % (pagenum + 1),
                query={
                    'tbm': 'vid',
                    'q': query,
                    'start': pagenum * 10,
                    'hl': 'en',
                })

            for hit_idx, mobj in enumerate(re.finditer(
                    r'<h3 class=""r""><a href=""([^""]+)""', webpage)):

                # Skip playlists
                if not re.search(r'id=""vidthumb%d""' % (hit_idx + 1), webpage):
                    continue

                entries.append({
                    '_type': 'url',
                    'url': mobj.group(1)
                })

            if (len(entries) >= n) or not re.search(r'id=""pnnext""', webpage):
                res['entries'] = entries[:n]
                return res",_5569.py,23,"for (hit_idx, mobj) in enumerate(re.finditer('<h3 class=""r""><a href=""([^""]+)""', webpage)):
    if not re.search('id=""vidthumb%d""' % (hit_idx + 1), webpage):
        continue
    entries.append({'_type': 'url', 'url': mobj.group(1)})","entries =[{
        '_type': 'url',
        'url': mobj.group(1)
    } for hit_idx, mobj in enumerate(re.finditer(
        r'<h3 class=""r""><a href=""([^""]+)""', webpage)) if re.search(r'id=""vidthumb%d""' % (hit_idx + 1), webpage)]",continue
https://github.com/domlysz/BlenderGIS/tree/master/operators/io_import_shp.py,"def listObjects(self, context):
		objs = []
		for index, object in enumerate(bpy.context.scene.objects):
			if object.type == 'MESH':
				#put each object in a tuple (key, label, tooltip) and add this to the objects list
				objs.append((object.name, object.name, ""Object named "" + object.name))
		return objs",_144.py,3,"for (index, object) in enumerate(bpy.context.scene.objects):
    if object.type == 'MESH':
        objs.append((object.name, object.name, 'Object named ' + object.name))","objs = [(object.name, object.name, 'Object named ' + object.name) for (index, object) in enumerate(bpy.context.scene.objects) if object.type == 'MESH']",1.0
https://github.com/taigaio/taiga-back/tree/master/scripts/manage_translations.py,"def lang_stats(resources=None, languages=None):
    """"""
    Output language statistics of committed translation files for each catalog.
    If resources is provided, it should be a list of translation resource to
    limit the output (e.g. ['main', 'taiga']).
    """"""
    locale_dirs = _get_locale_dirs(resources)

    for name, dir_ in locale_dirs:
        print(""\nShowing translations stats for '{res}':"".format(res=name))

        langs = []
        for d in os.listdir(dir_):
            if not d.startswith('_') and os.path.isdir(os.path.join(dir_, d)):
                langs.append(d)
        langs = sorted(langs)

        for lang in langs:
            if languages and lang not in languages:
                continue

            # TODO: merge first with the latest en catalog
            p = Popen(""msgfmt -vc -o /dev/null {path}/{lang}/LC_MESSAGES/django.po"".format(path=dir_, lang=lang),
                      stdout=PIPE, stderr=PIPE, shell=True)
            output, errors = p.communicate()

            if p.returncode == 0:
                # msgfmt output stats on stderr
                print(""{0}: {1}"".format(lang, errors.strip().decode(""utf-8"")))
            else:
                print(""Errors happened when checking {0} translation for {1}:\n{2}"".format(lang, name, errors))",_278.py,13,"for d in os.listdir(dir_):
    if not d.startswith('_') and os.path.isdir(os.path.join(dir_, d)):
        langs.append(d)","langs = [d for d in os.listdir(dir_) if not d.startswith('_') and os.path.isdir(os.path.join(dir_, d))]",1.0
https://github.com/tmulc18/Distributed-TensorFlow-Guide/tree/master/SAGN/SAGN.py,"def assign_local_to_global(local_to_global):
	""""""Assigns global variable value to local variables.

	local_to_global : dictionary with corresponding global variable for local key
	""""""
	r= []
	for v in local_to_global.keys():
		r.append(tf.assign(local_to_global[v],v))
	with tf.control_dependencies(r):
		a = tf.no_op()
	return a",_349.py,7,"for v in local_to_global.keys():
    r.append(tf.assign(local_to_global[v], v))","r = [tf.assign(local_to_global[v], v) for v in local_to_global.keys()]",1.0
https://github.com/shenweichen/GraphEmbedding/tree/master/examples/struc2vec_flight.py,"def plot_embeddings(embeddings,):

    X, Y = read_node_label('../data/flight/labels-brazil-airports.txt',skip_head=True)



    emb_list = []

    for k in X:

        emb_list.append(embeddings[k])

    emb_list = np.array(emb_list)



    model = TSNE(n_components=2)

    node_pos = model.fit_transform(emb_list)



    color_idx = {}

    for i in range(len(X)):

        color_idx.setdefault(Y[i][0], [])

        color_idx[Y[i][0]].append(i)



    for c, idx in color_idx.items():

        plt.scatter(node_pos[idx, 0], node_pos[idx, 1], label=c)  # c=node_colors)

    plt.legend()

    plt.show()",_381.py,9,"for k in X:
    emb_list.append(embeddings[k])",emb_list = [embeddings[k] for k in X],1.0
https://github.com/nucleic/enaml/tree/master/enaml/qt/docking/q_dock_bar.py,"def dockBarContainers(self):
        """""" Get the containers held in the dock bars.

        Returns
        -------
        result : list
            A list of tuples of the form (container, position).

        """"""
        res = []
        for value in self._widgets.values():
            if isinstance(value, QDockBarItem):
                res.append((value.widget(), value.position()))
        return res",_482.py,11,"for value in self._widgets.values():
    if isinstance(value, QDockBarItem):
        res.append((value.widget(), value.position()))","res = [(value.widget(), value.position()) for value in self._widgets.values() if isinstance(value, QDockBarItem)]",1.0
https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/xpu/test_label_smooth_op_xpu.py,"def dynamic_create_class(self):
        base_class = self.TestLabelSmoothOp
        classes = []
        batch_sizes = [1, 5, 1024]
        label_dims = [1, 7, 12]
        for bs in batch_sizes:
            for label_dim in label_dims:
                class_name = (
                    'XPUTestLabelSmooth_' + str(bs) + ""_"" + str(label_dim)
                )
                attr_dict = {'batch_size': bs, 'label_dim': label_dim}
                classes.append([class_name, attr_dict])
        classes.append(['XPUTestLabelSmooth_3d', {'is_3d': True}])
        return base_class, classes",_502.py,6,"for bs in batch_sizes:
    for label_dim in label_dims:
        class_name = 'XPUTestLabelSmooth_' + str(bs) + '_' + str(label_dim)
        attr_dict = {'batch_size': bs, 'label_dim': label_dim}
        classes.append([class_name, attr_dict])","classes = [['XPUTestLabelSmooth_' + str(bs) + '_' + str(label_dim), {'batch_size': bs, 'label_dim': label_dim}] for bs in batch_sizes for label_dim in label_dims]",1.0
https://github.com/gavin66/proxy_list/tree/master/persistence/redis_impl.py,"def list(self, count=1, query=None, columns=None):
        query_list = list()
        if query:
            for k, v in query.items():
                if k in self._index_keys:
                    query_list.append('index_%s_%s' % (k, v))
            keys = list(self._client.sinter(query_list))
            keys.sort(key=lambda x: float(self._client.zscore('index_speed', x)))
            if isinstance(count, int):
                keys = keys[:count]
        else:
            start = 0
            if isinstance(count, str):
                count = None
                start = None
            keys = list(self._client.zrangebyscore('index_speed', '-inf', '+inf', start=start, num=count))
        proxies = []
        for key in keys:
            proxy = self._client.hgetall(key)
            if isinstance(columns, tuple) and len(columns):
                x = {}
                for k in columns:
                    if k.encode('utf-8') in proxy.keys():
                        x[k] = proxy[k.encode('utf-8')].decode('utf-8')
                proxies.append(x)
            elif isinstance(columns, str) and columns == 'all':
                # ip, port, country, address, anonymity, protocol, speed
                proxies.append({x.decode('utf-8'): y.decode('utf-8') for x, y in proxy.items()})
            else:
                proxies.append(
                    (proxy[b'ip'].decode('utf-8'), proxy[b'port'].decode('utf-8'))
                )
        return proxies",_565.py,4,"for (k, v) in query.items():
    if k in self._index_keys:
        query_list.append('index_%s_%s' % (k, v))","query_list = ['index_%s_%s' % (k, v) for (k, v) in query.items() if k in self._index_keys]",1.0
https://github.com/r9y9/deepvoice3_pytorch/tree/master/vctk_preprocess/extract_feats.py,"def extract_intermediate_features(wav_path, txt_path, keep_silences=False,
                                  full_features=False, ehmm_max_n_itr=1):
    basedir = os.getcwd()
    latest_feature_dir = ""latest_features""
    if not os.path.exists(latest_feature_dir):
        os.mkdir(latest_feature_dir)

    os.chdir(latest_feature_dir)
    latest_feature_dir = os.getcwd()

    if not os.path.exists(""merlin""):
        clone_cmd = ""git clone https://github.com/kastnerkyle/merlin""
        pe(clone_cmd, shell=True)

    if keep_silences:
        # REMOVE SILENCES TO MATCH JOSE PREPROC
        os.chdir(""merlin/src"")
        pe(""sed -i.bak -e '708,712d;' run_merlin.py"", shell=True)
        pe(""sed -i.bak -e '695,706d;' run_merlin.py"", shell=True)
        os.chdir(latest_feature_dir)

    os.chdir(""merlin"")
    merlin_dir = os.getcwd()
    os.chdir(""egs/build_your_own_voice/s1"")
    experiment_dir = os.getcwd()

    if not os.path.exists(""database""):
        print(""Creating database and copying in files"")
        pe(""bash -x 01_setup.sh my_new_voice 2>&1"", shell=True)

        # Copy in wav files
        wav_partial_path = wav_path  # vctkdir + ""wav48/""
        """"""
        subfolders = sorted(os.listdir(wav_partial_path))
        # only p294 for now...
        subfolders = subfolder_select(subfolders)
        os.chdir(""database/wav"")
        for sf in subfolders:
            wav_path = wav_partial_path + sf + ""/*.wav""
            pe(""cp %s ."" % wav_path, shell=True)
        """"""
        to_copy = os.listdir(wav_partial_path)
        if len([tc for tc in to_copy if tc[-4:] == "".wav""]) == 0:
            raise IOError(
                ""Unable to find any wav files in %s, make sure the filenames end in .wav!"" % wav_partial_path)
        os.chdir(""database/wav"")
        if wav_partial_path[-1] != ""/"":
            wav_partial_path = wav_partial_path + ""/""
        wav_match_path = wav_partial_path + ""*.wav""
        for fi in glob.glob(wav_match_path):
            pe(""echo %s; cp %s ."" % (fi, fi), shell=True)
        # THIS MAY FAIL IF TOO MANY WAV FILES
        # pe(""cp %s ."" % wav_match_path, shell=True)
        for f in os.listdir("".""):
            # This is only necessary because of corrupted files...
            fs, d = wavfile.read(f)
            wavfile.write(f, fs, d)

        # downsample the files
        get_sr_cmd = 'file `ls *.wav | head -n 1` | cut -d "" "" -f 12'
        sr = pe(get_sr_cmd, shell=True)
        sr_int = int(sr[0].strip())
        print(""Got samplerate {}, converting to 16000"".format(sr_int))
        # was assuming all were 48000
        convert = estdir + \
            ""bin/ch_wave $i -o tmp_$i -itype wav -otype wav -F 16000 -f {}"".format(sr_int)
        pe(""for i in *.wav; do echo %s; %s; mv tmp_$i $i; done"" % (convert, convert), shell=True)

        os.chdir(experiment_dir)
        txt_partial_path = txt_path  # vctkdir + ""txt/""
        """"""
        subfolders = sorted(os.listdir(txt_partial_path))
        # only p294 for now...
        subfolders = subfolder_select(subfolders)
        os.chdir(""database/txt"")
        for sf in subfolders:
            txt_path = txt_partial_path + sf + ""/*.txt""
            pe(""cp %s ."" % txt_path, shell=True)
        """"""
        os.chdir(""database/txt"")
        to_copy = os.listdir(txt_partial_path)
        if len([tc for tc in to_copy if tc[-4:] == "".txt""]) == 0:
            raise IOError(
                ""Unable to find any txt files in %s. Be sure the filenames end in .txt!"" % txt_partial_path)
        txt_match_path = txt_partial_path + ""/*.txt""
        for fi in glob.glob(txt_match_path):
            # escape string...
            fi = re.escape(fi)
            try:
                pe(""echo %s; cp %s ."" % (fi, fi), shell=True)
            except:
                from IPython import embed
                embed()
                raise ValueError()

        #pe(""cp %s ."" % txt_match_path, shell=True)

    do_state_align = False
    if do_state_align:
        raise ValueError(""Replace these lies with something that points at the right place"")
        os.chdir(merlin_dir)
        os.chdir(""misc/scripts/alignment/state_align"")
        pe(""bash -x setup.sh 2>&1"", shell=True)

        with open(""config.cfg"", ""r"") as f:
            config_lines = f.readlines()

        # replace FESTDIR with the correct path
        festdir_replace_line = None
        for n, l in enumerate(config_lines):
            if ""FESTDIR="" in l:
                festdir_replace_line = n
                break

        config_lines[festdir_replace_line] = ""FESTDIR=%s\n"" % festdir

        # replace HTKDIR with the correct path
        htkdir_replace_line = None
        for n, l in enumerate(config_lines):
            if ""HTKDIR="" in l:
                htkdir_replace_line = n
                break

        config_lines[htkdir_replace_line] = ""HTKDIR=%s\n"" % htkdir

        with open(""config.cfg"", ""w"") as f:
            f.writelines(config_lines)

        pe(""bash -x run_aligner.sh config.cfg 2>&1"", shell=True)
    else:
        os.chdir(merlin_dir)
        if not os.path.exists(""misc/scripts/alignment/phone_align/full-context-labels/full""):
            os.chdir(""misc/scripts/alignment/phone_align"")
            pe(""bash -x setup.sh 2>&1"", shell=True)

            with open(""config.cfg"", ""r"") as f:
                config_lines = f.readlines()

            # replace ESTDIR with the correct path
            estdir_replace_line = None
            for n, l in enumerate(config_lines):
                if ""ESTDIR="" in l and l[0] == ""E"":
                    estdir_replace_line = n
                    break

            config_lines[estdir_replace_line] = ""ESTDIR=%s\n"" % estdir

            # replace FESTDIR with the correct path
            festdir_replace_line = None
            for n, l in enumerate(config_lines):
                # EST/FEST
                if ""FESTDIR="" in l and l[0] == ""F"":
                    festdir_replace_line = n
                    break

            config_lines[festdir_replace_line] = ""FESTDIR=%s\n"" % festdir

            # replace FESTVOXDIR with the correct path
            festvoxdir_replace_line = None
            for n, l in enumerate(config_lines):
                if ""FESTVOXDIR="" in l:
                    festvoxdir_replace_line = n
                    break

            config_lines[festvoxdir_replace_line] = ""FESTVOXDIR=%s\n"" % festvoxdir

            with open(""config.cfg"", ""w"") as f:
                f.writelines(config_lines)

            with open(""run_aligner.sh"", ""r"") as f:
                run_aligner_lines = f.readlines()

            replace_line = None
            for n, l in enumerate(run_aligner_lines):
                if ""cp ../cmuarctic.data"" in l:
                    replace_line = n
                    break

            run_aligner_lines[replace_line] = ""cp ../txt.done.data etc/txt.done.data\n""

            # Make the txt.done.data file
            def format_info_tup(info_tup):
                return ""( "" + str(info_tup[0]) + ' ""' + info_tup[1] + '"" )\n'

            # Now we need to get the text info
            txt_partial_path = txt_path  # vctkdir + ""txt/""
            cwd = os.getcwd()
            out_path = ""txt.done.data""
            out_file = open(out_path, ""w"")
            """"""
            subfolders = sorted(os.listdir(txt_partial_path))
            # TODO: Avoid this truncation and have an option to select subfolder(s)...
            subfolders = subfolder_select(subfolders)

            txt_ids = []
            for sf in subfolders:
                print(""Processing subfolder %s"" % sf)
                txt_sf_path = txt_partial_path + sf + ""/""
                for txtpath in os.listdir(txt_sf_path):
                    full_txtpath = txt_sf_path + txtpath
                    with open(full_txtpath, 'r') as f:
                        r = f.readlines()
                        assert len(r) == 1
                        # remove txt extension
                        name = txtpath.split(""."")[0]
                        text = r[0].strip()
                        info_tup = (name, text)
                        txt_ids.append(name)
                        out_file.writelines(format_info_tup(info_tup))
            """"""
            txt_ids = []
            txt_l_path = txt_partial_path
            for txtpath in os.listdir(txt_l_path):
                print(""Processing %s"" % txtpath)
                full_txtpath = txt_l_path + txtpath
                name = txtpath.split(""."")[0]
                wavpath_matches = [fname.split(""."")[0] for fname in os.listdir(wav_partial_path)
                                   if name in fname]
                for name in wavpath_matches:
                    # Need an extra level here for pavoque :/
                    with open(full_txtpath, 'r') as f:
                        r = f.readlines()
                    if len(r) == 0:
                        continue
                    if len(r) != 1:
                        new_r = []
                        for ri in r:
                            if ri != ""\n"":
                                new_r.append(ri)
                        r = new_r
                    if len(r) != 1:
                        print(""Something wrong in text extraction, cowardly bailing to IPython"")
                        from IPython import embed
                        embed()
                        raise ValueError()
                    assert len(r) == 1
                    # remove txt extension
                    text = r[0].strip()
                    info_tup = (name, text)
                    txt_ids.append(name)
                    out_file.writelines(format_info_tup(info_tup))
            out_file.close()
            pe(""cp %s %s/txt.done.data"" % (out_path, latest_feature_dir),
               shell=True)
            os.chdir(cwd)

            replace_line = None
            for n, l in enumerate(run_aligner_lines):
                if ""cp ../slt_wav/*.wav"" in l:
                    replace_line = n
                    break

            run_aligner_lines[replace_line] = ""cp ../wav/*.wav wav\n""

            # Put wav file in the correct place
            wav_partial_path = experiment_dir + ""/database/wav""
            """"""
            subfolders = sorted(os.listdir(wav_partial_path))
            """"""
            if not os.path.exists(""wav""):
                os.mkdir(""wav"")
            cwd = os.getcwd()
            os.chdir(""wav"")
            """"""
            for sf in subfolders:
                wav_path = wav_partial_path + ""/*.wav""
                pe(""cp %s ."" % wav_path, shell=True)
            """"""
            wav_match_path = wav_partial_path + ""/*.wav""
            for fi in glob.glob(wav_match_path):
                fi = re.escape(fi)
                try:
                    pe(""echo %s; cp %s ."" % (fi, fi), shell=True)
                except:
                    from IPython import embed
                    embed()
                    raise ValueError()
                #pe(""echo %s; cp %s ."" % (fi, fi), shell=True)
            #pe(""cp %s ."" % wav_match_path, shell=True)
            os.chdir(cwd)

            replace_line = None
            for n, l in enumerate(run_aligner_lines):
                if ""cat cmuarctic.data |"" in l:
                    replace_line = n
                    break

            run_aligner_lines[replace_line] = 'cat txt.done.data | cut -d "" "" -f 2 > file_id_list.scp\n'

            # FIXME
            # Hackaround to avoid harcoded 30 in festivox do_ehmm
            if not full_features:
                bdir = os.getcwd()

                # need to hack up run_aligner more..
                # do setup manually
                pe(""mkdir cmu_us_slt_arctic"", shell=True)
                os.chdir(""cmu_us_slt_arctic"")

                pe(""%s/src/clustergen/setup_cg cmu us slt_arctic"" % festvoxdir, shell=True)

                pe(""cp ../txt.done.data etc/txt.done.data"", shell=True)
                wmp = ""../wav/*.wav""
                for fi in glob.glob(wmp):
                    fi = re.escape(fi)
                    try:
                        pe(""echo %s; cp %s wav/"" % (fi, fi), shell=True)
                    except:
                        from IPython import embed
                        embed()
                        raise ValueError()
                    #pe(""echo %s; cp %s wav/"" % (fi, fi), shell=True)
                #pe(""cp ../wav/*.wav wav/"", shell=True)

                # remove top part but keep cd call
                run_aligner_lines = run_aligner_lines[:13] + \
                    [""cd cmu_us_slt_arctic\n""] + run_aligner_lines[35:]

                '''
                # need to change do_build
                # NO LONGER NECESSARY DUE TO FESTIVAL DEPENDENCE ON FILENAME

                os.chdir(""bin"")
                with open(""do_build"", ""r"") as f:
                    do_build_lines = f.readlines()

                replace_line = None
                for n, l in enumerate(do_build_lines):
                    if ""$FESTVOXDIR/src/ehmm/bin/do_ehmm"" in l:
                        replace_line = n
                        break

                do_build_lines[replace_line] = ""   $FESTVOXDIR/src/ehmm/bin/do_ehmm\n""

                # FIXME Why does this hang when not overwritten???
                with open(""edit_do_build"", ""w"") as f:
                    f.writelines(do_build_lines)
                '''

                # need to change do_ehmm
                os.chdir(festvoxdir)
                os.chdir(""src/ehmm/bin/"")

                # this is to fix festival if we somehow kill in the middle of training :(
                # all due to festival's apparent dependence on name of script!
                # really, really, REALLY weird
                if os.path.exists(""do_ehmm.bak""):
                    with open(""do_ehmm.bak"", ""r"") as f:
                        fix = f.readlines()

                    with open(""do_ehmm"", ""w"") as f:
                        f.writelines(fix)

                with open(""do_ehmm"", ""r"") as f:
                    do_ehmm_lines = f.readlines()

                with open(""do_ehmm.bak"", ""w"") as f:
                    f.writelines(do_ehmm_lines)

                replace_line = None
                for n, l in enumerate(do_ehmm_lines):
                    if ""$EHMMDIR/bin/ehmm ehmm/etc/ph_list.int"" in l:
                        replace_line = n
                        break

                max_n_itr = ehmm_max_n_itr
                do_ehmm_lines[replace_line] = ""    $EHMMDIR/bin/ehmm ehmm/etc/ph_list.int ehmm/etc/txt.phseq.data.int 1 0 ehmm/binfeat scaledft ehmm/mod 0 0 0 %s $num_cpus\n"" % str(
                    max_n_itr)

                # depends on *name* of the script?????????
                with open(""do_ehmm"", ""w"") as f:
                    f.writelines(do_ehmm_lines)

                # need to edit run_aligner....
                dbn = ""do_build""
                # FIXME
                # WHY DOES IT DEPEND ON FILENAME????!!!!!??????
                # should be able to call only edit_do_build label
                # but hangs indefinitely...
                replace_line = None
                for n, l in enumerate(run_aligner_lines):
                    if ""./bin/do_build build_prompts"" in l:
                        replace_line = n
                        break
                run_aligner_lines[replace_line] = ""./bin/%s build_prompts\n"" % dbn

                replace_line = None
                for n, l in enumerate(run_aligner_lines):
                    if ""./bin/do_build label"" in l:
                        replace_line = n
                        break
                run_aligner_lines[replace_line] = ""./bin/%s label\n"" % dbn

                replace_line = None
                for n, l in enumerate(run_aligner_lines):
                    if ""./bin/do_build build_utts"" in l:
                        replace_line = n
                        break
                run_aligner_lines[replace_line] = ""./bin/%s build_utts\n"" % dbn
                os.chdir(bdir)

            with open(""edit_run_aligner.sh"", ""w"") as f:
                f.writelines(run_aligner_lines)

            # 2>&1 needed to make it work?? really sketchy
            pe(""bash -x edit_run_aligner.sh config.cfg 2>&1"", shell=True)

    # compile vocoder
    os.chdir(merlin_dir)
    # set it to run on cpu
    pe(""sed -i.bak -e s/MERLIN_THEANO_FLAGS=.*/MERLIN_THEANO_FLAGS='device=cpu,floatX=float32,on_unused_input=ignore'/g src/setup_env.sh"", shell=True)
    os.chdir(""tools"")
    if not os.path.exists(""SPTK-3.9""):
        pe(""bash -x compile_tools.sh 2>&1"", shell=True)

    # slt_arctic stuff
    os.chdir(merlin_dir)
    os.chdir(""egs/slt_arctic/s1"")

    # This madness due to autogen configs...
    pe(""bash -x scripts/setup.sh slt_arctic_full 2>&1"", shell=True)

    global_config_file = ""conf/global_settings.cfg""
    replace_write(global_config_file, ""Labels"", ""phone_align"", replace_line=""%s=%s\n"")
    replace_write(global_config_file, ""Train"", ""1132"", replace_line=""%s=%s\n"")
    replace_write(global_config_file, ""Valid"", ""0"", replace_line=""%s=%s\n"")
    replace_write(global_config_file, ""Test"", ""0"", replace_line=""%s=%s\n"")

    pe(""bash -x scripts/prepare_config_files.sh %s 2>&1"" % global_config_file, shell=True)
    pe(""bash -x scripts/prepare_config_files_for_synthesis.sh %s 2>&1"" % global_config_file, shell=True)
    # delete the setup lines from run_full_voice.sh
    pe(""sed -i.bak -e '11d;12d;13d' run_full_voice.sh"", shell=True)

    pushd = os.getcwd()
    os.chdir(""conf"")

    acoustic_conf = ""acoustic_slt_arctic_full.conf""
    replace_write(acoustic_conf, ""train_file_number"", ""1132"")
    replace_write(acoustic_conf, ""valid_file_number"", ""0"")
    replace_write(acoustic_conf, ""test_file_number"", ""0"")

    replace_write(acoustic_conf, ""label_type"", ""phone_align"")
    replace_write(acoustic_conf, ""subphone_feats"", ""coarse_coding"")
    replace_write(acoustic_conf, ""dmgc"", ""60"")
    replace_write(acoustic_conf, ""dbap"", ""1"")
    # hack this to add an extra line in the config
    replace_write(acoustic_conf, ""dlf0"", ""1\ndo_MLPG: False"")

    if not full_features:
        replace_write(acoustic_conf, ""warmup_epoch"", ""1"")
        replace_write(acoustic_conf, ""training_epochs"", ""1"")
    replace_write(acoustic_conf, ""TRAINDNN"", ""False"")
    replace_write(acoustic_conf, ""DNNGEN"", ""False"")
    replace_write(acoustic_conf, ""GENWAV"", ""False"")
    replace_write(acoustic_conf, ""CALMCD"", ""False"")

    duration_conf = ""duration_slt_arctic_full.conf""
    replace_write(duration_conf, ""train_file_number"", ""1132"")
    replace_write(duration_conf, ""valid_file_number"", ""0"")
    replace_write(duration_conf, ""test_file_number"", ""0"")
    replace_write(duration_conf, ""label_type"", ""phone_align"")
    replace_write(duration_conf, ""dur"", ""1"")
    if not full_features:
        replace_write(duration_conf, ""warmup_epoch"", ""1"")
        replace_write(duration_conf, ""training_epochs"", ""1"")

    replace_write(duration_conf, ""TRAINDNN"", ""False"")
    replace_write(duration_conf, ""DNNGEN"", ""False"")
    replace_write(duration_conf, ""CALMCD"", ""False"")

    os.chdir(pushd)
    if not os.path.exists(""slt_arctic_full_data""):
        pe(""bash -x run_full_voice.sh 2>&1"", shell=True)

    pe(""mv run_full_voice.sh.bak run_full_voice.sh"", shell=True)

    os.chdir(merlin_dir)
    os.chdir(""misc/scripts/vocoder/world"")

    with open(""extract_features_for_merlin.sh"", ""r"") as f:
        ex_lines = f.readlines()

    ex_line_replace = None
    for n, l in enumerate(ex_lines):
        if ""merlin_dir="" in l:
            ex_line_replace = n
            break

    ex_lines[ex_line_replace] = 'merlin_dir=""%s""' % merlin_dir

    ex_line_replace = None
    for n, l in enumerate(ex_lines):
        if ""wav_dir="" in l:
            ex_line_replace = n
            break

    ex_lines[ex_line_replace] = 'wav_dir=""%s""' % (experiment_dir + ""/database/wav"")

    with open(""edit_extract_features_for_merlin.sh"", ""w"") as f:
        f.writelines(ex_lines)

    pe(""bash -x edit_extract_features_for_merlin.sh 2>&1"", shell=True)

    os.chdir(basedir)
    os.chdir(""latest_features"")
    os.symlink(merlin_dir + ""/egs/slt_arctic/s1/slt_arctic_full_data/feat"", ""audio_feat"")
    os.symlink(merlin_dir + ""/misc/scripts/alignment/phone_align/full-context-labels/full"", ""text_feat"")

    print(""Audio features in %s (and %s)"" % (os.getcwd() + ""/audio_feat"",
                                             merlin_dir + ""/egs/slt_arctic/s1/slt_arctic_full_data/feat""))
    print(""Text features in %s (and %s)"" % (os.getcwd() + ""/text_feat"", merlin_dir +
                                            ""/misc/scripts/alignment/phone_align/full-context-labels/full""))
    os.chdir(basedir)",_794.py,227,"for ri in r:
    if ri != '\n':
        new_r.append(ri)",new_r = [ri for ri in r if ri != '\n'],1.0
https://github.com/bonzanini/Book-SocialMediaMiningPython/tree/master/Chap08/youtube_search_video_pagination.py,"def search_video(self, query, n_results):
        search = self.service.search()
        request = search.list(q=query,
                              part=""id,snippet"",
                              maxResults=n_results,
                              type='video')
        all_results = []
        while request and len(all_results) <= n_results:
            response = request.execute()
            try:
                for video in response['items']:
                    all_results.append(video)
            except KeyError:
                break
            request = search.list_next(request, response)
        return all_results[:n_results]",_1054.py,11,"for video in response['items']:
    all_results.append(video)",all_results.extend([video for video in response['items']]),1.0
https://github.com/3b1b/videos/tree/master/_2020/chess.py,"def construct(self):
        # Camera stuffs
        frame = self.camera.frame
        light = self.camera.light_source
        light.move_to([-25, -20, 20])

        # Setup cube
        colors = [RED, GREEN, BLUE_D, YELLOW]
        cube = self.get_hypercube()
        for n, vert in enumerate(cube.verts):
            code = boolian_linear_combo(int_to_bit_coords(n, 4))
            cube.verts[n].set_color(colors[code])

        # Create trees
        trees = Group()
        original_trees = Group()
        for vert in cube.verts:
            tree = Group(
                vert,
                vert.edges,
                vert.neighbors,
            ).copy()
            original = tree.copy()
            original[0].set_color(GREY)
            original[0].scale(0)
            original_trees.add(original)
            trees.add(tree)
        for tree in trees:
            tree[0].set_color(GREY)
            tree[0].rotate(90 * DEGREES, LEFT)
            sorted_verts = Group(*tree[2])
            sorted_verts.submobjects.sort(key=lambda m: m.get_color().hex)
            sorted_verts.arrange(DOWN, buff=SMALL_BUFF)
            sorted_verts.next_to(tree[0], RIGHT, buff=0.75)
            for edge, neighbor in zip(tree[1], tree[2]):
                edge.become(Line3D(
                    tree[0].get_center(),
                    neighbor.get_center(),
                    resolution=edge.resolution,
                ))
                neighbor.rotate(90 * DEGREES, LEFT)

        trees.arrange_in_grid(4, 4, buff=MED_LARGE_BUFF)
        for i in range(4):
            trees[i::4].shift(0.5 * i * RIGHT)
        trees.center()
        trees.set_height(6)
        trees.rotate(PI / 2, RIGHT)
        trees.move_to(10 * LEFT, LEFT)

        frame.set_phi(90 * DEGREES)
        frame.move_to(5 * LEFT)
        self.add(trees)
        self.wait()

        # Show transition
        anims = []
        for tree, original in zip(trees, original_trees):
            anims.append(Transform(tree, original))
        self.play(
            frame.set_euler_angles, 20 * DEGREES, 70 * DEGREES,
            frame.move_to, ORIGIN,
            LaggedStart(*anims, lag_ratio=0.2),
            run_time=8,
        )
        self.remove(trees)
        self.add(cube)
        frame.add_updater(lambda m, dt: m.increment_theta(2 * dt * DEGREES))
        self.wait(30)",_1137.py,58,"for (tree, original) in zip(trees, original_trees):
    anims.append(Transform(tree, original))","anims = [Transform(tree, original) for (tree, original) in zip(trees, original_trees)]",1.0
https://github.com/DingXiaoH/ACNet/tree/master/deprecated/constants.py,"def wrn_convert_flattened_deps(flattened):
    assert len(flattened) in [16, 28, 40]
    n = int((len(flattened) - 4) // 6)
    assert n in [2, 4, 6]
    pacesetters = wrn_pacesetter_idxes(n)
    result = [flattened[0]]
    for ps in pacesetters:
        assert flattened[ps] == flattened[ps+2]
        stage_deps = []
        for i in range(n):
            stage_deps.append([flattened[ps + 1 + 2 * i], flattened[ps + 2 + 2 * i]])
        result.append(stage_deps)
    return result",_1249.py,10,"for i in range(n):
    stage_deps.append([flattened[ps + 1 + 2 * i], flattened[ps + 2 + 2 * i]])","stage_deps = [[flattened[ps + 1 + 2 * i], flattened[ps + 2 + 2 * i]] for i in range(n)]",1.0
https://github.com/EducationalTestingService/skll/tree/master/tests/test_featureset.py,"def featureset_creation_from_dataframe_helper(with_labels, use_feature_hasher):
    """"""
    Helper function for the two unit tests for FeatureSet.from_data_frame().
    Since labels are optional, run two tests, one with, one without.
    """"""

    # First, setup the test data.
    # get a 100 instances with 4 features each
    X, y = make_classification(n_samples=100, n_features=4,
                               n_informative=4, n_redundant=0,
                               n_classes=3, random_state=1234567890)

    # Not using 0 - 100 here because that would be pandas' default index names anyway.
    # So let's make sure pandas is using the ids we supply.
    ids = list(range(100, 200))

    featureset_name = 'test'

    # if use_feature_hashing, run these tests with a vectorizer
    feature_bins = 4
    vectorizer = (FeatureHasher(n_features=feature_bins)
                  if use_feature_hasher else None)

    # convert the features into a list of dictionaries
    feature_names = [f'f{n}' for n in range(1, 5)]
    features = []
    for row in X:
        features.append(dict(zip(feature_names, row)))

    # Now, create a FeatureSet object.
    if with_labels:
        expected = FeatureSet(featureset_name, ids, features=features, labels=y,
                              vectorizer=vectorizer)
    else:
        expected = FeatureSet(featureset_name, ids, features=features,
                              vectorizer=vectorizer)

    # Also create a DataFrame and then create a FeatureSet from it.
    df = pd.DataFrame(features, index=ids)
    if with_labels:
        df['y'] = y
        current = FeatureSet.from_data_frame(df, featureset_name, labels_column='y',
                                             vectorizer=vectorizer)
    else:
        current = FeatureSet.from_data_frame(df, featureset_name, vectorizer=vectorizer)

    return expected, current",_1366.py,27,"for row in X:
    features.append(dict(zip(feature_names, row)))","features = [dict(zip(feature_names, row)) for row in X]",1.0
https://github.com/MushroomRL/mushroom-rl/tree/master/mushroom_rl/approximators/_implementations/ensemble.py,"def __init__(self, model, n_models, prediction='mean', **params):
        """"""
        Constructor.

        Args:
            approximator (class): the model class to approximate the
                Q-function.
            n_models (int): number of regressors in the ensemble;
            prediction (str, ['mean', 'sum', 'min', 'max']): the type of
                prediction to make;
            **params: parameters dictionary to create each regressor.

        """"""
        self._model = list()
        self._prediction = prediction

        for _ in range(n_models):
            self._model.append(model(**params))

        self._add_save_attr(
            _model=self._get_serialization_method(model),
            _prediction='primitive'
        )",_1377.py,17,"for _ in range(n_models):
    self._model.append(model(**params))",self._model.extend([model(**params) for _ in range(n_models)]),1.0
https://github.com/conda/conda/tree/master/conda/_vendor/cpuinfo.py,"def program_paths(program_name):
    paths = []
    exts = filter(None, os.environ.get('PATHEXT', '').split(os.pathsep))
    path = os.environ['PATH']
    for p in os.environ['PATH'].split(os.pathsep):
        p = os.path.join(p, program_name)
        if os.access(p, os.X_OK):
            paths.append(p)
        for e in exts:
            pext = p + e
            if os.access(pext, os.X_OK):
                paths.append(pext)
    return paths",_1529.py,9,"for e in exts:
    pext = p + e
    if os.access(pext, os.X_OK):
        paths.append(pext)","paths.extend([p + e for e in exts if os.access(p + e, os.X_OK)])",1.0
https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/resource/custom.py,"def get_deployment_operations_at_resource_group(client, resource_group_name, deployment_name, operation_ids):
    result = []
    for op_id in operation_ids:
        dep = client.get(resource_group_name, deployment_name, op_id)
        result.append(dep)
    return result",_1541.py,3,"for op_id in operation_ids:
    dep = client.get(resource_group_name, deployment_name, op_id)
    result.append(dep)","result = [client.get(resource_group_name, deployment_name, op_id) for op_id in operation_ids]",1.0
https://github.com/jay0lee/GAM/tree/master/src/gam/gapi/cloudidentity/groups.py,"def print_():
    ci = gapi_cloudidentity.build('cloudidentity_beta')
    i = 3
    members = False
    membersCountOnly = False
    managers = False
    managersCountOnly = False
    owners = False
    ownersCountOnly = False
    memberRestrictions = False
    gapi_directory_customer.setTrueCustomerId()
    parent = f'customers/{GC_Values[GC_CUSTOMER_ID]}'
    usemember = None
    memberDelimiter = '\n'
    todrive = False
    titles = []
    csvRows = []
    roles = []
    sortHeaders = False
    while i < len(sys.argv):
        myarg = sys.argv[i].lower()
        if myarg == 'todrive':
            todrive = True
            i += 1
        elif myarg == 'enterprisemember':
            member = gam.convertUIDtoEmailAddress(sys.argv[i + 1], email_types=['user', 'group'])
            usemember = f""member_key_id == '{member}' && 'cloudidentity.googleapis.com/groups.discussion_forum' in labels""
            i += 2
        elif myarg == 'delimiter':
            memberDelimiter = sys.argv[i + 1]
            i += 2
        elif myarg == 'sortheaders':
            sortHeaders = True
            i += 1
        elif myarg in ['members', 'memberscount']:
            roles.append(ROLE_MEMBER)
            members = True
            if myarg == 'memberscount':
                membersCountOnly = True
            i += 1
        elif myarg in ['owners', 'ownerscount']:
            roles.append(ROLE_OWNER)
            owners = True
            if myarg == 'ownerscount':
                ownersCountOnly = True
            i += 1
        elif myarg in ['managers', 'managerscount']:
            roles.append(ROLE_MANAGER)
            managers = True
            if myarg == 'managerscount':
                managersCountOnly = True
            i += 1
        elif myarg in ['memberrestrictions']:
            memberRestrictions = True
            display.add_titles_to_csv_file(
                    ['memberRestrictionQuery',],
                    titles)
            display.add_titles_to_csv_file(
                    ['memberRestrictionEvaluation',],
                    titles)
            i += 1
        else:
            controlflow.invalid_argument_exit(sys.argv[i], 'gam print cigroups')
    if roles:
        if members:
            display.add_titles_to_csv_file([
                'MembersCount',
            ], titles)
            if not membersCountOnly:
                display.add_titles_to_csv_file([
                    'Members',
                ], titles)
        if managers:
            display.add_titles_to_csv_file([
                'ManagersCount',
            ], titles)
            if not managersCountOnly:
                display.add_titles_to_csv_file([
                    'Managers',
                ], titles)
        if owners:
            display.add_titles_to_csv_file([
                'OwnersCount',
            ], titles)
            if not ownersCountOnly:
                display.add_titles_to_csv_file([
                    'Owners',
                ], titles)
    gam.printGettingAllItems('Groups', usemember)
    page_message = gapi.got_total_items_first_last_msg('Groups')
    if usemember:
        try:
            result = gapi.get_all_pages(ci.groups().memberships(),
                                        'searchTransitiveGroups',
                                        'memberships',
                                        throw_reasons=[gapi_errors.ErrorReason.FOUR_O_O],
                                        page_message=page_message,
                                        message_attribute=['groupKey', 'id'],
                                        parent='groups/-', query=usemember,
                                        fields='nextPageToken,memberships(group,groupKey(id),relationType)',
                                        pageSize=1000)
        except googleapiclient.errors.HttpError:
            controlflow.system_error_exit(
                2,
                'enterprisemember requires Enterprise license')
        entityList = []
        for entity in result:
            if entity['relationType'] == 'DIRECT':
                entityList.append(gapi.call(ci.groups(), 'get', name=entity['group']))
    else:
        entityList = gapi.get_all_pages(ci.groups(),
                                        'list',
                                        'groups',
                                        page_message=page_message,
                                        message_attribute=['groupKey', 'id'],
                                        parent=parent,
                                        view='FULL',
                                        pageSize=500)
    i = 0
    count = len(entityList)
    for groupEntity in entityList:
        i += 1
        groupEmail = groupEntity['groupKey']['id']
        for k, v in iter(groupEntity.pop('labels', {}).items()):
            if v == '':
                groupEntity[f'labels.{k}'] = True
            else:
                groupEntity[f'labels.{k}'] = v
        group = utils.flatten_json(groupEntity)
        for a_key in group:
            if a_key not in titles:
                titles.append(a_key)
        groupKey_id = groupEntity['name']
        if roles:
            sys.stderr.write(
                f' Getting {roles} for {groupEmail}{gam.currentCountNL(i, count)}'
            )
            page_message = gapi.got_total_items_first_last_msg('Members')
            validRoles, _, _ = gam._getRoleVerification(
                '.'.join(roles), 'nextPageToken,members(email,id,role)')
            groupMembers = gapi.get_all_pages(ci.groups().memberships(),
                                              'list',
                                              'memberships',
                                              page_message=page_message,
                                              message_attribute=['preferredMemberKey', 'id'],
                                              soft_errors=True,
                                              parent=groupKey_id,
                                              view='BASIC')
            if members:
                membersList = []
                membersCount = 0
            if managers:
                managersList = []
                managersCount = 0
            if owners:
                ownersList = []
                ownersCount = 0
            for member in groupMembers:
                member_email = member['preferredMemberKey']['id']
                role = get_single_role(member.get('roles', []))
                if not validRoles or role in validRoles:
                    if role == ROLE_MEMBER:
                        if members:
                            membersCount += 1
                            if not membersCountOnly:
                                membersList.append(member_email)
                    elif role == ROLE_MANAGER:
                        if managers:
                            managersCount += 1
                            if not managersCountOnly:
                                managersList.append(member_email)
                    elif role == ROLE_OWNER:
                        if owners:
                            ownersCount += 1
                            if not ownersCountOnly:
                                ownersList.append(member_email)
                    elif members:
                        membersCount += 1
                        if not membersCountOnly:
                            membersList.append(member_email)
            if members:
                group['MembersCount'] = membersCount
                if not membersCountOnly:
                    group['Members'] = memberDelimiter.join(membersList)
            if managers:
                group['ManagersCount'] = managersCount
                if not managersCountOnly:
                    group['Managers'] = memberDelimiter.join(managersList)
            if owners:
                group['OwnersCount'] = ownersCount
                if not ownersCountOnly:
                    group['Owners'] = memberDelimiter.join(ownersList)
        if memberRestrictions:
           name = f'{groupKey_id}/securitySettings'
           print(f'Getting member restrictions for {groupEmail} ({i}/{count}')
           sec_info = gapi.call(ci.groups(),
                                'getSecuritySettings',
                                name=name,
                                readMask='*')
           if 'memberRestriction' in sec_info:
               group['memberRestrictionQuery'] = sec_info['memberRestriction'].get('query', '')
               group['memberRestrictionEvaluation'] = sec_info['memberRestriction'].get('evaluation', {}).get('state', '')
        csvRows.append(group)
    if sortHeaders:
        display.sort_csv_titles([
            'name', 'groupKey.id'
        ], titles)
    display.write_csv_file(csvRows, titles, 'Groups', todrive)",_1576.py,107,"for entity in result:
    if entity['relationType'] == 'DIRECT':
        entityList.append(gapi.call(ci.groups(), 'get', name=entity['group']))","entityList = [gapi.call(ci.groups(), 'get', name=entity['group']) for entity in result if entity['relationType'] == 'DIRECT']",1.0
https://github.com/jay0lee/GAM/tree/master/src/gam/gapi/cloudidentity/groups.py,"def print_():
    ci = gapi_cloudidentity.build('cloudidentity_beta')
    i = 3
    members = False
    membersCountOnly = False
    managers = False
    managersCountOnly = False
    owners = False
    ownersCountOnly = False
    memberRestrictions = False
    gapi_directory_customer.setTrueCustomerId()
    parent = f'customers/{GC_Values[GC_CUSTOMER_ID]}'
    usemember = None
    memberDelimiter = '\n'
    todrive = False
    titles = []
    csvRows = []
    roles = []
    sortHeaders = False
    while i < len(sys.argv):
        myarg = sys.argv[i].lower()
        if myarg == 'todrive':
            todrive = True
            i += 1
        elif myarg == 'enterprisemember':
            member = gam.convertUIDtoEmailAddress(sys.argv[i + 1], email_types=['user', 'group'])
            usemember = f""member_key_id == '{member}' && 'cloudidentity.googleapis.com/groups.discussion_forum' in labels""
            i += 2
        elif myarg == 'delimiter':
            memberDelimiter = sys.argv[i + 1]
            i += 2
        elif myarg == 'sortheaders':
            sortHeaders = True
            i += 1
        elif myarg in ['members', 'memberscount']:
            roles.append(ROLE_MEMBER)
            members = True
            if myarg == 'memberscount':
                membersCountOnly = True
            i += 1
        elif myarg in ['owners', 'ownerscount']:
            roles.append(ROLE_OWNER)
            owners = True
            if myarg == 'ownerscount':
                ownersCountOnly = True
            i += 1
        elif myarg in ['managers', 'managerscount']:
            roles.append(ROLE_MANAGER)
            managers = True
            if myarg == 'managerscount':
                managersCountOnly = True
            i += 1
        elif myarg in ['memberrestrictions']:
            memberRestrictions = True
            display.add_titles_to_csv_file(
                    ['memberRestrictionQuery',],
                    titles)
            display.add_titles_to_csv_file(
                    ['memberRestrictionEvaluation',],
                    titles)
            i += 1
        else:
            controlflow.invalid_argument_exit(sys.argv[i], 'gam print cigroups')
    if roles:
        if members:
            display.add_titles_to_csv_file([
                'MembersCount',
            ], titles)
            if not membersCountOnly:
                display.add_titles_to_csv_file([
                    'Members',
                ], titles)
        if managers:
            display.add_titles_to_csv_file([
                'ManagersCount',
            ], titles)
            if not managersCountOnly:
                display.add_titles_to_csv_file([
                    'Managers',
                ], titles)
        if owners:
            display.add_titles_to_csv_file([
                'OwnersCount',
            ], titles)
            if not ownersCountOnly:
                display.add_titles_to_csv_file([
                    'Owners',
                ], titles)
    gam.printGettingAllItems('Groups', usemember)
    page_message = gapi.got_total_items_first_last_msg('Groups')
    if usemember:
        try:
            result = gapi.get_all_pages(ci.groups().memberships(),
                                        'searchTransitiveGroups',
                                        'memberships',
                                        throw_reasons=[gapi_errors.ErrorReason.FOUR_O_O],
                                        page_message=page_message,
                                        message_attribute=['groupKey', 'id'],
                                        parent='groups/-', query=usemember,
                                        fields='nextPageToken,memberships(group,groupKey(id),relationType)',
                                        pageSize=1000)
        except googleapiclient.errors.HttpError:
            controlflow.system_error_exit(
                2,
                'enterprisemember requires Enterprise license')
        entityList = []
        for entity in result:
            if entity['relationType'] == 'DIRECT':
                entityList.append(gapi.call(ci.groups(), 'get', name=entity['group']))
    else:
        entityList = gapi.get_all_pages(ci.groups(),
                                        'list',
                                        'groups',
                                        page_message=page_message,
                                        message_attribute=['groupKey', 'id'],
                                        parent=parent,
                                        view='FULL',
                                        pageSize=500)
    i = 0
    count = len(entityList)
    for groupEntity in entityList:
        i += 1
        groupEmail = groupEntity['groupKey']['id']
        for k, v in iter(groupEntity.pop('labels', {}).items()):
            if v == '':
                groupEntity[f'labels.{k}'] = True
            else:
                groupEntity[f'labels.{k}'] = v
        group = utils.flatten_json(groupEntity)
        for a_key in group:
            if a_key not in titles:
                titles.append(a_key)
        groupKey_id = groupEntity['name']
        if roles:
            sys.stderr.write(
                f' Getting {roles} for {groupEmail}{gam.currentCountNL(i, count)}'
            )
            page_message = gapi.got_total_items_first_last_msg('Members')
            validRoles, _, _ = gam._getRoleVerification(
                '.'.join(roles), 'nextPageToken,members(email,id,role)')
            groupMembers = gapi.get_all_pages(ci.groups().memberships(),
                                              'list',
                                              'memberships',
                                              page_message=page_message,
                                              message_attribute=['preferredMemberKey', 'id'],
                                              soft_errors=True,
                                              parent=groupKey_id,
                                              view='BASIC')
            if members:
                membersList = []
                membersCount = 0
            if managers:
                managersList = []
                managersCount = 0
            if owners:
                ownersList = []
                ownersCount = 0
            for member in groupMembers:
                member_email = member['preferredMemberKey']['id']
                role = get_single_role(member.get('roles', []))
                if not validRoles or role in validRoles:
                    if role == ROLE_MEMBER:
                        if members:
                            membersCount += 1
                            if not membersCountOnly:
                                membersList.append(member_email)
                    elif role == ROLE_MANAGER:
                        if managers:
                            managersCount += 1
                            if not managersCountOnly:
                                managersList.append(member_email)
                    elif role == ROLE_OWNER:
                        if owners:
                            ownersCount += 1
                            if not ownersCountOnly:
                                ownersList.append(member_email)
                    elif members:
                        membersCount += 1
                        if not membersCountOnly:
                            membersList.append(member_email)
            if members:
                group['MembersCount'] = membersCount
                if not membersCountOnly:
                    group['Members'] = memberDelimiter.join(membersList)
            if managers:
                group['ManagersCount'] = managersCount
                if not managersCountOnly:
                    group['Managers'] = memberDelimiter.join(managersList)
            if owners:
                group['OwnersCount'] = ownersCount
                if not ownersCountOnly:
                    group['Owners'] = memberDelimiter.join(ownersList)
        if memberRestrictions:
           name = f'{groupKey_id}/securitySettings'
           print(f'Getting member restrictions for {groupEmail} ({i}/{count}')
           sec_info = gapi.call(ci.groups(),
                                'getSecuritySettings',
                                name=name,
                                readMask='*')
           if 'memberRestriction' in sec_info:
               group['memberRestrictionQuery'] = sec_info['memberRestriction'].get('query', '')
               group['memberRestrictionEvaluation'] = sec_info['memberRestriction'].get('evaluation', {}).get('state', '')
        csvRows.append(group)
    if sortHeaders:
        display.sort_csv_titles([
            'name', 'groupKey.id'
        ], titles)
    display.write_csv_file(csvRows, titles, 'Groups', todrive)",_1576.py,130,"for a_key in group:
    if a_key not in titles:
        titles.append(a_key)",titles.extend([a_key for a_key in group if a_key not in titles]),1.0
https://github.com/JiaxuanYou/graph-generation/tree/master//create_graphs.py,"def create(args):
### load datasets
    graphs=[]
    # synthetic graphs
    if args.graph_type=='ladder':
        graphs = []
        for i in range(100, 201):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='ladder_small':
        graphs = []
        for i in range(2, 11):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='tree':
        graphs = []
        for i in range(2,5):
            for j in range(3,5):
                graphs.append(nx.balanced_tree(i,j))
        args.max_prev_node = 256
    elif args.graph_type=='caveman':
        # graphs = []
        # for i in range(5,10):
        #     for j in range(5,25):
        #         for k in range(5):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(30, 81):
                for k in range(10):
                    graphs.append(caveman_special(i,j, p_edge=0.3))
        args.max_prev_node = 100
    elif args.graph_type=='caveman_small':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(6, 11):
                for k in range(20):
                    graphs.append(caveman_special(i, j, p_edge=0.8)) # default 0.8
        args.max_prev_node = 20
    elif args.graph_type=='caveman_small_single':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(8, 9):
                for k in range(100):
                    graphs.append(caveman_special(i, j, p_edge=0.5))
        args.max_prev_node = 20
    elif args.graph_type.startswith('community'):
        num_communities = int(args.graph_type[-1])
        print('Creating dataset with ', num_communities, ' communities')
        c_sizes = np.random.choice([12, 13, 14, 15, 16, 17], num_communities)
        #c_sizes = [15] * num_communities
        for k in range(3000):
            graphs.append(n_community(c_sizes, p_inter=0.01))
        args.max_prev_node = 80
    elif args.graph_type=='grid':
        graphs = []
        for i in range(10,20):
            for j in range(10,20):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 40
    elif args.graph_type=='grid_small':
        graphs = []
        for i in range(2,5):
            for j in range(2,6):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 15
    elif args.graph_type=='barabasi':
        graphs = []
        for i in range(100,200):
             for j in range(4,5):
                 for k in range(5):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 130
    elif args.graph_type=='barabasi_small':
        graphs = []
        for i in range(4,21):
             for j in range(3,4):
                 for k in range(10):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 20
    elif args.graph_type=='grid_big':
        graphs = []
        for i in range(36, 46):
            for j in range(36, 46):
                graphs.append(nx.grid_2d_graph(i, j))
        args.max_prev_node = 90

    elif 'barabasi_noise' in args.graph_type:
        graphs = []
        for i in range(100,101):
            for j in range(4,5):
                for k in range(500):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        graphs = perturb_new(graphs,p=args.noise/10.0)
        args.max_prev_node = 99

    # real graphs
    elif args.graph_type == 'enzymes':
        graphs= Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        args.max_prev_node = 25
    elif args.graph_type == 'enzymes_small':
        graphs_raw = Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        graphs = []
        for G in graphs_raw:
            if G.number_of_nodes()<=20:
                graphs.append(G)
        args.max_prev_node = 15
    elif args.graph_type == 'protein':
        graphs = Graph_load_batch(min_num_nodes=20, name='PROTEINS_full')
        args.max_prev_node = 80
    elif args.graph_type == 'DD':
        graphs = Graph_load_batch(min_num_nodes=100, max_num_nodes=500, name='DD',node_attributes=False,graph_labels=True)
        args.max_prev_node = 230
    elif args.graph_type == 'citeseer':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=3)
            if G_ego.number_of_nodes() >= 50 and (G_ego.number_of_nodes() <= 400):
                graphs.append(G_ego)
        args.max_prev_node = 250
    elif args.graph_type == 'citeseer_small':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=1)
            if (G_ego.number_of_nodes() >= 4) and (G_ego.number_of_nodes() <= 20):
                graphs.append(G_ego)
        shuffle(graphs)
        graphs = graphs[0:200]
        args.max_prev_node = 15

    return graphs",_1750.py,7,"for i in range(100, 201):
    graphs.append(nx.ladder_graph(i))","graphs = [nx.ladder_graph(i) for i in range(100, 201)]",1.0
https://github.com/JiaxuanYou/graph-generation/tree/master//create_graphs.py,"def create(args):
### load datasets
    graphs=[]
    # synthetic graphs
    if args.graph_type=='ladder':
        graphs = []
        for i in range(100, 201):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='ladder_small':
        graphs = []
        for i in range(2, 11):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='tree':
        graphs = []
        for i in range(2,5):
            for j in range(3,5):
                graphs.append(nx.balanced_tree(i,j))
        args.max_prev_node = 256
    elif args.graph_type=='caveman':
        # graphs = []
        # for i in range(5,10):
        #     for j in range(5,25):
        #         for k in range(5):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(30, 81):
                for k in range(10):
                    graphs.append(caveman_special(i,j, p_edge=0.3))
        args.max_prev_node = 100
    elif args.graph_type=='caveman_small':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(6, 11):
                for k in range(20):
                    graphs.append(caveman_special(i, j, p_edge=0.8)) # default 0.8
        args.max_prev_node = 20
    elif args.graph_type=='caveman_small_single':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(8, 9):
                for k in range(100):
                    graphs.append(caveman_special(i, j, p_edge=0.5))
        args.max_prev_node = 20
    elif args.graph_type.startswith('community'):
        num_communities = int(args.graph_type[-1])
        print('Creating dataset with ', num_communities, ' communities')
        c_sizes = np.random.choice([12, 13, 14, 15, 16, 17], num_communities)
        #c_sizes = [15] * num_communities
        for k in range(3000):
            graphs.append(n_community(c_sizes, p_inter=0.01))
        args.max_prev_node = 80
    elif args.graph_type=='grid':
        graphs = []
        for i in range(10,20):
            for j in range(10,20):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 40
    elif args.graph_type=='grid_small':
        graphs = []
        for i in range(2,5):
            for j in range(2,6):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 15
    elif args.graph_type=='barabasi':
        graphs = []
        for i in range(100,200):
             for j in range(4,5):
                 for k in range(5):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 130
    elif args.graph_type=='barabasi_small':
        graphs = []
        for i in range(4,21):
             for j in range(3,4):
                 for k in range(10):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 20
    elif args.graph_type=='grid_big':
        graphs = []
        for i in range(36, 46):
            for j in range(36, 46):
                graphs.append(nx.grid_2d_graph(i, j))
        args.max_prev_node = 90

    elif 'barabasi_noise' in args.graph_type:
        graphs = []
        for i in range(100,101):
            for j in range(4,5):
                for k in range(500):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        graphs = perturb_new(graphs,p=args.noise/10.0)
        args.max_prev_node = 99

    # real graphs
    elif args.graph_type == 'enzymes':
        graphs= Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        args.max_prev_node = 25
    elif args.graph_type == 'enzymes_small':
        graphs_raw = Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        graphs = []
        for G in graphs_raw:
            if G.number_of_nodes()<=20:
                graphs.append(G)
        args.max_prev_node = 15
    elif args.graph_type == 'protein':
        graphs = Graph_load_batch(min_num_nodes=20, name='PROTEINS_full')
        args.max_prev_node = 80
    elif args.graph_type == 'DD':
        graphs = Graph_load_batch(min_num_nodes=100, max_num_nodes=500, name='DD',node_attributes=False,graph_labels=True)
        args.max_prev_node = 230
    elif args.graph_type == 'citeseer':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=3)
            if G_ego.number_of_nodes() >= 50 and (G_ego.number_of_nodes() <= 400):
                graphs.append(G_ego)
        args.max_prev_node = 250
    elif args.graph_type == 'citeseer_small':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=1)
            if (G_ego.number_of_nodes() >= 4) and (G_ego.number_of_nodes() <= 20):
                graphs.append(G_ego)
        shuffle(graphs)
        graphs = graphs[0:200]
        args.max_prev_node = 15

    return graphs",_1750.py,12,"for i in range(2, 11):
    graphs.append(nx.ladder_graph(i))","graphs = [nx.ladder_graph(i) for i in range(2, 11)]",1.0
https://github.com/JiaxuanYou/graph-generation/tree/master//create_graphs.py,"def create(args):
### load datasets
    graphs=[]
    # synthetic graphs
    if args.graph_type=='ladder':
        graphs = []
        for i in range(100, 201):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='ladder_small':
        graphs = []
        for i in range(2, 11):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='tree':
        graphs = []
        for i in range(2,5):
            for j in range(3,5):
                graphs.append(nx.balanced_tree(i,j))
        args.max_prev_node = 256
    elif args.graph_type=='caveman':
        # graphs = []
        # for i in range(5,10):
        #     for j in range(5,25):
        #         for k in range(5):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(30, 81):
                for k in range(10):
                    graphs.append(caveman_special(i,j, p_edge=0.3))
        args.max_prev_node = 100
    elif args.graph_type=='caveman_small':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(6, 11):
                for k in range(20):
                    graphs.append(caveman_special(i, j, p_edge=0.8)) # default 0.8
        args.max_prev_node = 20
    elif args.graph_type=='caveman_small_single':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(8, 9):
                for k in range(100):
                    graphs.append(caveman_special(i, j, p_edge=0.5))
        args.max_prev_node = 20
    elif args.graph_type.startswith('community'):
        num_communities = int(args.graph_type[-1])
        print('Creating dataset with ', num_communities, ' communities')
        c_sizes = np.random.choice([12, 13, 14, 15, 16, 17], num_communities)
        #c_sizes = [15] * num_communities
        for k in range(3000):
            graphs.append(n_community(c_sizes, p_inter=0.01))
        args.max_prev_node = 80
    elif args.graph_type=='grid':
        graphs = []
        for i in range(10,20):
            for j in range(10,20):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 40
    elif args.graph_type=='grid_small':
        graphs = []
        for i in range(2,5):
            for j in range(2,6):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 15
    elif args.graph_type=='barabasi':
        graphs = []
        for i in range(100,200):
             for j in range(4,5):
                 for k in range(5):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 130
    elif args.graph_type=='barabasi_small':
        graphs = []
        for i in range(4,21):
             for j in range(3,4):
                 for k in range(10):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 20
    elif args.graph_type=='grid_big':
        graphs = []
        for i in range(36, 46):
            for j in range(36, 46):
                graphs.append(nx.grid_2d_graph(i, j))
        args.max_prev_node = 90

    elif 'barabasi_noise' in args.graph_type:
        graphs = []
        for i in range(100,101):
            for j in range(4,5):
                for k in range(500):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        graphs = perturb_new(graphs,p=args.noise/10.0)
        args.max_prev_node = 99

    # real graphs
    elif args.graph_type == 'enzymes':
        graphs= Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        args.max_prev_node = 25
    elif args.graph_type == 'enzymes_small':
        graphs_raw = Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        graphs = []
        for G in graphs_raw:
            if G.number_of_nodes()<=20:
                graphs.append(G)
        args.max_prev_node = 15
    elif args.graph_type == 'protein':
        graphs = Graph_load_batch(min_num_nodes=20, name='PROTEINS_full')
        args.max_prev_node = 80
    elif args.graph_type == 'DD':
        graphs = Graph_load_batch(min_num_nodes=100, max_num_nodes=500, name='DD',node_attributes=False,graph_labels=True)
        args.max_prev_node = 230
    elif args.graph_type == 'citeseer':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=3)
            if G_ego.number_of_nodes() >= 50 and (G_ego.number_of_nodes() <= 400):
                graphs.append(G_ego)
        args.max_prev_node = 250
    elif args.graph_type == 'citeseer_small':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=1)
            if (G_ego.number_of_nodes() >= 4) and (G_ego.number_of_nodes() <= 20):
                graphs.append(G_ego)
        shuffle(graphs)
        graphs = graphs[0:200]
        args.max_prev_node = 15

    return graphs",_1750.py,17,"for i in range(2, 5):
    for j in range(3, 5):
        graphs.append(nx.balanced_tree(i, j))","graphs = [nx.balanced_tree(i, j) for i in range(2, 5) for j in range(3, 5)]",1.0
https://github.com/JiaxuanYou/graph-generation/tree/master//create_graphs.py,"def create(args):
### load datasets
    graphs=[]
    # synthetic graphs
    if args.graph_type=='ladder':
        graphs = []
        for i in range(100, 201):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='ladder_small':
        graphs = []
        for i in range(2, 11):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='tree':
        graphs = []
        for i in range(2,5):
            for j in range(3,5):
                graphs.append(nx.balanced_tree(i,j))
        args.max_prev_node = 256
    elif args.graph_type=='caveman':
        # graphs = []
        # for i in range(5,10):
        #     for j in range(5,25):
        #         for k in range(5):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(30, 81):
                for k in range(10):
                    graphs.append(caveman_special(i,j, p_edge=0.3))
        args.max_prev_node = 100
    elif args.graph_type=='caveman_small':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(6, 11):
                for k in range(20):
                    graphs.append(caveman_special(i, j, p_edge=0.8)) # default 0.8
        args.max_prev_node = 20
    elif args.graph_type=='caveman_small_single':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(8, 9):
                for k in range(100):
                    graphs.append(caveman_special(i, j, p_edge=0.5))
        args.max_prev_node = 20
    elif args.graph_type.startswith('community'):
        num_communities = int(args.graph_type[-1])
        print('Creating dataset with ', num_communities, ' communities')
        c_sizes = np.random.choice([12, 13, 14, 15, 16, 17], num_communities)
        #c_sizes = [15] * num_communities
        for k in range(3000):
            graphs.append(n_community(c_sizes, p_inter=0.01))
        args.max_prev_node = 80
    elif args.graph_type=='grid':
        graphs = []
        for i in range(10,20):
            for j in range(10,20):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 40
    elif args.graph_type=='grid_small':
        graphs = []
        for i in range(2,5):
            for j in range(2,6):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 15
    elif args.graph_type=='barabasi':
        graphs = []
        for i in range(100,200):
             for j in range(4,5):
                 for k in range(5):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 130
    elif args.graph_type=='barabasi_small':
        graphs = []
        for i in range(4,21):
             for j in range(3,4):
                 for k in range(10):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 20
    elif args.graph_type=='grid_big':
        graphs = []
        for i in range(36, 46):
            for j in range(36, 46):
                graphs.append(nx.grid_2d_graph(i, j))
        args.max_prev_node = 90

    elif 'barabasi_noise' in args.graph_type:
        graphs = []
        for i in range(100,101):
            for j in range(4,5):
                for k in range(500):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        graphs = perturb_new(graphs,p=args.noise/10.0)
        args.max_prev_node = 99

    # real graphs
    elif args.graph_type == 'enzymes':
        graphs= Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        args.max_prev_node = 25
    elif args.graph_type == 'enzymes_small':
        graphs_raw = Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        graphs = []
        for G in graphs_raw:
            if G.number_of_nodes()<=20:
                graphs.append(G)
        args.max_prev_node = 15
    elif args.graph_type == 'protein':
        graphs = Graph_load_batch(min_num_nodes=20, name='PROTEINS_full')
        args.max_prev_node = 80
    elif args.graph_type == 'DD':
        graphs = Graph_load_batch(min_num_nodes=100, max_num_nodes=500, name='DD',node_attributes=False,graph_labels=True)
        args.max_prev_node = 230
    elif args.graph_type == 'citeseer':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=3)
            if G_ego.number_of_nodes() >= 50 and (G_ego.number_of_nodes() <= 400):
                graphs.append(G_ego)
        args.max_prev_node = 250
    elif args.graph_type == 'citeseer_small':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=1)
            if (G_ego.number_of_nodes() >= 4) and (G_ego.number_of_nodes() <= 20):
                graphs.append(G_ego)
        shuffle(graphs)
        graphs = graphs[0:200]
        args.max_prev_node = 15

    return graphs",_1750.py,28,"for i in range(2, 3):
    for j in range(30, 81):
        for k in range(10):
            graphs.append(caveman_special(i, j, p_edge=0.3))","graphs = [caveman_special(i, j, p_edge=0.3) for i in range(2, 3) for j in range(30, 81) for k in range(10)]",1.0
https://github.com/JiaxuanYou/graph-generation/tree/master//create_graphs.py,"def create(args):
### load datasets
    graphs=[]
    # synthetic graphs
    if args.graph_type=='ladder':
        graphs = []
        for i in range(100, 201):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='ladder_small':
        graphs = []
        for i in range(2, 11):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='tree':
        graphs = []
        for i in range(2,5):
            for j in range(3,5):
                graphs.append(nx.balanced_tree(i,j))
        args.max_prev_node = 256
    elif args.graph_type=='caveman':
        # graphs = []
        # for i in range(5,10):
        #     for j in range(5,25):
        #         for k in range(5):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(30, 81):
                for k in range(10):
                    graphs.append(caveman_special(i,j, p_edge=0.3))
        args.max_prev_node = 100
    elif args.graph_type=='caveman_small':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(6, 11):
                for k in range(20):
                    graphs.append(caveman_special(i, j, p_edge=0.8)) # default 0.8
        args.max_prev_node = 20
    elif args.graph_type=='caveman_small_single':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(8, 9):
                for k in range(100):
                    graphs.append(caveman_special(i, j, p_edge=0.5))
        args.max_prev_node = 20
    elif args.graph_type.startswith('community'):
        num_communities = int(args.graph_type[-1])
        print('Creating dataset with ', num_communities, ' communities')
        c_sizes = np.random.choice([12, 13, 14, 15, 16, 17], num_communities)
        #c_sizes = [15] * num_communities
        for k in range(3000):
            graphs.append(n_community(c_sizes, p_inter=0.01))
        args.max_prev_node = 80
    elif args.graph_type=='grid':
        graphs = []
        for i in range(10,20):
            for j in range(10,20):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 40
    elif args.graph_type=='grid_small':
        graphs = []
        for i in range(2,5):
            for j in range(2,6):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 15
    elif args.graph_type=='barabasi':
        graphs = []
        for i in range(100,200):
             for j in range(4,5):
                 for k in range(5):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 130
    elif args.graph_type=='barabasi_small':
        graphs = []
        for i in range(4,21):
             for j in range(3,4):
                 for k in range(10):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 20
    elif args.graph_type=='grid_big':
        graphs = []
        for i in range(36, 46):
            for j in range(36, 46):
                graphs.append(nx.grid_2d_graph(i, j))
        args.max_prev_node = 90

    elif 'barabasi_noise' in args.graph_type:
        graphs = []
        for i in range(100,101):
            for j in range(4,5):
                for k in range(500):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        graphs = perturb_new(graphs,p=args.noise/10.0)
        args.max_prev_node = 99

    # real graphs
    elif args.graph_type == 'enzymes':
        graphs= Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        args.max_prev_node = 25
    elif args.graph_type == 'enzymes_small':
        graphs_raw = Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        graphs = []
        for G in graphs_raw:
            if G.number_of_nodes()<=20:
                graphs.append(G)
        args.max_prev_node = 15
    elif args.graph_type == 'protein':
        graphs = Graph_load_batch(min_num_nodes=20, name='PROTEINS_full')
        args.max_prev_node = 80
    elif args.graph_type == 'DD':
        graphs = Graph_load_batch(min_num_nodes=100, max_num_nodes=500, name='DD',node_attributes=False,graph_labels=True)
        args.max_prev_node = 230
    elif args.graph_type == 'citeseer':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=3)
            if G_ego.number_of_nodes() >= 50 and (G_ego.number_of_nodes() <= 400):
                graphs.append(G_ego)
        args.max_prev_node = 250
    elif args.graph_type == 'citeseer_small':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=1)
            if (G_ego.number_of_nodes() >= 4) and (G_ego.number_of_nodes() <= 20):
                graphs.append(G_ego)
        shuffle(graphs)
        graphs = graphs[0:200]
        args.max_prev_node = 15

    return graphs",_1750.py,40,"for i in range(2, 3):
    for j in range(6, 11):
        for k in range(20):
            graphs.append(caveman_special(i, j, p_edge=0.8))","graphs = [caveman_special(i, j, p_edge=0.8) for i in range(2, 3) for j in range(6, 11) for k in range(20)]",1.0
https://github.com/JiaxuanYou/graph-generation/tree/master//create_graphs.py,"def create(args):
### load datasets
    graphs=[]
    # synthetic graphs
    if args.graph_type=='ladder':
        graphs = []
        for i in range(100, 201):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='ladder_small':
        graphs = []
        for i in range(2, 11):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='tree':
        graphs = []
        for i in range(2,5):
            for j in range(3,5):
                graphs.append(nx.balanced_tree(i,j))
        args.max_prev_node = 256
    elif args.graph_type=='caveman':
        # graphs = []
        # for i in range(5,10):
        #     for j in range(5,25):
        #         for k in range(5):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(30, 81):
                for k in range(10):
                    graphs.append(caveman_special(i,j, p_edge=0.3))
        args.max_prev_node = 100
    elif args.graph_type=='caveman_small':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(6, 11):
                for k in range(20):
                    graphs.append(caveman_special(i, j, p_edge=0.8)) # default 0.8
        args.max_prev_node = 20
    elif args.graph_type=='caveman_small_single':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(8, 9):
                for k in range(100):
                    graphs.append(caveman_special(i, j, p_edge=0.5))
        args.max_prev_node = 20
    elif args.graph_type.startswith('community'):
        num_communities = int(args.graph_type[-1])
        print('Creating dataset with ', num_communities, ' communities')
        c_sizes = np.random.choice([12, 13, 14, 15, 16, 17], num_communities)
        #c_sizes = [15] * num_communities
        for k in range(3000):
            graphs.append(n_community(c_sizes, p_inter=0.01))
        args.max_prev_node = 80
    elif args.graph_type=='grid':
        graphs = []
        for i in range(10,20):
            for j in range(10,20):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 40
    elif args.graph_type=='grid_small':
        graphs = []
        for i in range(2,5):
            for j in range(2,6):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 15
    elif args.graph_type=='barabasi':
        graphs = []
        for i in range(100,200):
             for j in range(4,5):
                 for k in range(5):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 130
    elif args.graph_type=='barabasi_small':
        graphs = []
        for i in range(4,21):
             for j in range(3,4):
                 for k in range(10):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 20
    elif args.graph_type=='grid_big':
        graphs = []
        for i in range(36, 46):
            for j in range(36, 46):
                graphs.append(nx.grid_2d_graph(i, j))
        args.max_prev_node = 90

    elif 'barabasi_noise' in args.graph_type:
        graphs = []
        for i in range(100,101):
            for j in range(4,5):
                for k in range(500):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        graphs = perturb_new(graphs,p=args.noise/10.0)
        args.max_prev_node = 99

    # real graphs
    elif args.graph_type == 'enzymes':
        graphs= Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        args.max_prev_node = 25
    elif args.graph_type == 'enzymes_small':
        graphs_raw = Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        graphs = []
        for G in graphs_raw:
            if G.number_of_nodes()<=20:
                graphs.append(G)
        args.max_prev_node = 15
    elif args.graph_type == 'protein':
        graphs = Graph_load_batch(min_num_nodes=20, name='PROTEINS_full')
        args.max_prev_node = 80
    elif args.graph_type == 'DD':
        graphs = Graph_load_batch(min_num_nodes=100, max_num_nodes=500, name='DD',node_attributes=False,graph_labels=True)
        args.max_prev_node = 230
    elif args.graph_type == 'citeseer':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=3)
            if G_ego.number_of_nodes() >= 50 and (G_ego.number_of_nodes() <= 400):
                graphs.append(G_ego)
        args.max_prev_node = 250
    elif args.graph_type == 'citeseer_small':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=1)
            if (G_ego.number_of_nodes() >= 4) and (G_ego.number_of_nodes() <= 20):
                graphs.append(G_ego)
        shuffle(graphs)
        graphs = graphs[0:200]
        args.max_prev_node = 15

    return graphs",_1750.py,52,"for i in range(2, 3):
    for j in range(8, 9):
        for k in range(100):
            graphs.append(caveman_special(i, j, p_edge=0.5))","graphs = [caveman_special(i, j, p_edge=0.5) for i in range(2, 3) for j in range(8, 9) for k in range(100)]",1.0
https://github.com/JiaxuanYou/graph-generation/tree/master//create_graphs.py,"def create(args):
### load datasets
    graphs=[]
    # synthetic graphs
    if args.graph_type=='ladder':
        graphs = []
        for i in range(100, 201):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='ladder_small':
        graphs = []
        for i in range(2, 11):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='tree':
        graphs = []
        for i in range(2,5):
            for j in range(3,5):
                graphs.append(nx.balanced_tree(i,j))
        args.max_prev_node = 256
    elif args.graph_type=='caveman':
        # graphs = []
        # for i in range(5,10):
        #     for j in range(5,25):
        #         for k in range(5):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(30, 81):
                for k in range(10):
                    graphs.append(caveman_special(i,j, p_edge=0.3))
        args.max_prev_node = 100
    elif args.graph_type=='caveman_small':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(6, 11):
                for k in range(20):
                    graphs.append(caveman_special(i, j, p_edge=0.8)) # default 0.8
        args.max_prev_node = 20
    elif args.graph_type=='caveman_small_single':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(8, 9):
                for k in range(100):
                    graphs.append(caveman_special(i, j, p_edge=0.5))
        args.max_prev_node = 20
    elif args.graph_type.startswith('community'):
        num_communities = int(args.graph_type[-1])
        print('Creating dataset with ', num_communities, ' communities')
        c_sizes = np.random.choice([12, 13, 14, 15, 16, 17], num_communities)
        #c_sizes = [15] * num_communities
        for k in range(3000):
            graphs.append(n_community(c_sizes, p_inter=0.01))
        args.max_prev_node = 80
    elif args.graph_type=='grid':
        graphs = []
        for i in range(10,20):
            for j in range(10,20):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 40
    elif args.graph_type=='grid_small':
        graphs = []
        for i in range(2,5):
            for j in range(2,6):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 15
    elif args.graph_type=='barabasi':
        graphs = []
        for i in range(100,200):
             for j in range(4,5):
                 for k in range(5):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 130
    elif args.graph_type=='barabasi_small':
        graphs = []
        for i in range(4,21):
             for j in range(3,4):
                 for k in range(10):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 20
    elif args.graph_type=='grid_big':
        graphs = []
        for i in range(36, 46):
            for j in range(36, 46):
                graphs.append(nx.grid_2d_graph(i, j))
        args.max_prev_node = 90

    elif 'barabasi_noise' in args.graph_type:
        graphs = []
        for i in range(100,101):
            for j in range(4,5):
                for k in range(500):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        graphs = perturb_new(graphs,p=args.noise/10.0)
        args.max_prev_node = 99

    # real graphs
    elif args.graph_type == 'enzymes':
        graphs= Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        args.max_prev_node = 25
    elif args.graph_type == 'enzymes_small':
        graphs_raw = Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        graphs = []
        for G in graphs_raw:
            if G.number_of_nodes()<=20:
                graphs.append(G)
        args.max_prev_node = 15
    elif args.graph_type == 'protein':
        graphs = Graph_load_batch(min_num_nodes=20, name='PROTEINS_full')
        args.max_prev_node = 80
    elif args.graph_type == 'DD':
        graphs = Graph_load_batch(min_num_nodes=100, max_num_nodes=500, name='DD',node_attributes=False,graph_labels=True)
        args.max_prev_node = 230
    elif args.graph_type == 'citeseer':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=3)
            if G_ego.number_of_nodes() >= 50 and (G_ego.number_of_nodes() <= 400):
                graphs.append(G_ego)
        args.max_prev_node = 250
    elif args.graph_type == 'citeseer_small':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=1)
            if (G_ego.number_of_nodes() >= 4) and (G_ego.number_of_nodes() <= 20):
                graphs.append(G_ego)
        shuffle(graphs)
        graphs = graphs[0:200]
        args.max_prev_node = 15

    return graphs",_1750.py,62,"for k in range(3000):
    graphs.append(n_community(c_sizes, p_inter=0.01))","graphs.extend([n_community(c_sizes, p_inter=0.01) for k in range(3000)])",1.0
https://github.com/JiaxuanYou/graph-generation/tree/master//create_graphs.py,"def create(args):
### load datasets
    graphs=[]
    # synthetic graphs
    if args.graph_type=='ladder':
        graphs = []
        for i in range(100, 201):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='ladder_small':
        graphs = []
        for i in range(2, 11):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='tree':
        graphs = []
        for i in range(2,5):
            for j in range(3,5):
                graphs.append(nx.balanced_tree(i,j))
        args.max_prev_node = 256
    elif args.graph_type=='caveman':
        # graphs = []
        # for i in range(5,10):
        #     for j in range(5,25):
        #         for k in range(5):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(30, 81):
                for k in range(10):
                    graphs.append(caveman_special(i,j, p_edge=0.3))
        args.max_prev_node = 100
    elif args.graph_type=='caveman_small':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(6, 11):
                for k in range(20):
                    graphs.append(caveman_special(i, j, p_edge=0.8)) # default 0.8
        args.max_prev_node = 20
    elif args.graph_type=='caveman_small_single':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(8, 9):
                for k in range(100):
                    graphs.append(caveman_special(i, j, p_edge=0.5))
        args.max_prev_node = 20
    elif args.graph_type.startswith('community'):
        num_communities = int(args.graph_type[-1])
        print('Creating dataset with ', num_communities, ' communities')
        c_sizes = np.random.choice([12, 13, 14, 15, 16, 17], num_communities)
        #c_sizes = [15] * num_communities
        for k in range(3000):
            graphs.append(n_community(c_sizes, p_inter=0.01))
        args.max_prev_node = 80
    elif args.graph_type=='grid':
        graphs = []
        for i in range(10,20):
            for j in range(10,20):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 40
    elif args.graph_type=='grid_small':
        graphs = []
        for i in range(2,5):
            for j in range(2,6):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 15
    elif args.graph_type=='barabasi':
        graphs = []
        for i in range(100,200):
             for j in range(4,5):
                 for k in range(5):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 130
    elif args.graph_type=='barabasi_small':
        graphs = []
        for i in range(4,21):
             for j in range(3,4):
                 for k in range(10):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 20
    elif args.graph_type=='grid_big':
        graphs = []
        for i in range(36, 46):
            for j in range(36, 46):
                graphs.append(nx.grid_2d_graph(i, j))
        args.max_prev_node = 90

    elif 'barabasi_noise' in args.graph_type:
        graphs = []
        for i in range(100,101):
            for j in range(4,5):
                for k in range(500):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        graphs = perturb_new(graphs,p=args.noise/10.0)
        args.max_prev_node = 99

    # real graphs
    elif args.graph_type == 'enzymes':
        graphs= Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        args.max_prev_node = 25
    elif args.graph_type == 'enzymes_small':
        graphs_raw = Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        graphs = []
        for G in graphs_raw:
            if G.number_of_nodes()<=20:
                graphs.append(G)
        args.max_prev_node = 15
    elif args.graph_type == 'protein':
        graphs = Graph_load_batch(min_num_nodes=20, name='PROTEINS_full')
        args.max_prev_node = 80
    elif args.graph_type == 'DD':
        graphs = Graph_load_batch(min_num_nodes=100, max_num_nodes=500, name='DD',node_attributes=False,graph_labels=True)
        args.max_prev_node = 230
    elif args.graph_type == 'citeseer':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=3)
            if G_ego.number_of_nodes() >= 50 and (G_ego.number_of_nodes() <= 400):
                graphs.append(G_ego)
        args.max_prev_node = 250
    elif args.graph_type == 'citeseer_small':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=1)
            if (G_ego.number_of_nodes() >= 4) and (G_ego.number_of_nodes() <= 20):
                graphs.append(G_ego)
        shuffle(graphs)
        graphs = graphs[0:200]
        args.max_prev_node = 15

    return graphs",_1750.py,67,"for i in range(10, 20):
    for j in range(10, 20):
        graphs.append(nx.grid_2d_graph(i, j))","graphs = [nx.grid_2d_graph(i, j) for i in range(10, 20) for j in range(10, 20)]",1.0
https://github.com/JiaxuanYou/graph-generation/tree/master//create_graphs.py,"def create(args):
### load datasets
    graphs=[]
    # synthetic graphs
    if args.graph_type=='ladder':
        graphs = []
        for i in range(100, 201):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='ladder_small':
        graphs = []
        for i in range(2, 11):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='tree':
        graphs = []
        for i in range(2,5):
            for j in range(3,5):
                graphs.append(nx.balanced_tree(i,j))
        args.max_prev_node = 256
    elif args.graph_type=='caveman':
        # graphs = []
        # for i in range(5,10):
        #     for j in range(5,25):
        #         for k in range(5):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(30, 81):
                for k in range(10):
                    graphs.append(caveman_special(i,j, p_edge=0.3))
        args.max_prev_node = 100
    elif args.graph_type=='caveman_small':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(6, 11):
                for k in range(20):
                    graphs.append(caveman_special(i, j, p_edge=0.8)) # default 0.8
        args.max_prev_node = 20
    elif args.graph_type=='caveman_small_single':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(8, 9):
                for k in range(100):
                    graphs.append(caveman_special(i, j, p_edge=0.5))
        args.max_prev_node = 20
    elif args.graph_type.startswith('community'):
        num_communities = int(args.graph_type[-1])
        print('Creating dataset with ', num_communities, ' communities')
        c_sizes = np.random.choice([12, 13, 14, 15, 16, 17], num_communities)
        #c_sizes = [15] * num_communities
        for k in range(3000):
            graphs.append(n_community(c_sizes, p_inter=0.01))
        args.max_prev_node = 80
    elif args.graph_type=='grid':
        graphs = []
        for i in range(10,20):
            for j in range(10,20):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 40
    elif args.graph_type=='grid_small':
        graphs = []
        for i in range(2,5):
            for j in range(2,6):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 15
    elif args.graph_type=='barabasi':
        graphs = []
        for i in range(100,200):
             for j in range(4,5):
                 for k in range(5):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 130
    elif args.graph_type=='barabasi_small':
        graphs = []
        for i in range(4,21):
             for j in range(3,4):
                 for k in range(10):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 20
    elif args.graph_type=='grid_big':
        graphs = []
        for i in range(36, 46):
            for j in range(36, 46):
                graphs.append(nx.grid_2d_graph(i, j))
        args.max_prev_node = 90

    elif 'barabasi_noise' in args.graph_type:
        graphs = []
        for i in range(100,101):
            for j in range(4,5):
                for k in range(500):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        graphs = perturb_new(graphs,p=args.noise/10.0)
        args.max_prev_node = 99

    # real graphs
    elif args.graph_type == 'enzymes':
        graphs= Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        args.max_prev_node = 25
    elif args.graph_type == 'enzymes_small':
        graphs_raw = Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        graphs = []
        for G in graphs_raw:
            if G.number_of_nodes()<=20:
                graphs.append(G)
        args.max_prev_node = 15
    elif args.graph_type == 'protein':
        graphs = Graph_load_batch(min_num_nodes=20, name='PROTEINS_full')
        args.max_prev_node = 80
    elif args.graph_type == 'DD':
        graphs = Graph_load_batch(min_num_nodes=100, max_num_nodes=500, name='DD',node_attributes=False,graph_labels=True)
        args.max_prev_node = 230
    elif args.graph_type == 'citeseer':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=3)
            if G_ego.number_of_nodes() >= 50 and (G_ego.number_of_nodes() <= 400):
                graphs.append(G_ego)
        args.max_prev_node = 250
    elif args.graph_type == 'citeseer_small':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=1)
            if (G_ego.number_of_nodes() >= 4) and (G_ego.number_of_nodes() <= 20):
                graphs.append(G_ego)
        shuffle(graphs)
        graphs = graphs[0:200]
        args.max_prev_node = 15

    return graphs",_1750.py,73,"for i in range(2, 5):
    for j in range(2, 6):
        graphs.append(nx.grid_2d_graph(i, j))","graphs = [nx.grid_2d_graph(i, j) for i in range(2, 5) for j in range(2, 6)]",1.0
https://github.com/JiaxuanYou/graph-generation/tree/master//create_graphs.py,"def create(args):
### load datasets
    graphs=[]
    # synthetic graphs
    if args.graph_type=='ladder':
        graphs = []
        for i in range(100, 201):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='ladder_small':
        graphs = []
        for i in range(2, 11):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='tree':
        graphs = []
        for i in range(2,5):
            for j in range(3,5):
                graphs.append(nx.balanced_tree(i,j))
        args.max_prev_node = 256
    elif args.graph_type=='caveman':
        # graphs = []
        # for i in range(5,10):
        #     for j in range(5,25):
        #         for k in range(5):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(30, 81):
                for k in range(10):
                    graphs.append(caveman_special(i,j, p_edge=0.3))
        args.max_prev_node = 100
    elif args.graph_type=='caveman_small':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(6, 11):
                for k in range(20):
                    graphs.append(caveman_special(i, j, p_edge=0.8)) # default 0.8
        args.max_prev_node = 20
    elif args.graph_type=='caveman_small_single':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(8, 9):
                for k in range(100):
                    graphs.append(caveman_special(i, j, p_edge=0.5))
        args.max_prev_node = 20
    elif args.graph_type.startswith('community'):
        num_communities = int(args.graph_type[-1])
        print('Creating dataset with ', num_communities, ' communities')
        c_sizes = np.random.choice([12, 13, 14, 15, 16, 17], num_communities)
        #c_sizes = [15] * num_communities
        for k in range(3000):
            graphs.append(n_community(c_sizes, p_inter=0.01))
        args.max_prev_node = 80
    elif args.graph_type=='grid':
        graphs = []
        for i in range(10,20):
            for j in range(10,20):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 40
    elif args.graph_type=='grid_small':
        graphs = []
        for i in range(2,5):
            for j in range(2,6):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 15
    elif args.graph_type=='barabasi':
        graphs = []
        for i in range(100,200):
             for j in range(4,5):
                 for k in range(5):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 130
    elif args.graph_type=='barabasi_small':
        graphs = []
        for i in range(4,21):
             for j in range(3,4):
                 for k in range(10):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 20
    elif args.graph_type=='grid_big':
        graphs = []
        for i in range(36, 46):
            for j in range(36, 46):
                graphs.append(nx.grid_2d_graph(i, j))
        args.max_prev_node = 90

    elif 'barabasi_noise' in args.graph_type:
        graphs = []
        for i in range(100,101):
            for j in range(4,5):
                for k in range(500):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        graphs = perturb_new(graphs,p=args.noise/10.0)
        args.max_prev_node = 99

    # real graphs
    elif args.graph_type == 'enzymes':
        graphs= Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        args.max_prev_node = 25
    elif args.graph_type == 'enzymes_small':
        graphs_raw = Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        graphs = []
        for G in graphs_raw:
            if G.number_of_nodes()<=20:
                graphs.append(G)
        args.max_prev_node = 15
    elif args.graph_type == 'protein':
        graphs = Graph_load_batch(min_num_nodes=20, name='PROTEINS_full')
        args.max_prev_node = 80
    elif args.graph_type == 'DD':
        graphs = Graph_load_batch(min_num_nodes=100, max_num_nodes=500, name='DD',node_attributes=False,graph_labels=True)
        args.max_prev_node = 230
    elif args.graph_type == 'citeseer':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=3)
            if G_ego.number_of_nodes() >= 50 and (G_ego.number_of_nodes() <= 400):
                graphs.append(G_ego)
        args.max_prev_node = 250
    elif args.graph_type == 'citeseer_small':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=1)
            if (G_ego.number_of_nodes() >= 4) and (G_ego.number_of_nodes() <= 20):
                graphs.append(G_ego)
        shuffle(graphs)
        graphs = graphs[0:200]
        args.max_prev_node = 15

    return graphs",_1750.py,79,"for i in range(100, 200):
    for j in range(4, 5):
        for k in range(5):
            graphs.append(nx.barabasi_albert_graph(i, j))","graphs = [nx.barabasi_albert_graph(i, j) for i in range(100, 200) for j in range(4, 5) for k in range(5)]",1.0
https://github.com/JiaxuanYou/graph-generation/tree/master//create_graphs.py,"def create(args):
### load datasets
    graphs=[]
    # synthetic graphs
    if args.graph_type=='ladder':
        graphs = []
        for i in range(100, 201):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='ladder_small':
        graphs = []
        for i in range(2, 11):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='tree':
        graphs = []
        for i in range(2,5):
            for j in range(3,5):
                graphs.append(nx.balanced_tree(i,j))
        args.max_prev_node = 256
    elif args.graph_type=='caveman':
        # graphs = []
        # for i in range(5,10):
        #     for j in range(5,25):
        #         for k in range(5):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(30, 81):
                for k in range(10):
                    graphs.append(caveman_special(i,j, p_edge=0.3))
        args.max_prev_node = 100
    elif args.graph_type=='caveman_small':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(6, 11):
                for k in range(20):
                    graphs.append(caveman_special(i, j, p_edge=0.8)) # default 0.8
        args.max_prev_node = 20
    elif args.graph_type=='caveman_small_single':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(8, 9):
                for k in range(100):
                    graphs.append(caveman_special(i, j, p_edge=0.5))
        args.max_prev_node = 20
    elif args.graph_type.startswith('community'):
        num_communities = int(args.graph_type[-1])
        print('Creating dataset with ', num_communities, ' communities')
        c_sizes = np.random.choice([12, 13, 14, 15, 16, 17], num_communities)
        #c_sizes = [15] * num_communities
        for k in range(3000):
            graphs.append(n_community(c_sizes, p_inter=0.01))
        args.max_prev_node = 80
    elif args.graph_type=='grid':
        graphs = []
        for i in range(10,20):
            for j in range(10,20):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 40
    elif args.graph_type=='grid_small':
        graphs = []
        for i in range(2,5):
            for j in range(2,6):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 15
    elif args.graph_type=='barabasi':
        graphs = []
        for i in range(100,200):
             for j in range(4,5):
                 for k in range(5):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 130
    elif args.graph_type=='barabasi_small':
        graphs = []
        for i in range(4,21):
             for j in range(3,4):
                 for k in range(10):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 20
    elif args.graph_type=='grid_big':
        graphs = []
        for i in range(36, 46):
            for j in range(36, 46):
                graphs.append(nx.grid_2d_graph(i, j))
        args.max_prev_node = 90

    elif 'barabasi_noise' in args.graph_type:
        graphs = []
        for i in range(100,101):
            for j in range(4,5):
                for k in range(500):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        graphs = perturb_new(graphs,p=args.noise/10.0)
        args.max_prev_node = 99

    # real graphs
    elif args.graph_type == 'enzymes':
        graphs= Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        args.max_prev_node = 25
    elif args.graph_type == 'enzymes_small':
        graphs_raw = Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        graphs = []
        for G in graphs_raw:
            if G.number_of_nodes()<=20:
                graphs.append(G)
        args.max_prev_node = 15
    elif args.graph_type == 'protein':
        graphs = Graph_load_batch(min_num_nodes=20, name='PROTEINS_full')
        args.max_prev_node = 80
    elif args.graph_type == 'DD':
        graphs = Graph_load_batch(min_num_nodes=100, max_num_nodes=500, name='DD',node_attributes=False,graph_labels=True)
        args.max_prev_node = 230
    elif args.graph_type == 'citeseer':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=3)
            if G_ego.number_of_nodes() >= 50 and (G_ego.number_of_nodes() <= 400):
                graphs.append(G_ego)
        args.max_prev_node = 250
    elif args.graph_type == 'citeseer_small':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=1)
            if (G_ego.number_of_nodes() >= 4) and (G_ego.number_of_nodes() <= 20):
                graphs.append(G_ego)
        shuffle(graphs)
        graphs = graphs[0:200]
        args.max_prev_node = 15

    return graphs",_1750.py,86,"for i in range(4, 21):
    for j in range(3, 4):
        for k in range(10):
            graphs.append(nx.barabasi_albert_graph(i, j))","graphs = [nx.barabasi_albert_graph(i, j) for i in range(4, 21) for j in range(3, 4) for k in range(10)]",1.0
https://github.com/JiaxuanYou/graph-generation/tree/master//create_graphs.py,"def create(args):
### load datasets
    graphs=[]
    # synthetic graphs
    if args.graph_type=='ladder':
        graphs = []
        for i in range(100, 201):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='ladder_small':
        graphs = []
        for i in range(2, 11):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='tree':
        graphs = []
        for i in range(2,5):
            for j in range(3,5):
                graphs.append(nx.balanced_tree(i,j))
        args.max_prev_node = 256
    elif args.graph_type=='caveman':
        # graphs = []
        # for i in range(5,10):
        #     for j in range(5,25):
        #         for k in range(5):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(30, 81):
                for k in range(10):
                    graphs.append(caveman_special(i,j, p_edge=0.3))
        args.max_prev_node = 100
    elif args.graph_type=='caveman_small':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(6, 11):
                for k in range(20):
                    graphs.append(caveman_special(i, j, p_edge=0.8)) # default 0.8
        args.max_prev_node = 20
    elif args.graph_type=='caveman_small_single':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(8, 9):
                for k in range(100):
                    graphs.append(caveman_special(i, j, p_edge=0.5))
        args.max_prev_node = 20
    elif args.graph_type.startswith('community'):
        num_communities = int(args.graph_type[-1])
        print('Creating dataset with ', num_communities, ' communities')
        c_sizes = np.random.choice([12, 13, 14, 15, 16, 17], num_communities)
        #c_sizes = [15] * num_communities
        for k in range(3000):
            graphs.append(n_community(c_sizes, p_inter=0.01))
        args.max_prev_node = 80
    elif args.graph_type=='grid':
        graphs = []
        for i in range(10,20):
            for j in range(10,20):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 40
    elif args.graph_type=='grid_small':
        graphs = []
        for i in range(2,5):
            for j in range(2,6):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 15
    elif args.graph_type=='barabasi':
        graphs = []
        for i in range(100,200):
             for j in range(4,5):
                 for k in range(5):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 130
    elif args.graph_type=='barabasi_small':
        graphs = []
        for i in range(4,21):
             for j in range(3,4):
                 for k in range(10):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 20
    elif args.graph_type=='grid_big':
        graphs = []
        for i in range(36, 46):
            for j in range(36, 46):
                graphs.append(nx.grid_2d_graph(i, j))
        args.max_prev_node = 90

    elif 'barabasi_noise' in args.graph_type:
        graphs = []
        for i in range(100,101):
            for j in range(4,5):
                for k in range(500):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        graphs = perturb_new(graphs,p=args.noise/10.0)
        args.max_prev_node = 99

    # real graphs
    elif args.graph_type == 'enzymes':
        graphs= Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        args.max_prev_node = 25
    elif args.graph_type == 'enzymes_small':
        graphs_raw = Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        graphs = []
        for G in graphs_raw:
            if G.number_of_nodes()<=20:
                graphs.append(G)
        args.max_prev_node = 15
    elif args.graph_type == 'protein':
        graphs = Graph_load_batch(min_num_nodes=20, name='PROTEINS_full')
        args.max_prev_node = 80
    elif args.graph_type == 'DD':
        graphs = Graph_load_batch(min_num_nodes=100, max_num_nodes=500, name='DD',node_attributes=False,graph_labels=True)
        args.max_prev_node = 230
    elif args.graph_type == 'citeseer':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=3)
            if G_ego.number_of_nodes() >= 50 and (G_ego.number_of_nodes() <= 400):
                graphs.append(G_ego)
        args.max_prev_node = 250
    elif args.graph_type == 'citeseer_small':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=1)
            if (G_ego.number_of_nodes() >= 4) and (G_ego.number_of_nodes() <= 20):
                graphs.append(G_ego)
        shuffle(graphs)
        graphs = graphs[0:200]
        args.max_prev_node = 15

    return graphs",_1750.py,93,"for i in range(36, 46):
    for j in range(36, 46):
        graphs.append(nx.grid_2d_graph(i, j))","graphs = [nx.grid_2d_graph(i, j) for i in range(36, 46) for j in range(36, 46)]",1.0
https://github.com/JiaxuanYou/graph-generation/tree/master//create_graphs.py,"def create(args):
### load datasets
    graphs=[]
    # synthetic graphs
    if args.graph_type=='ladder':
        graphs = []
        for i in range(100, 201):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='ladder_small':
        graphs = []
        for i in range(2, 11):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='tree':
        graphs = []
        for i in range(2,5):
            for j in range(3,5):
                graphs.append(nx.balanced_tree(i,j))
        args.max_prev_node = 256
    elif args.graph_type=='caveman':
        # graphs = []
        # for i in range(5,10):
        #     for j in range(5,25):
        #         for k in range(5):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(30, 81):
                for k in range(10):
                    graphs.append(caveman_special(i,j, p_edge=0.3))
        args.max_prev_node = 100
    elif args.graph_type=='caveman_small':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(6, 11):
                for k in range(20):
                    graphs.append(caveman_special(i, j, p_edge=0.8)) # default 0.8
        args.max_prev_node = 20
    elif args.graph_type=='caveman_small_single':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(8, 9):
                for k in range(100):
                    graphs.append(caveman_special(i, j, p_edge=0.5))
        args.max_prev_node = 20
    elif args.graph_type.startswith('community'):
        num_communities = int(args.graph_type[-1])
        print('Creating dataset with ', num_communities, ' communities')
        c_sizes = np.random.choice([12, 13, 14, 15, 16, 17], num_communities)
        #c_sizes = [15] * num_communities
        for k in range(3000):
            graphs.append(n_community(c_sizes, p_inter=0.01))
        args.max_prev_node = 80
    elif args.graph_type=='grid':
        graphs = []
        for i in range(10,20):
            for j in range(10,20):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 40
    elif args.graph_type=='grid_small':
        graphs = []
        for i in range(2,5):
            for j in range(2,6):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 15
    elif args.graph_type=='barabasi':
        graphs = []
        for i in range(100,200):
             for j in range(4,5):
                 for k in range(5):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 130
    elif args.graph_type=='barabasi_small':
        graphs = []
        for i in range(4,21):
             for j in range(3,4):
                 for k in range(10):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 20
    elif args.graph_type=='grid_big':
        graphs = []
        for i in range(36, 46):
            for j in range(36, 46):
                graphs.append(nx.grid_2d_graph(i, j))
        args.max_prev_node = 90

    elif 'barabasi_noise' in args.graph_type:
        graphs = []
        for i in range(100,101):
            for j in range(4,5):
                for k in range(500):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        graphs = perturb_new(graphs,p=args.noise/10.0)
        args.max_prev_node = 99

    # real graphs
    elif args.graph_type == 'enzymes':
        graphs= Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        args.max_prev_node = 25
    elif args.graph_type == 'enzymes_small':
        graphs_raw = Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        graphs = []
        for G in graphs_raw:
            if G.number_of_nodes()<=20:
                graphs.append(G)
        args.max_prev_node = 15
    elif args.graph_type == 'protein':
        graphs = Graph_load_batch(min_num_nodes=20, name='PROTEINS_full')
        args.max_prev_node = 80
    elif args.graph_type == 'DD':
        graphs = Graph_load_batch(min_num_nodes=100, max_num_nodes=500, name='DD',node_attributes=False,graph_labels=True)
        args.max_prev_node = 230
    elif args.graph_type == 'citeseer':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=3)
            if G_ego.number_of_nodes() >= 50 and (G_ego.number_of_nodes() <= 400):
                graphs.append(G_ego)
        args.max_prev_node = 250
    elif args.graph_type == 'citeseer_small':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=1)
            if (G_ego.number_of_nodes() >= 4) and (G_ego.number_of_nodes() <= 20):
                graphs.append(G_ego)
        shuffle(graphs)
        graphs = graphs[0:200]
        args.max_prev_node = 15

    return graphs",_1750.py,100,"for i in range(100, 101):
    for j in range(4, 5):
        for k in range(500):
            graphs.append(nx.barabasi_albert_graph(i, j))","graphs = [nx.barabasi_albert_graph(i, j) for i in range(100, 101) for j in range(4, 5) for k in range(500)]",1.0
https://github.com/JiaxuanYou/graph-generation/tree/master//create_graphs.py,"def create(args):
### load datasets
    graphs=[]
    # synthetic graphs
    if args.graph_type=='ladder':
        graphs = []
        for i in range(100, 201):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='ladder_small':
        graphs = []
        for i in range(2, 11):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='tree':
        graphs = []
        for i in range(2,5):
            for j in range(3,5):
                graphs.append(nx.balanced_tree(i,j))
        args.max_prev_node = 256
    elif args.graph_type=='caveman':
        # graphs = []
        # for i in range(5,10):
        #     for j in range(5,25):
        #         for k in range(5):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(30, 81):
                for k in range(10):
                    graphs.append(caveman_special(i,j, p_edge=0.3))
        args.max_prev_node = 100
    elif args.graph_type=='caveman_small':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(6, 11):
                for k in range(20):
                    graphs.append(caveman_special(i, j, p_edge=0.8)) # default 0.8
        args.max_prev_node = 20
    elif args.graph_type=='caveman_small_single':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(8, 9):
                for k in range(100):
                    graphs.append(caveman_special(i, j, p_edge=0.5))
        args.max_prev_node = 20
    elif args.graph_type.startswith('community'):
        num_communities = int(args.graph_type[-1])
        print('Creating dataset with ', num_communities, ' communities')
        c_sizes = np.random.choice([12, 13, 14, 15, 16, 17], num_communities)
        #c_sizes = [15] * num_communities
        for k in range(3000):
            graphs.append(n_community(c_sizes, p_inter=0.01))
        args.max_prev_node = 80
    elif args.graph_type=='grid':
        graphs = []
        for i in range(10,20):
            for j in range(10,20):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 40
    elif args.graph_type=='grid_small':
        graphs = []
        for i in range(2,5):
            for j in range(2,6):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 15
    elif args.graph_type=='barabasi':
        graphs = []
        for i in range(100,200):
             for j in range(4,5):
                 for k in range(5):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 130
    elif args.graph_type=='barabasi_small':
        graphs = []
        for i in range(4,21):
             for j in range(3,4):
                 for k in range(10):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 20
    elif args.graph_type=='grid_big':
        graphs = []
        for i in range(36, 46):
            for j in range(36, 46):
                graphs.append(nx.grid_2d_graph(i, j))
        args.max_prev_node = 90

    elif 'barabasi_noise' in args.graph_type:
        graphs = []
        for i in range(100,101):
            for j in range(4,5):
                for k in range(500):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        graphs = perturb_new(graphs,p=args.noise/10.0)
        args.max_prev_node = 99

    # real graphs
    elif args.graph_type == 'enzymes':
        graphs= Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        args.max_prev_node = 25
    elif args.graph_type == 'enzymes_small':
        graphs_raw = Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        graphs = []
        for G in graphs_raw:
            if G.number_of_nodes()<=20:
                graphs.append(G)
        args.max_prev_node = 15
    elif args.graph_type == 'protein':
        graphs = Graph_load_batch(min_num_nodes=20, name='PROTEINS_full')
        args.max_prev_node = 80
    elif args.graph_type == 'DD':
        graphs = Graph_load_batch(min_num_nodes=100, max_num_nodes=500, name='DD',node_attributes=False,graph_labels=True)
        args.max_prev_node = 230
    elif args.graph_type == 'citeseer':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=3)
            if G_ego.number_of_nodes() >= 50 and (G_ego.number_of_nodes() <= 400):
                graphs.append(G_ego)
        args.max_prev_node = 250
    elif args.graph_type == 'citeseer_small':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=1)
            if (G_ego.number_of_nodes() >= 4) and (G_ego.number_of_nodes() <= 20):
                graphs.append(G_ego)
        shuffle(graphs)
        graphs = graphs[0:200]
        args.max_prev_node = 15

    return graphs",_1750.py,114,"for G in graphs_raw:
    if G.number_of_nodes() <= 20:
        graphs.append(G)",graphs = [G for G in graphs_raw if G.number_of_nodes() <= 20],1.0
https://github.com/JiaxuanYou/graph-generation/tree/master//create_graphs.py,"def create(args):
### load datasets
    graphs=[]
    # synthetic graphs
    if args.graph_type=='ladder':
        graphs = []
        for i in range(100, 201):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='ladder_small':
        graphs = []
        for i in range(2, 11):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='tree':
        graphs = []
        for i in range(2,5):
            for j in range(3,5):
                graphs.append(nx.balanced_tree(i,j))
        args.max_prev_node = 256
    elif args.graph_type=='caveman':
        # graphs = []
        # for i in range(5,10):
        #     for j in range(5,25):
        #         for k in range(5):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(30, 81):
                for k in range(10):
                    graphs.append(caveman_special(i,j, p_edge=0.3))
        args.max_prev_node = 100
    elif args.graph_type=='caveman_small':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(6, 11):
                for k in range(20):
                    graphs.append(caveman_special(i, j, p_edge=0.8)) # default 0.8
        args.max_prev_node = 20
    elif args.graph_type=='caveman_small_single':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(8, 9):
                for k in range(100):
                    graphs.append(caveman_special(i, j, p_edge=0.5))
        args.max_prev_node = 20
    elif args.graph_type.startswith('community'):
        num_communities = int(args.graph_type[-1])
        print('Creating dataset with ', num_communities, ' communities')
        c_sizes = np.random.choice([12, 13, 14, 15, 16, 17], num_communities)
        #c_sizes = [15] * num_communities
        for k in range(3000):
            graphs.append(n_community(c_sizes, p_inter=0.01))
        args.max_prev_node = 80
    elif args.graph_type=='grid':
        graphs = []
        for i in range(10,20):
            for j in range(10,20):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 40
    elif args.graph_type=='grid_small':
        graphs = []
        for i in range(2,5):
            for j in range(2,6):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 15
    elif args.graph_type=='barabasi':
        graphs = []
        for i in range(100,200):
             for j in range(4,5):
                 for k in range(5):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 130
    elif args.graph_type=='barabasi_small':
        graphs = []
        for i in range(4,21):
             for j in range(3,4):
                 for k in range(10):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 20
    elif args.graph_type=='grid_big':
        graphs = []
        for i in range(36, 46):
            for j in range(36, 46):
                graphs.append(nx.grid_2d_graph(i, j))
        args.max_prev_node = 90

    elif 'barabasi_noise' in args.graph_type:
        graphs = []
        for i in range(100,101):
            for j in range(4,5):
                for k in range(500):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        graphs = perturb_new(graphs,p=args.noise/10.0)
        args.max_prev_node = 99

    # real graphs
    elif args.graph_type == 'enzymes':
        graphs= Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        args.max_prev_node = 25
    elif args.graph_type == 'enzymes_small':
        graphs_raw = Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        graphs = []
        for G in graphs_raw:
            if G.number_of_nodes()<=20:
                graphs.append(G)
        args.max_prev_node = 15
    elif args.graph_type == 'protein':
        graphs = Graph_load_batch(min_num_nodes=20, name='PROTEINS_full')
        args.max_prev_node = 80
    elif args.graph_type == 'DD':
        graphs = Graph_load_batch(min_num_nodes=100, max_num_nodes=500, name='DD',node_attributes=False,graph_labels=True)
        args.max_prev_node = 230
    elif args.graph_type == 'citeseer':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=3)
            if G_ego.number_of_nodes() >= 50 and (G_ego.number_of_nodes() <= 400):
                graphs.append(G_ego)
        args.max_prev_node = 250
    elif args.graph_type == 'citeseer_small':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=1)
            if (G_ego.number_of_nodes() >= 4) and (G_ego.number_of_nodes() <= 20):
                graphs.append(G_ego)
        shuffle(graphs)
        graphs = graphs[0:200]
        args.max_prev_node = 15

    return graphs",_1750.py,129,"for i in range(G.number_of_nodes()):
    G_ego = nx.ego_graph(G, i, radius=3)
    if G_ego.number_of_nodes() >= 50 and G_ego.number_of_nodes() <= 400:
        graphs.append(G_ego)","graphs = [nx.ego_graph(G, i, radius=3) for i in range(G.number_of_nodes()) if nx.ego_graph(G, i, radius=3).number_of_nodes() >= 50 and nx.ego_graph(G, i, radius=3).number_of_nodes() <= 400]",1.0
https://github.com/JiaxuanYou/graph-generation/tree/master//create_graphs.py,"def create(args):
### load datasets
    graphs=[]
    # synthetic graphs
    if args.graph_type=='ladder':
        graphs = []
        for i in range(100, 201):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='ladder_small':
        graphs = []
        for i in range(2, 11):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='tree':
        graphs = []
        for i in range(2,5):
            for j in range(3,5):
                graphs.append(nx.balanced_tree(i,j))
        args.max_prev_node = 256
    elif args.graph_type=='caveman':
        # graphs = []
        # for i in range(5,10):
        #     for j in range(5,25):
        #         for k in range(5):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(30, 81):
                for k in range(10):
                    graphs.append(caveman_special(i,j, p_edge=0.3))
        args.max_prev_node = 100
    elif args.graph_type=='caveman_small':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(6, 11):
                for k in range(20):
                    graphs.append(caveman_special(i, j, p_edge=0.8)) # default 0.8
        args.max_prev_node = 20
    elif args.graph_type=='caveman_small_single':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(8, 9):
                for k in range(100):
                    graphs.append(caveman_special(i, j, p_edge=0.5))
        args.max_prev_node = 20
    elif args.graph_type.startswith('community'):
        num_communities = int(args.graph_type[-1])
        print('Creating dataset with ', num_communities, ' communities')
        c_sizes = np.random.choice([12, 13, 14, 15, 16, 17], num_communities)
        #c_sizes = [15] * num_communities
        for k in range(3000):
            graphs.append(n_community(c_sizes, p_inter=0.01))
        args.max_prev_node = 80
    elif args.graph_type=='grid':
        graphs = []
        for i in range(10,20):
            for j in range(10,20):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 40
    elif args.graph_type=='grid_small':
        graphs = []
        for i in range(2,5):
            for j in range(2,6):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 15
    elif args.graph_type=='barabasi':
        graphs = []
        for i in range(100,200):
             for j in range(4,5):
                 for k in range(5):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 130
    elif args.graph_type=='barabasi_small':
        graphs = []
        for i in range(4,21):
             for j in range(3,4):
                 for k in range(10):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 20
    elif args.graph_type=='grid_big':
        graphs = []
        for i in range(36, 46):
            for j in range(36, 46):
                graphs.append(nx.grid_2d_graph(i, j))
        args.max_prev_node = 90

    elif 'barabasi_noise' in args.graph_type:
        graphs = []
        for i in range(100,101):
            for j in range(4,5):
                for k in range(500):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        graphs = perturb_new(graphs,p=args.noise/10.0)
        args.max_prev_node = 99

    # real graphs
    elif args.graph_type == 'enzymes':
        graphs= Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        args.max_prev_node = 25
    elif args.graph_type == 'enzymes_small':
        graphs_raw = Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        graphs = []
        for G in graphs_raw:
            if G.number_of_nodes()<=20:
                graphs.append(G)
        args.max_prev_node = 15
    elif args.graph_type == 'protein':
        graphs = Graph_load_batch(min_num_nodes=20, name='PROTEINS_full')
        args.max_prev_node = 80
    elif args.graph_type == 'DD':
        graphs = Graph_load_batch(min_num_nodes=100, max_num_nodes=500, name='DD',node_attributes=False,graph_labels=True)
        args.max_prev_node = 230
    elif args.graph_type == 'citeseer':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=3)
            if G_ego.number_of_nodes() >= 50 and (G_ego.number_of_nodes() <= 400):
                graphs.append(G_ego)
        args.max_prev_node = 250
    elif args.graph_type == 'citeseer_small':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=1)
            if (G_ego.number_of_nodes() >= 4) and (G_ego.number_of_nodes() <= 20):
                graphs.append(G_ego)
        shuffle(graphs)
        graphs = graphs[0:200]
        args.max_prev_node = 15

    return graphs",_1750.py,139,"for i in range(G.number_of_nodes()):
    G_ego = nx.ego_graph(G, i, radius=1)
    if G_ego.number_of_nodes() >= 4 and G_ego.number_of_nodes() <= 20:
        graphs.append(G_ego)","graphs = [nx.ego_graph(G, i, radius=1) for i in range(G.number_of_nodes()) if nx.ego_graph(G, i, radius=1).number_of_nodes() >= 4 and nx.ego_graph(G, i, radius=1).number_of_nodes() <= 20]",1.0
https://github.com/crytic/slither/tree/master/slither/tools/mutator/utils/command_line.py,"def output_mutators(mutators_classes):
    mutators_list = []
    for detector in mutators_classes:
        argument = detector.NAME
        help_info = detector.HELP
        fault_class = detector.FAULTCLASS.name
        fault_nature = detector.FAULTNATURE.name
        mutators_list.append((argument, help_info, fault_class, fault_nature))
    table = MyPrettyTable([""Num"", ""Name"", ""What it Does"", ""Fault Class"", ""Fault Nature""])

    # Sort by class, nature, name
    mutators_list = sorted(mutators_list, key=lambda element: (element[2], element[3], element[0]))
    idx = 1
    for (argument, help_info, fault_class, fault_nature) in mutators_list:
        table.add_row([idx, argument, help_info, fault_class, fault_nature])
        idx = idx + 1
    print(table)",_1782.py,3,"for detector in mutators_classes:
    argument = detector.NAME
    help_info = detector.HELP
    fault_class = detector.FAULTCLASS.name
    fault_nature = detector.FAULTNATURE.name
    mutators_list.append((argument, help_info, fault_class, fault_nature))","mutators_list = [(detector.NAME, detector.HELP, detector.FAULTCLASS.name, detector.FAULTNATURE.name) for detector in mutators_classes]",1.0
https://github.com/open-mmlab/mmskeleton/tree/master/mmskeleton/processor/skeleton_dataset.py,"def build(detection_cfg,
          estimation_cfg,
          tracker_cfg,
          video_dir,
          out_dir,
          gpus=1,
          worker_per_gpu=1,
          video_max_length=10000,
          category_annotation=None):

    cache_checkpoint(detection_cfg.checkpoint_file)
    cache_checkpoint(estimation_cfg.checkpoint_file)
    if tracker_cfg is not None:
        raise NotImplementedError

    if not os.path.isdir(out_dir):
        os.makedirs(out_dir)

    if category_annotation is None:
        video_categories = dict()
    else:
        with open(category_annotation) as f:
            video_categories = json.load(f)['annotations']

    inputs = Manager().Queue(video_max_length)
    results = Manager().Queue(video_max_length)

    num_worker = gpus * worker_per_gpu
    procs = []
    for i in range(num_worker):
        p = Process(
            target=worker,
            args=(inputs, results, i % gpus, detection_cfg, estimation_cfg))
        procs.append(p)
        p.start()

    video_file_list = os.listdir(video_dir)
    prog_bar = ProgressBar(len(video_file_list))
    for video_file in video_file_list:

        reader = mmcv.VideoReader(os.path.join(video_dir, video_file))
        video_frames = reader[:video_max_length]
        annotations = []
        num_keypoints = -1

        for i, image in enumerate(video_frames):
            inputs.put((i, image))

        for i in range(len(video_frames)):
            t = results.get()
            if not t['has_return']:
                continue

            num_person = len(t['joint_preds'])
            assert len(t['person_bbox']) == num_person

            for j in range(num_person):
                keypoints = [[p[0], p[1], round(s[0], 2)] for p, s in zip(
                    t['joint_preds'][j].round().astype(int).tolist(), t[
                        'joint_scores'][j].tolist())]
                num_keypoints = len(keypoints)
                person_info = dict(
                    person_bbox=t['person_bbox'][j].round().astype(int)
                    .tolist(),
                    frame_index=t['frame_index'],
                    id=j,
                    person_id=None,
                    keypoints=keypoints)
                annotations.append(person_info)

        # output results
        annotations = sorted(annotations, key=lambda x: x['frame_index'])
        category_id = video_categories[video_file][
            'category_id'] if video_file in video_categories else -1
        info = dict(
            video_name=video_file,
            resolution=reader.resolution,
            num_frame=len(video_frames),
            num_keypoints=num_keypoints,
            keypoint_channels=['x', 'y', 'score'],
            version='1.0')
        video_info = dict(
            info=info, category_id=category_id, annotations=annotations)
        with open(os.path.join(out_dir, video_file + '.json'), 'w') as f:
            json.dump(video_info, f)

        prog_bar.update()

    # send end signals
    for p in procs:
        inputs.put((-1, None))
    # wait to finish
    for p in procs:
        p.join()

    print('\nBuild skeleton dataset to {}.'.format(out_dir))
    return video_info",_1831.py,57,"for j in range(num_person):
    keypoints = [[p[0], p[1], round(s[0], 2)] for (p, s) in zip(t['joint_preds'][j].round().astype(int).tolist(), t['joint_scores'][j].tolist())]
    num_keypoints = len(keypoints)
    person_info = dict(person_bbox=t['person_bbox'][j].round().astype(int).tolist(), frame_index=t['frame_index'], id=j, person_id=None, keypoints=keypoints)
    annotations.append(person_info)","annotations.extend([dict(person_bbox=t['person_bbox'][j].round().astype(int).tolist(), frame_index=t['frame_index'], id=j, person_id=None, keypoints=[[p[0], p[1], round(s[0], 2)] for (p, s) in zip(t['joint_preds'][j].round().astype(int).tolist(), t['joint_scores'][j].tolist())]) for j in range(num_person)])",1.0
https://github.com/openstack/keystone/tree/master/keystone/models/token_model.py,"def _get_project_roles(self):
        roles = []
        project_roles = (
            PROVIDERS.assignment_api.get_roles_for_user_and_project(
                self.user_id, self.project_id
            )
        )
        for role_id in project_roles:
            r = PROVIDERS.role_api.get_role(role_id)
            roles.append({'id': r['id'], 'name': r['name']})

        return roles",_1832.py,8,"for role_id in project_roles:
    r = PROVIDERS.role_api.get_role(role_id)
    roles.append({'id': r['id'], 'name': r['name']})","roles = [{'id': PROVIDERS.role_api.get_role(role_id)['id'], 'name': PROVIDERS.role_api.get_role(role_id)['name']} for role_id in project_roles]",1.0
https://github.com/openvinotoolkit/open_model_zoo/tree/master/tools/accuracy_checker/openvino/tools/accuracy_checker/postprocessor/crop_segmentation_mask.py,"def process_image(self, annotation, prediction):

        @singledispatch
        def crop_segmentation_mask(entry, height, width, volume):
            return entry

        @crop_segmentation_mask.register(SegmentationAnnotation)
        @crop_segmentation_mask.register(SegmentationPrediction)
        def _(entry, height, width, volume):
            shape = len(entry.mask.shape)
            if shape == 2:
                entry.mask = Crop.process_data(entry.mask, height, width, None, False, False, True, {})
            elif shape == 3:
                entry_mask = []
                for class_mask in entry.mask:
                    mask_channel = Crop.process_data(class_mask, height, width, None, False, False, True, {})
                    entry_mask.append(mask_channel)
                entry.mask = np.array(entry_mask)
            else:
                raise ValueError(""'arr' does not have a suitable array shape for any mode."")
            return entry

        @crop_segmentation_mask.register(BrainTumorSegmentationAnnotation)
        @crop_segmentation_mask.register(BrainTumorSegmentationPrediction)
        def _(entry, height, width, volume):
            entry.mask = Crop3D.crop_center(entry.mask, height, width, volume)
            return entry

        for target in annotation:
            crop_segmentation_mask(target, self.dst_height, self.dst_width, self.dst_volume)

        for target in prediction:
            crop_segmentation_mask(target, self.dst_height, self.dst_width, self.dst_volume)

        return annotation, prediction",_1976.py,15,"for class_mask in entry.mask:
    mask_channel = Crop.process_data(class_mask, height, width, None, False, False, True, {})
    entry_mask.append(mask_channel)","entry_mask = [Crop.process_data(class_mask, height, width, None, False, False, True, {}) for class_mask in entry.mask]",1.0
https://github.com/PaddlePaddle/PaddleGAN/tree/master/ppgan/apps/first_order_predictor.py,"def make_animation(self,
                       source_image,
                       driving_video,
                       generator,
                       kp_detector,
                       relative=True,
                       adapt_movement_scale=True):
        with paddle.no_grad():
            predictions = []
            source = paddle.to_tensor(source_image[np.newaxis].astype(
                np.float32)).transpose([0, 3, 1, 2])

            driving_video_np = np.array(driving_video).astype(np.float32)
            driving_n, driving_h, driving_w, driving_c = driving_video_np.shape

            driving_slices = []

            if self.slice_size != 0:
                batch_count_in_slice = int(
                    np.floor(
                        float(self.slice_size) /
                        (self.batch_size * driving_h * driving_w * driving_c)))
                assert batch_count_in_slice > 0, ""batch_count_in_slice is 0, use smaller batch_size or bigger slice_size""
                frame_count_in_slice = batch_count_in_slice * self.batch_size
                for slice_start in range(0, driving_n, frame_count_in_slice):
                    slice_end = slice_start + min(frame_count_in_slice,
                                                  driving_n - slice_start)
                    current_slice = paddle.to_tensor(
                        driving_video_np[slice_start:slice_end, ]).transpose(
                            [0, 3, 1, 2])
                    driving_slices.append(current_slice)
            else:
                # whole driving as a single slice
                driving = paddle.to_tensor(
                    np.array(driving_video).astype(np.float32)).transpose(
                        [0, 3, 1, 2])
                frame_count_in_slice = driving_n
                driving_slices.append(driving)

            kp_source = kp_detector(source)
            kp_driving_initial = kp_detector(driving_slices[0][0:1])
            kp_source_batch = {}
            kp_source_batch[""value""] = paddle.tile(
                kp_source[""value""], repeat_times=[self.batch_size, 1, 1])
            kp_source_batch[""jacobian""] = paddle.tile(
                kp_source[""jacobian""], repeat_times=[self.batch_size, 1, 1, 1])
            source = paddle.tile(source,
                                 repeat_times=[self.batch_size, 1, 1, 1])
            begin_idx = 0
            for frame_idx in tqdm(
                    range(int(np.ceil(float(driving_n) / self.batch_size)))):
                frame_num = min(self.batch_size, driving_n - begin_idx)
                slice_id = int(frame_idx * self.batch_size /
                               frame_count_in_slice)

                internal_start = frame_idx - slice_id * frame_count_in_slice
                internal_end = frame_idx - slice_id * frame_count_in_slice + frame_num

                driving_frame = driving_slices[slice_id][
                    internal_start:internal_end]

                kp_driving = kp_detector(driving_frame)
                kp_source_img = {}
                kp_source_img[""value""] = kp_source_batch[""value""][0:frame_num]
                kp_source_img[""jacobian""] = kp_source_batch[""jacobian""][
                    0:frame_num]

                kp_norm = normalize_kp(
                    kp_source=kp_source,
                    kp_driving=kp_driving,
                    kp_driving_initial=kp_driving_initial,
                    use_relative_movement=relative,
                    use_relative_jacobian=relative,
                    adapt_movement_scale=adapt_movement_scale)

                out = generator(source[0:frame_num],
                                kp_source=kp_source_img,
                                kp_driving=kp_norm)
                img = np.transpose(out['prediction'].numpy(),
                                   [0, 2, 3, 1]) * 255.0

                if self.face_enhancement:
                    img = self.faceenhancer.enhance_from_batch(img)

                predictions.append(img)
                begin_idx += frame_num
        return np.concatenate(predictions)",_2033.py,25,"for slice_start in range(0, driving_n, frame_count_in_slice):
    slice_end = slice_start + min(frame_count_in_slice, driving_n - slice_start)
    current_slice = paddle.to_tensor(driving_video_np[slice_start:slice_end,]).transpose([0, 3, 1, 2])
    driving_slices.append(current_slice)","driving_slices = [paddle.to_tensor(driving_video_np[slice_start:slice_start + min(frame_count_in_slice, driving_n - slice_start),]).transpose([0, 3, 1, 2]) for slice_start in range(0, driving_n, frame_count_in_slice)]",1.0
https://github.com/NUAA-AL/ALiPy/tree/master/alipy/experiment/experiment_analyser.py,"def _check_and_get_total_cost(self):
        """"""Check if the total cost is the same for all folds.

        Returns
        -------
        same: bool
            If the total cost for all folds are the same.

        effective_cost: float
            If the total cost are the same, return the total cost.
            Otherwise, return the min value of total cost for all folds.

        method_cost: dict
            The effective cost for all methods.
        """"""
        # return value initialize
        effective_cost = set()
        method_cost = dict()

        # gathering information
        for method_name in self._data_extracted.keys():
            total_cost_folds = []
            for fold in self._data_extracted[method_name]:
                total_cost_fold = [np.sum(query_info[0]) for query_info in fold]
                total_cost_folds.append(np.sum(total_cost_fold))

            method_unique_cost = np.unique(total_cost_folds)
            effective_cost.update(set(method_unique_cost))
            method_cost[method_name] = method_unique_cost
        # return
        same = True if len(effective_cost) == 1 else False
        return same, min(effective_cost), method_cost",_2155.py,23,"for fold in self._data_extracted[method_name]:
    total_cost_fold = [np.sum(query_info[0]) for query_info in fold]
    total_cost_folds.append(np.sum(total_cost_fold))",total_cost_folds = [np.sum([np.sum(query_info[0]) for query_info in fold]) for fold in self._data_extracted[method_name]],1.0
https://github.com/uber-research/go-explore/tree/master/robustified/goexplore_py/goexplore.py,"def try_split_frames(self, frames):
        n_processes = multiprocessing.cpu_count()
        tqdm.write('Decoding frames')
        frames = [RLEArray.frombytes(f, dtype=np.uint8) for f in frames]
        tqdm.write('Frames decoded')
        unif_ent_cache = {}
        def get_dist_score(dist):
            if len(dist) == 1:
                return 0.0
            from math import log
            def ent(dist):
                return -sum(log(e) * e for e in dist)
            def unif_ent(l):
                if l not in unif_ent_cache:
                    return ent([1 / l] * l)
                return unif_ent_cache[l]
            def norment(dist):
                return ent(dist) / unif_ent(len(dist))
            target_len = len(frames) * self.args.cell_split_factor
            return norment(dist) / np.sqrt(abs(len(dist) - target_len) / target_len + 1)

        unif_score_cache = {}
        def unif_dist_score(l):
            if l not in unif_score_cache:
                unif_score_cache[l] = get_dist_score([1 / l] * l)
            return unif_score_cache[l]

        best_shape = (random.randint(1, self.normal_frame_shape[0] - 1), random.randint(1, self.normal_frame_shape[1] - 1))
        best_pix_val = random.randint(2, 255)
        best_score = -infinity #get_dist_score([1 / len(frames) for _ in range(len(frames))])
        best_n = 0
        seen = set()

        # Intuition: we want our batch size to be such that it will be processed in two passes
        BATCH_SIZE = len(frames) // (n_processes // 2 + 1) + 1

        def proc_downscale(to_process, returns):
            while True:
                start_batch, cur_shape, cur_pix_val = to_process.get()
                if start_batch == -1:
                    return
                results = []
                for i in range(start_batch, min(len(frames), start_batch + BATCH_SIZE)):
                    results.append(imdownscale(frames[i].to_np(), cur_shape, cur_pix_val).tobytes())
                returns.put(results)

        tqdm.write('Creating processes')
        to_process = multiprocessing.Queue()
        returns = multiprocessing.Queue()
        processes = [multiprocessing.Process(target=proc_downscale, args=(to_process, returns)) for _ in range(n_processes)]
        for p in processes:
            p.start()
        tqdm.write('Processes created')

        for _ in tqdm(range(self.args.split_iterations), desc='New representation'):
            cur_shape = best_shape
            cur_pix_val = best_pix_val
            while (cur_shape, cur_pix_val) in seen:
                cur_shape = list(best_shape)
                for idx in range(2):
                    while True:
                        cur_shape[idx] = np.random.geometric(min(1 / (best_shape[idx] + 1), 20 / self.normal_frame_shape[idx]))
                        if cur_shape[idx] >= 1 and cur_shape[idx] <= self.normal_frame_shape[idx] - 1:
                            break
                cur_shape = tuple(cur_shape)
                while True:
                    cur_pix_val = np.random.geometric(min(1 / best_pix_val, 1 / 12))
                    if cur_pix_val >= 2 and cur_pix_val <= 255:
                        break
            seen.add((cur_shape, cur_pix_val))

            for i in range(0, len(frames), BATCH_SIZE):
                to_process.put((i, cur_shape, cur_pix_val))
            downscaled = []
            for _ in range(0, len(frames), BATCH_SIZE):
                downscaled += returns.get()

            dist = np.array(list(Counter(downscaled).values())) / len(frames)
            cur_score = get_dist_score(dist)

            if cur_score >= best_score:
                if cur_score > best_score:
                    tqdm.write(f'NEW BEST score: {cur_score} n: {len(dist)} shape:{cur_shape} {cur_pix_val}')
                best_score = cur_score
                best_shape = cur_shape
                best_n = len(dist)
                best_pix_val = cur_pix_val

        for i in range(n_processes):
            to_process.put((-1, None, None))
        for p in processes:
            try:
                p.join(1)
            except Exception:
                p.terminate()

        return best_shape, best_pix_val, best_n",_2230.py,43,"for i in range(start_batch, min(len(frames), start_batch + BATCH_SIZE)):
    results.append(imdownscale(frames[i].to_np(), cur_shape, cur_pix_val).tobytes())","results = [imdownscale(frames[i].to_np(), cur_shape, cur_pix_val).tobytes() for i in range(start_batch, min(len(frames), start_batch + BATCH_SIZE))]",1.0
https://github.com/okfn-brasil/querido-diario/tree/master/data_collection/gazette/spiders/sp_campinas.py,"def parse_month_page(self, response):
        items = []
        month_year = response.css(
            "".tabelaDiario:first-child tr th:nth-child(2)::text""
        ).extract_first()  # ""janeiro 2018""
        links = response.css("".tabelaDiario:first-child tr td a"")
        for link in links:
            url = link.css(""::attr(href)"").extract_first().replace(""../"", """")
            day = link.css(""::text"").extract_first()
            date = parse(f""{day} {month_year}"", languages=[""pt""]).date()
            url = f""{self.sp_campinas_url}{url}""
            is_extra_edition = False
            power = ""executive_legislative""
            items.append(
                Gazette(
                    date=date,
                    file_urls=[url],
                    is_extra_edition=is_extra_edition,
                    power=power,
                )
            )
        return items",_2276.py,7,"for link in links:
    url = link.css('::attr(href)').extract_first().replace('../', '')
    day = link.css('::text').extract_first()
    date = parse(f'{day} {month_year}', languages=['pt']).date()
    url = f'{self.sp_campinas_url}{url}'
    is_extra_edition = False
    power = 'executive_legislative'
    items.append(Gazette(date=date, file_urls=[url], is_extra_edition=is_extra_edition, power=power))","items = [Gazette(date=parse(f""{link.css('::text').extract_first()} {month_year}"", languages=['pt']).date(), file_urls=[f""{self.sp_campinas_url}{link.css('::attr(href)').extract_first().replace('../', '')}""], is_extra_edition=False, power='executive_legislative') for link in links]",1.0
https://github.com/beerfactory/hbmqtt/tree/master/hbmqtt/broker.py,"def client_connected(self, listener_name, reader: ReaderAdapter, writer: WriterAdapter):
        # Wait for connection available on listener
        server = self._servers.get(listener_name, None)
        if not server:
            raise BrokerException(""Invalid listener name '%s'"" % listener_name)
        yield from server.acquire_connection()

        remote_address, remote_port = writer.get_peer_info()
        self.logger.info(""Connection from %s:%d on listener '%s'"" % (remote_address, remote_port, listener_name))

        # Wait for first packet and expect a CONNECT
        try:
            handler, client_session = yield from BrokerProtocolHandler.init_from_connect(reader, writer, self.plugins_manager, loop=self._loop)
        except HBMQTTException as exc:
            self.logger.warning(""[MQTT-3.1.0-1] %s: Can't read first packet an CONNECT: %s"" %
                                (format_client_message(address=remote_address, port=remote_port), exc))
            #yield from writer.close()
            self.logger.debug(""Connection closed"")
            return
        except MQTTException as me:
            self.logger.error('Invalid connection from %s : %s' %
                              (format_client_message(address=remote_address, port=remote_port), me))
            yield from writer.close()
            self.logger.debug(""Connection closed"")
            return

        if client_session.clean_session:
            # Delete existing session and create a new one
            if client_session.client_id is not None and client_session.client_id != """":
                self.delete_session(client_session.client_id)
            else:
                client_session.client_id = gen_client_id()
            client_session.parent = 0
        else:
            # Get session from cache
            if client_session.client_id in self._sessions:
                self.logger.debug(""Found old session %s"" % repr(self._sessions[client_session.client_id]))
                (client_session, h) = self._sessions[client_session.client_id]
                client_session.parent = 1
            else:
                client_session.parent = 0
        if client_session.keep_alive > 0:
            client_session.keep_alive += self.config['timeout-disconnect-delay']
        self.logger.debug(""Keep-alive timeout=%d"" % client_session.keep_alive)

        handler.attach(client_session, reader, writer)
        self._sessions[client_session.client_id] = (client_session, handler)

        authenticated = yield from self.authenticate(client_session, self.listeners_config[listener_name])
        if not authenticated:
            yield from writer.close()
            server.release_connection()  # Delete client from connections list
            return

        while True:
            try:
                client_session.transitions.connect()
                break
            except (MachineError, ValueError):
                # Backwards compat: MachineError is raised by transitions < 0.5.0.
                self.logger.warning(""Client %s is reconnecting too quickly, make it wait"" % client_session.client_id)
                # Wait a bit may be client is reconnecting too fast
                yield from asyncio.sleep(1, loop=self._loop)
        yield from handler.mqtt_connack_authorize(authenticated)

        yield from self.plugins_manager.fire_event(EVENT_BROKER_CLIENT_CONNECTED, client_id=client_session.client_id)

        self.logger.debug(""%s Start messages handling"" % client_session.client_id)
        yield from handler.start()
        self.logger.debug(""Retained messages queue size: %d"" % client_session.retained_messages.qsize())
        yield from self.publish_session_retained_messages(client_session)

        # Init and start loop for handling client messages (publish, subscribe/unsubscribe, disconnect)
        disconnect_waiter = asyncio.ensure_future(handler.wait_disconnect(), loop=self._loop)
        subscribe_waiter = asyncio.ensure_future(handler.get_next_pending_subscription(), loop=self._loop)
        unsubscribe_waiter = asyncio.ensure_future(handler.get_next_pending_unsubscription(), loop=self._loop)
        wait_deliver = asyncio.ensure_future(handler.mqtt_deliver_next_message(), loop=self._loop)
        connected = True
        while connected:
            try:
                done, pending = yield from asyncio.wait(
                    [disconnect_waiter, subscribe_waiter, unsubscribe_waiter, wait_deliver],
                    return_when=asyncio.FIRST_COMPLETED, loop=self._loop)
                if disconnect_waiter in done:
                    result = disconnect_waiter.result()
                    self.logger.debug(""%s Result from wait_diconnect: %s"" % (client_session.client_id, result))
                    if result is None:
                        self.logger.debug(""Will flag: %s"" % client_session.will_flag)
                        # Connection closed anormally, send will message
                        if client_session.will_flag:
                            self.logger.debug(""Client %s disconnected abnormally, sending will message"" %
                                              format_client_message(client_session))
                            yield from self._broadcast_message(
                                client_session,
                                client_session.will_topic,
                                client_session.will_message,
                                client_session.will_qos)
                            if client_session.will_retain:
                                self.retain_message(client_session,
                                                    client_session.will_topic,
                                                    client_session.will_message,
                                                    client_session.will_qos)
                    self.logger.debug(""%s Disconnecting session"" % client_session.client_id)
                    yield from self._stop_handler(handler)
                    client_session.transitions.disconnect()
                    yield from self.plugins_manager.fire_event(EVENT_BROKER_CLIENT_DISCONNECTED, client_id=client_session.client_id)
                    connected = False
                if unsubscribe_waiter in done:
                    self.logger.debug(""%s handling unsubscription"" % client_session.client_id)
                    unsubscription = unsubscribe_waiter.result()
                    for topic in unsubscription['topics']:
                        self._del_subscription(topic, client_session)
                        yield from self.plugins_manager.fire_event(
                            EVENT_BROKER_CLIENT_UNSUBSCRIBED,
                            client_id=client_session.client_id,
                            topic=topic)
                    yield from handler.mqtt_acknowledge_unsubscription(unsubscription['packet_id'])
                    unsubscribe_waiter = asyncio.Task(handler.get_next_pending_unsubscription(), loop=self._loop)
                if subscribe_waiter in done:
                    self.logger.debug(""%s handling subscription"" % client_session.client_id)
                    subscriptions = subscribe_waiter.result()
                    return_codes = []
                    for subscription in subscriptions['topics']:
                        result = yield from self.add_subscription(subscription, client_session)
                        return_codes.append(result)
                    yield from handler.mqtt_acknowledge_subscription(subscriptions['packet_id'], return_codes)
                    for index, subscription in enumerate(subscriptions['topics']):
                        if return_codes[index] != 0x80:
                            yield from self.plugins_manager.fire_event(
                                EVENT_BROKER_CLIENT_SUBSCRIBED,
                                client_id=client_session.client_id,
                                topic=subscription[0],
                                qos=subscription[1])
                            yield from self.publish_retained_messages_for_subscription(subscription, client_session)
                    subscribe_waiter = asyncio.Task(handler.get_next_pending_subscription(), loop=self._loop)
                    self.logger.debug(repr(self._subscriptions))
                if wait_deliver in done:
                    if self.logger.isEnabledFor(logging.DEBUG):
                        self.logger.debug(""%s handling message delivery"" % client_session.client_id)
                    app_message = wait_deliver.result()
                    if not app_message.topic:
                        self.logger.warning(""[MQTT-4.7.3-1] - %s invalid TOPIC sent in PUBLISH message, closing connection"" % client_session.client_id)
                        break
                    if ""#"" in app_message.topic or ""+"" in app_message.topic:
                        self.logger.warning(""[MQTT-3.3.2-2] - %s invalid TOPIC sent in PUBLISH message, closing connection"" % client_session.client_id)
                        break
                    yield from self.plugins_manager.fire_event(EVENT_BROKER_MESSAGE_RECEIVED,
                                                               client_id=client_session.client_id,
                                                               message=app_message)
                    yield from self._broadcast_message(client_session, app_message.topic, app_message.data)
                    if app_message.publish_packet.retain_flag:
                        self.retain_message(client_session, app_message.topic, app_message.data, app_message.qos)
                    wait_deliver = asyncio.Task(handler.mqtt_deliver_next_message(), loop=self._loop)
            except asyncio.CancelledError:
                self.logger.debug(""Client loop cancelled"")
                break
        disconnect_waiter.cancel()
        subscribe_waiter.cancel()
        unsubscribe_waiter.cancel()
        wait_deliver.cancel()

        self.logger.debug(""%s Client disconnected"" % client_session.client_id)
        server.release_connection()",_2280.py,123,"for subscription in subscriptions['topics']:
    result = (yield from self.add_subscription(subscription, client_session))
    return_codes.append(result)","return_codes = [(yield from self.add_subscription(subscription, client_session)) for subscription in subscriptions['topics']]",1.0
https://github.com/apache/airflow/tree/master/dev/import_all_classes.py,"def import_all_classes(
    paths: List[str],
    prefix: str,
    provider_ids: List[str] = None,
    print_imports: bool = False,
    print_skips: bool = False,
) -> Tuple[List[str], List[WarningMessage]]:
    """"""
    Imports all classes in providers packages. This method loads and imports
    all the classes found in providers, so that we can find all the subclasses
    of operators/sensors etc.

    :param paths: list of paths to look the provider packages in
    :param prefix: prefix to add
    :param provider_ids - provider ids that should be loaded.
    :param print_imports - if imported class should also be printed in output
    :param print_skips - if skipped classes should also be printed in output
    :return: tuple of list of all imported classes and all warnings generated
    """"""
    imported_classes = []
    tracebacks = []
    printed_packages: Set[str] = set()

    def mk_prefix(provider_id):
        return f'{prefix}{provider_id}'

    if provider_ids:
        provider_prefixes = [mk_prefix(provider_id) for provider_id in provider_ids]
    else:
        provider_prefixes = [prefix]

    def onerror(_):
        nonlocal tracebacks
        exception_string = traceback.format_exc()
        if any(provider_prefix in exception_string for provider_prefix in provider_prefixes):
            tracebacks.append(exception_string)

    all_warnings: List[WarningMessage] = []
    for modinfo in pkgutil.walk_packages(path=paths, prefix=prefix, onerror=onerror):
        if not any(modinfo.name.startswith(provider_prefix) for provider_prefix in provider_prefixes):
            if print_skips:
                print(f""Skipping module: {modinfo.name}"")
            continue
        if print_imports:
            package_to_print = ""."".join(modinfo.name.split(""."")[:-1])
            if package_to_print not in printed_packages:
                printed_packages.add(package_to_print)
                print(f""Importing package: {package_to_print}"")
        try:
            with warnings.catch_warnings(record=True) as w:
                warnings.filterwarnings(""always"", category=DeprecationWarning)
                _module = importlib.import_module(modinfo.name)
                for attribute_name in dir(_module):
                    class_name = modinfo.name + ""."" + attribute_name
                    attribute = getattr(_module, attribute_name)
                    if isclass(attribute):
                        imported_classes.append(class_name)
            if w:
                all_warnings.extend(w)
        except Exception:
            exception_str = traceback.format_exc()
            tracebacks.append(exception_str)
    if tracebacks:
        print(
            """"""
[red]ERROR: There were some import errors[/]
"""""",
            file=sys.stderr,
        )
        for trace in tracebacks:
            print(""[red]----------------------------------------[/]"", file=sys.stderr)
            print(trace, file=sys.stderr)
            print(""[red]----------------------------------------[/]"", file=sys.stderr)
        sys.exit(1)
    else:
        return imported_classes, all_warnings",_2354.py,53,"for attribute_name in dir(_module):
    class_name = modinfo.name + '.' + attribute_name
    attribute = getattr(_module, attribute_name)
    if isclass(attribute):
        imported_classes.append(class_name)","imported_classes.extend([modinfo.name + '.' + attribute_name for attribute_name in dir(_module) if isclass(getattr(_module, attribute_name))])",1.0
https://github.com/zhimingshenjun/DD_Monitor/tree/master//LiverSelect.py,"def run(self):
        try:
            roomInfoSummary = []
            for area in [9, 2, 3, 6, 1]:
                pageSummary = []
                for p in range(1, 6):
                    api = 'https://api.live.bilibili.com/xlive/web-interface/v1/second/getList?platform=web&parent_area_id=%s&page=%s' % (area, p)
                    r = requests.get(api)
                    data = json.loads(r.text)['data']['list']
                    if data:
                        for info in data:
                            pageSummary.append([info['uname'], info['title'], str(info['roomid'])])
                    time.sleep(0.1)
                roomInfoSummary.append(pageSummary)
            if roomInfoSummary:
                self.roomInfoSummary.emit(roomInfoSummary)
        except:
            logging.exception('房间信息获取失败')",_2389.py,11,"for info in data:
    pageSummary.append([info['uname'], info['title'], str(info['roomid'])])","pageSummary.extend([[info['uname'], info['title'], str(info['roomid'])] for info in data])",1.0
https://github.com/saltstack/salt/tree/master/salt/returners/cassandra_cql_return.py,"def get_minions():
    """"""
    Return a list of minions
    """"""
    query = """"""SELECT DISTINCT minion_id FROM salt.minions;""""""

    ret = []

    # cassandra_cql.cql_query may raise a CommandExecutionError
    try:
        data = __salt__[""cassandra_cql.cql_query""](query)
        if data:
            for row in data:
                minion = row.get(""minion_id"")
                if minion:
                    ret.append(minion)
    except CommandExecutionError:
        log.critical(""Could not get the list of minions."")
        raise
    except Exception as e:  # pylint: disable=broad-except
        log.critical(""Unexpected error while getting list of minions: %s"", e)
        raise

    return ret",_2511.py,13,"for row in data:
    minion = row.get('minion_id')
    if minion:
        ret.append(minion)",ret = [row.get('minion_id') for row in data if row.get('minion_id')],1.0
https://github.com/pfnet-research/sngan_projection/tree/master/source/inception/inception_score_tf.py,"def get_inception_score(images, splits=10):
    preds = inception_forward(images, softmax)
    scores = []
    for i in range(splits):
        part = preds[(i * preds.shape[0] // splits):((i + 1) * preds.shape[0] // splits), :]
        kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))
        kl = np.mean(np.sum(kl, 1))
        scores.append(np.exp(kl))
    return np.mean(scores), np.std(scores)",_2662.py,4,"for i in range(splits):
    part = preds[i * preds.shape[0] // splits:(i + 1) * preds.shape[0] // splits, :]
    kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))
    kl = np.mean(np.sum(kl, 1))
    scores.append(np.exp(kl))","scores = [np.exp(np.mean(np.sum(preds[i * preds.shape[0] // splits:(i + 1) * preds.shape[0] // splits, :] * (np.log(preds[i * preds.shape[0] // splits:(i + 1) * preds.shape[0] // splits, :]) - np.log(np.expand_dims(np.mean(preds[i * preds.shape[0] // splits:(i + 1) * preds.shape[0] // splits, :], 0), 0))), 1))) for i in range(splits)]",1.0
https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/vm/_actions.py,"def load_extension_images_thru_services(cli_ctx, publisher, name, version, location,
                                        show_latest=False, partial_match=True):
    from concurrent.futures import ThreadPoolExecutor, as_completed
    from packaging.version import parse  # pylint: disable=no-name-in-module,import-error
    all_images = []
    client = _compute_client_factory(cli_ctx)
    if location is None:
        location = get_one_of_subscription_locations(cli_ctx)

    def _load_extension_images_from_publisher(publisher):
        from azure.core.exceptions import ResourceNotFoundError
        try:
            types = client.virtual_machine_extension_images.list_types(location, publisher)
        except ResourceNotFoundError as e:
            # PIR image publishers might not have any extension images, exception could raise
            logger.warning(str(e))
            types = []
        if name:
            types = [t for t in types if _matched(name, t.name, partial_match)]
        for t in types:
            try:
                versions = client.virtual_machine_extension_images.list_versions(
                    location, publisher, t.name)
            except ResourceNotFoundError as e:
                logger.warning(str(e))
                continue
            if version:
                versions = [v for v in versions if _matched(version, v.name, partial_match)]

            if show_latest:
                # pylint: disable=no-member
                versions.sort(key=lambda v: parse(v.name), reverse=True)
                try:
                    all_images.append({
                        'publisher': publisher,
                        'name': t.name,
                        'version': versions[0].name})
                except IndexError:
                    pass  # if no versions for this type continue to next type.
            else:
                for v in versions:
                    all_images.append({
                        'publisher': publisher,
                        'name': t.name,
                        'version': v.name})

    publishers = client.virtual_machine_images.list_publishers(location)
    if publisher:
        publishers = [p for p in publishers if _matched(publisher, p.name, partial_match)]

    publisher_num = len(publishers)
    if publisher_num > 1:
        with ThreadPoolExecutor(max_workers=_get_thread_count()) as executor:
            tasks = [executor.submit(_load_extension_images_from_publisher,
                                     p.name) for p in publishers]
            for t in as_completed(tasks):
                t.result()  # don't use the result but expose exceptions from the threads
    elif publisher_num == 1:
        _load_extension_images_from_publisher(publishers[0].name)

    return all_images",_2772.py,41,"for v in versions:
    all_images.append({'publisher': publisher, 'name': t.name, 'version': v.name})","all_images.extend([{'publisher': publisher, 'name': t.name, 'version': v.name} for v in versions])",1.0
https://github.com/fizyr/keras-retinanet/tree/master/keras_retinanet/preprocessing/kitti.py,"def __init__(
        self,
        base_dir,
        subset='train',
        **kwargs
    ):
        """""" Initialize a KITTI data generator.

        Args
            base_dir: Directory w.r.t. where the files are to be searched (defaults to the directory containing the csv_data_file).
            subset: The subset to generate data for (defaults to 'train').
        """"""
        self.base_dir = base_dir

        label_dir = os.path.join(self.base_dir, subset, 'labels')
        image_dir = os.path.join(self.base_dir, subset, 'images')

        """"""
        1    type         Describes the type of object: 'Car', 'Van', 'Truck',
                             'Pedestrian', 'Person_sitting', 'Cyclist', 'Tram',
                             'Misc' or 'DontCare'
        1    truncated    Float from 0 (non-truncated) to 1 (truncated), where
                         truncated refers to the object leaving image boundaries
        1    occluded     Integer (0,1,2,3) indicating occlusion state:
                         0 = fully visible, 1 = partly occluded
                         2 = largely occluded, 3 = unknown
        1    alpha        Observation angle of object, ranging [-pi..pi]
        4    bbox         2D bounding box of object in the image (0-based index):
                         contains left, top, right, bottom pixel coordinates
        3    dimensions   3D object dimensions: height, width, length (in meters)
        3    location     3D object location x,y,z in camera coordinates (in meters)
        1    rotation_y   Rotation ry around Y-axis in camera coordinates [-pi..pi]
        """"""

        self.labels = {}
        self.classes = kitti_classes
        for name, label in self.classes.items():
            self.labels[label] = name

        self.image_data = dict()
        self.images = []
        for i, fn in enumerate(os.listdir(label_dir)):
            label_fp = os.path.join(label_dir, fn)
            image_fp = os.path.join(image_dir, fn.replace('.txt', '.png'))

            self.images.append(image_fp)

            fieldnames = ['type', 'truncated', 'occluded', 'alpha', 'left', 'top', 'right', 'bottom', 'dh', 'dw', 'dl',
                          'lx', 'ly', 'lz', 'ry']
            with open(label_fp, 'r') as csv_file:
                reader = csv.DictReader(csv_file, delimiter=' ', fieldnames=fieldnames)
                boxes = []
                for line, row in enumerate(reader):
                    label = row['type']
                    cls_id = kitti_classes[label]

                    annotation = {'cls_id': cls_id, 'x1': row['left'], 'x2': row['right'], 'y2': row['bottom'], 'y1': row['top']}
                    boxes.append(annotation)

                self.image_data[i] = boxes

        super(KittiGenerator, self).__init__(**kwargs)",_2791.py,53,"for (line, row) in enumerate(reader):
    label = row['type']
    cls_id = kitti_classes[label]
    annotation = {'cls_id': cls_id, 'x1': row['left'], 'x2': row['right'], 'y2': row['bottom'], 'y1': row['top']}
    boxes.append(annotation)","boxes = [{'cls_id': kitti_classes[row['type']], 'x1': row['left'], 'x2': row['right'], 'y2': row['bottom'], 'y1': row['top']} for (line, row) in enumerate(reader)]",1.0
https://github.com/haotian-liu/yolact_edge/tree/master/yolact_edge/yolact.py,"def build_flow_convs(encode_layers, in_features, out_features, stride=1, groups=1):
    conv = []
    conv.append(conv_lrelu(in_features, cfg.flow.encode_channels * encode_layers[0], groups=groups, stride=stride))
    for encode_idx, encode_layer in enumerate(encode_layers[:-1]):
        conv.append(conv_lrelu(cfg.flow.encode_channels * encode_layers[encode_idx], cfg.flow.encode_channels * encode_layers[encode_idx + 1], groups=groups))
    conv.append(conv_lrelu(cfg.flow.encode_channels * encode_layers[-1], out_features))
    return nn.Sequential(*conv)",_3073.py,4,"for (encode_idx, encode_layer) in enumerate(encode_layers[:-1]):
    conv.append(conv_lrelu(cfg.flow.encode_channels * encode_layers[encode_idx], cfg.flow.encode_channels * encode_layers[encode_idx + 1], groups=groups))","conv.extend([conv_lrelu(cfg.flow.encode_channels * encode_layers[encode_idx], cfg.flow.encode_channels * encode_layers[encode_idx + 1], groups=groups) for (encode_idx, encode_layer) in enumerate(encode_layers[:-1])])",1.0
https://github.com/DinoTools/dionaea/tree/master/modules/python/dionaea/emu_scripts/handler.py,"def run(self, data):
        urls = []
        for m in self._regex_url.finditer(data):
            urls.append(m.group(""url""))
        return urls",_3241.py,3,"for m in self._regex_url.finditer(data):
    urls.append(m.group('url'))",urls = [m.group('url') for m in self._regex_url.finditer(data)],1.0
https://github.com/tensorlayer/hyperpose/tree/master/hyperpose/Model/pose_proposal/processor.py,"def process(self, predict_x, scale_w_rate=1,scale_h_rate=1, resize=True):
        predict_x = to_numpy_dict(predict_x)
        batch_size = list(predict_x.values())[0].shape[0]
        humans_list = []
        for batch_idx in range(0,batch_size):
            predict_x_one = {key:value[batch_idx] for key,value in predict_x.items()}
            humans_list.append(self.process_one(predict_x_one, scale_w_rate, scale_h_rate, resize=resize))        
        return humans_list",_3253.py,5,"for batch_idx in range(0, batch_size):
    predict_x_one = {key: value[batch_idx] for (key, value) in predict_x.items()}
    humans_list.append(self.process_one(predict_x_one, scale_w_rate, scale_h_rate, resize=resize))","humans_list = [self.process_one({key: value[batch_idx] for (key, value) in predict_x.items()}, scale_w_rate, scale_h_rate, resize=resize) for batch_idx in range(0, batch_size)]",1.0
https://github.com/timoschick/pet/tree/master/pet/tasks.py,"def _create_examples(path: str, set_type: str) -> List[InputExample]:
        examples = []

        with open(path, encoding='utf8') as f:
            for line in f:
                example_json = json.loads(line)
                idx = example_json['idx']
                label = str(example_json['label']) if 'label' in example_json else None
                guid = ""%s-%s"" % (set_type, idx)
                text_a = example_json['passage']
                text_b = example_json['question']
                example = InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label, idx=idx)
                examples.append(example)

        return examples",_3277.py,5,"for line in f:
    example_json = json.loads(line)
    idx = example_json['idx']
    label = str(example_json['label']) if 'label' in example_json else None
    guid = '%s-%s' % (set_type, idx)
    text_a = example_json['passage']
    text_b = example_json['question']
    example = InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label, idx=idx)
    examples.append(example)","examples = [InputExample(guid='%s-%s' % (set_type, json.loads(line)['idx']), text_a=json.loads(line)['passage'], text_b=json.loads(line)['question'], label=str(json.loads(line)['label']) if 'label' in json.loads(line) else None, idx=json.loads(line)['idx']) for line in f]",1.0
https://github.com/nlplab/brat/tree/master/server/src/verify_annotations.py,"def verify_equivs(ann_obj, projectconf):
    issues = []

    # shortcut
    def disp(s):
        return projectconf.preferred_display_form(s)

    for eq in ann_obj.get_equivs():
        # get the equivalent annotations
        equiv_anns = [ann_obj.get_ann_by_id(eid) for eid in eq.entities]

        # all pairs of entity types in the Equiv group must be allowed
        # to have an Equiv. Create type-level pairs to avoid N^2
        # search where N=entities.
        eq_type = {}
        for e in equiv_anns:
            eq_type[e.type] = True
        type_pairs = []
        for t1 in eq_type:
            for t2 in eq_type:
                type_pairs.append((t1, t2))

        # do avoid marking both (a1,a2) and (a2,a1), remember what's
        # already included
        marked = {}

        for t1, t2 in type_pairs:
            reltypes = projectconf.relation_types_from_to(t1, t2)
            # TODO: this is too convoluted; use projectconf directly
            equiv_type_found = False
            for rt in reltypes:
                if projectconf.is_equiv_type(rt):
                    equiv_type_found = True
            if not equiv_type_found:
                # Avoid redundant output
                if (t2, t1) in marked:
                    continue
                # TODO: mark this error on the Eq relation, not the entities
                for e in equiv_anns:
                    issues.append(
                        AnnotationIssue(
                            e.id, AnnotationError, ""Equivalence relation %s not allowed between %s and %s"" %
                            (eq.type, disp(t1), disp(t2))))
                marked[(t1, t2)] = True

    return issues",_3364.py,19,"for t1 in eq_type:
    for t2 in eq_type:
        type_pairs.append((t1, t2))","type_pairs = [(t1, t2) for t1 in eq_type for t2 in eq_type]",1.0
https://github.com/nlplab/brat/tree/master/server/src/verify_annotations.py,"def verify_equivs(ann_obj, projectconf):
    issues = []

    # shortcut
    def disp(s):
        return projectconf.preferred_display_form(s)

    for eq in ann_obj.get_equivs():
        # get the equivalent annotations
        equiv_anns = [ann_obj.get_ann_by_id(eid) for eid in eq.entities]

        # all pairs of entity types in the Equiv group must be allowed
        # to have an Equiv. Create type-level pairs to avoid N^2
        # search where N=entities.
        eq_type = {}
        for e in equiv_anns:
            eq_type[e.type] = True
        type_pairs = []
        for t1 in eq_type:
            for t2 in eq_type:
                type_pairs.append((t1, t2))

        # do avoid marking both (a1,a2) and (a2,a1), remember what's
        # already included
        marked = {}

        for t1, t2 in type_pairs:
            reltypes = projectconf.relation_types_from_to(t1, t2)
            # TODO: this is too convoluted; use projectconf directly
            equiv_type_found = False
            for rt in reltypes:
                if projectconf.is_equiv_type(rt):
                    equiv_type_found = True
            if not equiv_type_found:
                # Avoid redundant output
                if (t2, t1) in marked:
                    continue
                # TODO: mark this error on the Eq relation, not the entities
                for e in equiv_anns:
                    issues.append(
                        AnnotationIssue(
                            e.id, AnnotationError, ""Equivalence relation %s not allowed between %s and %s"" %
                            (eq.type, disp(t1), disp(t2))))
                marked[(t1, t2)] = True

    return issues",_3364.py,39,"for e in equiv_anns:
    issues.append(AnnotationIssue(e.id, AnnotationError, 'Equivalence relation %s not allowed between %s and %s' % (eq.type, disp(t1), disp(t2))))","issues.extend([AnnotationIssue(e.id, AnnotationError, 'Equivalence relation %s not allowed between %s and %s' % (eq.type, disp(t1), disp(t2))) for e in equiv_anns])",1.0
https://github.com/keras-team/keras/tree/master/keras/optimizer_v2/optimizer_v2_test.py,"def identify_redundant_ops(graph):
  """"""Implements basic common subexpression elimination.

  This is not intended to replicate the graph semantics of TensorFlow Graphs
  (for instance it does not handle stateful op ordering), nor is it intended to
  replace the common subexpression elimination Grappler pass. Rather, it
  provides a high level sanity check that clearly redundant ops are not being
  created.

  Args:
    graph: The graph to be analyzed.

  Returns:
    A count of the duplicate ops and a description of the structure of each.
  """"""
  sorted_ops = topological_sort(graph)
  duplicates = collections.defaultdict(list)
  unified_node_defs = {}
  name_map = {}

  for op in sorted_ops:
    input_names = []
    for op_input, name in zip(*get_inputs(op)):
      input_def = op_input.node_def

      # Operations can have multiple outputs. We track which is used to prevent
      # overzealous elimination.
      input_def.name = name

      input_def.input[:] = [name_map.get(i, i) for i in input_def.input]
      strip_name(input_def)

      # NodeDef.SerializeToString() does not provide identical serialized
      # representations for identical NodeDefs, so we instead use string
      # representation as a dict key.
      key = repr(input_def)

      if key in unified_node_defs:
        input_names.append(unified_node_defs[key])

      else:
        unified_node_defs[key] = op_input.name
        input_names.append(name)

    node_def = op.node_def
    node_def.input[:] = input_names
    strip_name(node_def)

    key = repr(node_def)
    duplicates[key].append(op)
    name_map[op.name] = duplicates[key][0].name

  num_duplicates = 0
  duplicate_types = []
  for standard_def, op_defs in duplicates.items():
    # We are only interested in testing the apply method of the optimizer
    op_defs = [i for i in op_defs if APPLY_SCOPE in i.name]

    # We only check for per-apply redundant ops.
    if len(op_defs) < _NUM_LEARNERS:
      continue

    # Certain ops are simply not worth eliminating, and are instead simply
    # ignored.
    name, op_type = op_defs[0].name, op_defs[0].type
    if any(allowlisted_scope in name and op_type == allowlisted_type
           for allowlisted_scope, allowlisted_type in ALLOWLIST):
      continue

    num_duplicates += len(op_defs)
    traceback = []
    for level in op_defs[0].traceback:
      traceback.append('  {} {}:{}'.format(level[0], level[2], level[1]))

    duplicate_types.append(
        '# Example name: {}\n# Op creation stack:\n{}\n{}'.format(
            op_defs[0].name,
            '\n'.join(traceback),
            standard_def))

  return num_duplicates, duplicate_types",_3432.py,72,"for level in op_defs[0].traceback:
    traceback.append('  {} {}:{}'.format(level[0], level[2], level[1]))","traceback = ['  {} {}:{}'.format(level[0], level[2], level[1]) for level in op_defs[0].traceback]",1.0
https://github.com/nucleic/enaml/tree/master/enaml/qt/styleutil.py,"def translate_style(name, style):
    parts = ['#%s' % name]
    if style.pseudo_element:
        root = parts.pop()
        for pe in style.pseudo_element.split(','):
            parts.append(root + '::%s' % pe.strip())
    if style.pseudo_class:
        these = parts[:]
        parts = []
        for pc in style.pseudo_class.split(','):
            pc = pc.strip()
            for this in these:
                parts.append(this + ':%s' % pc)
    selector = ','.join(parts)
    body = '{\n%s\n}' % _translate_style_body(style)
    return '%s %s' % (selector, body)",_3497.py,10,"for pc in style.pseudo_class.split(','):
    pc = pc.strip()
    for this in these:
        parts.append(this + ':%s' % pc)","parts = [this + ':%s' % pc.strip() for pc in style.pseudo_class.split(',') for this in these]",1.0
https://github.com/jhpyle/docassemble/tree/master/docassemble_webapp/docassemble/webapp/jsonstore.py,"def read_answer_json(user_code, filename, tags=None, all_tags=False):
    if all_tags:
        entries = list()
        for entry in JsonDb.execute(select(JsonStorage).filter_by(filename=filename, key=user_code, tags=tags)).scalars():
            entries.append(dict(data=entry.data, tags=entry.tags, modtime=entry.modtime))
        return entries
    existing_entry = JsonDb.execute(select(JsonStorage).filter_by(filename=filename, key=user_code, tags=tags)).scalar()
    if existing_entry is not None:
        return existing_entry.data
    return None",_3638.py,4,"for entry in JsonDb.execute(select(JsonStorage).filter_by(filename=filename, key=user_code, tags=tags)).scalars():
    entries.append(dict(data=entry.data, tags=entry.tags, modtime=entry.modtime))","entries = [dict(data=entry.data, tags=entry.tags, modtime=entry.modtime) for entry in JsonDb.execute(select(JsonStorage).filter_by(filename=filename, key=user_code, tags=tags)).scalars()]",1.0
https://github.com/tensorflow/tfx/tree/master/tfx/examples/tfjs_next_page_prediction/bigquery_beam_data_generation.py,"def ga_session_to_tensorflow_examples(session: List[Any]):
  """"""Converts a Google Analytics Session to Tensorflow Examples.""""""
  examples = []
  for i in range(len(session) - 1):
    features = {
        # Add any additional desired training features here.
        'cur_page': [_sanitize_page_path(session[i]['page']['pagePath'])],
        'label': [_sanitize_page_path(session[i + 1]['page']['pagePath'])],
        'session_index': [i],
    }
    examples.append(create_tensorflow_example(features))
  return examples",_3673.py,4,"for i in range(len(session) - 1):
    features = {'cur_page': [_sanitize_page_path(session[i]['page']['pagePath'])], 'label': [_sanitize_page_path(session[i + 1]['page']['pagePath'])], 'session_index': [i]}
    examples.append(create_tensorflow_example(features))","examples = [create_tensorflow_example({'cur_page': [_sanitize_page_path(session[i]['page']['pagePath'])], 'label': [_sanitize_page_path(session[i + 1]['page']['pagePath'])], 'session_index': [i]}) for i in range(len(session) - 1)]",1.0
https://github.com/saeeddhqan/Maryam/tree/master/maryam/core/util/osint/hunter.py,"def json_emails(self):
		emails = []
		if self.acceptable:
			for x in range(self.limit):
				emails.append(self._json_pages['data']['emails'][x]['value'])
			return emails
		return []",_3688.py,4,"for x in range(self.limit):
    emails.append(self._json_pages['data']['emails'][x]['value'])",emails = [self._json_pages['data']['emails'][x]['value'] for x in range(self.limit)],1.0
https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/functions.py,"def verb_present_de(*pargs, **kwargs):
    ensure_definition(*pargs, **kwargs)
    new_args = list()
    for arg in pargs:
        new_args.append(str(arg))
    if len(new_args) < 2:
        new_args.append('3sg')
    if new_args[1] == 'pl':
        new_args[1] = '3pl'
    output = docassemble_pattern.de.conjugate(*new_args, **kwargs)
    if 'capitalize' in kwargs and kwargs['capitalize']:
        return(capitalize(output))
    else:
        return(output)",_3953.py,4,"for arg in pargs:
    new_args.append(str(arg))",new_args = [str(arg) for arg in pargs],1.0
https://github.com/SFTtech/openage/tree/master/openage/convert/processor/conversion/de2/nyan_subprocessor.py,"def terrain_group_to_terrain(terrain_group: GenieTerrainGroup) -> None:
        """"""
        Creates raw API objects for a terrain group.

        :param terrain_group: Terrain group that gets converted to a tech.
        :type terrain_group: ..dataformat.converter_object.ConverterObjectGroup
        """"""
        terrain_index = terrain_group.get_id()

        dataset = terrain_group.data

        # name_lookup_dict = internal_name_lookups.get_entity_lookups(dataset.game_version)
        terrain_lookup_dict = internal_name_lookups.get_terrain_lookups(dataset.game_version)
        terrain_type_lookup_dict = internal_name_lookups.get_terrain_type_lookups(
            dataset.game_version)

        if terrain_index not in terrain_lookup_dict:
            # TODO: Not all terrains are used in DE2; filter out the unused terrains
            # in pre-processor
            return

        # Start with the Terrain object
        terrain_name = terrain_lookup_dict[terrain_index][1]
        raw_api_object = RawAPIObject(terrain_name, terrain_name,
                                      dataset.nyan_api_objects)
        raw_api_object.add_raw_parent(""engine.util.terrain.Terrain"")
        obj_location = f""data/terrain/{terrain_lookup_dict[terrain_index][2]}/""
        raw_api_object.set_location(obj_location)
        raw_api_object.set_filename(terrain_lookup_dict[terrain_index][2])
        terrain_group.add_raw_api_object(raw_api_object)

        # =======================================================================
        # Types
        # =======================================================================
        terrain_types = []

        for terrain_type in terrain_type_lookup_dict.values():
            if terrain_index in terrain_type[0]:
                type_name = f""util.terrain_type.types.{terrain_type[2]}""
                type_obj = dataset.pregen_nyan_objects[type_name].get_nyan_object()
                terrain_types.append(type_obj)

        raw_api_object.add_raw_member(""types"", terrain_types, ""engine.util.terrain.Terrain"")

        # =======================================================================
        # Name
        # =======================================================================
        name_ref = f""{terrain_name}.{terrain_name}Name""
        name_raw_api_object = RawAPIObject(name_ref,
                                           f""{terrain_name}Name"",
                                           dataset.nyan_api_objects)
        name_raw_api_object.add_raw_parent(""engine.util.language.translated.type.TranslatedString"")
        name_location = ForwardRef(terrain_group, terrain_name)
        name_raw_api_object.set_location(name_location)

        name_raw_api_object.add_raw_member(""translations"",
                                           [],
                                           ""engine.util.language.translated.type.TranslatedString"")

        name_forward_ref = ForwardRef(terrain_group, name_ref)
        raw_api_object.add_raw_member(""name"", name_forward_ref, ""engine.util.terrain.Terrain"")
        terrain_group.add_raw_api_object(name_raw_api_object)

        # =======================================================================
        # Sound
        # =======================================================================
        sound_name = f""{terrain_name}.Sound""
        sound_raw_api_object = RawAPIObject(sound_name, ""Sound"",
                                            dataset.nyan_api_objects)
        sound_raw_api_object.add_raw_parent(""engine.util.sound.Sound"")
        sound_location = ForwardRef(terrain_group, terrain_name)
        sound_raw_api_object.set_location(sound_location)

        # TODO: Sounds
        sounds = []

        sound_raw_api_object.add_raw_member(""play_delay"",
                                            0,
                                            ""engine.util.sound.Sound"")
        sound_raw_api_object.add_raw_member(""sounds"",
                                            sounds,
                                            ""engine.util.sound.Sound"")

        sound_forward_ref = ForwardRef(terrain_group, sound_name)
        raw_api_object.add_raw_member(""sound"",
                                      sound_forward_ref,
                                      ""engine.util.terrain.Terrain"")

        terrain_group.add_raw_api_object(sound_raw_api_object)

        # =======================================================================
        # Ambience
        # =======================================================================
        terrain = terrain_group.get_terrain()
        # ambients_count = terrain[""terrain_units_used_count""].value

        ambience = []
        # TODO: Ambience
# ===============================================================================
#         for ambient_index in range(ambients_count):
#             ambient_id = terrain[""terrain_unit_id""][ambient_index].value
#
#             if ambient_id == -1:
#                 continue
#
#             ambient_line = dataset.unit_ref[ambient_id]
#             ambient_name = name_lookup_dict[ambient_line.get_head_unit_id()][0]
#
#             ambient_ref = ""%s.Ambient%s"" % (terrain_name, str(ambient_index))
#             ambient_raw_api_object = RawAPIObject(ambient_ref,
#                                                   ""Ambient%s"" % (str(ambient_index)),
#                                                   dataset.nyan_api_objects)
#             ambient_raw_api_object.add_raw_parent(""engine.util.terrain.TerrainAmbient"")
#             ambient_location = ForwardRef(terrain_group, terrain_name)
#             ambient_raw_api_object.set_location(ambient_location)
#
#             # Game entity reference
#             ambient_line_forward_ref = ForwardRef(ambient_line, ambient_name)
#             ambient_raw_api_object.add_raw_member(""object"",
#                                                   ambient_line_forward_ref,
#                                                   ""engine.util.terrain.TerrainAmbient"")
#
#             # Max density
#             max_density = terrain[""terrain_unit_density""][ambient_index].value
#             ambient_raw_api_object.add_raw_member(""max_density"",
#                                                   max_density,
#                                                   ""engine.util.terrain.TerrainAmbient"")
#
#             terrain_group.add_raw_api_object(ambient_raw_api_object)
#             terrain_ambient_forward_ref = ForwardRef(terrain_group, ambient_ref)
#             ambience.append(terrain_ambient_forward_ref)
# ===============================================================================

        raw_api_object.add_raw_member(""ambience"", ambience, ""engine.util.terrain.Terrain"")

        # =======================================================================
        # Graphic
        # =======================================================================
        texture_id = terrain.get_id()

        # Create animation object
        graphic_name = f""{terrain_name}.TerrainTexture""
        graphic_raw_api_object = RawAPIObject(graphic_name, ""TerrainTexture"",
                                              dataset.nyan_api_objects)
        graphic_raw_api_object.add_raw_parent(""engine.util.graphics.Terrain"")
        graphic_location = ForwardRef(terrain_group, terrain_name)
        graphic_raw_api_object.set_location(graphic_location)

        if texture_id in dataset.combined_terrains.keys():
            terrain_graphic = dataset.combined_terrains[texture_id]

        else:
            terrain_graphic = CombinedTerrain(texture_id,
                                              f""texture_{terrain_lookup_dict[terrain_index][2]}"",
                                              dataset)
            dataset.combined_terrains.update({terrain_graphic.get_id(): terrain_graphic})

        terrain_graphic.add_reference(graphic_raw_api_object)

        graphic_raw_api_object.add_raw_member(""sprite"", terrain_graphic,
                                              ""engine.util.graphics.Terrain"")

        terrain_group.add_raw_api_object(graphic_raw_api_object)
        graphic_forward_ref = ForwardRef(terrain_group, graphic_name)
        raw_api_object.add_raw_member(""terrain_graphic"", graphic_forward_ref,
                                      ""engine.util.terrain.Terrain"")",_4097.py,37,"for terrain_type in terrain_type_lookup_dict.values():
    if terrain_index in terrain_type[0]:
        type_name = f'util.terrain_type.types.{terrain_type[2]}'
        type_obj = dataset.pregen_nyan_objects[type_name].get_nyan_object()
        terrain_types.append(type_obj)",terrain_types = [dataset.pregen_nyan_objects[f'util.terrain_type.types.{terrain_type[2]}'].get_nyan_object() for terrain_type in terrain_type_lookup_dict.values() if terrain_index in terrain_type[0]],1.0
https://github.com/yenchenlin/DeepLearningFlappyBird/tree/master//deep_q_network.py,"def trainNetwork(s, readout, h_fc1, sess):
    # define the cost function
    a = tf.placeholder(""float"", [None, ACTIONS])
    y = tf.placeholder(""float"", [None])
    readout_action = tf.reduce_sum(tf.multiply(readout, a), reduction_indices=1)
    cost = tf.reduce_mean(tf.square(y - readout_action))
    train_step = tf.train.AdamOptimizer(1e-6).minimize(cost)

    # open up a game state to communicate with emulator
    game_state = game.GameState()

    # store the previous observations in replay memory
    D = deque()

    # printing
    a_file = open(""logs_"" + GAME + ""/readout.txt"", 'w')
    h_file = open(""logs_"" + GAME + ""/hidden.txt"", 'w')

    # get the first state by doing nothing and preprocess the image to 80x80x4
    do_nothing = np.zeros(ACTIONS)
    do_nothing[0] = 1
    x_t, r_0, terminal = game_state.frame_step(do_nothing)
    x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)
    ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)
    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)

    # saving and loading networks
    saver = tf.train.Saver()
    sess.run(tf.initialize_all_variables())
    checkpoint = tf.train.get_checkpoint_state(""saved_networks"")
    if checkpoint and checkpoint.model_checkpoint_path:
        saver.restore(sess, checkpoint.model_checkpoint_path)
        print(""Successfully loaded:"", checkpoint.model_checkpoint_path)
    else:
        print(""Could not find old network weights"")

    # start training
    epsilon = INITIAL_EPSILON
    t = 0
    while ""flappy bird"" != ""angry bird"":
        # choose an action epsilon greedily
        readout_t = readout.eval(feed_dict={s : [s_t]})[0]
        a_t = np.zeros([ACTIONS])
        action_index = 0
        if t % FRAME_PER_ACTION == 0:
            if random.random() <= epsilon:
                print(""----------Random Action----------"")
                action_index = random.randrange(ACTIONS)
                a_t[random.randrange(ACTIONS)] = 1
            else:
                action_index = np.argmax(readout_t)
                a_t[action_index] = 1
        else:
            a_t[0] = 1 # do nothing

        # scale down epsilon
        if epsilon > FINAL_EPSILON and t > OBSERVE:
            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE

        # run the selected action and observe next state and reward
        x_t1_colored, r_t, terminal = game_state.frame_step(a_t)
        x_t1 = cv2.cvtColor(cv2.resize(x_t1_colored, (80, 80)), cv2.COLOR_BGR2GRAY)
        ret, x_t1 = cv2.threshold(x_t1, 1, 255, cv2.THRESH_BINARY)
        x_t1 = np.reshape(x_t1, (80, 80, 1))
        #s_t1 = np.append(x_t1, s_t[:,:,1:], axis = 2)
        s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)

        # store the transition in D
        D.append((s_t, a_t, r_t, s_t1, terminal))
        if len(D) > REPLAY_MEMORY:
            D.popleft()

        # only train if done observing
        if t > OBSERVE:
            # sample a minibatch to train on
            minibatch = random.sample(D, BATCH)

            # get the batch variables
            s_j_batch = [d[0] for d in minibatch]
            a_batch = [d[1] for d in minibatch]
            r_batch = [d[2] for d in minibatch]
            s_j1_batch = [d[3] for d in minibatch]

            y_batch = []
            readout_j1_batch = readout.eval(feed_dict = {s : s_j1_batch})
            for i in range(0, len(minibatch)):
                terminal = minibatch[i][4]
                # if terminal, only equals reward
                if terminal:
                    y_batch.append(r_batch[i])
                else:
                    y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))

            # perform gradient step
            train_step.run(feed_dict = {
                y : y_batch,
                a : a_batch,
                s : s_j_batch}
            )

        # update the old values
        s_t = s_t1
        t += 1

        # save progress every 10000 iterations
        if t % 10000 == 0:
            saver.save(sess, 'saved_networks/' + GAME + '-dqn', global_step = t)

        # print info
        state = """"
        if t <= OBSERVE:
            state = ""observe""
        elif t > OBSERVE and t <= OBSERVE + EXPLORE:
            state = ""explore""
        else:
            state = ""train""

        print(""TIMESTEP"", t, ""/ STATE"", state, \
            ""/ EPSILON"", epsilon, ""/ ACTION"", action_index, ""/ REWARD"", r_t, \
            ""/ Q_MAX %e"" % np.max(readout_t))
        # write info to files
        '''
        if t % 10000 <= 100:
            a_file.write("","".join([str(x) for x in readout_t]) + '\n')
            h_file.write("","".join([str(x) for x in h_fc1.eval(feed_dict={s:[s_t]})[0]]) + '\n')
            cv2.imwrite(""logs_tetris/frame"" + str(t) + "".png"", x_t1)
        '''",_4107.py,86,"for i in range(0, len(minibatch)):
    terminal = minibatch[i][4]
    if terminal:
        y_batch.append(r_batch[i])
    else:
        y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))","y_batch = [r_batch[i] if minibatch[i][4] else r_batch[i] + GAMMA * np.max(readout_j1_batch[i]) for i in range(0, len(minibatch))]",1.0
https://github.com/mwielgoszewski/doorman/tree/master/doorman/extract_ddl.py,"def extract_schema(filename):
    namespace = {
        'Column': Column,
        'schema': schema,
        'table_name': table_name,
        'extended_schema': extended_schema,
        'current_spec': {},
    }

    for fn in DUMMY_FUNCTIONS:
        namespace[fn] = lambda *args, **kwargs: None

    for ty in SQL_TYPES:
        namespace[ty] = ty

    with open(filename, 'rU') as f:
        tree = ast.parse(f.read())
        exec(compile(tree, '<string>', 'exec'), namespace)

    columns = ', '.join('%s %s' % (x[0], x[1]) for x in current_spec['schema'])
    statements = []
    statements = []
    statements.append('CREATE TABLE %s (%s);' % (current_spec['name'], columns))
    if 'extended_schema' in current_spec:
        statement = 'ALTER TABLE %s ADD %%s %%s;' % (current_spec['name'], )
        for column_name, column_definition in current_spec['extended_schema']:
            statements.append(statement % (column_name, column_definition))
        del current_spec['extended_schema']
    return '\n'.join(statements)",_4127.py,26,"for (column_name, column_definition) in current_spec['extended_schema']:
    statements.append(statement % (column_name, column_definition))","statements.extend([statement % (column_name, column_definition) for (column_name, column_definition) in current_spec['extended_schema']])",1.0
https://github.com/shibing624/pycorrector/tree/master/pycorrector/utils/tokenizer.py,"def segment(sentence, cut_type='word', pos=False):
    """"""
    切词
    :param sentence:
    :param cut_type: 'word' use jieba.lcut; 'char' use list(sentence)
    :param pos: enable POS
    :return: list
    """"""
    if pos:
        if cut_type == 'word':
            word_pos_seq = posseg.lcut(sentence)
            word_seq, pos_seq = [], []
            for w, p in word_pos_seq:
                word_seq.append(w)
                pos_seq.append(p)
            return word_seq, pos_seq
        elif cut_type == 'char':
            word_seq = list(sentence)
            pos_seq = []
            for w in word_seq:
                w_p = posseg.lcut(w)
                pos_seq.append(w_p[0].flag)
            return word_seq, pos_seq
    else:
        if cut_type == 'word':
            return jieba.lcut(sentence)
        elif cut_type == 'char':
            return list(sentence)",_4177.py,20,"for w in word_seq:
    w_p = posseg.lcut(w)
    pos_seq.append(w_p[0].flag)",pos_seq = [posseg.lcut(w)[0].flag for w in word_seq],1.0
https://github.com/jessevdk/cldoc/tree/master/cldoc/cmdgir.py,"def find_ref(self, node, name, goup):
        if node is None:
            return []

        ret = []

        for child in node.resolve_nodes:
            if self.match_ref(child, name):
                ret.append(child)

        if goup and len(ret) == 0:
            return self.find_ref(node.parent, name, True)
        else:
            return ret",_4277.py,7,"for child in node.resolve_nodes:
    if self.match_ref(child, name):
        ret.append(child)","ret = [child for child in node.resolve_nodes if self.match_ref(child, name)]",1.0
https://github.com/Alexey-T/CudaText/tree/master/app/py/sys/urllib3/fields.py,"def render_headers(self):
        """"""
        Renders the headers for this request field.
        """"""
        lines = []

        sort_keys = [""Content-Disposition"", ""Content-Type"", ""Content-Location""]
        for sort_key in sort_keys:
            if self.headers.get(sort_key, False):
                lines.append(u""%s: %s"" % (sort_key, self.headers[sort_key]))

        for header_name, header_value in self.headers.items():
            if header_name not in sort_keys:
                if header_value:
                    lines.append(u""%s: %s"" % (header_name, header_value))

        lines.append(u""\r\n"")
        return u""\r\n"".join(lines)",_4382.py,8,"for sort_key in sort_keys:
    if self.headers.get(sort_key, False):
        lines.append(u'%s: %s' % (sort_key, self.headers[sort_key]))","lines = [u'%s: %s' % (sort_key, self.headers[sort_key]) for sort_key in sort_keys if self.headers.get(sort_key, False)]",1.0
https://github.com/Alexey-T/CudaText/tree/master/app/py/sys/urllib3/fields.py,"def render_headers(self):
        """"""
        Renders the headers for this request field.
        """"""
        lines = []

        sort_keys = [""Content-Disposition"", ""Content-Type"", ""Content-Location""]
        for sort_key in sort_keys:
            if self.headers.get(sort_key, False):
                lines.append(u""%s: %s"" % (sort_key, self.headers[sort_key]))

        for header_name, header_value in self.headers.items():
            if header_name not in sort_keys:
                if header_value:
                    lines.append(u""%s: %s"" % (header_name, header_value))

        lines.append(u""\r\n"")
        return u""\r\n"".join(lines)",_4382.py,12,"for (header_name, header_value) in self.headers.items():
    if header_name not in sort_keys:
        if header_value:
            lines.append(u'%s: %s' % (header_name, header_value))","lines.extend([u'%s: %s' % (header_name, header_value) for (header_name, header_value) in self.headers.items() if header_name not in sort_keys if header_value])",1.0
https://github.com/open-mmlab/mmpose/tree/master/mmpose/models/backbones/hrnet.py,"def _make_branches(self, num_branches, block, num_blocks, num_channels):
        """"""Make branches.""""""
        branches = []

        for i in range(num_branches):
            branches.append(
                self._make_one_branch(i, block, num_blocks, num_channels))

        return nn.ModuleList(branches)",_4461.py,5,"for i in range(num_branches):
    branches.append(self._make_one_branch(i, block, num_blocks, num_channels))","branches = [self._make_one_branch(i, block, num_blocks, num_channels) for i in range(num_branches)]",1.0
https://github.com/graph4ai/graph4nlp/tree/master/graph4nlp/pytorch/modules/prediction/generation/decoder_strategy.py,"def beam_search_for_tree_decoding(
        self,
        decoder_initial_state,
        decoder_initial_input,
        parent_state,
        graph_node_embedding,
        rnn_node_embedding=None,
        src_seq=None,
        oov_dict=None,
        sibling_state=None,
        device=None,
        topk=1,
        enc_batch=None,
    ):
        min_out_len = 1
        max_out_len = self.max_decoder_step
        batch_size = graph_node_embedding.size(0)
        assert batch_size == 1
        decoder_hidden = decoder_initial_state

        batch_results = []
        for _ in range(batch_size):
            single_graph_node_embedding = graph_node_embedding.expand(
                self.beam_size, -1, -1
            ).contiguous()
            single_parent_state = parent_state.expand(self.beam_size, -1).contiguous()

            step = 0
            results, backup_results = [], []
            hypos = [
                Hypothesis(
                    tokens=[decoder_initial_input],
                    log_probs=[],
                    dec_state=decoder_hidden,
                    input_feed=None,
                    num_non_words=1,
                    enc_attn_weights=[],
                    use_coverage=self.use_coverage,
                    states_for_tree=[decoder_hidden],
                )
            ]

            while len(hypos) > 0 and step <= self.max_decoder_step:
                n_hypos = len(hypos)
                if n_hypos < self.beam_size:
                    hypos.extend(
                        hypos[-1] for _ in range(self.beam_size - n_hypos)
                    )  # check deep copy
                decoder_input = torch.tensor([h.tokens[-1] for h in hypos]).to(
                    graph_node_embedding.device
                )
                decoder_hidden = (
                    torch.cat([h.dec_state[0] for h in hypos], 0),
                    torch.cat([h.dec_state[1] for h in hypos], 0),
                )

                prediction, decoder_hidden, dec_attn_scores = self.decoder.decode_step(
                    tgt_batch_size=self.beam_size,
                    dec_single_input=decoder_input,
                    dec_single_state=decoder_hidden,
                    memory=single_graph_node_embedding,
                    parent_state=single_parent_state,
                    oov_dict=oov_dict,
                    enc_batch=enc_batch,
                )
                prediction = torch.log(prediction + 1e-31)
                top_v, top_i = prediction.data.topk(self.beam_size)

                new_hypos = []
                for in_idx in range(n_hypos):
                    for out_idx in range(self.beam_size):
                        new_tok = top_i[in_idx][out_idx].item()
                        new_prob = top_v[in_idx][out_idx].item()
                        new_enc_attn_weights = dec_attn_scores[in_idx, :].unsqueeze(0).unsqueeze(0)

                        non_word = new_tok == self.vocab.get_symbol_idx(
                            self.vocab.end_token
                        )  # only SOS & EOS don't count
                        tmp_decoder_state = (
                            decoder_hidden[0][in_idx, :].unsqueeze(0),
                            decoder_hidden[1][in_idx, :].unsqueeze(0),
                        )

                        new_hypo = hypos[in_idx].create_next(
                            token=new_tok,
                            log_prob=new_prob,
                            dec_state=tmp_decoder_state,
                            input_feed=None,
                            non_word=non_word,
                            add_enc_attn_weights=new_enc_attn_weights,
                        )
                        new_hypos.append(new_hypo)

                new_hypos = sorted(new_hypos, key=lambda h: -h.avg_log_prob)[: self.beam_size]
                hypos = []
                new_complete_results, new_incomplete_results = [], []
                for nh in new_hypos:
                    length = len(nh)  # Does not count SOS and EOS
                    if nh.tokens[-1] == self.vocab.get_symbol_idx(
                        self.vocab.end_token
                    ):  # a complete hypothesis
                        if (
                            len(new_complete_results) < self.beam_size
                            and min_out_len <= length <= max_out_len
                        ):
                            new_complete_results.append(nh)
                    elif (
                        len(hypos) < self.beam_size and length < max_out_len
                    ):  # an incomplete hypothesis
                        hypos.append(nh)
                    elif length == max_out_len and len(new_incomplete_results) < self.beam_size:
                        new_incomplete_results.append(nh)
                if new_complete_results:
                    results.extend(new_complete_results)
                elif new_incomplete_results:
                    backup_results.extend(new_incomplete_results)
                step += 1

            if (
                not results
            ):  # if no sequence ends with EOS within desired length, fallback to sequences
                results = backup_results  # that are ""truncated"" at the end to max_out_len
            batch_results.append(sorted(results, key=lambda h: -h.avg_log_prob)[:topk])

        ret = torch.zeros(batch_size, topk, self.max_decoder_step).long()
        states = []
        for sent_id, each in enumerate(batch_results):
            for i in range(topk):
                ids = torch.Tensor(each[i].tokens[:])[: self.max_decoder_step]
                if ids.shape[0] < self.max_decoder_step:
                    pad = torch.zeros(self.max_decoder_step - ids.shape[0])
                    ids = torch.cat((ids, pad), dim=0)
                ret[sent_id, i, :] = ids
                states.append(each[i].states_for_tree[:][: self.max_decoder_step])
        assert batch_size == 1 and topk == 1
        # for id
        output_results = [[[]]]

        token_id_list = ret[0][0]
        states_emb_list = states[0]
        for index in range(self.max_decoder_step):
            if token_id_list[index] != self.vocab.get_symbol_idx(self.vocab.pad_token):
                output_results[0][0].append(
                    BeamSearchNode(
                        hiddenstate=states_emb_list[index],
                        enc_attn_weights_average=None,
                        previousNode=None,
                        wordId=token_id_list[index],
                        logProb=0,
                        length=-1,
                    )
                )

        return output_results",_4627.py,70,"for in_idx in range(n_hypos):
    for out_idx in range(self.beam_size):
        new_tok = top_i[in_idx][out_idx].item()
        new_prob = top_v[in_idx][out_idx].item()
        new_enc_attn_weights = dec_attn_scores[in_idx, :].unsqueeze(0).unsqueeze(0)
        non_word = new_tok == self.vocab.get_symbol_idx(self.vocab.end_token)
        tmp_decoder_state = (decoder_hidden[0][in_idx, :].unsqueeze(0), decoder_hidden[1][in_idx, :].unsqueeze(0))
        new_hypo = hypos[in_idx].create_next(token=new_tok, log_prob=new_prob, dec_state=tmp_decoder_state, input_feed=None, non_word=non_word, add_enc_attn_weights=new_enc_attn_weights)
        new_hypos.append(new_hypo)","new_hypos = [hypos[in_idx].create_next(token=top_i[in_idx][out_idx].item(), log_prob=top_v[in_idx][out_idx].item(), dec_state=(decoder_hidden[0][in_idx, :].unsqueeze(0), decoder_hidden[1][in_idx, :].unsqueeze(0)), input_feed=None, non_word=top_i[in_idx][out_idx].item() == self.vocab.get_symbol_idx(self.vocab.end_token), add_enc_attn_weights=dec_attn_scores[in_idx, :].unsqueeze(0).unsqueeze(0)) for in_idx in range(n_hypos) for out_idx in range(self.beam_size)]",1.0
https://github.com/commixproject/commix/tree/master/src/core/requests/authentication.py,"def define_wordlists():

  while True:
    message = ""Do you want to use default wordlists for dictionary-based attack? [Y/n] > ""
    do_update = common.read_input(message, default=""Y"", check_batch=True)
    if do_update in settings.CHOICE_YES:
      username_txt_file = settings.USERNAMES_TXT_FILE
      passwords_txt_file = settings.PASSWORDS_TXT_FILE
      info_msg = ""Setting default wordlists for dictionary-based attack.""
      print(settings.print_info_msg(info_msg))
      break
    elif do_update in settings.CHOICE_NO:
      message = ""Please enter usernames wordlist > ""
      username_txt_file = common.read_input(message, default=None, check_batch=True)
      message = ""Please enter passwords wordlist > ""
      passwords_txt_file = common.read_input(message, default=None, check_batch=True)
      break
    elif do_update in settings.CHOICE_QUIT:
      raise SystemExit()
    else:
      common.invalid_option(do_update)  
      pass

  try:
    usernames = []
    if settings.VERBOSITY_LEVEL != 0:
      debug_msg = ""Parsing usernames wordlist '"" + username_txt_file + ""'.""
      print(settings.print_debug_msg(debug_msg))
    if not os.path.isfile(username_txt_file):
      err_msg = ""The specified file '"" + str(username_txt_file) + ""' does not exist.""
      print(settings.print_critical_msg(err_msg))
      raise SystemExit() 
    if len(username_txt_file) == 0:
      err_msg = ""The specified file '"" + str(username_txt_file) + ""' seems empty.""
      print(settings.print_critical_msg(err_msg))
      raise SystemExit()
    with open(username_txt_file, ""r"") as f: 
      for line in f:
        line = line.strip()
        usernames.append(line)
  except IOError: 
    err_msg = "" Check if file '"" + str(username_txt_file) + ""' is readable or corrupted.""
    print(settings.print_critical_msg(err_msg))
    raise SystemExit()

  try:
    passwords = []
    if settings.VERBOSITY_LEVEL != 0:
      debug_msg = ""Parsing passwords wordlist '"" + passwords_txt_file + ""'.""
      print(settings.print_debug_msg(debug_msg))
    if not os.path.isfile(passwords_txt_file):
      err_msg = ""The specified file '"" + str(passwords_txt_file) + ""' does not exist.""
      print(settings.print_critical_msg(err_msg))
      raise SystemExit() 
    if len(passwords_txt_file) == 0:
      err_msg = ""The specified file '"" + str(passwords_txt_file) + ""' seems empty.""
      print(settings.print_critical_msg(err_msg))
      raise SystemExit() 
    with open(passwords_txt_file, ""r"") as f: 
      for line in f:
        line = line.strip()
        passwords.append(line)
  except IOError: 
    err_msg = "" Check if file '"" + str(passwords_txt_file) + ""' is readable or corrupted.""
    print(settings.print_critical_msg(err_msg))
    raise SystemExit()

  return usernames, passwords",_4639.py,38,"for line in f:
    line = line.strip()
    usernames.append(line)",usernames = [line.strip() for line in f],1.0
https://github.com/commixproject/commix/tree/master/src/core/requests/authentication.py,"def define_wordlists():

  while True:
    message = ""Do you want to use default wordlists for dictionary-based attack? [Y/n] > ""
    do_update = common.read_input(message, default=""Y"", check_batch=True)
    if do_update in settings.CHOICE_YES:
      username_txt_file = settings.USERNAMES_TXT_FILE
      passwords_txt_file = settings.PASSWORDS_TXT_FILE
      info_msg = ""Setting default wordlists for dictionary-based attack.""
      print(settings.print_info_msg(info_msg))
      break
    elif do_update in settings.CHOICE_NO:
      message = ""Please enter usernames wordlist > ""
      username_txt_file = common.read_input(message, default=None, check_batch=True)
      message = ""Please enter passwords wordlist > ""
      passwords_txt_file = common.read_input(message, default=None, check_batch=True)
      break
    elif do_update in settings.CHOICE_QUIT:
      raise SystemExit()
    else:
      common.invalid_option(do_update)  
      pass

  try:
    usernames = []
    if settings.VERBOSITY_LEVEL != 0:
      debug_msg = ""Parsing usernames wordlist '"" + username_txt_file + ""'.""
      print(settings.print_debug_msg(debug_msg))
    if not os.path.isfile(username_txt_file):
      err_msg = ""The specified file '"" + str(username_txt_file) + ""' does not exist.""
      print(settings.print_critical_msg(err_msg))
      raise SystemExit() 
    if len(username_txt_file) == 0:
      err_msg = ""The specified file '"" + str(username_txt_file) + ""' seems empty.""
      print(settings.print_critical_msg(err_msg))
      raise SystemExit()
    with open(username_txt_file, ""r"") as f: 
      for line in f:
        line = line.strip()
        usernames.append(line)
  except IOError: 
    err_msg = "" Check if file '"" + str(username_txt_file) + ""' is readable or corrupted.""
    print(settings.print_critical_msg(err_msg))
    raise SystemExit()

  try:
    passwords = []
    if settings.VERBOSITY_LEVEL != 0:
      debug_msg = ""Parsing passwords wordlist '"" + passwords_txt_file + ""'.""
      print(settings.print_debug_msg(debug_msg))
    if not os.path.isfile(passwords_txt_file):
      err_msg = ""The specified file '"" + str(passwords_txt_file) + ""' does not exist.""
      print(settings.print_critical_msg(err_msg))
      raise SystemExit() 
    if len(passwords_txt_file) == 0:
      err_msg = ""The specified file '"" + str(passwords_txt_file) + ""' seems empty.""
      print(settings.print_critical_msg(err_msg))
      raise SystemExit() 
    with open(passwords_txt_file, ""r"") as f: 
      for line in f:
        line = line.strip()
        passwords.append(line)
  except IOError: 
    err_msg = "" Check if file '"" + str(passwords_txt_file) + ""' is readable or corrupted.""
    print(settings.print_critical_msg(err_msg))
    raise SystemExit()

  return usernames, passwords",_4639.py,60,"for line in f:
    line = line.strip()
    passwords.append(line)",passwords = [line.strip() for line in f],1.0
https://github.com/microsoft/torchgeo/tree/master/torchgeo/datasets/zuericrop.py,"def _verify(self) -> None:
        """"""Verify the integrity of the dataset.

        Raises:
            RuntimeError: if ``download=False`` but dataset is missing or checksum fails
        """"""
        # Check if the files already exist
        exists = []
        for filename in self.filenames:
            filepath = os.path.join(self.root, filename)
            exists.append(os.path.exists(filepath))

        if all(exists):
            return

        # Check if the user requested to download the dataset
        if not self.download:
            raise RuntimeError(
                ""Dataset not found in `root` directory and `download=False`, ""
                ""either specify a different `root` directory or use `download=True` ""
                ""to automatically download the dataset.""
            )

        # Download the dataset
        self._download()",_4643.py,9,"for filename in self.filenames:
    filepath = os.path.join(self.root, filename)
    exists.append(os.path.exists(filepath))","exists = [os.path.exists(os.path.join(self.root, filename)) for filename in self.filenames]",1.0
https://github.com/dandelin/ViLT/tree/master/vilt/modules/objectives.py,"def vqa_test_wrapup(outs, model_name):
    rank = torch.distributed.get_rank()
    qids, preds = list(), list()
    for out in outs:
        qids += out[""qids""]
        preds += out[""preds""]

    rets = list()
    for qid, pred in zip(qids, preds):
        rets.append({""question_id"": qid, ""answer"": pred})
    with open(f""vqa_submit_{rank}.json"", ""w"") as fp:
        json.dump(rets, fp, indent=4)

    torch.distributed.barrier()

    if rank == 0:
        jsons = list()
        paths = list(glob.glob(""vqa_submit_*.json""))
        for path in paths:
            with open(path, ""r"") as fp:
                jsons += json.load(fp)
        os.makedirs(""result"", exist_ok=True)
        with open(f""result/vqa_submit_{model_name}.json"", ""w"") as fp:
            json.dump(jsons, fp, indent=4)

    torch.distributed.barrier()
    os.remove(f""vqa_submit_{rank}.json"")",_4652.py,9,"for (qid, pred) in zip(qids, preds):
    rets.append({'question_id': qid, 'answer': pred})","rets = [{'question_id': qid, 'answer': pred} for (qid, pred) in zip(qids, preds)]",1.0
https://github.com/tartiflette/tartiflette/tree/master/tartiflette/schema/schema.py,"def _validate_non_empty_object(self) -> List[str]:
        """"""
        Validates that object types implement at least one fields.
        :return: a list of errors
        :rtype: List[str]
        """"""
        errors = []
        for type_name, gql_type in self.type_definitions.items():
            if isinstance(gql_type, GraphQLObjectType) and not [
                field_name
                for field_name in gql_type.implemented_fields
                if not field_name.startswith(""__"")
            ]:
                errors.append(f""Type < {type_name} > has no fields."")
        return errors",_4680.py,8,"for (type_name, gql_type) in self.type_definitions.items():
    if isinstance(gql_type, GraphQLObjectType) and (not [field_name for field_name in gql_type.implemented_fields if not field_name.startswith('__')]):
        errors.append(f'Type < {type_name} > has no fields.')","errors = [f'Type < {type_name} > has no fields.' for (type_name, gql_type) in self.type_definitions.items() if isinstance(gql_type, GraphQLObjectType) and (not [field_name for field_name in gql_type.implemented_fields if not field_name.startswith('__')])]",1.0
https://github.com/microsoft/nni/tree/master/nni/algorithms/hpo/networkmorphism_tuner/graph.py,"def __init__(self, graph):
        data = dict()
        node_list = list()
        layer_list = list()
        operation_history = list()

        data[""input_shape""] = graph.input_shape
        vis = graph.vis
        data[""vis""] = list(vis.keys()) if vis is not None else None
        data[""weighted""] = graph.weighted

        for item in graph.operation_history:
            if item[0] == ""to_deeper_model"":
                operation_history.append(
                    [
                        item[0],
                        item[1],
                        layer_description_extractor(item[2], graph.node_to_id),
                    ]
                )
            else:
                operation_history.append(item)
        data[""operation_history""] = operation_history
        data[""layer_id_to_input_node_ids""] = graph.layer_id_to_input_node_ids
        data[""layer_id_to_output_node_ids""] = graph.layer_id_to_output_node_ids
        data[""adj_list""] = graph.adj_list
        data[""reverse_adj_list""] = graph.reverse_adj_list

        for node in graph.node_list:
            node_id = graph.node_to_id[node]
            node_information = node.shape
            node_list.append((node_id, node_information))

        for layer_id, item in enumerate(graph.layer_list):
            layer = graph.layer_list[layer_id]
            layer_information = layer_description_extractor(
                layer, graph.node_to_id)
            layer_list.append((layer_id, layer_information))

        data[""node_list""] = node_list
        data[""layer_list""] = layer_list

        self.data = data",_4799.py,12,"for item in graph.operation_history:
    if item[0] == 'to_deeper_model':
        operation_history.append([item[0], item[1], layer_description_extractor(item[2], graph.node_to_id)])
    else:
        operation_history.append(item)","operation_history = [[item[0], item[1], layer_description_extractor(item[2], graph.node_to_id)] if item[0] == 'to_deeper_model' else item for item in graph.operation_history]",1.0
https://github.com/microsoft/nni/tree/master/nni/algorithms/hpo/networkmorphism_tuner/graph.py,"def __init__(self, graph):
        data = dict()
        node_list = list()
        layer_list = list()
        operation_history = list()

        data[""input_shape""] = graph.input_shape
        vis = graph.vis
        data[""vis""] = list(vis.keys()) if vis is not None else None
        data[""weighted""] = graph.weighted

        for item in graph.operation_history:
            if item[0] == ""to_deeper_model"":
                operation_history.append(
                    [
                        item[0],
                        item[1],
                        layer_description_extractor(item[2], graph.node_to_id),
                    ]
                )
            else:
                operation_history.append(item)
        data[""operation_history""] = operation_history
        data[""layer_id_to_input_node_ids""] = graph.layer_id_to_input_node_ids
        data[""layer_id_to_output_node_ids""] = graph.layer_id_to_output_node_ids
        data[""adj_list""] = graph.adj_list
        data[""reverse_adj_list""] = graph.reverse_adj_list

        for node in graph.node_list:
            node_id = graph.node_to_id[node]
            node_information = node.shape
            node_list.append((node_id, node_information))

        for layer_id, item in enumerate(graph.layer_list):
            layer = graph.layer_list[layer_id]
            layer_information = layer_description_extractor(
                layer, graph.node_to_id)
            layer_list.append((layer_id, layer_information))

        data[""node_list""] = node_list
        data[""layer_list""] = layer_list

        self.data = data",_4799.py,29,"for node in graph.node_list:
    node_id = graph.node_to_id[node]
    node_information = node.shape
    node_list.append((node_id, node_information))","node_list = [(graph.node_to_id[node], node.shape) for node in graph.node_list]",1.0
https://github.com/microsoft/nni/tree/master/nni/algorithms/hpo/networkmorphism_tuner/graph.py,"def __init__(self, graph):
        data = dict()
        node_list = list()
        layer_list = list()
        operation_history = list()

        data[""input_shape""] = graph.input_shape
        vis = graph.vis
        data[""vis""] = list(vis.keys()) if vis is not None else None
        data[""weighted""] = graph.weighted

        for item in graph.operation_history:
            if item[0] == ""to_deeper_model"":
                operation_history.append(
                    [
                        item[0],
                        item[1],
                        layer_description_extractor(item[2], graph.node_to_id),
                    ]
                )
            else:
                operation_history.append(item)
        data[""operation_history""] = operation_history
        data[""layer_id_to_input_node_ids""] = graph.layer_id_to_input_node_ids
        data[""layer_id_to_output_node_ids""] = graph.layer_id_to_output_node_ids
        data[""adj_list""] = graph.adj_list
        data[""reverse_adj_list""] = graph.reverse_adj_list

        for node in graph.node_list:
            node_id = graph.node_to_id[node]
            node_information = node.shape
            node_list.append((node_id, node_information))

        for layer_id, item in enumerate(graph.layer_list):
            layer = graph.layer_list[layer_id]
            layer_information = layer_description_extractor(
                layer, graph.node_to_id)
            layer_list.append((layer_id, layer_information))

        data[""node_list""] = node_list
        data[""layer_list""] = layer_list

        self.data = data",_4799.py,34,"for (layer_id, item) in enumerate(graph.layer_list):
    layer = graph.layer_list[layer_id]
    layer_information = layer_description_extractor(layer, graph.node_to_id)
    layer_list.append((layer_id, layer_information))","layer_list = [(layer_id, layer_description_extractor(graph.layer_list[layer_id], graph.node_to_id)) for (layer_id, item) in enumerate(graph.layer_list)]",1.0
https://github.com/pytorch/fairseq/tree/master/examples/speech_to_text/prep_mtedx_data.py,"def __init__(self, root: str, lang: str, split: str) -> None:
        assert split in self.SPLITS and lang in self.LANGPAIRS
        _root = Path(root) / f""{lang}"" / ""data"" / split
        wav_root, txt_root = _root / ""wav"", _root / ""txt""
        assert _root.is_dir() and wav_root.is_dir() and txt_root.is_dir()
        # Load audio segments
        try:
            import yaml
        except ImportError:
            print(
                ""Please install PyYAML to load the Multilingual TEDx YAML files""
            )
        with open(txt_root / f""{split}.yaml"") as f:
            segments = yaml.load(f, Loader=yaml.BaseLoader)
        # Load source and target utterances
        src, tgt = lang.split(""-"")
        for _lang in [src, tgt]:
            with open(txt_root / f""{split}.{_lang}"") as f:
                utterances = [r.strip() for r in f]
            assert len(segments) == len(utterances)
            for i, u in enumerate(utterances):
                segments[i][_lang] = u
        # Gather info
        self.data = []
        for wav_filename, _seg_group in groupby(segments, lambda x: x[""wav""]):
            wav_filename = wav_filename.replace("".wav"", "".flac"")
            wav_path = wav_root / wav_filename
            sample_rate = sf.info(wav_path.as_posix()).samplerate
            seg_group = sorted(_seg_group, key=lambda x: float(x[""offset""]))
            for i, segment in enumerate(seg_group):
                offset = int(float(segment[""offset""]) * sample_rate)
                n_frames = int(float(segment[""duration""]) * sample_rate)
                _id = f""{wav_path.stem}_{i}""
                self.data.append(
                    (
                        wav_path.as_posix(),
                        offset,
                        n_frames,
                        sample_rate,
                        segment[src],
                        segment[tgt],
                        segment[""speaker_id""],
                        tgt,
                        _id,
                    )
                )",_4943.py,25,"for (wav_filename, _seg_group) in groupby(segments, lambda x: x['wav']):
    wav_filename = wav_filename.replace('.wav', '.flac')
    wav_path = wav_root / wav_filename
    sample_rate = sf.info(wav_path.as_posix()).samplerate
    seg_group = sorted(_seg_group, key=lambda x: float(x['offset']))
    for (i, segment) in enumerate(seg_group):
        offset = int(float(segment['offset']) * sample_rate)
        n_frames = int(float(segment['duration']) * sample_rate)
        _id = f'{wav_path.stem}_{i}'
        self.data.append((wav_path.as_posix(), offset, n_frames, sample_rate, segment[src], segment[tgt], segment['speaker_id'], tgt, _id))","self.data = [((wav_root / wav_filename.replace('.wav', '.flac')).as_posix(), int(float(segment['offset']) * sf.info((wav_root / wav_filename.replace('.wav', '.flac')).as_posix()).samplerate), int(float(segment['duration']) * sf.info((wav_root / wav_filename.replace('.wav', '.flac')).as_posix()).samplerate), sf.info((wav_root / wav_filename.replace('.wav', '.flac')).as_posix()).samplerate, segment[src], segment[tgt], segment['speaker_id'], tgt, f""{(wav_root / wav_filename.replace('.wav', '.flac')).stem}_{i}"") for (wav_filename, _seg_group) in groupby(segments, lambda x: x['wav']) for (i, segment) in enumerate(sorted(_seg_group, key=lambda x: float(x['offset'])))]",1.0
https://github.com/jarun/buku/tree/master/tests/test_import_firefox_json.py,"def test_load_invalid_typecode():
    """"""test method.""""""
    # Arrange
    data = json.loads(""""""
        {
            ""title"" : ""title"",
            ""children"": [
                {
                    ""title"" : ""title1"",
                    ""typeCode"" : 99,
                    ""uri"" : ""http://uri1"",
                    ""annos"" : [{
                         ""name"": ""bookmarkProperties/description"",
                         ""value"": ""desc""
                     }]
                }]
        }"""""")
    # Act
    items = import_firefox_json(data)

    # Assert
    result = []
    for item in items:
        result.append(item)

    assert len(result) == 0",_5122.py,23,"for item in items:
    result.append(item)",result = [item for item in items],1.0
https://github.com/yongzhuo/nlp_xiaojiang/tree/master/ChatBot/chatbot_search/chatbot_tfserving/TFServing_postprocess.py,"def postprocess(predictions):
    """""" 后处理 """"""
    predicts = predictions.get(""predictions"", {})
    token_ids = []
    for p in predicts:
        doc_id = str(p.get(""doc_id"", """"))
        score = p.get(""score"", """")
        answer = id2answer.get(doc_id, """")
        doc = id2doc.get(doc_id, """")
        token_ids.append({""score"": round(score, 6), ""doc"": doc, ""answer"": answer, ""doc_id"": doc_id})
    return {""instances"": token_ids}",_5171.py,5,"for p in predicts:
    doc_id = str(p.get('doc_id', ''))
    score = p.get('score', '')
    answer = id2answer.get(doc_id, '')
    doc = id2doc.get(doc_id, '')
    token_ids.append({'score': round(score, 6), 'doc': doc, 'answer': answer, 'doc_id': doc_id})","token_ids = [{'score': round(p.get('score', ''), 6), 'doc': id2doc.get(str(p.get('doc_id', '')), ''), 'answer': id2answer.get(str(p.get('doc_id', '')), ''), 'doc_id': str(p.get('doc_id', ''))} for p in predicts]",1.0
https://github.com/searx/searx/tree/master/searx/engines/google_scholar.py,"def response(resp):
    """"""Get response from google's search request""""""
    results = []

    detect_google_sorry(resp)

    # which subdomain ?
    # subdomain = resp.search_params.get('google_subdomain')

    # convert the text to dom
    dom = html.fromstring(resp.text)

    # parse results
    for result in eval_xpath_list(dom, '//div[@class=""gs_ri""]'):

        title = extract_text(eval_xpath(result, './h3[1]//a'))

        if not title:
            # this is a [ZITATION] block
            continue

        url = eval_xpath(result, './h3[1]//a/@href')[0]
        content = extract_text(eval_xpath(result, './div[@class=""gs_rs""]')) or ''

        pub_info = extract_text(eval_xpath(result, './div[@class=""gs_a""]'))
        if pub_info:
            content += ""[%s]"" % pub_info

        pub_type = extract_text(eval_xpath(result, './/span[@class=""gs_ct1""]'))
        if pub_type:
            title = title + "" "" + pub_type

        results.append({
            'url':      url,
            'title':    title,
            'content':  content,
        })

    # parse suggestion
    for suggestion in eval_xpath(dom, '//div[contains(@class, ""gs_qsuggest_wrap"")]//li//a'):
        # append suggestion
        results.append({'suggestion': extract_text(suggestion)})

    for correction in eval_xpath(dom, '//div[@class=""gs_r gs_pda""]/a'):
        results.append({'correction': extract_text(correction)})

    return results",_5422.py,40,"for suggestion in eval_xpath(dom, '//div[contains(@class, ""gs_qsuggest_wrap"")]//li//a'):
    results.append({'suggestion': extract_text(suggestion)})","results.extend([{'suggestion': extract_text(suggestion)} for suggestion in eval_xpath(dom, '//div[contains(@class, ""gs_qsuggest_wrap"")]//li//a')])",1.0
https://github.com/searx/searx/tree/master/searx/engines/google_scholar.py,"def response(resp):
    """"""Get response from google's search request""""""
    results = []

    detect_google_sorry(resp)

    # which subdomain ?
    # subdomain = resp.search_params.get('google_subdomain')

    # convert the text to dom
    dom = html.fromstring(resp.text)

    # parse results
    for result in eval_xpath_list(dom, '//div[@class=""gs_ri""]'):

        title = extract_text(eval_xpath(result, './h3[1]//a'))

        if not title:
            # this is a [ZITATION] block
            continue

        url = eval_xpath(result, './h3[1]//a/@href')[0]
        content = extract_text(eval_xpath(result, './div[@class=""gs_rs""]')) or ''

        pub_info = extract_text(eval_xpath(result, './div[@class=""gs_a""]'))
        if pub_info:
            content += ""[%s]"" % pub_info

        pub_type = extract_text(eval_xpath(result, './/span[@class=""gs_ct1""]'))
        if pub_type:
            title = title + "" "" + pub_type

        results.append({
            'url':      url,
            'title':    title,
            'content':  content,
        })

    # parse suggestion
    for suggestion in eval_xpath(dom, '//div[contains(@class, ""gs_qsuggest_wrap"")]//li//a'):
        # append suggestion
        results.append({'suggestion': extract_text(suggestion)})

    for correction in eval_xpath(dom, '//div[@class=""gs_r gs_pda""]/a'):
        results.append({'correction': extract_text(correction)})

    return results",_5422.py,44,"for correction in eval_xpath(dom, '//div[@class=""gs_r gs_pda""]/a'):
    results.append({'correction': extract_text(correction)})","results.extend([{'correction': extract_text(correction)} for correction in eval_xpath(dom, '//div[@class=""gs_r gs_pda""]/a')])",1.0
https://github.com/adipandas/multi-object-tracker/tree/master/motrackers/tracker.py,"def _get_tracks(tracks):
        """"""
        Output the information of tracks.

        Args:
            tracks (OrderedDict): Tracks dictionary with (key, value) as (track_id, corresponding `Track` objects).

        Returns:
            list: List of tracks being currently tracked by the tracker.
        """"""

        outputs = []
        for trackid, track in tracks.items():
            if not track.lost:
                outputs.append(track.output())
        return outputs",_5558.py,13,"for (trackid, track) in tracks.items():
    if not track.lost:
        outputs.append(track.output())","outputs = [track.output() for (trackid, track) in tracks.items() if not track.lost]",1.0
https://github.com/HashPals/Name-That-Hash/tree/master/name_that_hash/hash_namer.py,"def identify_all(self, chash: str):
        logging.debug(f""In identify all with {chash}"")
        chash = chash.strip()
        output = []
        for prototype in self.prototypes:
            if prototype.regex.findall(chash):
                logging.debug(f""Found all matched a regex {prototype.regex}"")
                for mode in prototype.modes:
                    output.append(mode)
        return output",_5671.py,8,"for mode in prototype.modes:
    output.append(mode)",output.extend([mode for mode in prototype.modes]),1.0
https://github.com/open-mmlab/mmgeneration/tree/master/mmgen/models/architectures/ddpm/denoising.py,"Check
def forward(self, x_t, t, label=None, return_noise=False):
        """"""Forward function.
        Args:
            x_t (torch.Tensor): Diffused image at timestep `t` to denoise.
            t (torch.Tensor): Current timestep.
            label (torch.Tensor | callable | None): You can directly give a
                batch of label through a ``torch.Tensor`` or offer a callable
                function to sample a batch of label data. Otherwise, the
                ``None`` indicates to use the default label sampler.
            return_noise (bool, optional): If True, inputted ``x_t`` and ``t``
                will be returned in a dict with output desired by
                ``output_cfg``. Defaults to False.

        Returns:
            torch.Tensor | dict: If not ``return_noise``
        """"""

        if self.use_rescale_timesteps:
            t = t.float() * (1000.0 / self.num_timesteps)
        embedding = self.time_embedding(t)

        if label is not None:
            assert hasattr(self, 'label_embedding')
            embedding = self.label_embedding(label) + embedding

        h, hs = x_t, []
        # forward downsample blocks
        for block in self.in_blocks:
            h = block(h, embedding)
            hs.append(h)

        # forward middle blocks
        h = self.mid_blocks(h, embedding)

        # forward upsample blocks
        for block in self.out_blocks:
            h = block(torch.cat([h, hs.pop()], dim=1), embedding)
        outputs = self.out(h)

        output_dict = dict()
        if 'FIXED' not in self.var_mode.upper():
            # split mean and learned from output
            mean, var = outputs.split(self.out_channels // 2, dim=1)
            if self.var_mode.upper() == 'LEARNED_RANGE':
                # rescale [-1, 1] to [0, 1]
                output_dict['factor'] = (var + 1) / 2
            elif self.var_mode.upper() == 'LEARNED':
                output_dict['logvar'] = var
            else:
                raise AttributeError(
                    'Only support \'FIXED\', \'LEARNED_RANGE\' '
                    'and \'LEARNED\' for variance output format. But receive '
                    f'\'{self.var_mode}\'.')
        else:
            mean = outputs

        if self.mean_mode.upper() == 'EPS':
            output_dict['eps_t_pred'] = mean
        elif self.mean_mode.upper() == 'START_X':
            output_dict['x_0_pred'] = mean
        elif self.mean_mode.upper() == 'PREVIOUS_X':
            output_dict['x_tm1_pred'] = mean
        else:
            raise AttributeError(
                'Only support \'EPS\', \'START_X\' and \'PREVIOUS_X\' for '
                f'mean output format. But receive \'{self.mean_mode}\'.')

        if return_noise:
            output_dict['x_t'] = x_t
            output_dict['t_rescaled'] = t
            if self.num_classes > 0:
                output_dict['label'] = label

        return output_dict",_5684.py,28,"for block in self.in_blocks:
    h = block(h, embedding)
    hs.append(h)","hs= [block(h, embedding)  for block in self.in_blocks]",
https://github.com/rwightman/pytorch-image-models/tree/master//avg_checkpoints.py,"ridiom
def main():
    args = parser.parse_args()
    # by default use the EMA weights (if present)
    args.use_ema = not args.no_use_ema
    # by default sort by checkpoint metric (if present) and avg top n checkpoints
    args.sort = not args.no_sort

    if os.path.exists(args.output):
        print(""Error: Output filename ({}) already exists."".format(args.output))
        exit(1)

    pattern = args.input
    if not args.input.endswith(os.path.sep) and not args.filter.startswith(os.path.sep):
        pattern += os.path.sep
    pattern += args.filter
    checkpoints = glob.glob(pattern, recursive=True)

    if args.sort:
        checkpoint_metrics = []
        for c in checkpoints:
            metric = checkpoint_metric(c)
            if metric is not None:
                checkpoint_metrics.append((metric, c))
        checkpoint_metrics = list(sorted(checkpoint_metrics))
        checkpoint_metrics = checkpoint_metrics[-args.n:]
        print(""Selected checkpoints:"")
        [print(m, c) for m, c in checkpoint_metrics]
        avg_checkpoints = [c for m, c in checkpoint_metrics]
    else:
        avg_checkpoints = checkpoints
        print(""Selected checkpoints:"")
        [print(c) for c in checkpoints]

    avg_state_dict = {}
    avg_counts = {}
    for c in avg_checkpoints:
        new_state_dict = load_state_dict(c, args.use_ema)
        if not new_state_dict:
            print(""Error: Checkpoint ({}) doesn't exist"".format(args.checkpoint))
            continue

        for k, v in new_state_dict.items():
            if k not in avg_state_dict:
                avg_state_dict[k] = v.clone().to(dtype=torch.float64)
                avg_counts[k] = 1
            else:
                avg_state_dict[k] += v.to(dtype=torch.float64)
                avg_counts[k] += 1

    for k, v in avg_state_dict.items():
        v.div_(avg_counts[k])

    # float32 overflow seems unlikely based on weights seen to date, but who knows
    float32_info = torch.finfo(torch.float32)
    final_state_dict = {}
    for k, v in avg_state_dict.items():
        v = v.clamp(float32_info.min, float32_info.max)
        final_state_dict[k] = v.to(dtype=torch.float32)

    try:
        torch.save(final_state_dict, args.output, _use_new_zipfile_serialization=False)
    except:
        torch.save(final_state_dict, args.output)

    with open(args.output, 'rb') as f:
        sha_hash = hashlib.sha256(f.read()).hexdigest()
    print(""=> Saved state_dict to '{}, SHA256: {}'"".format(args.output, sha_hash))",_5723.py,20,"for c in checkpoints:
    metric = checkpoint_metric(c)
    if metric is not None:
        checkpoint_metrics.append((metric, c))","checkpoint_metrics = [(checkpoint_metric(c), c) for c in checkpoints if checkpoint_metric(c) is not None]",1
https://github.com/kizniche/Mycodo/tree/master/mycodo/scripts/generate_manual_outputs.py,"ridiom
if __name__ == ""__main__"":
    for output_id, output_data in parse_output_information(exclude_custom=True).items():
        name_str = """"
        if 'output_name' in output_data and output_data['output_name']:
            name_str += ""{}"".format(output_data['output_name'])
        if 'output_manufacturer' in output_data and output_data['output_manufacturer']:
            name_str += "": {}"".format(output_data['output_manufacturer'])
        if 'measurements_name' in output_data and output_data['measurements_name']:
            name_str += "": {}"".format(output_data['measurements_name'])
        if 'output_library' in output_data and output_data['output_library']:
            name_str += "": {}"".format(output_data['output_library'])

        if ('output_manufacturer' in output_data and
                output_data['output_manufacturer'] in ['Linux', 'Mycodo', 'Raspberry Pi', 'System']):

            if name_str in mycodo_info and 'dependencies_module' in mycodo_info[name_str]:
                # Multiple sets of dependencies, append library
                mycodo_info[name_str]['dependencies_module'].append(output_data['dependencies_module'])
            else:
                # Only one set of dependencies
                mycodo_info[name_str] = output_data
                if 'dependencies_module' in output_data:
                    mycodo_info[name_str]['dependencies_module'] = [output_data['dependencies_module']]  # turn into list
        else:
            if name_str in outputs_info and 'dependencies_module' in outputs_info[name_str]:
                # Multiple sets of dependencies, append library
                outputs_info[name_str]['dependencies_module'].append(output_data['dependencies_module'])
            else:
                # Only one set of dependencies
                outputs_info[name_str] = output_data
                if 'dependencies_module' in output_data:
                    outputs_info[name_str]['dependencies_module'] = [output_data['dependencies_module']]  # turn into list

    mycodo_info = dict(OrderedDict(sorted(mycodo_info.items(), key = lambda t: t[0])))
    outputs_info = dict(OrderedDict(sorted(outputs_info.items(), key = lambda t: t[0])))

    list_outputs = [
        (mycodo_info, ""Built-In Outputs (System)""),
        (outputs_info, ""Built-In Outputs (Devices)"")
    ]

    with open(save_path, 'w') as out_file:
        for each_list in list_outputs:
            out_file.write(""## {}\n\n"".format(each_list[1]))

            for each_id, each_data in each_list[0].items():
                if 'output_name' in each_data and each_data['output_name']:
                    out_file.write(""### {}\n\n"".format(each_data['output_name']))
                else:
                    out_file.write(""### {}\n\n"".format(each_id))

                if 'output_manufacturer' in each_data and each_data['output_manufacturer']:
                    out_file.write(""- Manufacturer: {}\n"".format(each_data['output_manufacturer']))

                if 'measurements_name' in each_data and each_data['measurements_name']:
                    out_file.write(""- Measurements: {}\n"".format(each_data['measurements_name']))

                if 'interfaces' in each_data and each_data['interfaces']:
                    list_interfaces = []
                    for each_type in each_data['interfaces']:
                        if each_type == 'I2C':
                            list_interfaces.append(""I<sup>2</sup>C"")
                        elif each_type == 'MYCODO':
                            list_interfaces.append(""Mycodo"")
                        elif each_type == '1WIRE':
                            list_interfaces.append(""1-Wire"")
                        elif each_type == 'HTTP':
                            list_interfaces.append(""HTTP"")
                        elif each_type == 'FTDI':
                            list_interfaces.append(""FTDI"")
                        elif each_type == 'UART':
                            list_interfaces.append(""UART"")
                        elif each_type == 'GPIO':
                            list_interfaces.append(""GPIO"")
                        elif each_type == 'PYTHON':
                            list_interfaces.append(""Python"")
                        elif each_type == 'SHELL':
                            list_interfaces.append(""Shell"")
                        else:
                            list_interfaces.append(each_type)
                    out_file.write(""- Interfaces: {}\n"".format("", "".join(list_interfaces)))

                if 'output_types' in each_data and each_data['output_types']:
                    list_output_types = []
                    for each_type in each_data['output_types']:
                        if each_type == 'on_off':
                            list_output_types.append(""On/Off"")
                        elif each_type == 'volume':
                            list_output_types.append(""Volume"")
                        elif each_type == 'pwm':
                            list_output_types.append(""PWM"")
                        elif each_type == 'value':
                            list_output_types.append(""Value"")
                    out_file.write(""- Output Types: {}\n"".format("", "".join(list_output_types)))

                if 'output_library' in each_data and each_data['output_library']:
                    out_file.write(""- Libraries: {}\n"".format(each_data['output_library']))

                generate_controller_doc(out_file, each_data)",_5899.py,60,"for each_type in each_data['interfaces']:
    if each_type == 'I2C':
        list_interfaces.append('I<sup>2</sup>C')
    elif each_type == 'MYCODO':
        list_interfaces.append('Mycodo')
    elif each_type == '1WIRE':
        list_interfaces.append('1-Wire')
    elif each_type == 'HTTP':
        list_interfaces.append('HTTP')
    elif each_type == 'FTDI':
        list_interfaces.append('FTDI')
    elif each_type == 'UART':
        list_interfaces.append('UART')
    elif each_type == 'GPIO':
        list_interfaces.append('GPIO')
    elif each_type == 'PYTHON':
        list_interfaces.append('Python')
    elif each_type == 'SHELL':
        list_interfaces.append('Shell')
    else:
        list_interfaces.append(each_type)",list_interfaces = ['I<sup>2</sup>C' if each_type == 'I2C' else 'Mycodo' if each_type == 'MYCODO' else '1-Wire' if each_type == '1WIRE' else 'HTTP' if each_type == 'HTTP' else 'FTDI' if each_type == 'FTDI' else 'UART' if each_type == 'UART' else 'GPIO' if each_type == 'GPIO' else 'Python' if each_type == 'PYTHON' else 'Shell' if each_type == 'SHELL' else each_type for each_type in each_data['interfaces']],
https://github.com/yihong0618/GitHubPoster/tree/master/github_poster/loader/gitlab_loader.py,"check
def make_track_dict(self):
        self.make_latest_date_dict()
        self.make_left_data_dict()
        for _, v in self.number_by_date_dict.items():
            self.number_list.append(v)",_5800.py,4,"for (_, v) in self.number_by_date_dict.items():
    self.number_list.append(v)","self.number_list = [ v for (_, v) in self.number_by_date_dict.items()]",不确定是否为list
https://github.com/sympy/sympy/tree/master/sympy/physics/wigner.py,"check
def _calc_factlist(nn):
    r""""""
    Function calculates a list of precomputed factorials in order to
    massively accelerate future calculations of the various
    coefficients.

    Parameters
    ==========

    nn : integer
        Highest factorial to be computed.

    Returns
    =======

    list of integers :
        The list of precomputed factorials.

    Examples
    ========

    Calculate list of factorials::

        sage: from sage.functions.wigner import _calc_factlist
        sage: _calc_factlist(10)
        [1, 1, 2, 6, 24, 120, 720, 5040, 40320, 362880, 3628800]
    """"""
    if nn >= len(_Factlist):
        for ii in range(len(_Factlist), int(nn + 1)):
            _Factlist.append(_Factlist[ii - 1] * ii)
    return _Factlist[:int(nn) + 1]",_5989.py,29,"for ii in range(len(_Factlist), int(nn + 1)):
    _Factlist.append(_Factlist[ii - 1] * ii)","_Factlist = [_Factlist[ii - 1] * ii for ii in range(len(_Factlist), int(nn + 1))]",
https://github.com/idealo/imagededup/tree/master/imagededup/utils/general_utils.py,"Idiom
def save_json(results: Dict, filename: str, float_scores: bool = False) -> None:
    """"""
    Save results with a filename.

    Args:
        results: Dictionary of results to be saved.
        filename: Name of the file to be saved.
        float_scores: boolean to indicate if scores are floats.
    """"""
    logger.info('Start: Saving duplicates as json!')

    if float_scores:
        for _file, dup_list in results.items():
            if dup_list:
                typecasted_dup_list = []
                for dup in dup_list:
                    typecasted_dup_list.append((dup[0], float(dup[1])))

                results[_file] = typecasted_dup_list

    with open(filename, 'w') as f:
        json.dump(results, f, indent=2, sort_keys=True)

    logger.info('End: Saving duplicates as json!’)",_5908.py,16,"for dup in dup_list:
    typecasted_dup_list.append((dup[0], float(dup[1])))","typecasted_dup_list = [(dup[0], float(dup[1])) for dup in dup_list]",
https://github.com/PaddlePaddle/PaddleHub/tree/master/modules/text/language_model/lda_webpage/module.py,"Idiom
def infer_doc_topic_distribution(self, document):
        """"""
        This interface infers the topic distribution of document.

        Args:
            document(str): the input document text.

        Returns:
            results(list): returns the topic distribution of document.
        """"""
        tokens = self.__tokenizer.tokenize(document)
        if tokens == []:
            return []
        results = []
        doc = LDADoc()
        self.__engine.infer(tokens, doc)
        topics = doc.sparse_topic_dist()
        for topic in topics:
            results.append({""topic id"": topic.tid, ""distribution"": topic.prob})
        return results",_5960.py,18,"for topic in topics:
    results.append({'topic id': topic.tid, 'distribution': topic.prob})","results = [{'topic id': topic.tid, 'distribution': topic.prob} for topic in topics]",
