file_html,method_content,file_name,lineno,old_code,new_code
https://github.com/quantopian/trading_calendars/tree/master/trading_calendars/trading_calendar.py,"def _special_dates(self, calendars, ad_hoc_dates, start_date, end_date):
        """"""
        Compute a Series of times associated with special dates.

        Parameters
        ----------
        holiday_calendars : list[(datetime.time, HolidayCalendar)]
            Pairs of time and calendar describing when that time occurs. These
            are used to describe regularly-scheduled late opens or early
            closes.
        ad_hoc_dates : list[(datetime.time, list[pd.Timestamp])]
            Pairs of time and list of dates associated with the given times.
            These are used to describe late opens or early closes that occurred
            for unscheduled or otherwise irregular reasons.
        start_date : pd.Timestamp
            Start of the range for which we should calculate special dates.
        end_date : pd.Timestamp
            End of the range for which we should calculate special dates.

        Returns
        -------
        special_dates : pd.Series
            Series mapping trading sessions with special opens/closes to the
            special open/close for that session.
        """"""
        # List of Series for regularly-scheduled times.
        regular = [
            scheduled_special_times(
                calendar,
                start_date,
                end_date,
                time_,
                self.tz,
            )
            for time_, calendar in calendars
        ]

        # List of Series for ad-hoc times.
        ad_hoc = [
            pd.Series(
                index=pd.to_datetime(datetimes, utc=True),
                data=days_at_time(datetimes, time_, self.tz),
            )
            for time_, datetimes in ad_hoc_dates
        ]

        merged = regular + ad_hoc
        if not merged:
            # Concat barfs if the input has length 0.
            return pd.Series([], dtype=""datetime64[ns, UTC]"")

        result = pd.concat(merged).sort_index()
        return result.loc[(result >= start_date) & (result <= end_date)]",_283.py,53,(result >= start_date) & (result <= end_date),start_date <= result <= end_date
https://github.com/microsoft/forecasting/tree/master/fclib/fclib/dataset/ojdata.py,"def split_train_test(data_dir, n_splits=1, horizon=2, gap=2, first_week=40, last_week=156, write_csv=False):
    """"""Generate training, testing, and auxiliary datasets. Training data includes the historical 
    sales and external features; testing data contains the future sales and external features; 
    auxiliary data includes the future price, deal, and advertisement information which can be 
    used for making predictions (we assume such auxiliary information is available at the time 
    when we generate the forecasts). Use this function to generate the train, test, aux data for
    each forecast period on the fly, or use write_csv flag to write data to files.

    Note that train_*.csv files in /train folder contain all the features in the training period
    and aux_*.csv files in /train folder contain all the features except 'logmove', 'constant',
    'profit' up until the forecast period end week. Both train_*.csv and auxi_*csv can be used for
    generating forecasts in each split. However, test_*.csv files in /test folder can only be used
    for model performance evaluation.

    Example:
        data_dir = ""/home/ojdata""

        train, test, aux = split_train_test(data_dir=data_dir, n_splits=5, horizon=3, write_csv=True)

        print(len(train))
        print(len(test))
        print(len(aux))

    Args:
        data_dir (str): location of the download directory
        n_splits (int, optional): number of splits (folds) to generate (default: 1) 
        horizon (int, optional): forecasting horizon, number of weeks to forecast (default: 2) 
        gap (int, optional): gap between training and testing, number of weeks between last training 
            week and first test week (default: 2) 
        first_week (int, optional): first available week (default: 40) 
        last_week (int, optional): last available week (default: 156)
        write_csv (Boolean, optional): Whether to write out the data files or not (default: False)
    
    Returns:
        list[pandas.DataFrame]: a list containing train data frames for each split
        list[pandas.DataFrame]: a list containing test data frames for each split
        list[pandas.DataFrame]: a list containing aux data frames for each split
        
    """"""
    # Read sales data into dataframe
    sales = pd.read_csv(os.path.join(data_dir, ""yx.csv""), index_col=0)

    if write_csv:
        TRAIN_DATA_DIR = os.path.join(data_dir, ""train"")
        TEST_DATA_DIR = os.path.join(data_dir, ""test"")
        if not os.path.isdir(TRAIN_DATA_DIR):
            os.mkdir(TRAIN_DATA_DIR)
        if not os.path.isdir(TEST_DATA_DIR):
            os.mkdir(TEST_DATA_DIR)

    train_df_list = list()
    test_df_list = list()
    aux_df_list = list()

    test_start_week_list, test_end_week_list, train_end_week_list = _gen_split_indices(
        n_splits, horizon, gap, first_week, last_week
    )

    for i in range(n_splits):
        data_mask = (sales.week >= first_week) & (sales.week <= train_end_week_list[i])
        train_df = sales[data_mask].copy()
        data_mask = (sales.week >= test_start_week_list[i]) & (sales.week <= test_end_week_list[i])
        test_df = sales[data_mask].copy()
        data_mask = (sales.week >= first_week) & (sales.week <= test_end_week_list[i])
        aux_df = sales[data_mask].copy()
        aux_df.drop([""logmove"", ""constant"", ""profit""], axis=1, inplace=True)

        if write_csv:
            roundstr = ""_"" + str(i + 1) if n_splits > 1 else """"
            train_df.to_csv(os.path.join(TRAIN_DATA_DIR, ""train"" + roundstr + "".csv""))
            test_df.to_csv(os.path.join(TEST_DATA_DIR, ""test"" + roundstr + "".csv""))
            aux_df.to_csv(os.path.join(TRAIN_DATA_DIR, ""auxi"" + roundstr + "".csv""))

        train_df_list.append(train_df)
        test_df_list.append(test_df)
        aux_df_list.append(aux_df)

    return train_df_list, test_df_list, aux_df_list",_2825.py,62,(sales.week >= test_start_week_list[i]) & (sales.week <= test_end_week_list[i]),test_start_week_list[I] <= sales.week <= test_end_week_list[i]
https://github.com/xonsh/xonsh/tree/master/xonsh/ply/ply/yacc.py,"def infinite_cycles(self):
        terminates = {}

        # Terminals:
        for t in self.Terminals:
            terminates[t] = True

        terminates['$end'] = True

        # Nonterminals:

        # Initialize to false:
        for n in self.Nonterminals:
            terminates[n] = False

        # Then propagate termination until no change:
        while True:
            some_change = False
            for (n, pl) in self.Prodnames.items():
                # Nonterminal n terminates iff any of its productions terminates.
                for p in pl:
                    # Production p terminates iff all of its rhs symbols terminate.
                    for s in p.prod:
                        if not terminates[s]:
                            # The symbol s does not terminate,
                            # so production p does not terminate.
                            p_terminates = False
                            break
                    else:
                        # didn't break from the loop,
                        # so every symbol s terminates
                        # so production p terminates.
                        p_terminates = True

                    if p_terminates:
                        # symbol n terminates!
                        if not terminates[n]:
                            terminates[n] = True
                            some_change = True
                        # Don't need to consider any more productions for this n.
                        break

            if not some_change:
                break

        infinite = []
        for (s, term) in terminates.items():
            if not term:
                if s not in self.Prodnames and s not in self.Terminals and s != 'error':
                    # s is used-but-not-defined, and we've already warned of that,
                    # so it would be overkill to say that it's also non-terminating.
                    pass
                else:
                    infinite.append(s)

        return infinite",_3452.py,49,s not in self.Prodnames and s not in self.Terminals and (s != 'error'),s not in self.Prodnames and â€˜error' != s not in self.Terminals
https://github.com/xonsh/xonsh/tree/master/xonsh/ply/ply/yacc.py,"def infinite_cycles(self):
        terminates = {}

        # Terminals:
        for t in self.Terminals:
            terminates[t] = True

        terminates['$end'] = True

        # Nonterminals:

        # Initialize to false:
        for n in self.Nonterminals:
            terminates[n] = False

        # Then propagate termination until no change:
        while True:
            some_change = False
            for (n, pl) in self.Prodnames.items():
                # Nonterminal n terminates iff any of its productions terminates.
                for p in pl:
                    # Production p terminates iff all of its rhs symbols terminate.
                    for s in p.prod:
                        if not terminates[s]:
                            # The symbol s does not terminate,
                            # so production p does not terminate.
                            p_terminates = False
                            break
                    else:
                        # didn't break from the loop,
                        # so every symbol s terminates
                        # so production p terminates.
                        p_terminates = True

                    if p_terminates:
                        # symbol n terminates!
                        if not terminates[n]:
                            terminates[n] = True
                            some_change = True
                        # Don't need to consider any more productions for this n.
                        break

            if not some_change:
                break

        infinite = []
        for (s, term) in terminates.items():
            if not term:
                if s not in self.Prodnames and s not in self.Terminals and s != 'error':
                    # s is used-but-not-defined, and we've already warned of that,
                    # so it would be overkill to say that it's also non-terminating.
                    pass
                else:
                    infinite.append(s)

        return infinite",_3452.py,49,s not in self.Prodnames and s not in self.Terminals and (s != 'error'),error' != s not in self.Prodnames and (s != 'error')
https://github.com/angr/angr/tree/master/angr/storage/memory_mixins/paged_memory/paged_memory_mixin.py,"def changed_pages(self, other) -> Dict[int,Optional[Set[int]]]:
        my_pages = set(self._pages)
        other_pages = set(other._pages)
        intersection = my_pages.intersection(other_pages)
        difference = my_pages.difference(other_pages)

        changes: Dict[int,Optional[Set[int]]] = dict((d, None) for d in difference)

        for pageno in intersection:
            my_page = self._pages[pageno]
            other_page = other._pages[pageno]

            if (my_page is None) ^ (other_page is None):
                changes[pageno] = None
            elif my_page is None:
                pass
            else:
                changed_offsets = my_page.changed_bytes(other_page, page_addr=pageno * self.page_size)
                if changed_offsets:
                    changes[pageno] = changed_offsets

        return changes",_8521.py,13,(my_page is None) ^ (other_page is None),other_page is None is my_page
https://github.com/facebookresearch/ReAgent/tree/master/reagent/gym/envs/pomdp/pocman.py,"def _smell_food(self):
        smell_range = self.board[""_smell_range""]
        agent_pos = self.internal_state.agent_pos

        for x in range(-smell_range, smell_range + 1):
            for y in range(-smell_range, smell_range + 1):
                smell_x = agent_pos.x + x
                smell_y = agent_pos.y + y
                if (
                    0 <= smell_x < self.maze.shape[0]
                    and 0 <= smell_y < self.maze.shape[1]
                    and self.maze[smell_x, smell_y] == Element.FOOD_PELLET
                ):
                    return True
        return False",_253.py,10,"0 <= smell_x < self.maze.shape[0] and 0 <= smell_y < self.maze.shape[1] and (self.maze[smell_x, smell_y] == Element.FOOD_PELLET)","self.maze.shape[0] > smell_x >= 0 <= smell_y < self.maze.shape[1] and self.maze[smell_x, smell_y] == Element.FOOD_PELLET"
https://github.com/MultiAgentLearning/playground/tree/master/pommerman/characters.py,"def set_agent_id(self, agent_id):
        self.agent_id = agent_id
        if self._game_type == constants.GameType.FFA:
            self.teammate = constants.Item.AgentDummy
            self.enemies = [
                getattr(constants.Item, 'Agent%d' % id_)
                for id_ in range(4)
                if id_ != agent_id
            ]
        elif self._game_type == constants.GameType.OneVsOne:
            self.teammate = constants.Item.AgentDummy
            self.enemies = [
                getattr(constants.Item, 'Agent%d' % id_)
                for id_ in range(2)
                if id_ != agent_id
            ]
        else:
            teammate_id = (agent_id + 2) % 4
            self.teammate = getattr(constants.Item, 'Agent%d' % teammate_id)
            self.enemies = [
                getattr(constants.Item, 'Agent%d' % id_)
                for id_ in range(4)
                if id_ != agent_id and id_ != teammate_id
            ]
            self.enemies.append(constants.Item.AgentDummy)",_308.py,23,id_ != agent_id and id_ != teammate_id,agent_id != id_ != teammate_id
https://github.com/hukkelas/DeepPrivacy/tree/master/deep_privacy/utils/bufferless_videocapture.py,"def __init__(self, name, width=None, height=None):
        self.cap = cv2.VideoCapture(name)
        if width is not None and height is not None:
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)
        self.q = queue.Queue()
        t = threading.Thread(target=self._reader)
        t.daemon = True
        t.start()",_417.py,3,width is not None and height is not None,width is not None is not height
https://github.com/hydrusnetwork/hydrus/tree/master/hydrus/client/gui/pages/ClientGUIPages.py,"def pageJustChanged( self, index ):
        
        old_selection = self._previous_page_index
        selection = index
        
        if old_selection != -1 and old_selection < self.count():
            
            self.widget( old_selection ).PageHidden()
            
        
        if selection != -1:
            
            new_page = self.widget( selection )
            
            new_page.PageShown()
            
        
        self._controller.gui.RefreshStatusBar()
        
        self._previous_page_index = index
        
        self._controller.pub( 'notify_page_change' )",_419.py,6,old_selection != -1 and old_selection < self.count(),-1 != old_selection < self.count()
https://github.com/patrys/httmock/tree/master//httmock.py,"def decorator(func):
        @wraps(func)
        def inner(self_or_url, url_or_request, *args, **kwargs):
            if isinstance(self_or_url, urlparse.SplitResult):
                url = self_or_url
                request = url_or_request
            else:
                url = url_or_request
                request = args[0]
            if scheme is not None and scheme != url.scheme:
                return
            if netloc is not None and not re.match(netloc, url.netloc):
                return
            if path is not None and not re.match(path, url.path):
                return
            if query is not None and not re.match(query, url.query):
                return
            if method is not None and method.upper() != request.method:
                return
            return func(self_or_url, url_or_request, *args, **kwargs)
        return inner",_540.py,10,scheme is not None and scheme != url.scheme,None is not scheme != url.scheme
https://github.com/confluentinc/confluent-kafka-python/tree/master/src/confluent_kafka/avro/cached_schema_registry_client.py,"def get_version(self, subject, avro_schema):
        """"""
        POST /subjects/(string: subject)

        Get the version of a schema for a given subject.

        Returns None if not found.
        :param str subject: subject name
        :param: schema avro_schema: Avro schema
        :returns: version
        :rtype: int
        """"""
        schemas_to_version = self.subject_to_schema_versions[subject]
        version = schemas_to_version.get(avro_schema, None)
        if version is not None:
            return version

        url = '/'.join([self.url, 'subjects', subject])
        body = {'schema': str(avro_schema)}

        result, code = self._send_request(url, method='POST', body=body)
        if code == 404:
            log.error(""Not found:"" + str(code))
            return None
        elif not (code >= 200 and code <= 299):
            log.error(""Unable to get version of a schema:"" + str(code))
            return None
        schema_id = result['id']
        version = result['version']
        self._cache_schema(avro_schema, schema_id, subject, version)
        return version",_568.py,25,code >= 200 and code <= 299,200 <= code <= 299
https://github.com/django/django/tree/master/django/http/multipartparser.py,"def parse(self):
        """"""
        Parse the POST data and break it into a FILES MultiValueDict and a POST
        MultiValueDict.

        Return a tuple containing the POST and FILES dictionary, respectively.
        """"""
        from django.http import QueryDict

        encoding = self._encoding
        handlers = self._upload_handlers

        # HTTP spec says that Content-Length >= 0 is valid
        # handling content-length == 0 before continuing
        if self._content_length == 0:
            return QueryDict(encoding=self._encoding), MultiValueDict()

        # See if any of the handlers take care of the parsing.
        # This allows overriding everything if need be.
        for handler in handlers:
            result = handler.handle_raw_input(
                self._input_data,
                self._meta,
                self._content_length,
                self._boundary,
                encoding,
            )
            # Check to see if it was handled
            if result is not None:
                return result[0], result[1]

        # Create the data structures to be used later.
        self._post = QueryDict(mutable=True)
        self._files = MultiValueDict()

        # Instantiate the parser and stream:
        stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))

        # Whether or not to signal a file-completion at the beginning of the loop.
        old_field_name = None
        counters = [0] * len(handlers)

        # Number of bytes that have been read.
        num_bytes_read = 0
        # To count the number of keys in the request.
        num_post_keys = 0
        # To limit the amount of data read from the request.
        read_size = None
        # Whether a file upload is finished.
        uploaded_file = True

        try:
            for item_type, meta_data, field_stream in Parser(stream, self._boundary):
                if old_field_name:
                    # We run this at the beginning of the next loop
                    # since we cannot be sure a file is complete until
                    # we hit the next boundary/part of the multipart content.
                    self.handle_file_complete(old_field_name, counters)
                    old_field_name = None
                    uploaded_file = True

                try:
                    disposition = meta_data['content-disposition'][1]
                    field_name = disposition['name'].strip()
                except (KeyError, IndexError, AttributeError):
                    continue

                transfer_encoding = meta_data.get('content-transfer-encoding')
                if transfer_encoding is not None:
                    transfer_encoding = transfer_encoding[0].strip()
                field_name = force_str(field_name, encoding, errors='replace')

                if item_type == FIELD:
                    # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.
                    num_post_keys += 1
                    if (settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None and
                            settings.DATA_UPLOAD_MAX_NUMBER_FIELDS < num_post_keys):
                        raise TooManyFieldsSent(
                            'The number of GET/POST parameters exceeded '
                            'settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.'
                        )

                    # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.
                    if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:
                        read_size = settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read

                    # This is a post field, we can just set it in the post
                    if transfer_encoding == 'base64':
                        raw_data = field_stream.read(size=read_size)
                        num_bytes_read += len(raw_data)
                        try:
                            data = base64.b64decode(raw_data)
                        except binascii.Error:
                            data = raw_data
                    else:
                        data = field_stream.read(size=read_size)
                        num_bytes_read += len(data)

                    # Add two here to make the check consistent with the
                    # x-www-form-urlencoded check that includes '&='.
                    num_bytes_read += len(field_name) + 2
                    if (settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None and
                            num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE):
                        raise RequestDataTooBig('Request body exceeded settings.DATA_UPLOAD_MAX_MEMORY_SIZE.')

                    self._post.appendlist(field_name, force_str(data, encoding, errors='replace'))
                elif item_type == FILE:
                    # This is a file, use the handler...
                    file_name = disposition.get('filename')
                    if file_name:
                        file_name = force_str(file_name, encoding, errors='replace')
                        file_name = self.sanitize_file_name(file_name)
                    if not file_name:
                        continue

                    content_type, content_type_extra = meta_data.get('content-type', ('', {}))
                    content_type = content_type.strip()
                    charset = content_type_extra.get('charset')

                    try:
                        content_length = int(meta_data.get('content-length')[0])
                    except (IndexError, TypeError, ValueError):
                        content_length = None

                    counters = [0] * len(handlers)
                    uploaded_file = False
                    try:
                        for handler in handlers:
                            try:
                                handler.new_file(
                                    field_name, file_name, content_type,
                                    content_length, charset, content_type_extra,
                                )
                            except StopFutureHandlers:
                                break

                        for chunk in field_stream:
                            if transfer_encoding == 'base64':
                                # We only special-case base64 transfer encoding
                                # We should always decode base64 chunks by multiple of 4,
                                # ignoring whitespace.

                                stripped_chunk = b"""".join(chunk.split())

                                remaining = len(stripped_chunk) % 4
                                while remaining != 0:
                                    over_chunk = field_stream.read(4 - remaining)
                                    stripped_chunk += b"""".join(over_chunk.split())
                                    remaining = len(stripped_chunk) % 4

                                try:
                                    chunk = base64.b64decode(stripped_chunk)
                                except Exception as exc:
                                    # Since this is only a chunk, any error is an unfixable error.
                                    raise MultiPartParserError(""Could not decode base64 data."") from exc

                            for i, handler in enumerate(handlers):
                                chunk_length = len(chunk)
                                chunk = handler.receive_data_chunk(chunk, counters[i])
                                counters[i] += chunk_length
                                if chunk is None:
                                    # Don't continue if the chunk received by
                                    # the handler is None.
                                    break

                    except SkipFile:
                        self._close_files()
                        # Just use up the rest of this file...
                        exhaust(field_stream)
                    else:
                        # Handle file upload completions on next iteration.
                        old_field_name = field_name
                else:
                    # If this is neither a FIELD or a FILE, just exhaust the stream.
                    exhaust(stream)
        except StopUpload as e:
            self._close_files()
            if not e.connection_reset:
                exhaust(self._input_data)
        else:
            if not uploaded_file:
                for handler in handlers:
                    handler.upload_interrupted()
            # Make sure that the request data is all fed
            exhaust(self._input_data)

        # Signal that the upload has completed.
        # any() shortcircuits if a handler's upload_complete() returns a value.
        any(handler.upload_complete() for handler in handlers)
        self._post._mutable = False
        return self._post, self._files",_896.py,76,settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None and settings.DATA_UPLOAD_MAX_NUMBER_FIELDS < num_post_keys,None is not settings.DATA_UPLOAD_MAX_NUMBER_FIELDS < num_post_keys
https://github.com/django/django/tree/master/django/http/multipartparser.py,"def parse(self):
        """"""
        Parse the POST data and break it into a FILES MultiValueDict and a POST
        MultiValueDict.

        Return a tuple containing the POST and FILES dictionary, respectively.
        """"""
        from django.http import QueryDict

        encoding = self._encoding
        handlers = self._upload_handlers

        # HTTP spec says that Content-Length >= 0 is valid
        # handling content-length == 0 before continuing
        if self._content_length == 0:
            return QueryDict(encoding=self._encoding), MultiValueDict()

        # See if any of the handlers take care of the parsing.
        # This allows overriding everything if need be.
        for handler in handlers:
            result = handler.handle_raw_input(
                self._input_data,
                self._meta,
                self._content_length,
                self._boundary,
                encoding,
            )
            # Check to see if it was handled
            if result is not None:
                return result[0], result[1]

        # Create the data structures to be used later.
        self._post = QueryDict(mutable=True)
        self._files = MultiValueDict()

        # Instantiate the parser and stream:
        stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))

        # Whether or not to signal a file-completion at the beginning of the loop.
        old_field_name = None
        counters = [0] * len(handlers)

        # Number of bytes that have been read.
        num_bytes_read = 0
        # To count the number of keys in the request.
        num_post_keys = 0
        # To limit the amount of data read from the request.
        read_size = None
        # Whether a file upload is finished.
        uploaded_file = True

        try:
            for item_type, meta_data, field_stream in Parser(stream, self._boundary):
                if old_field_name:
                    # We run this at the beginning of the next loop
                    # since we cannot be sure a file is complete until
                    # we hit the next boundary/part of the multipart content.
                    self.handle_file_complete(old_field_name, counters)
                    old_field_name = None
                    uploaded_file = True

                try:
                    disposition = meta_data['content-disposition'][1]
                    field_name = disposition['name'].strip()
                except (KeyError, IndexError, AttributeError):
                    continue

                transfer_encoding = meta_data.get('content-transfer-encoding')
                if transfer_encoding is not None:
                    transfer_encoding = transfer_encoding[0].strip()
                field_name = force_str(field_name, encoding, errors='replace')

                if item_type == FIELD:
                    # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.
                    num_post_keys += 1
                    if (settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None and
                            settings.DATA_UPLOAD_MAX_NUMBER_FIELDS < num_post_keys):
                        raise TooManyFieldsSent(
                            'The number of GET/POST parameters exceeded '
                            'settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.'
                        )

                    # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.
                    if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:
                        read_size = settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read

                    # This is a post field, we can just set it in the post
                    if transfer_encoding == 'base64':
                        raw_data = field_stream.read(size=read_size)
                        num_bytes_read += len(raw_data)
                        try:
                            data = base64.b64decode(raw_data)
                        except binascii.Error:
                            data = raw_data
                    else:
                        data = field_stream.read(size=read_size)
                        num_bytes_read += len(data)

                    # Add two here to make the check consistent with the
                    # x-www-form-urlencoded check that includes '&='.
                    num_bytes_read += len(field_name) + 2
                    if (settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None and
                            num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE):
                        raise RequestDataTooBig('Request body exceeded settings.DATA_UPLOAD_MAX_MEMORY_SIZE.')

                    self._post.appendlist(field_name, force_str(data, encoding, errors='replace'))
                elif item_type == FILE:
                    # This is a file, use the handler...
                    file_name = disposition.get('filename')
                    if file_name:
                        file_name = force_str(file_name, encoding, errors='replace')
                        file_name = self.sanitize_file_name(file_name)
                    if not file_name:
                        continue

                    content_type, content_type_extra = meta_data.get('content-type', ('', {}))
                    content_type = content_type.strip()
                    charset = content_type_extra.get('charset')

                    try:
                        content_length = int(meta_data.get('content-length')[0])
                    except (IndexError, TypeError, ValueError):
                        content_length = None

                    counters = [0] * len(handlers)
                    uploaded_file = False
                    try:
                        for handler in handlers:
                            try:
                                handler.new_file(
                                    field_name, file_name, content_type,
                                    content_length, charset, content_type_extra,
                                )
                            except StopFutureHandlers:
                                break

                        for chunk in field_stream:
                            if transfer_encoding == 'base64':
                                # We only special-case base64 transfer encoding
                                # We should always decode base64 chunks by multiple of 4,
                                # ignoring whitespace.

                                stripped_chunk = b"""".join(chunk.split())

                                remaining = len(stripped_chunk) % 4
                                while remaining != 0:
                                    over_chunk = field_stream.read(4 - remaining)
                                    stripped_chunk += b"""".join(over_chunk.split())
                                    remaining = len(stripped_chunk) % 4

                                try:
                                    chunk = base64.b64decode(stripped_chunk)
                                except Exception as exc:
                                    # Since this is only a chunk, any error is an unfixable error.
                                    raise MultiPartParserError(""Could not decode base64 data."") from exc

                            for i, handler in enumerate(handlers):
                                chunk_length = len(chunk)
                                chunk = handler.receive_data_chunk(chunk, counters[i])
                                counters[i] += chunk_length
                                if chunk is None:
                                    # Don't continue if the chunk received by
                                    # the handler is None.
                                    break

                    except SkipFile:
                        self._close_files()
                        # Just use up the rest of this file...
                        exhaust(field_stream)
                    else:
                        # Handle file upload completions on next iteration.
                        old_field_name = field_name
                else:
                    # If this is neither a FIELD or a FILE, just exhaust the stream.
                    exhaust(stream)
        except StopUpload as e:
            self._close_files()
            if not e.connection_reset:
                exhaust(self._input_data)
        else:
            if not uploaded_file:
                for handler in handlers:
                    handler.upload_interrupted()
            # Make sure that the request data is all fed
            exhaust(self._input_data)

        # Signal that the upload has completed.
        # any() shortcircuits if a handler's upload_complete() returns a value.
        any(handler.upload_complete() for handler in handlers)
        self._post._mutable = False
        return self._post, self._files",_896.py,102,settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None and num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE,None is not settings.DATA_UPLOAD_MAX_MEMORY_SIZE < num_bytes_read
https://github.com/mozilla/addons-server/tree/master/src/olympia/versions/tests/test_models.py,"def test_track_upload_time(self):
        # Set created time back (just for sanity) otherwise the delta
        # would be in the microsecond range.
        self.upload.update(created=datetime.now() - timedelta(days=1))

        mock_timing_path = 'olympia.versions.models.statsd.timing'
        with mock.patch(mock_timing_path) as mock_timing:
            Version.from_upload(
                self.upload,
                self.addon,
                amo.CHANNEL_LISTED,
                selected_apps=[self.selected_app],
                parsed_data=self.dummy_parsed_data,
            )

            upload_start = utc_millesecs_from_epoch(self.upload.created)
            now = utc_millesecs_from_epoch()
            rough_delta = now - upload_start
            actual_delta = mock_timing.call_args[0][1]

            fuzz = 2000  # 2 seconds
            assert actual_delta >= (rough_delta - fuzz) and actual_delta <= (
                rough_delta + fuzz
            )",_1117.py,22,actual_delta >= rough_delta - fuzz and actual_delta <= rough_delta + fuzz,rough_delta - fuzz <= actual_delta <= rough_delta + fuzz
https://github.com/shadowsocksr-backup/shadowsocksr/tree/master/shadowsocks/udprelay.py,"def _handle_server(self):
        server = self._server_socket
        data, r_addr = server.recvfrom(BUF_SIZE)
        ogn_data = data
        if not data:
            logging.debug('UDP handle_server: data is empty')
        if self._stat_callback:
            self._stat_callback(self._listen_port, len(data))
        if self._is_local:
            frag = common.ord(data[2])
            if frag != 0:
                logging.warn('drop a message since frag is not 0')
                return
            else:
                data = data[3:]
        else:
            data = encrypt.encrypt_all(self._password, self._method, 0, data)
            # decrypt data
            if not data:
                logging.debug('UDP handle_server: data is empty after decrypt')
                return

        #logging.info(""UDP data %s"" % (binascii.hexlify(data),))
        if not self._is_local:
            data = pre_parse_header(data)

            data = self._pre_parse_udp_header(data)
            if data is None:
                return

            if type(data) is tuple:
                #(cmd, request_id, data)
                #logging.info(""UDP data %d %d %s"" % (data[0], data[1], binascii.hexlify(data[2])))
                try:
                    if data[0] == 0:
                        if len(data[2]) >= 4:
                            for i in range(64):
                                req_id = random.randint(1, 65535)
                                if req_id not in self._reqid_to_hd:
                                    break
                            if req_id in self._reqid_to_hd:
                                for i in range(64):
                                    req_id = random.randint(1, 65535)
                                    if type(self._reqid_to_hd[req_id]) is tuple:
                                        break
                            # return req id
                            self._reqid_to_hd[req_id] = (data[2][0:4], None)
                            rsp_data = self._pack_rsp_data(CMD_RSP_CONNECT, req_id, RSP_STATE_CONNECTED)
                            data_to_send = encrypt.encrypt_all(self._password, self._method, 1, rsp_data)
                            self.write_to_server_socket(data_to_send, r_addr)
                    elif data[0] == CMD_CONNECT_REMOTE:
                        if len(data[2]) > 4 and data[1] in self._reqid_to_hd:
                            # create
                            if type(self._reqid_to_hd[data[1]]) is tuple:
                                if data[2][0:4] == self._reqid_to_hd[data[1]][0]:
                                    handle = TCPRelayHandler(self, self._reqid_to_hd, self._fd_to_handlers,
                                        self._eventloop, self._server_socket,
                                        self._reqid_to_hd[data[1]][0], self._reqid_to_hd[data[1]][1],
                                        self._config, self._dns_resolver, self._is_local)
                                    self._reqid_to_hd[data[1]] = handle
                                    handle.handle_client(r_addr, CMD_CONNECT, data[1], data[2])
                                    handle.handle_client(r_addr, *data)
                                    self.update_activity(handle)
                                else:
                                    # disconnect
                                    rsp_data = self._pack_rsp_data(CMD_DISCONNECT, data[1], RSP_STATE_EMPTY)
                                    data_to_send = encrypt.encrypt_all(self._password, self._method, 1, rsp_data)
                                    self.write_to_server_socket(data_to_send, r_addr)
                            else:
                                self.update_activity(self._reqid_to_hd[data[1]])
                                self._reqid_to_hd[data[1]].handle_client(r_addr, *data)
                        else:
                            # disconnect
                            rsp_data = self._pack_rsp_data(CMD_DISCONNECT, data[1], RSP_STATE_EMPTY)
                            data_to_send = encrypt.encrypt_all(self._password, self._method, 1, rsp_data)
                            self.write_to_server_socket(data_to_send, r_addr)
                    elif data[0] > CMD_CONNECT_REMOTE and data[0] <= CMD_DISCONNECT:
                        if data[1] in self._reqid_to_hd:
                            if type(self._reqid_to_hd[data[1]]) is tuple:
                                pass
                            else:
                                self.update_activity(self._reqid_to_hd[data[1]])
                                self._reqid_to_hd[data[1]].handle_client(r_addr, *data)
                        else:
                            # disconnect
                            rsp_data = self._pack_rsp_data(CMD_DISCONNECT, data[1], RSP_STATE_EMPTY)
                            data_to_send = encrypt.encrypt_all(self._password, self._method, 1, rsp_data)
                            self.write_to_server_socket(data_to_send, r_addr)
                    return
                except Exception as e:
                    trace = traceback.format_exc()
                    logging.error(trace)
                    return

        try:
            header_result = parse_header(data)
        except:
            self._handel_protocol_error(r_addr, ogn_data)
            return

        if header_result is None:
            self._handel_protocol_error(r_addr, ogn_data)
            return
        connecttype, dest_addr, dest_port, header_length = header_result

        if self._is_local:
            server_addr, server_port = self._get_a_server()
        else:
            server_addr, server_port = dest_addr, dest_port

        addrs = self._dns_cache.get(server_addr, None)
        if addrs is None:
            addrs = socket.getaddrinfo(server_addr, server_port, 0,
                                       socket.SOCK_DGRAM, socket.SOL_UDP)
            if not addrs:
                # drop
                return
            else:
                self._dns_cache[server_addr] = addrs

        af, socktype, proto, canonname, sa = addrs[0]
        key = client_key(r_addr, af)
        client = self._cache.get(key, None)
        if not client:
            # TODO async getaddrinfo
            if self._forbidden_iplist:
                if common.to_str(sa[0]) in self._forbidden_iplist:
                    logging.debug('IP %s is in forbidden list, drop' %
                                  common.to_str(sa[0]))
                    # drop
                    return
            client = socket.socket(af, socktype, proto)
            client.setblocking(False)
            self._cache[key] = client
            self._client_fd_to_server_addr[client.fileno()] = r_addr

            self._sockets.add(client.fileno())
            self._eventloop.add(client, eventloop.POLL_IN, self)

            logging.debug('UDP port %5d sockets %d' % (self._listen_port, len(self._sockets)))

            logging.info('UDP data to %s:%d from %s:%d' %
                        (common.to_str(server_addr), server_port,
                            r_addr[0], r_addr[1]))

        if self._is_local:
            data = encrypt.encrypt_all(self._password, self._method, 1, data)
            if not data:
                return
        else:
            data = data[header_length:]
        if not data:
            return
        try:
            #logging.info('UDP handle_server sendto %s:%d %d bytes' % (common.to_str(server_addr), server_port, len(data)))
            client.sendto(data, (server_addr, server_port))
        except IOError as e:
            err = eventloop.errno_from_exception(e)
            if err in (errno.EINPROGRESS, errno.EAGAIN):
                pass
            else:
                shell.print_exception(e)",_1154.py,77,data[0] > CMD_CONNECT_REMOTE and data[0] <= CMD_DISCONNECT,CMD_CONNECT_REMOTE < data[0] <= CMD_DISCONNECT
https://github.com/machin3io/MACHIN3tools/tree/master/ui/operators/call_pie.py,"def invoke(self, context, event):
        if context.space_data.type == 'VIEW_3D':

            # SHADING PIE

            if self.idname == 'shading_pie':
                engine = context.scene.render.engine
                device = context.scene.cycles.device
                shading = context.space_data.shading

                # sync render engine settings
                if engine != context.scene.M3.render_engine and engine in ['BLENDER_EEVEE', 'CYCLES']:
                    context.scene.M3.avoid_update = True
                    context.scene.M3.render_engine = engine

                # sync cyclces device settings
                if engine == 'CYCLES' and device != context.scene.M3.cycles_device:
                    context.scene.M3.avoid_update = True
                    context.scene.M3.cycles_device = device

                # sync shading.light
                if shading.light != context.scene.M3.shading_light:
                    context.scene.M3.avoid_update = True
                    context.scene.M3.shading_light = shading.light

                    context.scene.M3.avoid_update = True
                    context.scene.M3.use_flat_shadows = shading.show_shadows


                bpy.ops.wm.call_menu_pie(name='MACHIN3_MT_%s' % (self.idname))

            # TOOLS PIE

            elif self.idname == 'tools_pie':
                if context.mode in ['OBJECT', 'EDIT_MESH']:
                    bpy.ops.wm.call_menu_pie(name='MACHIN3_MT_%s' % (self.idname))

                else:
                    return {'PASS_THROUGH'}

        return {'FINISHED'}",_1186.py,12,"engine != context.scene.M3.render_engine and engine in ['BLENDER_EEVEE', 'CYCLES']","context.scene.M3.render_engine != engine in ['BLENDER_EEVEE', 'CYCLES']"
https://github.com/ansible/awx/tree/master/awx/conf/registry.py,"def get_registered_settings(self, category_slug=None, read_only=None, slugs_to_ignore=set()):
        setting_names = []
        if category_slug == 'user-defaults':
            category_slug = 'user'
        if category_slug == 'changed':
            category_slug = 'all'
        for setting, kwargs in self._registry.items():
            if category_slug not in {None, 'all', kwargs.get('category_slug', None)}:
                continue
            if kwargs.get('category_slug', None) in slugs_to_ignore:
                continue
            if read_only in {True, False} and kwargs.get('read_only', False) != read_only and setting != 'INSTALL_UUID':
                # Note: Doesn't catch fields that set read_only via __init__;
                # read-only field kwargs should always include read_only=True.
                continue
            setting_names.append(setting)
        return setting_names",_1195.py,12,"read_only in {True, False} and kwargs.get('read_only', False) != read_only and (setting != 'INSTALL_UUID')","kwargs.get('read_only', False) != read_only in {True, False} and setting != 'INSTALL_UUID'"
https://github.com/spulec/moto/tree/master/moto/sagemaker/models.py,"def response_object(self):
        response_object = self.gen_response_object()
        return {
            k: v for k, v in response_object.items() if v is not None and v != [None]
        }",_1215.py,4,v is not None and v != [None],None is not v != [None]
https://github.com/Shmuma/ptan/tree/master/ptan/common/wrappers.py,"def step(self, action):
        obs, reward, done, info = self.env.step(action)
        self.was_real_done = done
        # check current lives, make loss of life terminal,
        # then update lives to handle bonus lives
        lives = self.env.unwrapped.ale.lives()
        if lives < self.lives and lives > 0:
            # for Qbert somtimes we stay in lives == 0 condtion for a few frames
            # so its important to keep lives > 0, so that we only reset once
            # the environment advertises done.
            done = True
        self.lives = lives
        return obs, reward, done, info",_1315.py,7,lives < self.lives and lives > 0,self.lives > lives > 0
https://github.com/adafruit/Adafruit_Python_SSD1306/tree/master/Adafruit_SSD1306/SSD1306.py,"def __init__(self, width, height, rst, dc=None, sclk=None, din=None, cs=None,
                 gpio=None, spi=None, i2c_bus=None, i2c_address=SSD1306_I2C_ADDRESS,
                 i2c=None):
        self._log = logging.getLogger('Adafruit_SSD1306.SSD1306Base')
        self._spi = None
        self._i2c = None
        self.width = width
        self.height = height
        self._pages = height//8
        self._buffer = [0]*(width*self._pages)
        # Default to platform GPIO if not provided.
        self._gpio = gpio
        if self._gpio is None:
            self._gpio = GPIO.get_platform_gpio()
        # Setup reset pin.
        self._rst = rst
        if not self._rst is None:
            self._gpio.setup(self._rst, GPIO.OUT)
        # Handle hardware SPI
        if spi is not None:
            self._log.debug('Using hardware SPI')
            self._spi = spi
            self._spi.set_clock_hz(8000000)
        # Handle software SPI
        elif sclk is not None and din is not None and cs is not None:
            self._log.debug('Using software SPI')
            self._spi = SPI.BitBang(self._gpio, sclk, din, None, cs)
        # Handle hardware I2C
        elif i2c is not None:
            self._log.debug('Using hardware I2C with custom I2C provider.')
            self._i2c = i2c.get_i2c_device(i2c_address)
        else:
            self._log.debug('Using hardware I2C with platform I2C provider.')
            import Adafruit_GPIO.I2C as I2C
            if i2c_bus is None:
                self._i2c = I2C.get_i2c_device(i2c_address)
            else:
                self._i2c = I2C.get_i2c_device(i2c_address, busnum=i2c_bus)
        # Initialize DC pin if using SPI.
        if self._spi is not None:
            if dc is None:
                raise ValueError('DC pin must be provided when using SPI.')
            self._dc = dc
            self._gpio.setup(self._dc, GPIO.OUT)",_1362.py,25,sclk is not None and din is not None and (cs is not None),sclk is not None is not din and cs is not None
https://github.com/apache/tvm/tree/master/python/tvm/topi/nn/conv2d.py,"def _conv2d_winograd_nhwc_impl(
    data,
    weight,
    strides,
    padding,
    dilation,
    out_dtype,
    tile_size,
    pre_computed=False,
    write_cache_level=None,
    auto_scheduler_rewritten_layout="""",
    meta_schedule_original_shape=None,
):
    """"""Conv2D Winograd implementation in NHWC layout.
    This is a clean version to be used by the auto-scheduler for both CPU and GPU.

    Parameters
    ----------
    data : tvm.te.Tensor
        4-D with shape [batch, in_height, in_width, in_channel]
    weight : tvm.te.Tensor
        4-D with shape [filter_height, filter_width, in_channel, num_filter]
    strides : int or a list/tuple of two ints
        stride size, or [stride_height, stride_width]
    padding : int or a list/tuple of two ints
        padding size, or [pad_height, pad_width]
    dilation: int or a list/tuple of two ints
        dilation size, or [dilation_height, dilation_width]
    out_dtype : str, optional
        Specifies the output data type.
    tile_size : int
        The size of the tile to use for the Winograd filter
    pre_computed: bool = False
        Whether the kernel is precomputed
    write_cache_level: Optional[int] = None
        The cache level to write to in multi-level tiling rule in MetaSchedule.
    auto_scheduler_rewritten_layout: str = """"
        The layout after auto-scheduler's layout rewrite pass.
    meta_schedule_original_shape: Optional[List[PrimExpr]] = None
        The original shape of the input tensor.

    Returns
    -------
    output : tvm.te.Tensor
        4-D with shape [batch, out_height, out_width, out_channel]
    """"""
    N, H, W, CI = get_const_tuple(data.shape)
    if isinstance(dilation, int):
        dilation_h = dilation_w = dilation
    else:
        dilation_h, dilation_w = dilation
    if meta_schedule_original_shape:
        auto_scheduler.rewrite_tensor_shape(weight, meta_schedule_original_shape)

    assert (dilation_h, dilation_w) == (1, 1), ""Does not support dilation""
    if not pre_computed:
        KH, KW, CI, CO = get_const_tuple(weight.shape)
    else:
        if auto_scheduler_rewritten_layout:
            H_CAT, W_CAT, CO, CI = get_const_tuple(
                auto_scheduler.get_shape_from_rewritten_layout(
                    auto_scheduler_rewritten_layout, [""eps"", ""nu"", ""co"", ""ci""]
                )
            )
            auto_scheduler.remove_index_check(weight)
        else:
            H_CAT, W_CAT, CO, CI = get_const_tuple(weight.shape)

        KH, KW = H_CAT - tile_size + 1, W_CAT - tile_size + 1

    pad_t, pad_l, pad_b, pad_r = get_pad_tuple(padding, (KH, KW))
    HSTR, WSTR = (strides, strides) if isinstance(strides, int) else strides
    assert HSTR == 1 and WSTR == 1 and KH == 3 and KW == 3

    r = KW
    m = tile_size
    alpha = m + r - 1
    A, B, G = winograd_transform_matrices(m, r, out_dtype)

    H = (H + pad_t + pad_b - KH) // HSTR + 1
    W = (W + pad_l + pad_r - KW) // WSTR + 1
    nH, nW = (H + m - 1) // m, (W + m - 1) // m
    P = N * nH * nW

    pad_extra = (nW - 1) * m + alpha - (H + pad_t + pad_b)
    data_pad = pad(
        data,
        (0, pad_t, pad_l, 0),
        (0, pad_b + pad_extra, pad_r + pad_extra, 0),
        name=""data_pad"",
        attrs={""schedule_rule"": ""None""},
    )

    if not pre_computed:
        r_kh = te.reduce_axis((0, KH), name=""r_kh"")
        r_kw = te.reduce_axis((0, KW), name=""r_kw"")
        kernel_pack = te.compute(
            (alpha, alpha, CO, CI),
            lambda eps, nu, co, ci: te.sum(
                weight[r_kh, r_kw, ci, co] * G[eps, r_kh] * G[nu, r_kw],
                axis=[r_kh, r_kw],
            ),
            name=""kernel_pack"",
        )
        bgemm_attrs = {}
    else:
        kernel_pack = weight
        bgemm_attrs = {""layout_free_placeholders"": [kernel_pack]}
    if write_cache_level is not None:
        if not isinstance(write_cache_level, int):
            bgemm_attrs[""meta_schedule.write_cache_level""] = write_cache_level
        else:
            bgemm_attrs[""meta_schedule.write_cache_level""] = [write_cache_level]

    # pack data tile
    input_tile = te.compute(
        (alpha, alpha, P, CI),
        lambda eps, nu, p, ci: data_pad[
            p // (nH * nW),
            ((p // nW) % nH) * m + eps,
            (p % nW) * m + nu,
            ci,
        ],
        name=""input_tile"",
        attrs={""schedule_rule"": ""None""},
    )

    # transform data
    r_a = te.reduce_axis((0, alpha), ""r_a"")
    r_b = te.reduce_axis((0, alpha), ""r_b"")
    data_pack = te.compute(
        (alpha, alpha, P, CI),
        lambda eps, nu, p, ci: te.sum(
            input_tile[r_a, r_b, p, ci] * B[r_a, eps] * B[r_b, nu],
            axis=[r_a, r_b],
        ),
        name=""data_pack"",
        attrs={
            ""auto_scheduler_simplify_const_tensor_indices"": [""eps"", ""nu"", ""r_a"", ""r_b""],
            ""schedule_rule"": ""conv2d_nhwc_winograd_data_pack"",
        },
    )

    # do batch gemm
    ci = te.reduce_axis((0, CI), name=""ci"")
    bgemm = te.compute(
        (alpha, alpha, P, CO),
        lambda eps, nu, p, co: te.sum(
            data_pack[eps, nu, p, ci] * kernel_pack[eps, nu, co, ci],
            axis=[ci],
        ),
        name=""bgemm"",
        attrs=bgemm_attrs,
    )

    if auto_scheduler_rewritten_layout:
        bgemm = auto_scheduler.rewrite_compute_body(bgemm, auto_scheduler_rewritten_layout)

    # inverse transform

    r_a = te.reduce_axis((0, alpha), ""r_a"")
    r_b = te.reduce_axis((0, alpha), ""r_b"")
    inverse = te.compute(
        (m, m, P, CO),
        lambda vh, vw, p, co: te.sum(
            bgemm[r_a, r_b, p, co] * A[r_a, vh] * A[r_b, vw],
            axis=[r_a, r_b],
        ),
        name=""inverse"",
        attrs={
            ""auto_scheduler_simplify_const_tensor_indices"": [""vh"", ""vw"", ""r_a"", ""r_b""],
            ""schedule_rule"": ""conv2d_nhwc_winograd_inverse"",
        },
    )

    # output
    output = te.compute(
        (N, H, W, CO),
        lambda n, h, w, co: inverse[
            h % m,
            w % m,
            n * nH * nW + (h // m) * nW + (w // m),
            co,
        ],
        name=""conv2d_winograd"",
    )

    return output",_1517.py,73,HSTR == 1 and WSTR == 1 and (KH == 3) and (KW == 3),HSTR == 1 == WSTR and KH == 3 == KW
https://github.com/ansible/ansible-modules-core/tree/master/system/user.py,"def create_user(self):
        cmd = [
            self.module.get_bin_path('pw', True),
            'useradd',
            '-n',
            self.name,
        ]

        if self.uid is not None:
            cmd.append('-u')
            cmd.append(self.uid)

            if self.non_unique:
                cmd.append('-o')

        if self.comment is not None:
            cmd.append('-c')
            cmd.append(self.comment)

        if self.home is not None:
            cmd.append('-d')
            cmd.append(self.home)

        if self.group is not None:
            if not self.group_exists(self.group):
                self.module.fail_json(msg=""Group %s does not exist"" % self.group)
            cmd.append('-g')
            cmd.append(self.group)

        if self.groups is not None:
            groups = self.get_groups_set()
            cmd.append('-G')
            cmd.append(','.join(groups))

        if self.createhome:
            cmd.append('-m')

            if self.skeleton is not None:
                cmd.append('-k')
                cmd.append(self.skeleton)

        if self.shell is not None:
            cmd.append('-s')
            cmd.append(self.shell)

        if self.login_class is not None:
            cmd.append('-L')
            cmd.append(self.login_class)

        if self.expires:
            days =( time.mktime(self.expires) - time.time() ) / 86400
            cmd.append('-e')
            cmd.append(str(int(days)))

        # system cannot be handled currently - should we error if its requested?
        # create the user
        (rc, out, err) = self.execute_command(cmd)
        if rc is not None and rc != 0:
            self.module.fail_json(name=self.name, msg=err, rc=rc)

        # we have to set the password in a second command
        if self.password is not None:
            cmd = [
                self.module.get_bin_path('chpass', True),
                '-p',
                self.password,
                self.name
            ]
            return self.execute_command(cmd)

        return (rc, out, err)",_1614.py,58,rc is not None and rc != 0,None is not rc != 0
https://github.com/home-assistant/core/tree/master/homeassistant/components/crownstone/config_flow.py,"def async_create_new_entry(self) -> FlowResult:
        """"""Create a new entry.""""""
        # these attributes will only change when a usb was configured
        if self.usb_path is not None and self.usb_sphere_id is not None:
            self.updated_options[CONF_USB_PATH] = self.usb_path
            self.updated_options[CONF_USB_SPHERE] = self.usb_sphere_id

        return super().async_create_entry(title="""", data=self.updated_options)",_1745.py,4,self.usb_path is not None and self.usb_sphere_id is not None,self.usb_path is not None is not self.usb_sphere_id
https://github.com/JiaxuanYou/graph-generation/tree/master//create_graphs.py,"def create(args):
### load datasets
    graphs=[]
    # synthetic graphs
    if args.graph_type=='ladder':
        graphs = []
        for i in range(100, 201):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='ladder_small':
        graphs = []
        for i in range(2, 11):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='tree':
        graphs = []
        for i in range(2,5):
            for j in range(3,5):
                graphs.append(nx.balanced_tree(i,j))
        args.max_prev_node = 256
    elif args.graph_type=='caveman':
        # graphs = []
        # for i in range(5,10):
        #     for j in range(5,25):
        #         for k in range(5):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(30, 81):
                for k in range(10):
                    graphs.append(caveman_special(i,j, p_edge=0.3))
        args.max_prev_node = 100
    elif args.graph_type=='caveman_small':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(6, 11):
                for k in range(20):
                    graphs.append(caveman_special(i, j, p_edge=0.8)) # default 0.8
        args.max_prev_node = 20
    elif args.graph_type=='caveman_small_single':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(8, 9):
                for k in range(100):
                    graphs.append(caveman_special(i, j, p_edge=0.5))
        args.max_prev_node = 20
    elif args.graph_type.startswith('community'):
        num_communities = int(args.graph_type[-1])
        print('Creating dataset with ', num_communities, ' communities')
        c_sizes = np.random.choice([12, 13, 14, 15, 16, 17], num_communities)
        #c_sizes = [15] * num_communities
        for k in range(3000):
            graphs.append(n_community(c_sizes, p_inter=0.01))
        args.max_prev_node = 80
    elif args.graph_type=='grid':
        graphs = []
        for i in range(10,20):
            for j in range(10,20):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 40
    elif args.graph_type=='grid_small':
        graphs = []
        for i in range(2,5):
            for j in range(2,6):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 15
    elif args.graph_type=='barabasi':
        graphs = []
        for i in range(100,200):
             for j in range(4,5):
                 for k in range(5):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 130
    elif args.graph_type=='barabasi_small':
        graphs = []
        for i in range(4,21):
             for j in range(3,4):
                 for k in range(10):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 20
    elif args.graph_type=='grid_big':
        graphs = []
        for i in range(36, 46):
            for j in range(36, 46):
                graphs.append(nx.grid_2d_graph(i, j))
        args.max_prev_node = 90

    elif 'barabasi_noise' in args.graph_type:
        graphs = []
        for i in range(100,101):
            for j in range(4,5):
                for k in range(500):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        graphs = perturb_new(graphs,p=args.noise/10.0)
        args.max_prev_node = 99

    # real graphs
    elif args.graph_type == 'enzymes':
        graphs= Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        args.max_prev_node = 25
    elif args.graph_type == 'enzymes_small':
        graphs_raw = Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        graphs = []
        for G in graphs_raw:
            if G.number_of_nodes()<=20:
                graphs.append(G)
        args.max_prev_node = 15
    elif args.graph_type == 'protein':
        graphs = Graph_load_batch(min_num_nodes=20, name='PROTEINS_full')
        args.max_prev_node = 80
    elif args.graph_type == 'DD':
        graphs = Graph_load_batch(min_num_nodes=100, max_num_nodes=500, name='DD',node_attributes=False,graph_labels=True)
        args.max_prev_node = 230
    elif args.graph_type == 'citeseer':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=3)
            if G_ego.number_of_nodes() >= 50 and (G_ego.number_of_nodes() <= 400):
                graphs.append(G_ego)
        args.max_prev_node = 250
    elif args.graph_type == 'citeseer_small':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=1)
            if (G_ego.number_of_nodes() >= 4) and (G_ego.number_of_nodes() <= 20):
                graphs.append(G_ego)
        shuffle(graphs)
        graphs = graphs[0:200]
        args.max_prev_node = 15

    return graphs",_1750.py,131,G_ego.number_of_nodes() >= 50 and G_ego.number_of_nodes() <= 400,50 <= G_ego.number_of_nodes() <= 400
https://github.com/JiaxuanYou/graph-generation/tree/master//create_graphs.py,"def create(args):
### load datasets
    graphs=[]
    # synthetic graphs
    if args.graph_type=='ladder':
        graphs = []
        for i in range(100, 201):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='ladder_small':
        graphs = []
        for i in range(2, 11):
            graphs.append(nx.ladder_graph(i))
        args.max_prev_node = 10
    elif args.graph_type=='tree':
        graphs = []
        for i in range(2,5):
            for j in range(3,5):
                graphs.append(nx.balanced_tree(i,j))
        args.max_prev_node = 256
    elif args.graph_type=='caveman':
        # graphs = []
        # for i in range(5,10):
        #     for j in range(5,25):
        #         for k in range(5):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(30, 81):
                for k in range(10):
                    graphs.append(caveman_special(i,j, p_edge=0.3))
        args.max_prev_node = 100
    elif args.graph_type=='caveman_small':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(6, 11):
                for k in range(20):
                    graphs.append(caveman_special(i, j, p_edge=0.8)) # default 0.8
        args.max_prev_node = 20
    elif args.graph_type=='caveman_small_single':
        # graphs = []
        # for i in range(2,5):
        #     for j in range(2,6):
        #         for k in range(10):
        #             graphs.append(nx.relaxed_caveman_graph(i, j, p=0.1))
        graphs = []
        for i in range(2, 3):
            for j in range(8, 9):
                for k in range(100):
                    graphs.append(caveman_special(i, j, p_edge=0.5))
        args.max_prev_node = 20
    elif args.graph_type.startswith('community'):
        num_communities = int(args.graph_type[-1])
        print('Creating dataset with ', num_communities, ' communities')
        c_sizes = np.random.choice([12, 13, 14, 15, 16, 17], num_communities)
        #c_sizes = [15] * num_communities
        for k in range(3000):
            graphs.append(n_community(c_sizes, p_inter=0.01))
        args.max_prev_node = 80
    elif args.graph_type=='grid':
        graphs = []
        for i in range(10,20):
            for j in range(10,20):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 40
    elif args.graph_type=='grid_small':
        graphs = []
        for i in range(2,5):
            for j in range(2,6):
                graphs.append(nx.grid_2d_graph(i,j))
        args.max_prev_node = 15
    elif args.graph_type=='barabasi':
        graphs = []
        for i in range(100,200):
             for j in range(4,5):
                 for k in range(5):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 130
    elif args.graph_type=='barabasi_small':
        graphs = []
        for i in range(4,21):
             for j in range(3,4):
                 for k in range(10):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        args.max_prev_node = 20
    elif args.graph_type=='grid_big':
        graphs = []
        for i in range(36, 46):
            for j in range(36, 46):
                graphs.append(nx.grid_2d_graph(i, j))
        args.max_prev_node = 90

    elif 'barabasi_noise' in args.graph_type:
        graphs = []
        for i in range(100,101):
            for j in range(4,5):
                for k in range(500):
                    graphs.append(nx.barabasi_albert_graph(i,j))
        graphs = perturb_new(graphs,p=args.noise/10.0)
        args.max_prev_node = 99

    # real graphs
    elif args.graph_type == 'enzymes':
        graphs= Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        args.max_prev_node = 25
    elif args.graph_type == 'enzymes_small':
        graphs_raw = Graph_load_batch(min_num_nodes=10, name='ENZYMES')
        graphs = []
        for G in graphs_raw:
            if G.number_of_nodes()<=20:
                graphs.append(G)
        args.max_prev_node = 15
    elif args.graph_type == 'protein':
        graphs = Graph_load_batch(min_num_nodes=20, name='PROTEINS_full')
        args.max_prev_node = 80
    elif args.graph_type == 'DD':
        graphs = Graph_load_batch(min_num_nodes=100, max_num_nodes=500, name='DD',node_attributes=False,graph_labels=True)
        args.max_prev_node = 230
    elif args.graph_type == 'citeseer':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=3)
            if G_ego.number_of_nodes() >= 50 and (G_ego.number_of_nodes() <= 400):
                graphs.append(G_ego)
        args.max_prev_node = 250
    elif args.graph_type == 'citeseer_small':
        _, _, G = Graph_load(dataset='citeseer')
        G = max(nx.connected_component_subgraphs(G), key=len)
        G = nx.convert_node_labels_to_integers(G)
        graphs = []
        for i in range(G.number_of_nodes()):
            G_ego = nx.ego_graph(G, i, radius=1)
            if (G_ego.number_of_nodes() >= 4) and (G_ego.number_of_nodes() <= 20):
                graphs.append(G_ego)
        shuffle(graphs)
        graphs = graphs[0:200]
        args.max_prev_node = 15

    return graphs",_1750.py,141,G_ego.number_of_nodes() >= 4 and G_ego.number_of_nodes() <= 20,4 <= G_ego.number_of_nodes() <= 20
https://github.com/wbond/package_control/tree/master/package_control/deps/oscrypto/trust_list.py,"def get_path(temp_dir=None, cache_length=24, cert_callback=None):
    """"""
    Get the filesystem path to a file that contains OpenSSL-compatible CA certs.

    On OS X and Windows, there are extracted from the system certificate store
    and cached in a file on the filesystem. This path should not be writable
    by other users, otherwise they could inject CA certs into the trust list.

    :param temp_dir:
        The temporary directory to cache the CA certs in on OS X and Windows.
        Needs to have secure permissions so other users can not modify the
        contents.

    :param cache_length:
        The number of hours to cache the CA certs on OS X and Windows

    :param cert_callback:
        A callback that is called once for each certificate in the trust store.
        It should accept two parameters: an asn1crypto.x509.Certificate object,
        and a reason. The reason will be None if the certificate is being
        exported, otherwise it will be a unicode string of the reason it won't.
        This is only called on Windows and OS X when passed to this function.

    :raises:
        oscrypto.errors.CACertsError - when an error occurs exporting/locating certs

    :return:
        The full filesystem path to a CA certs file
    """"""

    ca_path, temp = _ca_path(temp_dir)

    # Windows and OS X
    if temp and _cached_path_needs_update(ca_path, cache_length):
        empty_set = set()

        any_purpose = '2.5.29.37.0'
        apple_ssl = '1.2.840.113635.100.1.3'
        win_server_auth = '1.3.6.1.5.5.7.3.1'

        with path_lock:
            if _cached_path_needs_update(ca_path, cache_length):
                with open(ca_path, 'wb') as f:
                    for cert, trust_oids, reject_oids in extract_from_system(cert_callback, True):
                        if sys.platform == 'darwin':
                            if trust_oids != empty_set and any_purpose not in trust_oids \
                                    and apple_ssl not in trust_oids:
                                if cert_callback:
                                    cert_callback(Certificate.load(cert), 'implicitly distrusted for TLS')
                                continue
                            if reject_oids != empty_set and (apple_ssl in reject_oids
                                                             or any_purpose in reject_oids):
                                if cert_callback:
                                    cert_callback(Certificate.load(cert), 'explicitly distrusted for TLS')
                                continue
                        elif sys.platform == 'win32':
                            if trust_oids != empty_set and any_purpose not in trust_oids \
                                    and win_server_auth not in trust_oids:
                                if cert_callback:
                                    cert_callback(Certificate.load(cert), 'implicitly distrusted for TLS')
                                continue
                            if reject_oids != empty_set and (win_server_auth in reject_oids
                                                             or any_purpose in reject_oids):
                                if cert_callback:
                                    cert_callback(Certificate.load(cert), 'explicitly distrusted for TLS')
                                continue
                        if cert_callback:
                            cert_callback(Certificate.load(cert), None)
                        f.write(armor('CERTIFICATE', cert))

    if not ca_path:
        raise CACertsError('No CA certs found')

    return ca_path",_1904.py,46,trust_oids != empty_set and any_purpose not in trust_oids and (apple_ssl not in trust_oids),any_purpose not in trust_oids != empty_set and apple_ssl not in trust_oids
https://github.com/wbond/package_control/tree/master/package_control/deps/oscrypto/trust_list.py,"def get_path(temp_dir=None, cache_length=24, cert_callback=None):
    """"""
    Get the filesystem path to a file that contains OpenSSL-compatible CA certs.

    On OS X and Windows, there are extracted from the system certificate store
    and cached in a file on the filesystem. This path should not be writable
    by other users, otherwise they could inject CA certs into the trust list.

    :param temp_dir:
        The temporary directory to cache the CA certs in on OS X and Windows.
        Needs to have secure permissions so other users can not modify the
        contents.

    :param cache_length:
        The number of hours to cache the CA certs on OS X and Windows

    :param cert_callback:
        A callback that is called once for each certificate in the trust store.
        It should accept two parameters: an asn1crypto.x509.Certificate object,
        and a reason. The reason will be None if the certificate is being
        exported, otherwise it will be a unicode string of the reason it won't.
        This is only called on Windows and OS X when passed to this function.

    :raises:
        oscrypto.errors.CACertsError - when an error occurs exporting/locating certs

    :return:
        The full filesystem path to a CA certs file
    """"""

    ca_path, temp = _ca_path(temp_dir)

    # Windows and OS X
    if temp and _cached_path_needs_update(ca_path, cache_length):
        empty_set = set()

        any_purpose = '2.5.29.37.0'
        apple_ssl = '1.2.840.113635.100.1.3'
        win_server_auth = '1.3.6.1.5.5.7.3.1'

        with path_lock:
            if _cached_path_needs_update(ca_path, cache_length):
                with open(ca_path, 'wb') as f:
                    for cert, trust_oids, reject_oids in extract_from_system(cert_callback, True):
                        if sys.platform == 'darwin':
                            if trust_oids != empty_set and any_purpose not in trust_oids \
                                    and apple_ssl not in trust_oids:
                                if cert_callback:
                                    cert_callback(Certificate.load(cert), 'implicitly distrusted for TLS')
                                continue
                            if reject_oids != empty_set and (apple_ssl in reject_oids
                                                             or any_purpose in reject_oids):
                                if cert_callback:
                                    cert_callback(Certificate.load(cert), 'explicitly distrusted for TLS')
                                continue
                        elif sys.platform == 'win32':
                            if trust_oids != empty_set and any_purpose not in trust_oids \
                                    and win_server_auth not in trust_oids:
                                if cert_callback:
                                    cert_callback(Certificate.load(cert), 'implicitly distrusted for TLS')
                                continue
                            if reject_oids != empty_set and (win_server_auth in reject_oids
                                                             or any_purpose in reject_oids):
                                if cert_callback:
                                    cert_callback(Certificate.load(cert), 'explicitly distrusted for TLS')
                                continue
                        if cert_callback:
                            cert_callback(Certificate.load(cert), None)
                        f.write(armor('CERTIFICATE', cert))

    if not ca_path:
        raise CACertsError('No CA certs found')

    return ca_path",_1904.py,57,trust_oids != empty_set and any_purpose not in trust_oids and (win_server_auth not in trust_oids),any_purpose not in trust_oids != empty_set and win_server_auth not in trust_oids
https://github.com/chainer/chainer/tree/master/chainer/functions/connection/convolution_nd.py,"def _use_cudnn(self, x, gy):
        if cuda._cudnn_version < 6000 and any(d != 1 for d in self.dilate):
            # cuDNN < 6.0 does not support dilated convolutions
            return False
        if cuda._cudnn_version < 7000 and 1 < self.groups:
            # cuDNN < 7.0 does not support grouped convolutions
            return False
        return (
            chainer.should_use_cudnn('>=auto')
            and not self.cover_all
            and x.dtype == self.W_dtype
            and gy.dtype == self.W_dtype
            and self.ndim > 1)",_1911.py,9,chainer.should_use_cudnn('>=auto') and (not self.cover_all) and (x.dtype == self.W_dtype) and (gy.dtype == self.W_dtype) and (self.ndim > 1),x.dtype == self.W_dtype == gy.dtype and self.ndim > 1 and chainer.should_use_cudnn('>=auto') and (not self.cover_all)
https://github.com/bitbrute/evillimiter/tree/master/evillimiter/menus/parser.py,"def parse(self, command):
        """"""
        Parses a given list of arguments
        """"""
        names = [x.name for x in (self._flag_commands + self._parameter_commands)]
        result_dict = dict.fromkeys(names, None)

        # indicates whether or not to skip the next command argument
        skip_next = False

        for i, arg in enumerate(command):
            if skip_next:
                skip_next = False
                continue

            if i == 0:
                # check if the first argument is a subparser
                for sp in self._subparsers:
                    if sp.identifier == arg:
                        # if subparser present, parse arguments there
                        result = sp.subparser.parse(command[(i + 1):])
                        if result is not None and sp.handler is not None:
                            # call the subparser's handler if available
                            sp.handler(result)

                        return result
            
            # indicates whether or not the argument has been processed
            is_arg_processed = False

            for cmd in self._flag_commands:
                if cmd.identifier == arg:
                    if cmd.type == CommandParser.CommandType.FLAG_COMMAND:
                        # if its a flag, set the flag to true
                        result_dict[cmd.name] = True
                        is_arg_processed = True
                        break
                    elif cmd.type == CommandParser.CommandType.PARAMETERIZED_FLAG_COMMAND:
                        if (len(command) - 1) < (i + 1):
                            # no more command arguments to process
                            IO.error('parameter for flag {}{}{} is missing'.format(IO.Fore.LIGHTYELLOW_EX, cmd.name, IO.Style.RESET_ALL))
                            return

                        # if parameterized flag, set value to next argument
                        value = command[i + 1]
                        result_dict[cmd.name] = value

                        # skip the next argument (already processed)
                        skip_next = True

                        is_arg_processed = True
                        break
            
            if not is_arg_processed:
                for cmd in self._parameter_commands:
                    # parameter command, since a flag could not be found
                    if result_dict[cmd.name] is None:
                        # set parameter value
                        result_dict[cmd.name] = arg
                        is_arg_processed = True
                        break

            if not is_arg_processed:
                IO.error('{}{}{} is an unknown command.'.format(IO.Fore.LIGHTYELLOW_EX, arg, IO.Style.RESET_ALL))
                return

        # check if there are any parameters missing
        for cmd in self._parameter_commands:
            if result_dict[cmd.name] is None:
                IO.error('parameter {}{}{} is missing'.format(IO.Fore.LIGHTYELLOW_EX, cmd.name, IO.Style.RESET_ALL))
                return

        # set unspecified flags to False instead of None
        for cmd in self._flag_commands:
            if cmd.type == CommandParser.CommandType.FLAG_COMMAND:
                if result_dict[cmd.name] is None:
                    result_dict[cmd.name] = False

        result_tuple = collections.namedtuple('ParseResult', sorted(result_dict))
        return result_tuple(**result_dict)",_1978.py,22,result is not None and sp.handler is not None,result is not None is not sp.handler
https://github.com/tensorflow/data-validation/tree/master/tensorflow_data_validation/statistics/generators/basic_stats_generator.py,"def merge_accumulators(
      self, accumulators: Iterable[_BasicAcctype]) -> _BasicAcctype:
    it = iter(accumulators)
    result = next(it)
    for accumulator in it:
      result.num_examples += accumulator.num_examples
      result.weighted_num_examples += accumulator.weighted_num_examples
      for feature_path, basic_stats in accumulator.items():
        current_type = basic_stats.common_stats.type
        existing_stats = result.get(feature_path)
        if existing_stats is None:
          existing_stats = basic_stats
          result[feature_path] = basic_stats
        else:
          # Check if the types from the two partial statistics are not
          # compatible. If so, raise an error. We consider types to be
          # compatible if both types are same or one of them is None.
          left_type = existing_stats.common_stats.type
          right_type = current_type
          if (left_type is not None and right_type is not None and
              left_type != right_type):
            raise TypeError('Cannot determine the type of feature %s. '
                            'Found values of types %s and %s.' %
                            (feature_path, left_type, right_type))

          existing_stats.common_stats.merge_with(feature_path,
                                                 basic_stats.common_stats)

          if current_type is not None:
            if feature_path in self._bytes_features:
              existing_stats.bytes_stats += basic_stats.bytes_stats
            elif (top_k_uniques_stats_util.output_categorical_numeric(
                self._categorical_numeric_types, feature_path, current_type) or
                  current_type == statistics_pb2.FeatureNameStatistics.STRING):
              existing_stats.string_stats += basic_stats.string_stats
            elif current_type in (statistics_pb2.FeatureNameStatistics.INT,
                                  statistics_pb2.FeatureNameStatistics.FLOAT):
              existing_stats.numeric_stats += basic_stats.numeric_stats

    return result",_2112.py,20,left_type is not None and right_type is not None and (left_type != right_type),left_type is not None is not right_type != left_type
https://github.com/armory3d/armory/tree/master/blender/arm/logicnode/math/LN_math_expression.py,"def get_invalid_characters(value):
        value = value.replace(' ', '')
        len_v = len(value)
        arg = ['a', 'b', 'c', 'd', 'e', 'x', 'y', 'h', 'i', 'k']
        for i in range(len_v):
            s = value[i]
            if s == '.':
                if ((i - 1) < 0) or ((i + 1) >= len_v) or (not value[i - 1].isnumeric()) or (not value[i + 1].isnumeric()):
                    return False
            oper = ['+', '-', '*', '/', '%', '^']
            if s == '(':
                if (i > 0) and ((value[i - 1] not in oper) and (value[i - 1] != '(')):
                    return False
                if (i < (len_v - 1)) and ((value[i + 1] not in arg) and (not value[i + 1].isnumeric()) and (value[i + 1] != '(')):
                    return False
            if s == ')':
                if (i > 0) and ((value[i - 1] not in arg) and (not value[i - 1].isnumeric()) and (value[i - 1] != ')')):
                    return False
                if (i < (len_v - 1)) and (not value[i + 1].isnumeric()) and ((value[i + 1] not in oper) and (value[i + 1] != ')')):
                    return False
            if s in oper:
                if ((i > 0) and (value[i - 1] in oper)) or ((i < (len_v - 1)) and (value[i + 1] in oper)):
                    return False
        last_sym = value[len_v - 1]
        if (not last_sym.isnumeric()) and (last_sym not in arg) and (last_sym != ')'):
            return False
        return True",_2130.py,25,not last_sym.isnumeric() and last_sym not in arg and (last_sym != ')'),')' != last_sym not in arg and (not last_sym.isnumeric())
https://github.com/armory3d/armory/tree/master/blender/arm/logicnode/math/LN_math_expression.py,"def get_invalid_characters(value):
        value = value.replace(' ', '')
        len_v = len(value)
        arg = ['a', 'b', 'c', 'd', 'e', 'x', 'y', 'h', 'i', 'k']
        for i in range(len_v):
            s = value[i]
            if s == '.':
                if ((i - 1) < 0) or ((i + 1) >= len_v) or (not value[i - 1].isnumeric()) or (not value[i + 1].isnumeric()):
                    return False
            oper = ['+', '-', '*', '/', '%', '^']
            if s == '(':
                if (i > 0) and ((value[i - 1] not in oper) and (value[i - 1] != '(')):
                    return False
                if (i < (len_v - 1)) and ((value[i + 1] not in arg) and (not value[i + 1].isnumeric()) and (value[i + 1] != '(')):
                    return False
            if s == ')':
                if (i > 0) and ((value[i - 1] not in arg) and (not value[i - 1].isnumeric()) and (value[i - 1] != ')')):
                    return False
                if (i < (len_v - 1)) and (not value[i + 1].isnumeric()) and ((value[i + 1] not in oper) and (value[i + 1] != ')')):
                    return False
            if s in oper:
                if ((i > 0) and (value[i - 1] in oper)) or ((i < (len_v - 1)) and (value[i + 1] in oper)):
                    return False
        last_sym = value[len_v - 1]
        if (not last_sym.isnumeric()) and (last_sym not in arg) and (last_sym != ')'):
            return False
        return True",_2130.py,12,value[i - 1] not in oper and value[i - 1] != '(','(' != value[i - 1] not in oper
https://github.com/armory3d/armory/tree/master/blender/arm/logicnode/math/LN_math_expression.py,"def get_invalid_characters(value):
        value = value.replace(' ', '')
        len_v = len(value)
        arg = ['a', 'b', 'c', 'd', 'e', 'x', 'y', 'h', 'i', 'k']
        for i in range(len_v):
            s = value[i]
            if s == '.':
                if ((i - 1) < 0) or ((i + 1) >= len_v) or (not value[i - 1].isnumeric()) or (not value[i + 1].isnumeric()):
                    return False
            oper = ['+', '-', '*', '/', '%', '^']
            if s == '(':
                if (i > 0) and ((value[i - 1] not in oper) and (value[i - 1] != '(')):
                    return False
                if (i < (len_v - 1)) and ((value[i + 1] not in arg) and (not value[i + 1].isnumeric()) and (value[i + 1] != '(')):
                    return False
            if s == ')':
                if (i > 0) and ((value[i - 1] not in arg) and (not value[i - 1].isnumeric()) and (value[i - 1] != ')')):
                    return False
                if (i < (len_v - 1)) and (not value[i + 1].isnumeric()) and ((value[i + 1] not in oper) and (value[i + 1] != ')')):
                    return False
            if s in oper:
                if ((i > 0) and (value[i - 1] in oper)) or ((i < (len_v - 1)) and (value[i + 1] in oper)):
                    return False
        last_sym = value[len_v - 1]
        if (not last_sym.isnumeric()) and (last_sym not in arg) and (last_sym != ')'):
            return False
        return True",_2130.py,14,value[i + 1] not in arg and (not value[i + 1].isnumeric()) and (value[i + 1] != '('),'(' != value[i + 1] not in arg and (not value[i + 1].isnumeric())
https://github.com/armory3d/armory/tree/master/blender/arm/logicnode/math/LN_math_expression.py,"def get_invalid_characters(value):
        value = value.replace(' ', '')
        len_v = len(value)
        arg = ['a', 'b', 'c', 'd', 'e', 'x', 'y', 'h', 'i', 'k']
        for i in range(len_v):
            s = value[i]
            if s == '.':
                if ((i - 1) < 0) or ((i + 1) >= len_v) or (not value[i - 1].isnumeric()) or (not value[i + 1].isnumeric()):
                    return False
            oper = ['+', '-', '*', '/', '%', '^']
            if s == '(':
                if (i > 0) and ((value[i - 1] not in oper) and (value[i - 1] != '(')):
                    return False
                if (i < (len_v - 1)) and ((value[i + 1] not in arg) and (not value[i + 1].isnumeric()) and (value[i + 1] != '(')):
                    return False
            if s == ')':
                if (i > 0) and ((value[i - 1] not in arg) and (not value[i - 1].isnumeric()) and (value[i - 1] != ')')):
                    return False
                if (i < (len_v - 1)) and (not value[i + 1].isnumeric()) and ((value[i + 1] not in oper) and (value[i + 1] != ')')):
                    return False
            if s in oper:
                if ((i > 0) and (value[i - 1] in oper)) or ((i < (len_v - 1)) and (value[i + 1] in oper)):
                    return False
        last_sym = value[len_v - 1]
        if (not last_sym.isnumeric()) and (last_sym not in arg) and (last_sym != ')'):
            return False
        return True",_2130.py,17,value[i - 1] not in arg and (not value[i - 1].isnumeric()) and (value[i - 1] != ')'),')' != value[i - 1] not in arg and (not value[i - 1].isnumeric())
https://github.com/armory3d/armory/tree/master/blender/arm/logicnode/math/LN_math_expression.py,"def get_invalid_characters(value):
        value = value.replace(' ', '')
        len_v = len(value)
        arg = ['a', 'b', 'c', 'd', 'e', 'x', 'y', 'h', 'i', 'k']
        for i in range(len_v):
            s = value[i]
            if s == '.':
                if ((i - 1) < 0) or ((i + 1) >= len_v) or (not value[i - 1].isnumeric()) or (not value[i + 1].isnumeric()):
                    return False
            oper = ['+', '-', '*', '/', '%', '^']
            if s == '(':
                if (i > 0) and ((value[i - 1] not in oper) and (value[i - 1] != '(')):
                    return False
                if (i < (len_v - 1)) and ((value[i + 1] not in arg) and (not value[i + 1].isnumeric()) and (value[i + 1] != '(')):
                    return False
            if s == ')':
                if (i > 0) and ((value[i - 1] not in arg) and (not value[i - 1].isnumeric()) and (value[i - 1] != ')')):
                    return False
                if (i < (len_v - 1)) and (not value[i + 1].isnumeric()) and ((value[i + 1] not in oper) and (value[i + 1] != ')')):
                    return False
            if s in oper:
                if ((i > 0) and (value[i - 1] in oper)) or ((i < (len_v - 1)) and (value[i + 1] in oper)):
                    return False
        last_sym = value[len_v - 1]
        if (not last_sym.isnumeric()) and (last_sym not in arg) and (last_sym != ')'):
            return False
        return True",_2130.py,19,value[i + 1] not in oper and value[i + 1] != ')',')' != value[i + 1] not in oper
https://github.com/e2nIEE/pandapower/tree/master/pandapower/pypower/opf_execute.py,"def opf_execute(om, ppopt):
    """"""Executes the OPF specified by an OPF model object.

    C{results} are returned with internal indexing, all equipment
    in-service, etc.

    @see: L{opf}, L{opf_setup}

    @author: Ray Zimmerman (PSERC Cornell)
    @author: Richard Lincoln
    """"""
    ##-----  setup  -----
    ## options
    dc  = ppopt['PF_DC']        ## 1 = DC OPF, 0 = AC OPF
    alg = ppopt['OPF_ALG']
    verbose = ppopt['VERBOSE']

    ## build user-defined costs
    om.build_cost_params()

    ## get indexing
    vv, ll, nn, _ = om.get_idx()

    if verbose > 0:
        v = ppver('all')
        stdout.write('PYPOWER Version %s, %s' % (v['Version'], v['Date']))

    ##-----  run DC OPF solver  -----
    if dc:
        if verbose > 0:
            stdout.write(' -- DC Optimal Power Flow\n')

        results, success, raw = dcopf_solver(om, ppopt)
    else:
        ##-----  run AC OPF solver  -----
        if verbose > 0:
            stdout.write(' -- AC Optimal Power Flow\n')

        ## if OPF_ALG not set, choose best available option
        if alg == 0:
            alg = 560                ## MIPS

        ## update deprecated algorithm codes to new, generalized formulation equivalents
        if alg == 100 | alg == 200:        ## CONSTR
            alg = 300
        elif alg == 120 | alg == 220:      ## dense LP
            alg = 320
        elif alg == 140 | alg == 240:      ## sparse (relaxed) LP
            alg = 340
        elif alg == 160 | alg == 260:      ## sparse (full) LP
            alg = 360

        ppopt['OPF_ALG_POLY'] = alg

        ## run specific AC OPF solver
        if alg == 560 or alg == 565:                   ## PIPS
            results, success, raw = pipsopf_solver(om, ppopt)
#        elif alg == 580:                              ## IPOPT # pragma: no cover
#            try:
#                __import__('pyipopt')
#                results, success, raw = ipoptopf_solver(om, ppopt)
#            except ImportError:
#                raise ImportError('OPF_ALG %d requires IPOPT '
#                                  '(see https://projects.coin-or.org/Ipopt/)' %
#                                  alg)
        else:
            stderr.write('opf_execute: OPF_ALG %d is not a valid algorithm code\n' % alg)

    if ('output' not in raw) or ('alg' not in raw['output']):
        raw['output']['alg'] = alg

    if success:
        if not dc:
            ## copy bus voltages back to gen matrix
            results['gen'][:, VG] = results['bus'][results['gen'][:, GEN_BUS].astype(int), VM]

            ## gen PQ capability curve multipliers
            if (ll['N']['PQh'] > 0) | (ll['N']['PQl'] > 0): # pragma: no cover
                mu_PQh = results['mu']['lin']['l'][ll['i1']['PQh']:ll['iN']['PQh']] - results['mu']['lin']['u'][ll['i1']['PQh']:ll['iN']['PQh']]
                mu_PQl = results['mu']['lin']['l'][ll['i1']['PQl']:ll['iN']['PQl']] - results['mu']['lin']['u'][ll['i1']['PQl']:ll['iN']['PQl']]
                Apqdata = om.userdata('Apqdata')
                results['gen'] = update_mupq(results['baseMVA'], results['gen'], mu_PQh, mu_PQl, Apqdata)

            ## compute g, dg, f, df, d2f if requested by RETURN_RAW_DER = 1
            if ppopt['RETURN_RAW_DER']: # pragma: no cover
                ## move from results to raw if using v4.0 of MINOPF or TSPOPF
                if 'dg' in results:
                    raw = {}
                    raw['dg'] = results['dg']
                    raw['g'] = results['g']

                ## compute g, dg, unless already done by post-v4.0 MINOPF or TSPOPF
                if 'dg' not in raw:
                    ppc = om.get_ppc()
                    Ybus, Yf, Yt = makeYbus(ppc['baseMVA'], ppc['bus'], ppc['branch'])
                    g, geq, dg, dgeq = opf_consfcn(results['x'], om, Ybus, Yf, Yt, ppopt)
                    raw['g'] = r_[geq, g]
                    raw['dg'] = r_[dgeq.T, dg.T]   ## true Jacobian organization

                ## compute df, d2f
                _, df, d2f = opf_costfcn(results['x'], om, True)
                raw['df'] = df
                raw['d2f'] = d2f

        ## delete g and dg fieldsfrom results if using v4.0 of MINOPF or TSPOPF
        if 'dg' in results:
            del results['dg']
            del results['g']

        ## angle limit constraint multipliers
        if ll['N']['ang'] > 0:
            iang = om.userdata('iang')
            results['branch'][iang, MU_ANGMIN] = results['mu']['lin']['l'][ll['i1']['ang']:ll['iN']['ang']] * pi / 180
            results['branch'][iang, MU_ANGMAX] = results['mu']['lin']['u'][ll['i1']['ang']:ll['iN']['ang']] * pi / 180
    else:
        ## assign empty g, dg, f, df, d2f if requested by RETURN_RAW_DER = 1
        if not dc and ppopt['RETURN_RAW_DER']:
            raw['dg'] = array([])
            raw['g'] = array([])
            raw['df'] = array([])
            raw['d2f'] = array([])

    ## assign values and limit shadow prices for variables
    if om.var['order']:
        results['var'] = {'val': {}, 'mu': {'l': {}, 'u': {}}}
    for name in om.var['order']:
        if om.getN('var', name):
            idx = arange(vv['i1'][name], vv['iN'][name])
            results['var']['val'][name] = results['x'][idx]
            results['var']['mu']['l'][name] = results['mu']['var']['l'][idx]
            results['var']['mu']['u'][name] = results['mu']['var']['u'][idx]

    ## assign shadow prices for linear constraints
    if om.lin['order']:
        results['lin'] = {'mu': {'l': {}, 'u': {}}}
    for name in om.lin['order']:
        if om.getN('lin', name):
            idx = arange(ll['i1'][name], ll['iN'][name])
            results['lin']['mu']['l'][name] = results['mu']['lin']['l'][idx]
            results['lin']['mu']['u'][name] = results['mu']['lin']['u'][idx]

    ## assign shadow prices for nonlinear constraints
    if not dc:
        if om.nln['order']:
            results['nln'] = {'mu': {'l': {}, 'u': {}}}
        for name in om.nln['order']:
            if om.getN('nln', name):
                idx = arange(nn['i1'][name], nn['iN'][name])
                results['nln']['mu']['l'][name] = results['mu']['nln']['l'][idx]
                results['nln']['mu']['u'][name] = results['mu']['nln']['u'][idx]

    ## assign values for components of user cost
    if om.cost['order']:
        results['cost'] = {}
    for name in om.cost['order']:
        if om.getN('cost', name):
            results['cost'][name] = om.compute_cost(results['x'], name)

    ## if single-block PWL costs were converted to POLY, insert dummy y into x
    ## Note: The ""y"" portion of x will be nonsense, but everything should at
    ##       least be in the expected locations.
    pwl1 = om.userdata('pwl1')
    if (len(pwl1) > 0) and (alg != 545) and (alg != 550):
        ## get indexing
        vv, _, _, _ = om.get_idx()
        if dc:
            nx = vv['iN']['Pg']
        else:
            nx = vv['iN']['Qg']

        y = zeros(len(pwl1))
        raw['xr'] = r_[raw['xr'][:nx], y, raw['xr'][nx:]]
        results['x'] = r_[results['x'][:nx], y, results['x'][nx:]]

    return results, success, raw",_2131.py,163,len(pwl1) > 0 and alg != 545 and (alg != 550),len(pwl1) > 0 and 545 != alg != 550
https://github.com/willmcgugan/textual/tree/master/src/textual/widget.py,"def scroll_end(
        self,
        *,
        animate: bool = True,
        speed: float | None = None,
        duration: float | None = None,
        easing: EasingFunction | str | None = None,
    ) -> bool:
        """"""Scroll to the end of the container.

        Args:
            animate (bool, optional): Animate scroll. Defaults to True.
            speed (float | None, optional): Speed of scroll if animate is True. Or None to use duration.
            duration (float | None, optional): Duration of animation, if animate is True and speed is None.
            easing (EasingFunction | str | None, optional): An easing method for the scrolling animation. Defaults to ""None"",
                which will result in Textual choosing the configured default scrolling easing function.

        Returns:
            bool: True if any scrolling was done.

        """"""
        if speed is None and duration is None:
            duration = 1.0
        return self.scroll_to(
            0,
            self.max_scroll_y,
            animate=animate,
            speed=speed,
            duration=duration,
            easing=easing,
        )",_2168.py,22,speed is None and duration is None,speed is None is duration
https://github.com/osmr/imgclsmob/tree/master/gluon/gluoncv2/models/seresnet.py,"def __init__(self,
                 channels,
                 init_block_channels,
                 bottleneck,
                 conv1_stride,
                 bn_use_global_stats=False,
                 in_channels=3,
                 in_size=(224, 224),
                 classes=1000,
                 **kwargs):
        super(SEResNet, self).__init__(**kwargs)
        self.in_size = in_size
        self.classes = classes

        with self.name_scope():
            self.features = nn.HybridSequential(prefix="""")
            self.features.add(ResInitBlock(
                in_channels=in_channels,
                out_channels=init_block_channels,
                bn_use_global_stats=bn_use_global_stats))
            in_channels = init_block_channels
            for i, channels_per_stage in enumerate(channels):
                stage = nn.HybridSequential(prefix=""stage{}_"".format(i + 1))
                with stage.name_scope():
                    for j, out_channels in enumerate(channels_per_stage):
                        strides = 2 if (j == 0) and (i != 0) else 1
                        stage.add(SEResUnit(
                            in_channels=in_channels,
                            out_channels=out_channels,
                            strides=strides,
                            bn_use_global_stats=bn_use_global_stats,
                            bottleneck=bottleneck,
                            conv1_stride=conv1_stride))
                        in_channels = out_channels
                self.features.add(stage)
            self.features.add(nn.AvgPool2D(
                pool_size=7,
                strides=1))

            self.output = nn.HybridSequential(prefix="""")
            self.output.add(nn.Flatten())
            self.output.add(nn.Dense(
                units=classes,
                in_units=in_channels))",_2173.py,26,j == 0 and i != 0,j == 0 != i
https://github.com/uber-research/go-explore/tree/master/robustified/goexplore_py/goexplore.py,"def try_split_frames(self, frames):
        n_processes = multiprocessing.cpu_count()
        tqdm.write('Decoding frames')
        frames = [RLEArray.frombytes(f, dtype=np.uint8) for f in frames]
        tqdm.write('Frames decoded')
        unif_ent_cache = {}
        def get_dist_score(dist):
            if len(dist) == 1:
                return 0.0
            from math import log
            def ent(dist):
                return -sum(log(e) * e for e in dist)
            def unif_ent(l):
                if l not in unif_ent_cache:
                    return ent([1 / l] * l)
                return unif_ent_cache[l]
            def norment(dist):
                return ent(dist) / unif_ent(len(dist))
            target_len = len(frames) * self.args.cell_split_factor
            return norment(dist) / np.sqrt(abs(len(dist) - target_len) / target_len + 1)

        unif_score_cache = {}
        def unif_dist_score(l):
            if l not in unif_score_cache:
                unif_score_cache[l] = get_dist_score([1 / l] * l)
            return unif_score_cache[l]

        best_shape = (random.randint(1, self.normal_frame_shape[0] - 1), random.randint(1, self.normal_frame_shape[1] - 1))
        best_pix_val = random.randint(2, 255)
        best_score = -infinity #get_dist_score([1 / len(frames) for _ in range(len(frames))])
        best_n = 0
        seen = set()

        # Intuition: we want our batch size to be such that it will be processed in two passes
        BATCH_SIZE = len(frames) // (n_processes // 2 + 1) + 1

        def proc_downscale(to_process, returns):
            while True:
                start_batch, cur_shape, cur_pix_val = to_process.get()
                if start_batch == -1:
                    return
                results = []
                for i in range(start_batch, min(len(frames), start_batch + BATCH_SIZE)):
                    results.append(imdownscale(frames[i].to_np(), cur_shape, cur_pix_val).tobytes())
                returns.put(results)

        tqdm.write('Creating processes')
        to_process = multiprocessing.Queue()
        returns = multiprocessing.Queue()
        processes = [multiprocessing.Process(target=proc_downscale, args=(to_process, returns)) for _ in range(n_processes)]
        for p in processes:
            p.start()
        tqdm.write('Processes created')

        for _ in tqdm(range(self.args.split_iterations), desc='New representation'):
            cur_shape = best_shape
            cur_pix_val = best_pix_val
            while (cur_shape, cur_pix_val) in seen:
                cur_shape = list(best_shape)
                for idx in range(2):
                    while True:
                        cur_shape[idx] = np.random.geometric(min(1 / (best_shape[idx] + 1), 20 / self.normal_frame_shape[idx]))
                        if cur_shape[idx] >= 1 and cur_shape[idx] <= self.normal_frame_shape[idx] - 1:
                            break
                cur_shape = tuple(cur_shape)
                while True:
                    cur_pix_val = np.random.geometric(min(1 / best_pix_val, 1 / 12))
                    if cur_pix_val >= 2 and cur_pix_val <= 255:
                        break
            seen.add((cur_shape, cur_pix_val))

            for i in range(0, len(frames), BATCH_SIZE):
                to_process.put((i, cur_shape, cur_pix_val))
            downscaled = []
            for _ in range(0, len(frames), BATCH_SIZE):
                downscaled += returns.get()

            dist = np.array(list(Counter(downscaled).values())) / len(frames)
            cur_score = get_dist_score(dist)

            if cur_score >= best_score:
                if cur_score > best_score:
                    tqdm.write(f'NEW BEST score: {cur_score} n: {len(dist)} shape:{cur_shape} {cur_pix_val}')
                best_score = cur_score
                best_shape = cur_shape
                best_n = len(dist)
                best_pix_val = cur_pix_val

        for i in range(n_processes):
            to_process.put((-1, None, None))
        for p in processes:
            try:
                p.join(1)
            except Exception:
                p.terminate()

        return best_shape, best_pix_val, best_n",_2230.py,68,cur_pix_val >= 2 and cur_pix_val <= 255,2 <= cur_pix_val <= 255
https://github.com/uber-research/go-explore/tree/master/robustified/goexplore_py/goexplore.py,"def try_split_frames(self, frames):
        n_processes = multiprocessing.cpu_count()
        tqdm.write('Decoding frames')
        frames = [RLEArray.frombytes(f, dtype=np.uint8) for f in frames]
        tqdm.write('Frames decoded')
        unif_ent_cache = {}
        def get_dist_score(dist):
            if len(dist) == 1:
                return 0.0
            from math import log
            def ent(dist):
                return -sum(log(e) * e for e in dist)
            def unif_ent(l):
                if l not in unif_ent_cache:
                    return ent([1 / l] * l)
                return unif_ent_cache[l]
            def norment(dist):
                return ent(dist) / unif_ent(len(dist))
            target_len = len(frames) * self.args.cell_split_factor
            return norment(dist) / np.sqrt(abs(len(dist) - target_len) / target_len + 1)

        unif_score_cache = {}
        def unif_dist_score(l):
            if l not in unif_score_cache:
                unif_score_cache[l] = get_dist_score([1 / l] * l)
            return unif_score_cache[l]

        best_shape = (random.randint(1, self.normal_frame_shape[0] - 1), random.randint(1, self.normal_frame_shape[1] - 1))
        best_pix_val = random.randint(2, 255)
        best_score = -infinity #get_dist_score([1 / len(frames) for _ in range(len(frames))])
        best_n = 0
        seen = set()

        # Intuition: we want our batch size to be such that it will be processed in two passes
        BATCH_SIZE = len(frames) // (n_processes // 2 + 1) + 1

        def proc_downscale(to_process, returns):
            while True:
                start_batch, cur_shape, cur_pix_val = to_process.get()
                if start_batch == -1:
                    return
                results = []
                for i in range(start_batch, min(len(frames), start_batch + BATCH_SIZE)):
                    results.append(imdownscale(frames[i].to_np(), cur_shape, cur_pix_val).tobytes())
                returns.put(results)

        tqdm.write('Creating processes')
        to_process = multiprocessing.Queue()
        returns = multiprocessing.Queue()
        processes = [multiprocessing.Process(target=proc_downscale, args=(to_process, returns)) for _ in range(n_processes)]
        for p in processes:
            p.start()
        tqdm.write('Processes created')

        for _ in tqdm(range(self.args.split_iterations), desc='New representation'):
            cur_shape = best_shape
            cur_pix_val = best_pix_val
            while (cur_shape, cur_pix_val) in seen:
                cur_shape = list(best_shape)
                for idx in range(2):
                    while True:
                        cur_shape[idx] = np.random.geometric(min(1 / (best_shape[idx] + 1), 20 / self.normal_frame_shape[idx]))
                        if cur_shape[idx] >= 1 and cur_shape[idx] <= self.normal_frame_shape[idx] - 1:
                            break
                cur_shape = tuple(cur_shape)
                while True:
                    cur_pix_val = np.random.geometric(min(1 / best_pix_val, 1 / 12))
                    if cur_pix_val >= 2 and cur_pix_val <= 255:
                        break
            seen.add((cur_shape, cur_pix_val))

            for i in range(0, len(frames), BATCH_SIZE):
                to_process.put((i, cur_shape, cur_pix_val))
            downscaled = []
            for _ in range(0, len(frames), BATCH_SIZE):
                downscaled += returns.get()

            dist = np.array(list(Counter(downscaled).values())) / len(frames)
            cur_score = get_dist_score(dist)

            if cur_score >= best_score:
                if cur_score > best_score:
                    tqdm.write(f'NEW BEST score: {cur_score} n: {len(dist)} shape:{cur_shape} {cur_pix_val}')
                best_score = cur_score
                best_shape = cur_shape
                best_n = len(dist)
                best_pix_val = cur_pix_val

        for i in range(n_processes):
            to_process.put((-1, None, None))
        for p in processes:
            try:
                p.join(1)
            except Exception:
                p.terminate()

        return best_shape, best_pix_val, best_n",_2230.py,63,cur_shape[idx] >= 1 and cur_shape[idx] <= self.normal_frame_shape[idx] - 1,1 <= cur_shape[idx] <= self.normal_frame_shape[idx] - 1
https://github.com/beerfactory/hbmqtt/tree/master/hbmqtt/broker.py,"def client_connected(self, listener_name, reader: ReaderAdapter, writer: WriterAdapter):
        # Wait for connection available on listener
        server = self._servers.get(listener_name, None)
        if not server:
            raise BrokerException(""Invalid listener name '%s'"" % listener_name)
        yield from server.acquire_connection()

        remote_address, remote_port = writer.get_peer_info()
        self.logger.info(""Connection from %s:%d on listener '%s'"" % (remote_address, remote_port, listener_name))

        # Wait for first packet and expect a CONNECT
        try:
            handler, client_session = yield from BrokerProtocolHandler.init_from_connect(reader, writer, self.plugins_manager, loop=self._loop)
        except HBMQTTException as exc:
            self.logger.warning(""[MQTT-3.1.0-1] %s: Can't read first packet an CONNECT: %s"" %
                                (format_client_message(address=remote_address, port=remote_port), exc))
            #yield from writer.close()
            self.logger.debug(""Connection closed"")
            return
        except MQTTException as me:
            self.logger.error('Invalid connection from %s : %s' %
                              (format_client_message(address=remote_address, port=remote_port), me))
            yield from writer.close()
            self.logger.debug(""Connection closed"")
            return

        if client_session.clean_session:
            # Delete existing session and create a new one
            if client_session.client_id is not None and client_session.client_id != """":
                self.delete_session(client_session.client_id)
            else:
                client_session.client_id = gen_client_id()
            client_session.parent = 0
        else:
            # Get session from cache
            if client_session.client_id in self._sessions:
                self.logger.debug(""Found old session %s"" % repr(self._sessions[client_session.client_id]))
                (client_session, h) = self._sessions[client_session.client_id]
                client_session.parent = 1
            else:
                client_session.parent = 0
        if client_session.keep_alive > 0:
            client_session.keep_alive += self.config['timeout-disconnect-delay']
        self.logger.debug(""Keep-alive timeout=%d"" % client_session.keep_alive)

        handler.attach(client_session, reader, writer)
        self._sessions[client_session.client_id] = (client_session, handler)

        authenticated = yield from self.authenticate(client_session, self.listeners_config[listener_name])
        if not authenticated:
            yield from writer.close()
            server.release_connection()  # Delete client from connections list
            return

        while True:
            try:
                client_session.transitions.connect()
                break
            except (MachineError, ValueError):
                # Backwards compat: MachineError is raised by transitions < 0.5.0.
                self.logger.warning(""Client %s is reconnecting too quickly, make it wait"" % client_session.client_id)
                # Wait a bit may be client is reconnecting too fast
                yield from asyncio.sleep(1, loop=self._loop)
        yield from handler.mqtt_connack_authorize(authenticated)

        yield from self.plugins_manager.fire_event(EVENT_BROKER_CLIENT_CONNECTED, client_id=client_session.client_id)

        self.logger.debug(""%s Start messages handling"" % client_session.client_id)
        yield from handler.start()
        self.logger.debug(""Retained messages queue size: %d"" % client_session.retained_messages.qsize())
        yield from self.publish_session_retained_messages(client_session)

        # Init and start loop for handling client messages (publish, subscribe/unsubscribe, disconnect)
        disconnect_waiter = asyncio.ensure_future(handler.wait_disconnect(), loop=self._loop)
        subscribe_waiter = asyncio.ensure_future(handler.get_next_pending_subscription(), loop=self._loop)
        unsubscribe_waiter = asyncio.ensure_future(handler.get_next_pending_unsubscription(), loop=self._loop)
        wait_deliver = asyncio.ensure_future(handler.mqtt_deliver_next_message(), loop=self._loop)
        connected = True
        while connected:
            try:
                done, pending = yield from asyncio.wait(
                    [disconnect_waiter, subscribe_waiter, unsubscribe_waiter, wait_deliver],
                    return_when=asyncio.FIRST_COMPLETED, loop=self._loop)
                if disconnect_waiter in done:
                    result = disconnect_waiter.result()
                    self.logger.debug(""%s Result from wait_diconnect: %s"" % (client_session.client_id, result))
                    if result is None:
                        self.logger.debug(""Will flag: %s"" % client_session.will_flag)
                        # Connection closed anormally, send will message
                        if client_session.will_flag:
                            self.logger.debug(""Client %s disconnected abnormally, sending will message"" %
                                              format_client_message(client_session))
                            yield from self._broadcast_message(
                                client_session,
                                client_session.will_topic,
                                client_session.will_message,
                                client_session.will_qos)
                            if client_session.will_retain:
                                self.retain_message(client_session,
                                                    client_session.will_topic,
                                                    client_session.will_message,
                                                    client_session.will_qos)
                    self.logger.debug(""%s Disconnecting session"" % client_session.client_id)
                    yield from self._stop_handler(handler)
                    client_session.transitions.disconnect()
                    yield from self.plugins_manager.fire_event(EVENT_BROKER_CLIENT_DISCONNECTED, client_id=client_session.client_id)
                    connected = False
                if unsubscribe_waiter in done:
                    self.logger.debug(""%s handling unsubscription"" % client_session.client_id)
                    unsubscription = unsubscribe_waiter.result()
                    for topic in unsubscription['topics']:
                        self._del_subscription(topic, client_session)
                        yield from self.plugins_manager.fire_event(
                            EVENT_BROKER_CLIENT_UNSUBSCRIBED,
                            client_id=client_session.client_id,
                            topic=topic)
                    yield from handler.mqtt_acknowledge_unsubscription(unsubscription['packet_id'])
                    unsubscribe_waiter = asyncio.Task(handler.get_next_pending_unsubscription(), loop=self._loop)
                if subscribe_waiter in done:
                    self.logger.debug(""%s handling subscription"" % client_session.client_id)
                    subscriptions = subscribe_waiter.result()
                    return_codes = []
                    for subscription in subscriptions['topics']:
                        result = yield from self.add_subscription(subscription, client_session)
                        return_codes.append(result)
                    yield from handler.mqtt_acknowledge_subscription(subscriptions['packet_id'], return_codes)
                    for index, subscription in enumerate(subscriptions['topics']):
                        if return_codes[index] != 0x80:
                            yield from self.plugins_manager.fire_event(
                                EVENT_BROKER_CLIENT_SUBSCRIBED,
                                client_id=client_session.client_id,
                                topic=subscription[0],
                                qos=subscription[1])
                            yield from self.publish_retained_messages_for_subscription(subscription, client_session)
                    subscribe_waiter = asyncio.Task(handler.get_next_pending_subscription(), loop=self._loop)
                    self.logger.debug(repr(self._subscriptions))
                if wait_deliver in done:
                    if self.logger.isEnabledFor(logging.DEBUG):
                        self.logger.debug(""%s handling message delivery"" % client_session.client_id)
                    app_message = wait_deliver.result()
                    if not app_message.topic:
                        self.logger.warning(""[MQTT-4.7.3-1] - %s invalid TOPIC sent in PUBLISH message, closing connection"" % client_session.client_id)
                        break
                    if ""#"" in app_message.topic or ""+"" in app_message.topic:
                        self.logger.warning(""[MQTT-3.3.2-2] - %s invalid TOPIC sent in PUBLISH message, closing connection"" % client_session.client_id)
                        break
                    yield from self.plugins_manager.fire_event(EVENT_BROKER_MESSAGE_RECEIVED,
                                                               client_id=client_session.client_id,
                                                               message=app_message)
                    yield from self._broadcast_message(client_session, app_message.topic, app_message.data)
                    if app_message.publish_packet.retain_flag:
                        self.retain_message(client_session, app_message.topic, app_message.data, app_message.qos)
                    wait_deliver = asyncio.Task(handler.mqtt_deliver_next_message(), loop=self._loop)
            except asyncio.CancelledError:
                self.logger.debug(""Client loop cancelled"")
                break
        disconnect_waiter.cancel()
        subscribe_waiter.cancel()
        unsubscribe_waiter.cancel()
        wait_deliver.cancel()

        self.logger.debug(""%s Client disconnected"" % client_session.client_id)
        server.release_connection()",_2280.py,29,client_session.client_id is not None and client_session.client_id != '',None is not client_session.client_id != ''
https://github.com/plasma-disassembler/plasma/tree/master/plasma/lib/disassembler.py,"def dump_asm(self, ctx, lines=NB_LINES_TO_DISASM, until=-1):
        ARCH = self.load_arch_module()
        ARCH_OUTPUT = ARCH.output
        ARCH_UTILS = ARCH.utils

        ad = ctx.entry
        s = self.binary.get_section(ad)

        if s is None:
            # until is != -1 only from the visual mode
            # It allows to not go before the first section.
            if until != -1:
                return None
            # Get the next section, it's not mandatory that sections
            # are consecutives !
            s = self.binary.get_next_section(ad)
            if s is None:
                return None
            ad = s.start

        o = ARCH_OUTPUT.Output(ctx)
        o._new_line()
        o.curr_section = s
        o.mode_dump = True
        l = 0
        api = ctx.gctx.api

        # For mips: after a jump we add a newline, but for mips we should
        # add this newline after the prefetch instruction.
        prefetch_after_branch = False

        while 1:
            if ad == s.start:
                if not o.last_2_lines_are_empty():
                    o._new_line()
                o._dash()
                o._section(s.name)
                o._add(""  0x%x -> 0x%x"" % (s.start, s.end))
                o._new_line()
                o._new_line()

            while ((l < lines and until == -1) or (ad < until and until != -1)) \
                    and ad <= s.end:

                ty = self.mem.get_type(ad)

                # A PE import should not be displayed as a subroutine
                if not(self.binary.type == T_BIN_PE and ad in self.binary.imports) \
                        and self.mem.is_code(ad):

                    is_func = ad in self.functions

                    if is_func:
                        if not o.last_2_lines_are_empty():
                            o._new_line()
                        o._dash()
                        o._user_comment(""; SUBROUTINE"")
                        o._new_line()
                        o._dash()

                    i = self.lazy_disasm(ad, s.start)

                    if not is_func and ad in self.xrefs and \
                            not o.last_2_lines_are_empty():
                        o._new_line()

                    o._asm_inst(i)

                    is_end = ad in self.end_functions

                    # mips
                    if prefetch_after_branch:
                        prefetch_after_branch = False
                        if not is_end:
                            o._new_line()

                    if is_end:
                        for fad in self.end_functions[ad]:
                            sy = api.get_symbol(fad)
                            o._user_comment(""; end function %s"" % sy)
                            o._new_line()
                        o._new_line()

                    elif ARCH_UTILS.is_uncond_jump(i) or ARCH_UTILS.is_ret(i):
                        if self.is_mips:
                            prefetch_after_branch = True
                        else:
                            o._new_line()

                    elif ARCH_UTILS.is_call(i):
                        op = i.operands[0]
                        if op.type == self.capstone.CS_OP_IMM:
                            imm = unsigned(op.value.imm)
                            if imm in self.functions and self.is_noreturn(imm):
                                if self.is_mips:
                                    prefetch_after_branch = True
                                else:
                                    o._new_line()

                    ad += i.size

                elif MEM_WOFFSET <= ty <= MEM_QOFFSET:
                    prefetch_after_branch = False
                    o._label_and_address(ad)
                    o.set_line(ad)
                    sz = self.mem.get_size(ad)
                    off = s.read_int(ad, sz)
                    if off is None:
                        continue
                    if ctx.gctx.print_bytes:
                        o._bytes(s.read(ad, sz))
                    o._data_prefix(sz)
                    o._add("" "")
                    o._imm(off, sz, True, print_data=False, force_dont_print_data=True)
                    o._new_line()
                    ad += sz

                elif ty == MEM_ASCII:
                    prefetch_after_branch = False
                    o._label_and_address(ad)
                    o.set_line(ad)
                    sz = self.mem.get_size(ad)
                    buf = self.binary.get_string(ad, sz)

                    if buf is not None:
                        if ctx.gctx.print_bytes:
                            o._bytes(s.read(ad, sz))

                        # Split the string into multi lines

                        splitted = buf.split(""\n"")

                        j = 0
                        for i, st in enumerate(splitted):
                            if i > 0 and len(st) != 0:
                                o._new_line()
                                o.set_line(ad + j)
                                o._address(ad + j)

                            ibs = 0
                            bs = 65
                            while ibs < len(st):
                                if ibs > 0:
                                    o._new_line()
                                    o.set_line(ad + j)
                                    o._address(ad + j)

                                blk = st[ibs:ibs + bs]

                                if i < len(splitted) - 1 and ibs + bs >= len(st):
                                    o._string('""' + blk + '\\n""')
                                    j += len(blk) + 1
                                else:
                                    o._string('""' + blk + '""')
                                    j += len(blk)

                                ibs += bs

                    o._add("", 0"")
                    o._new_line()
                    ad += sz

                elif ty == MEM_ARRAY:
                    prefetch_after_branch = False
                    o._label_and_address(ad)

                    array_info = self.mem.mm[ad]
                    total_size = array_info[0]
                    entry_type = array_info[2]
                    entry_size = self.mem.get_size_from_type(entry_type)

                    n = int(total_size / entry_size)

                    o.set_line(ad)
                    o._data_prefix(entry_size)

                    k = 0
                    while k < total_size:
                        if o.curr_index > 70:
                            o._new_line()
                            o.set_line(ad)
                            o._address(ad)
                            o._data_prefix(entry_size)
                            l += 1

                        val = s.read_int(ad, entry_size)
                        if MEM_WOFFSET <= entry_type <= MEM_QOFFSET:
                            o._add("" "")
                            o._imm(val, entry_size, True,
                                   print_data=False, force_dont_print_data=True)
                        else:
                            o._word(val, entry_size, is_from_array=True)

                        ad += entry_size
                        k += entry_size

                        if k < total_size:
                            o._add("","")

                    o._new_line()

                else:
                    prefetch_after_branch = False
                    o._label_and_address(ad)
                    o.set_line(ad)
                    sz = self.mem.get_size_from_type(ty)
                    if ctx.gctx.print_bytes:
                        o._bytes(s.read(ad, sz))
                    o._word(s.read_int(ad, sz), sz)
                    o._new_line()
                    ad += sz

                l += 1

            s = self.binary.get_section(ad)
            if s is None:
                # Get the next section, it's not mandatory that sections
                # are consecutives !
                s = self.binary.get_next_section(ad)
                if s is None:
                    break
                o._new_line()
                ad = s.start
                if until != -1 and ad >= until:
                    break

            if (l >= lines and until == -1) or (ad >= until and until != -1):
                break

            o.curr_section = s

        if until == ad:
            if self.mem.is_code(ad) and ad in self.xrefs or \
                    s is not None and ad == s.start:
                if not o.last_2_lines_are_empty():
                    o._new_line()

        # remove the last empty line
        o.lines.pop(-1)
        o.token_lines.pop(-1)

        o.join_lines()

        return o",_2533.py,224,until != -1 and ad >= until,ad >= until != -1
https://github.com/plasma-disassembler/plasma/tree/master/plasma/lib/disassembler.py,"def dump_asm(self, ctx, lines=NB_LINES_TO_DISASM, until=-1):
        ARCH = self.load_arch_module()
        ARCH_OUTPUT = ARCH.output
        ARCH_UTILS = ARCH.utils

        ad = ctx.entry
        s = self.binary.get_section(ad)

        if s is None:
            # until is != -1 only from the visual mode
            # It allows to not go before the first section.
            if until != -1:
                return None
            # Get the next section, it's not mandatory that sections
            # are consecutives !
            s = self.binary.get_next_section(ad)
            if s is None:
                return None
            ad = s.start

        o = ARCH_OUTPUT.Output(ctx)
        o._new_line()
        o.curr_section = s
        o.mode_dump = True
        l = 0
        api = ctx.gctx.api

        # For mips: after a jump we add a newline, but for mips we should
        # add this newline after the prefetch instruction.
        prefetch_after_branch = False

        while 1:
            if ad == s.start:
                if not o.last_2_lines_are_empty():
                    o._new_line()
                o._dash()
                o._section(s.name)
                o._add(""  0x%x -> 0x%x"" % (s.start, s.end))
                o._new_line()
                o._new_line()

            while ((l < lines and until == -1) or (ad < until and until != -1)) \
                    and ad <= s.end:

                ty = self.mem.get_type(ad)

                # A PE import should not be displayed as a subroutine
                if not(self.binary.type == T_BIN_PE and ad in self.binary.imports) \
                        and self.mem.is_code(ad):

                    is_func = ad in self.functions

                    if is_func:
                        if not o.last_2_lines_are_empty():
                            o._new_line()
                        o._dash()
                        o._user_comment(""; SUBROUTINE"")
                        o._new_line()
                        o._dash()

                    i = self.lazy_disasm(ad, s.start)

                    if not is_func and ad in self.xrefs and \
                            not o.last_2_lines_are_empty():
                        o._new_line()

                    o._asm_inst(i)

                    is_end = ad in self.end_functions

                    # mips
                    if prefetch_after_branch:
                        prefetch_after_branch = False
                        if not is_end:
                            o._new_line()

                    if is_end:
                        for fad in self.end_functions[ad]:
                            sy = api.get_symbol(fad)
                            o._user_comment(""; end function %s"" % sy)
                            o._new_line()
                        o._new_line()

                    elif ARCH_UTILS.is_uncond_jump(i) or ARCH_UTILS.is_ret(i):
                        if self.is_mips:
                            prefetch_after_branch = True
                        else:
                            o._new_line()

                    elif ARCH_UTILS.is_call(i):
                        op = i.operands[0]
                        if op.type == self.capstone.CS_OP_IMM:
                            imm = unsigned(op.value.imm)
                            if imm in self.functions and self.is_noreturn(imm):
                                if self.is_mips:
                                    prefetch_after_branch = True
                                else:
                                    o._new_line()

                    ad += i.size

                elif MEM_WOFFSET <= ty <= MEM_QOFFSET:
                    prefetch_after_branch = False
                    o._label_and_address(ad)
                    o.set_line(ad)
                    sz = self.mem.get_size(ad)
                    off = s.read_int(ad, sz)
                    if off is None:
                        continue
                    if ctx.gctx.print_bytes:
                        o._bytes(s.read(ad, sz))
                    o._data_prefix(sz)
                    o._add("" "")
                    o._imm(off, sz, True, print_data=False, force_dont_print_data=True)
                    o._new_line()
                    ad += sz

                elif ty == MEM_ASCII:
                    prefetch_after_branch = False
                    o._label_and_address(ad)
                    o.set_line(ad)
                    sz = self.mem.get_size(ad)
                    buf = self.binary.get_string(ad, sz)

                    if buf is not None:
                        if ctx.gctx.print_bytes:
                            o._bytes(s.read(ad, sz))

                        # Split the string into multi lines

                        splitted = buf.split(""\n"")

                        j = 0
                        for i, st in enumerate(splitted):
                            if i > 0 and len(st) != 0:
                                o._new_line()
                                o.set_line(ad + j)
                                o._address(ad + j)

                            ibs = 0
                            bs = 65
                            while ibs < len(st):
                                if ibs > 0:
                                    o._new_line()
                                    o.set_line(ad + j)
                                    o._address(ad + j)

                                blk = st[ibs:ibs + bs]

                                if i < len(splitted) - 1 and ibs + bs >= len(st):
                                    o._string('""' + blk + '\\n""')
                                    j += len(blk) + 1
                                else:
                                    o._string('""' + blk + '""')
                                    j += len(blk)

                                ibs += bs

                    o._add("", 0"")
                    o._new_line()
                    ad += sz

                elif ty == MEM_ARRAY:
                    prefetch_after_branch = False
                    o._label_and_address(ad)

                    array_info = self.mem.mm[ad]
                    total_size = array_info[0]
                    entry_type = array_info[2]
                    entry_size = self.mem.get_size_from_type(entry_type)

                    n = int(total_size / entry_size)

                    o.set_line(ad)
                    o._data_prefix(entry_size)

                    k = 0
                    while k < total_size:
                        if o.curr_index > 70:
                            o._new_line()
                            o.set_line(ad)
                            o._address(ad)
                            o._data_prefix(entry_size)
                            l += 1

                        val = s.read_int(ad, entry_size)
                        if MEM_WOFFSET <= entry_type <= MEM_QOFFSET:
                            o._add("" "")
                            o._imm(val, entry_size, True,
                                   print_data=False, force_dont_print_data=True)
                        else:
                            o._word(val, entry_size, is_from_array=True)

                        ad += entry_size
                        k += entry_size

                        if k < total_size:
                            o._add("","")

                    o._new_line()

                else:
                    prefetch_after_branch = False
                    o._label_and_address(ad)
                    o.set_line(ad)
                    sz = self.mem.get_size_from_type(ty)
                    if ctx.gctx.print_bytes:
                        o._bytes(s.read(ad, sz))
                    o._word(s.read_int(ad, sz), sz)
                    o._new_line()
                    ad += sz

                l += 1

            s = self.binary.get_section(ad)
            if s is None:
                # Get the next section, it's not mandatory that sections
                # are consecutives !
                s = self.binary.get_next_section(ad)
                if s is None:
                    break
                o._new_line()
                ad = s.start
                if until != -1 and ad >= until:
                    break

            if (l >= lines and until == -1) or (ad >= until and until != -1):
                break

            o.curr_section = s

        if until == ad:
            if self.mem.is_code(ad) and ad in self.xrefs or \
                    s is not None and ad == s.start:
                if not o.last_2_lines_are_empty():
                    o._new_line()

        # remove the last empty line
        o.lines.pop(-1)
        o.token_lines.pop(-1)

        o.join_lines()

        return o",_2533.py,227,ad >= until and until != -1,ad >= until != -1
https://github.com/plasma-disassembler/plasma/tree/master/plasma/lib/disassembler.py,"def dump_asm(self, ctx, lines=NB_LINES_TO_DISASM, until=-1):
        ARCH = self.load_arch_module()
        ARCH_OUTPUT = ARCH.output
        ARCH_UTILS = ARCH.utils

        ad = ctx.entry
        s = self.binary.get_section(ad)

        if s is None:
            # until is != -1 only from the visual mode
            # It allows to not go before the first section.
            if until != -1:
                return None
            # Get the next section, it's not mandatory that sections
            # are consecutives !
            s = self.binary.get_next_section(ad)
            if s is None:
                return None
            ad = s.start

        o = ARCH_OUTPUT.Output(ctx)
        o._new_line()
        o.curr_section = s
        o.mode_dump = True
        l = 0
        api = ctx.gctx.api

        # For mips: after a jump we add a newline, but for mips we should
        # add this newline after the prefetch instruction.
        prefetch_after_branch = False

        while 1:
            if ad == s.start:
                if not o.last_2_lines_are_empty():
                    o._new_line()
                o._dash()
                o._section(s.name)
                o._add(""  0x%x -> 0x%x"" % (s.start, s.end))
                o._new_line()
                o._new_line()

            while ((l < lines and until == -1) or (ad < until and until != -1)) \
                    and ad <= s.end:

                ty = self.mem.get_type(ad)

                # A PE import should not be displayed as a subroutine
                if not(self.binary.type == T_BIN_PE and ad in self.binary.imports) \
                        and self.mem.is_code(ad):

                    is_func = ad in self.functions

                    if is_func:
                        if not o.last_2_lines_are_empty():
                            o._new_line()
                        o._dash()
                        o._user_comment(""; SUBROUTINE"")
                        o._new_line()
                        o._dash()

                    i = self.lazy_disasm(ad, s.start)

                    if not is_func and ad in self.xrefs and \
                            not o.last_2_lines_are_empty():
                        o._new_line()

                    o._asm_inst(i)

                    is_end = ad in self.end_functions

                    # mips
                    if prefetch_after_branch:
                        prefetch_after_branch = False
                        if not is_end:
                            o._new_line()

                    if is_end:
                        for fad in self.end_functions[ad]:
                            sy = api.get_symbol(fad)
                            o._user_comment(""; end function %s"" % sy)
                            o._new_line()
                        o._new_line()

                    elif ARCH_UTILS.is_uncond_jump(i) or ARCH_UTILS.is_ret(i):
                        if self.is_mips:
                            prefetch_after_branch = True
                        else:
                            o._new_line()

                    elif ARCH_UTILS.is_call(i):
                        op = i.operands[0]
                        if op.type == self.capstone.CS_OP_IMM:
                            imm = unsigned(op.value.imm)
                            if imm in self.functions and self.is_noreturn(imm):
                                if self.is_mips:
                                    prefetch_after_branch = True
                                else:
                                    o._new_line()

                    ad += i.size

                elif MEM_WOFFSET <= ty <= MEM_QOFFSET:
                    prefetch_after_branch = False
                    o._label_and_address(ad)
                    o.set_line(ad)
                    sz = self.mem.get_size(ad)
                    off = s.read_int(ad, sz)
                    if off is None:
                        continue
                    if ctx.gctx.print_bytes:
                        o._bytes(s.read(ad, sz))
                    o._data_prefix(sz)
                    o._add("" "")
                    o._imm(off, sz, True, print_data=False, force_dont_print_data=True)
                    o._new_line()
                    ad += sz

                elif ty == MEM_ASCII:
                    prefetch_after_branch = False
                    o._label_and_address(ad)
                    o.set_line(ad)
                    sz = self.mem.get_size(ad)
                    buf = self.binary.get_string(ad, sz)

                    if buf is not None:
                        if ctx.gctx.print_bytes:
                            o._bytes(s.read(ad, sz))

                        # Split the string into multi lines

                        splitted = buf.split(""\n"")

                        j = 0
                        for i, st in enumerate(splitted):
                            if i > 0 and len(st) != 0:
                                o._new_line()
                                o.set_line(ad + j)
                                o._address(ad + j)

                            ibs = 0
                            bs = 65
                            while ibs < len(st):
                                if ibs > 0:
                                    o._new_line()
                                    o.set_line(ad + j)
                                    o._address(ad + j)

                                blk = st[ibs:ibs + bs]

                                if i < len(splitted) - 1 and ibs + bs >= len(st):
                                    o._string('""' + blk + '\\n""')
                                    j += len(blk) + 1
                                else:
                                    o._string('""' + blk + '""')
                                    j += len(blk)

                                ibs += bs

                    o._add("", 0"")
                    o._new_line()
                    ad += sz

                elif ty == MEM_ARRAY:
                    prefetch_after_branch = False
                    o._label_and_address(ad)

                    array_info = self.mem.mm[ad]
                    total_size = array_info[0]
                    entry_type = array_info[2]
                    entry_size = self.mem.get_size_from_type(entry_type)

                    n = int(total_size / entry_size)

                    o.set_line(ad)
                    o._data_prefix(entry_size)

                    k = 0
                    while k < total_size:
                        if o.curr_index > 70:
                            o._new_line()
                            o.set_line(ad)
                            o._address(ad)
                            o._data_prefix(entry_size)
                            l += 1

                        val = s.read_int(ad, entry_size)
                        if MEM_WOFFSET <= entry_type <= MEM_QOFFSET:
                            o._add("" "")
                            o._imm(val, entry_size, True,
                                   print_data=False, force_dont_print_data=True)
                        else:
                            o._word(val, entry_size, is_from_array=True)

                        ad += entry_size
                        k += entry_size

                        if k < total_size:
                            o._add("","")

                    o._new_line()

                else:
                    prefetch_after_branch = False
                    o._label_and_address(ad)
                    o.set_line(ad)
                    sz = self.mem.get_size_from_type(ty)
                    if ctx.gctx.print_bytes:
                        o._bytes(s.read(ad, sz))
                    o._word(s.read_int(ad, sz), sz)
                    o._new_line()
                    ad += sz

                l += 1

            s = self.binary.get_section(ad)
            if s is None:
                # Get the next section, it's not mandatory that sections
                # are consecutives !
                s = self.binary.get_next_section(ad)
                if s is None:
                    break
                o._new_line()
                ad = s.start
                if until != -1 and ad >= until:
                    break

            if (l >= lines and until == -1) or (ad >= until and until != -1):
                break

            o.curr_section = s

        if until == ad:
            if self.mem.is_code(ad) and ad in self.xrefs or \
                    s is not None and ad == s.start:
                if not o.last_2_lines_are_empty():
                    o._new_line()

        # remove the last empty line
        o.lines.pop(-1)
        o.token_lines.pop(-1)

        o.join_lines()

        return o",_2533.py,42,ad < until and until != -1,ad < until != -1
https://github.com/plasma-disassembler/plasma/tree/master/plasma/lib/disassembler.py,"def dump_asm(self, ctx, lines=NB_LINES_TO_DISASM, until=-1):
        ARCH = self.load_arch_module()
        ARCH_OUTPUT = ARCH.output
        ARCH_UTILS = ARCH.utils

        ad = ctx.entry
        s = self.binary.get_section(ad)

        if s is None:
            # until is != -1 only from the visual mode
            # It allows to not go before the first section.
            if until != -1:
                return None
            # Get the next section, it's not mandatory that sections
            # are consecutives !
            s = self.binary.get_next_section(ad)
            if s is None:
                return None
            ad = s.start

        o = ARCH_OUTPUT.Output(ctx)
        o._new_line()
        o.curr_section = s
        o.mode_dump = True
        l = 0
        api = ctx.gctx.api

        # For mips: after a jump we add a newline, but for mips we should
        # add this newline after the prefetch instruction.
        prefetch_after_branch = False

        while 1:
            if ad == s.start:
                if not o.last_2_lines_are_empty():
                    o._new_line()
                o._dash()
                o._section(s.name)
                o._add(""  0x%x -> 0x%x"" % (s.start, s.end))
                o._new_line()
                o._new_line()

            while ((l < lines and until == -1) or (ad < until and until != -1)) \
                    and ad <= s.end:

                ty = self.mem.get_type(ad)

                # A PE import should not be displayed as a subroutine
                if not(self.binary.type == T_BIN_PE and ad in self.binary.imports) \
                        and self.mem.is_code(ad):

                    is_func = ad in self.functions

                    if is_func:
                        if not o.last_2_lines_are_empty():
                            o._new_line()
                        o._dash()
                        o._user_comment(""; SUBROUTINE"")
                        o._new_line()
                        o._dash()

                    i = self.lazy_disasm(ad, s.start)

                    if not is_func and ad in self.xrefs and \
                            not o.last_2_lines_are_empty():
                        o._new_line()

                    o._asm_inst(i)

                    is_end = ad in self.end_functions

                    # mips
                    if prefetch_after_branch:
                        prefetch_after_branch = False
                        if not is_end:
                            o._new_line()

                    if is_end:
                        for fad in self.end_functions[ad]:
                            sy = api.get_symbol(fad)
                            o._user_comment(""; end function %s"" % sy)
                            o._new_line()
                        o._new_line()

                    elif ARCH_UTILS.is_uncond_jump(i) or ARCH_UTILS.is_ret(i):
                        if self.is_mips:
                            prefetch_after_branch = True
                        else:
                            o._new_line()

                    elif ARCH_UTILS.is_call(i):
                        op = i.operands[0]
                        if op.type == self.capstone.CS_OP_IMM:
                            imm = unsigned(op.value.imm)
                            if imm in self.functions and self.is_noreturn(imm):
                                if self.is_mips:
                                    prefetch_after_branch = True
                                else:
                                    o._new_line()

                    ad += i.size

                elif MEM_WOFFSET <= ty <= MEM_QOFFSET:
                    prefetch_after_branch = False
                    o._label_and_address(ad)
                    o.set_line(ad)
                    sz = self.mem.get_size(ad)
                    off = s.read_int(ad, sz)
                    if off is None:
                        continue
                    if ctx.gctx.print_bytes:
                        o._bytes(s.read(ad, sz))
                    o._data_prefix(sz)
                    o._add("" "")
                    o._imm(off, sz, True, print_data=False, force_dont_print_data=True)
                    o._new_line()
                    ad += sz

                elif ty == MEM_ASCII:
                    prefetch_after_branch = False
                    o._label_and_address(ad)
                    o.set_line(ad)
                    sz = self.mem.get_size(ad)
                    buf = self.binary.get_string(ad, sz)

                    if buf is not None:
                        if ctx.gctx.print_bytes:
                            o._bytes(s.read(ad, sz))

                        # Split the string into multi lines

                        splitted = buf.split(""\n"")

                        j = 0
                        for i, st in enumerate(splitted):
                            if i > 0 and len(st) != 0:
                                o._new_line()
                                o.set_line(ad + j)
                                o._address(ad + j)

                            ibs = 0
                            bs = 65
                            while ibs < len(st):
                                if ibs > 0:
                                    o._new_line()
                                    o.set_line(ad + j)
                                    o._address(ad + j)

                                blk = st[ibs:ibs + bs]

                                if i < len(splitted) - 1 and ibs + bs >= len(st):
                                    o._string('""' + blk + '\\n""')
                                    j += len(blk) + 1
                                else:
                                    o._string('""' + blk + '""')
                                    j += len(blk)

                                ibs += bs

                    o._add("", 0"")
                    o._new_line()
                    ad += sz

                elif ty == MEM_ARRAY:
                    prefetch_after_branch = False
                    o._label_and_address(ad)

                    array_info = self.mem.mm[ad]
                    total_size = array_info[0]
                    entry_type = array_info[2]
                    entry_size = self.mem.get_size_from_type(entry_type)

                    n = int(total_size / entry_size)

                    o.set_line(ad)
                    o._data_prefix(entry_size)

                    k = 0
                    while k < total_size:
                        if o.curr_index > 70:
                            o._new_line()
                            o.set_line(ad)
                            o._address(ad)
                            o._data_prefix(entry_size)
                            l += 1

                        val = s.read_int(ad, entry_size)
                        if MEM_WOFFSET <= entry_type <= MEM_QOFFSET:
                            o._add("" "")
                            o._imm(val, entry_size, True,
                                   print_data=False, force_dont_print_data=True)
                        else:
                            o._word(val, entry_size, is_from_array=True)

                        ad += entry_size
                        k += entry_size

                        if k < total_size:
                            o._add("","")

                    o._new_line()

                else:
                    prefetch_after_branch = False
                    o._label_and_address(ad)
                    o.set_line(ad)
                    sz = self.mem.get_size_from_type(ty)
                    if ctx.gctx.print_bytes:
                        o._bytes(s.read(ad, sz))
                    o._word(s.read_int(ad, sz), sz)
                    o._new_line()
                    ad += sz

                l += 1

            s = self.binary.get_section(ad)
            if s is None:
                # Get the next section, it's not mandatory that sections
                # are consecutives !
                s = self.binary.get_next_section(ad)
                if s is None:
                    break
                o._new_line()
                ad = s.start
                if until != -1 and ad >= until:
                    break

            if (l >= lines and until == -1) or (ad >= until and until != -1):
                break

            o.curr_section = s

        if until == ad:
            if self.mem.is_code(ad) and ad in self.xrefs or \
                    s is not None and ad == s.start:
                if not o.last_2_lines_are_empty():
                    o._new_line()

        # remove the last empty line
        o.lines.pop(-1)
        o.token_lines.pop(-1)

        o.join_lines()

        return o",_2533.py,135,i > 0 and len(st) != 0,i > 0 != len(st)
https://github.com/LoRexxar/Kunlun-M/tree/master/core/detection.py,"def count_java_line(filename):
        count = {'count_code': 0, 'count_blank': 0, 'count_pound': 0}
        fi = open(filename, 'r')
        file_line = fi.readline()
        while fi.tell() != os.path.getsize(filename):
            file_line = file_line.lstrip()
            if len(file_line) == 0:
                count['count_blank'] += 1
            elif file_line.startswith('//'):
                count['count_pound'] += 1
            elif file_line.count('/*') == 1 and file_line.count('*/') == 1:
                if file_line.startswith('/*'):
                    count['count_pound'] += 1
                else:
                    count['count_code'] += 1
            elif file_line.count('/*') == 1 and file_line.count('*/') == 0:
                if file_line.startswith('/*'):
                    count['count_pound'] += 1
                    while True:
                        file_line = fi.readline()
                        if len(file_line) == 0 or file_line == ""\n"":
                            count['count_blank'] += 1
                        else:
                            count['count_pound'] += 1
                        if file_line.endswith('*/\n'):
                            break
                else:
                    count['count_code'] += 1
                    while True:
                        file_line = fi.readline()
                        if len(file_line) == 0 or file_line == ""\n"":
                            count['count_blank'] += 1
                        else:
                            count['count_code'] += 1
                        if file_line.find('*/'):
                            break
            else:
                count['count_code'] += 1
            file_line = fi.readline()
        fi.close()
        return count",_2624.py,11,file_line.count('/*') == 1 and file_line.count('*/') == 1,file_line.count('/*') == 1 == file_line.count('*/')
https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/core.py,"def __str__(self):
        if hasattr(self, 'name'):
            name = str(self.name)
        else:
            name = ''
        if hasattr(self, 'empty') and self.empty:
            return ''
        if self.address == '' and name == '':
            return 'EMAIL NOT DEFINED'
        if self.address == '' and name != '':
            return name
        if docassemble.base.functions.this_thread.evaluation_context == 'docx':
            return str(self.address)
        if name == '' and self.address != '':
            return '[' + str(self.address) + '](mailto:' + str(self.address) + ')'
        return '[' + str(name) + '](mailto:' + str(self.address) + ')'",_2688.py,8,self.address == '' and name == '',self.address == '' == name
https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/core.py,"def __str__(self):
        if hasattr(self, 'name'):
            name = str(self.name)
        else:
            name = ''
        if hasattr(self, 'empty') and self.empty:
            return ''
        if self.address == '' and name == '':
            return 'EMAIL NOT DEFINED'
        if self.address == '' and name != '':
            return name
        if docassemble.base.functions.this_thread.evaluation_context == 'docx':
            return str(self.address)
        if name == '' and self.address != '':
            return '[' + str(self.address) + '](mailto:' + str(self.address) + ')'
        return '[' + str(name) + '](mailto:' + str(self.address) + ')'",_2688.py,10,self.address == '' and name != '',self.address == '' != name
https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/core.py,"def __str__(self):
        if hasattr(self, 'name'):
            name = str(self.name)
        else:
            name = ''
        if hasattr(self, 'empty') and self.empty:
            return ''
        if self.address == '' and name == '':
            return 'EMAIL NOT DEFINED'
        if self.address == '' and name != '':
            return name
        if docassemble.base.functions.this_thread.evaluation_context == 'docx':
            return str(self.address)
        if name == '' and self.address != '':
            return '[' + str(self.address) + '](mailto:' + str(self.address) + ')'
        return '[' + str(name) + '](mailto:' + str(self.address) + ')'",_2688.py,14,name == '' and self.address != '',name == '' != self.address
https://github.com/bayespy/bayespy/tree/master/bayespy/inference/vmp/transformations.py,"def __init__(self, X, B, S, B_rotator):
        self.X_node = X
        self.B_node = B
        self.S_node = S
        self.B_rotator = B_rotator

        if len(S.plates) > 0 and S.plates[-1] > 1:
            raise ValueError(""The length of the last plate of S must be 1."")
        if len(B.plates) > 1 and B.plates[-2] > 1:
            raise ValueError(""The length of the last plate of B must be 1."")

        if len(S.dims[0]) != 1:
            raise ValueError(""S should have exactly one variable axis"")
        if len(B.dims[0]) != 2:
            raise ValueError(""B should have exactly two variable axes"")

        super().__init__(X, B_rotator)",_2754.py,9,len(B.plates) > 1 and B.plates[-2] > 1,len(B.plates) > 1 < B.plates[-2]
https://github.com/sunpy/sunpy/tree/master/sunpy/timeseries/metadata.py,"def _truncate(self, timerange):
        """"""
        Removes metadata entries outside of the new (truncated)
        `sunpy.time.TimeRange`. Also adjusts start and end times of time ranges
        going outside of the truncated time range.

        Parameters
        ----------
        timerange : `sunpy.time.TimeRange`
            The time range to truncate to.
        """"""
        truncated = []
        for metatuple in self.metadata:
            # Get metadata time range parameters
            start = metatuple[0].start
            end = metatuple[0].end
            out_of_range = False

            # Find truncations
            if start < timerange.start and end > timerange.start:
                # Truncate the start
                start = timerange.start
            elif start > timerange.end:
                # Metadata time range starts after truncated data ends.
                out_of_range = True
            if end > timerange.end and start < timerange.end:
                # Truncate the end
                end = timerange.end
            elif end < timerange.start:
                # Metadata time range finishes before truncated data starts.
                out_of_range = True

            # Add the values if applicable
            if not out_of_range:
                truncated.append((TimeRange(start, end), metatuple[1], metatuple[2]))

        # Update the original list
        self.metadata = truncated",_3383.py,20,start < timerange.start and end > timerange.start,start < timerange.start < end
https://github.com/sunpy/sunpy/tree/master/sunpy/timeseries/metadata.py,"def _truncate(self, timerange):
        """"""
        Removes metadata entries outside of the new (truncated)
        `sunpy.time.TimeRange`. Also adjusts start and end times of time ranges
        going outside of the truncated time range.

        Parameters
        ----------
        timerange : `sunpy.time.TimeRange`
            The time range to truncate to.
        """"""
        truncated = []
        for metatuple in self.metadata:
            # Get metadata time range parameters
            start = metatuple[0].start
            end = metatuple[0].end
            out_of_range = False

            # Find truncations
            if start < timerange.start and end > timerange.start:
                # Truncate the start
                start = timerange.start
            elif start > timerange.end:
                # Metadata time range starts after truncated data ends.
                out_of_range = True
            if end > timerange.end and start < timerange.end:
                # Truncate the end
                end = timerange.end
            elif end < timerange.start:
                # Metadata time range finishes before truncated data starts.
                out_of_range = True

            # Add the values if applicable
            if not out_of_range:
                truncated.append((TimeRange(start, end), metatuple[1], metatuple[2]))

        # Update the original list
        self.metadata = truncated",_3383.py,26,end > timerange.end and start < timerange.end,end > timerange.end > start
https://github.com/facebookresearch/dlrm/tree/master//dlrm_s_caffe2.py,"if __name__ == ""__main__"":
    ### import packages ###
    import sys
    import argparse

    ### parse arguments ###
    parser = argparse.ArgumentParser(
        description=""Train Deep Learning Recommendation Model (DLRM)""
    )
    # model related parameters
    parser.add_argument(""--arch-sparse-feature-size"", type=int, default=2)
    parser.add_argument(""--arch-embedding-size"", type=str, default=""4-3-2"")
    parser.add_argument(""--arch-mlp-bot"", type=str, default=""4-3-2"")
    parser.add_argument(""--arch-mlp-top"", type=str, default=""4-2-1"")
    parser.add_argument(""--arch-interaction-op"", type=str, default=""dot"")
    parser.add_argument(""--arch-interaction-itself"", action=""store_true"", default=False)
    # activations and loss
    parser.add_argument(""--activation-function"", type=str, default=""relu"")
    parser.add_argument(""--loss-function"", type=str, default=""mse"")   # or bce
    parser.add_argument(""--loss-threshold"", type=float, default=0.0)  # 1.0e-7
    parser.add_argument(""--round-targets"", type=bool, default=False)
    parser.add_argument(""--weighted-pooling"", type=str, default=None)
    # data
    parser.add_argument(""--data-size"", type=int, default=1)
    parser.add_argument(""--num-batches"", type=int, default=0)
    parser.add_argument(""--data-generation"", type=str, default=""random"")  # or synthetic or dataset
    parser.add_argument(""--rand-data-dist"", type=str, default=""uniform"")  # uniform or gaussian
    parser.add_argument(""--rand-data-min"", type=float, default=0)
    parser.add_argument(""--rand-data-max"", type=float, default=1)
    parser.add_argument(""--rand-data-mu"", type=float, default=-1)
    parser.add_argument(""--rand-data-sigma"", type=float, default=1)
    parser.add_argument(""--data-trace-file"", type=str, default=""./input/dist_emb_j.log"")
    parser.add_argument(""--data-set"", type=str, default=""kaggle"")  # or terabyte
    parser.add_argument(""--raw-data-file"", type=str, default="""")
    parser.add_argument(""--processed-data-file"", type=str, default="""")
    parser.add_argument(""--data-randomize"", type=str, default=""total"")  # or day or none
    parser.add_argument(""--data-trace-enable-padding"", type=bool, default=False)
    parser.add_argument(""--max-ind-range"", type=int, default=-1)
    parser.add_argument(""--data-sub-sample-rate"", type=float, default=0.0)  # in [0, 1]
    parser.add_argument(""--num-indices-per-lookup"", type=int, default=10)
    parser.add_argument(""--num-indices-per-lookup-fixed"", type=bool, default=False)
    parser.add_argument(""--num-workers"", type=int, default=0)
    parser.add_argument(""--memory-map"", action=""store_true"", default=False)
    # training
    parser.add_argument(""--mini-batch-size"", type=int, default=1)
    parser.add_argument(""--nepochs"", type=int, default=1)
    parser.add_argument(""--learning-rate"", type=float, default=0.01)
    parser.add_argument(""--print-precision"", type=int, default=5)
    parser.add_argument(""--numpy-rand-seed"", type=int, default=123)
    parser.add_argument(""--sync-dense-params"", type=bool, default=True)
    parser.add_argument(""--caffe2-net-type"", type=str, default="""")
    parser.add_argument(""--optimizer"", type=str, default=""sgd"",
        help=""""""This is the optimizer for embedding tables."""""")
    parser.add_argument(
        ""--dataset-multiprocessing"",
        action=""store_true"",
        default=False,
        help=""The Kaggle dataset can be multiprocessed in an environment \
                        with more than 7 CPU cores and more than 20 GB of memory. \n \
                        The Terabyte dataset can be multiprocessed in an environment \
                        with more than 24 CPU cores and at least 1 TB of memory."",
    )
    # inference
    parser.add_argument(""--inference-only"", action=""store_true"", default=False)
    # onnx (or protobuf with shapes)
    parser.add_argument(""--save-onnx"", action=""store_true"", default=False)
    parser.add_argument(""--save-proto-types-shapes"", action=""store_true"", default=False)
    # gpu
    parser.add_argument(""--use-gpu"", action=""store_true"", default=False)
    # debugging and profiling
    parser.add_argument(""--print-freq"", type=int, default=1)
    parser.add_argument(""--test-freq"", type=int, default=-1)
    parser.add_argument(""--test-mini-batch-size"", type=int, default=-1)
    parser.add_argument(""--test-num-workers"", type=int, default=-1)
    parser.add_argument(""--print-time"", action=""store_true"", default=False)
    parser.add_argument(""--debug-mode"", action=""store_true"", default=False)
    parser.add_argument(""--enable-profiling"", action=""store_true"", default=False)
    parser.add_argument(""--plot-compute-graph"", action=""store_true"", default=False)
    # mlperf logging (disables other output and stops early)
    parser.add_argument(""--mlperf-logging"", action=""store_true"", default=False)
    # stop at target accuracy Kaggle 0.789, Terabyte (sub-sampled=0.875) 0.8107
    parser.add_argument(""--mlperf-acc-threshold"", type=float, default=0.0)
    # stop at target AUC Terabyte (no subsampling) 0.8025
    parser.add_argument(""--mlperf-auc-threshold"", type=float, default=0.0)
    args = parser.parse_args()

    if args.dataset_multiprocessing:
        assert float(sys.version[:3]) > 3.7, ""The dataset_multiprocessing "" + \
        ""flag is susceptible to a bug in Python 3.7 and under. "" + \
        ""https://github.com/facebookresearch/dlrm/issues/172""

    ### some basic setup ###
    # WARNING: to obtain exactly the same initialization for
    # the weights we need to start from the same random seed.
    np.random.seed(args.numpy_rand_seed)

    np.set_printoptions(precision=args.print_precision)
    if (args.test_mini_batch_size < 0):
        # if the parameter is not set, use the training batch size
        args.test_mini_batch_size = args.mini_batch_size
    if (args.test_num_workers < 0):
        # if the parameter is not set, use the same parameter for training
        args.test_num_workers = args.num_workers

    use_gpu = args.use_gpu
    if use_gpu:
        device_opt = core.DeviceOption(workspace.GpuDeviceType, 0)
        ngpus = workspace.NumGpuDevices()  # 1
        print(""Using {} GPU(s)..."".format(ngpus))
    else:
        device_opt = core.DeviceOption(caffe2_pb2.CPU)
        print(""Using CPU..."")

    ### prepare training data ###
    ln_bot = np.fromstring(args.arch_mlp_bot, dtype=int, sep=""-"")
    if args.data_generation == ""dataset"":
        if args.num_workers > 0 or args.test_num_workers > 0:
            print(""WARNING: non default --num-workers or --test-num-workers options""
                    + "" are not supported and will be ignored"")
        if args.mini_batch_size != args.test_mini_batch_size:
            print(""WARNING: non default ----test-mini-batch-size option""
                    + "" is not supported and will be ignored"")

        # input and target from dataset

        train_data, train_ld, test_data, test_ld = \
            dp.make_criteo_data_and_loaders(
                args,
                offset_to_length_converter=True,
            )

        nbatches = args.num_batches if args.num_batches > 0 \
            else len(train_ld)

        nbatches_test = len(test_ld)

        ln_emb = train_data.counts
        m_den = train_data.m_den

        # enforce maximum limit on number of vectors per embedding
        if args.max_ind_range > 0:
            ln_emb = np.array(list(map(
                lambda x: x if x < args.max_ind_range else args.max_ind_range,
                ln_emb
            )))
        ln_bot[0] = m_den

    else:
        if args.num_workers > 0 or args.test_num_workers > 0:
            print(""WARNING: non default --num-workers or --test-num-workers options""
                  + "" are not supported and will be ignored"")
        if args.mini_batch_size != args.test_mini_batch_size:
            print(""WARNING: non default ----test-mini-batch-size option""
                  + "" is not supported and will be ignored"")

        # input and target at random
        ln_emb = np.fromstring(args.arch_embedding_size, dtype=int, sep=""-"")
        m_den = ln_bot[0]
        train_data, train_ld, test_data, test_ld = dp.make_random_data_and_loader(args, ln_emb, m_den, \
            offset_to_length_converter=True,
        )
        nbatches = args.num_batches if args.num_batches > 0 else len(train_ld)
        nbatches_test = len(test_ld)
        # table_feature_map = {idx : idx for idx in range(len(ln_emb))}

    ### parse command line arguments ###
    m_spa = args.arch_sparse_feature_size
    ln_emb = np.asarray(ln_emb)
    num_fea = ln_emb.size + 1  # num sparse + num dense features
    m_den_out = ln_bot[ln_bot.size - 1]
    if args.arch_interaction_op == ""dot"":
        # approach 1: all
        # num_int = num_fea * num_fea + m_den_out
        # approach 2: unique
        if args.arch_interaction_itself:
            num_int = (num_fea * (num_fea + 1)) // 2 + m_den_out
        else:
            num_int = (num_fea * (num_fea - 1)) // 2 + m_den_out
    elif args.arch_interaction_op == ""cat"":
        num_int = num_fea * m_den_out
    else:
        sys.exit(""ERROR: --arch-interaction-op=""
                 + args.arch_interaction_op + "" is not supported"")
    arch_mlp_top_adjusted = str(num_int) + ""-"" + args.arch_mlp_top
    ln_top = np.fromstring(arch_mlp_top_adjusted, dtype=int, sep=""-"")
    # sanity check: feature sizes and mlp dimensions must match
    if m_den != ln_bot[0]:
        sys.exit(""ERROR: arch-dense-feature-size ""
            + str(m_den) + "" does not match first dim of bottom mlp "" + str(ln_bot[0]))
    if m_spa != m_den_out:
        sys.exit(""ERROR: arch-sparse-feature-size ""
            + str(m_spa) + "" does not match last dim of bottom mlp "" + str(m_den_out))
    if num_int != ln_top[0]:
        sys.exit(""ERROR: # of feature interactions ""
            + str(num_int) + "" does not match first dim of top mlp "" + str(ln_top[0]))

    # test prints (model arch)
    if args.debug_mode:
        print(""model arch:"")
        print(""mlp top arch "" + str(ln_top.size - 1)
              + "" layers, with input to output dimensions:"")
        print(ln_top)

        print(""# of interactions"")
        print(num_int)
        print(""mlp bot arch "" + str(ln_bot.size - 1)
              + "" layers, with input to output dimensions:"")
        print(ln_bot)
        print(""# of features (sparse and dense)"")
        print(num_fea)
        print(""dense feature size"")
        print(m_den)
        print(""sparse feature size"")
        print(m_spa)
        print(""# of embeddings (= # of sparse features) "" + str(ln_emb.size)
              + "", with dimensions "" + str(m_spa) + ""x:"")
        print(ln_emb)

        print(""data (inputs and targets):"")
        for j, inputBatch in enumerate(train_ld):
            lX_j, lS_l_j, lS_i_j, lT_j = inputBatch
            print(""mini-batch: %d"" % j)
            print(lX_j)
            print(lS_l_j)
            print(lS_i_j)
            print(lT_j)

    ### construct the neural network specified above ###
    # WARNING: to obtain exactly the same initialization for
    # the weights we need to start from the same random seed.
    # np.random.seed(args.numpy_rand_seed)
    ndevices = min(ngpus, args.mini_batch_size, num_fea - 1) if use_gpu else -1
    flag_types_shapes = args.save_onnx or args.save_proto_types_shapes
    flag_forward_ops = not (use_gpu and ndevices > 1)
    with core.DeviceScope(device_opt):
        dlrm = DLRM_Net(
            m_spa,
            ln_emb,
            ln_bot,
            ln_top,
            args.arch_interaction_op,
            arch_interaction_itself=args.arch_interaction_itself,
            sigmoid_bot=-1,
            sigmoid_top=ln_top.size - 1,
            save_onnx=flag_types_shapes,
            ndevices=ndevices,
            # forward_ops = flag_forward_ops
            enable_prof=args.enable_profiling,
            weighted_pooling=args.weighted_pooling,
            emb_optimizer=args.optimizer
        )
    # load nccl if using multiple devices
    if args.sync_dense_params and ndevices > 1:
        dyndep.InitOpsLibrary(""//caffe2/caffe2/contrib/nccl:nccl_ops"")
    # set the net type for better performance (dag, async_scheduling, etc)
    if args.caffe2_net_type:
        dlrm.parameters().net.Proto().type = args.caffe2_net_type
    # plot compute graph
    if args.plot_compute_graph:
        graph = net_drawer.GetPydotGraph(
            dlrm.parameters().net,
            ""dlrm_s_caffe2_graph"",
            ""BT""
        )
        graph.write_pdf(graph.get_name() + "".pdf"")
    # test prints
    if args.debug_mode:
        print(""initial parameters (weights and bias):"")
        dlrm.print_weights()

    # add training loss if needed
    if not args.inference_only:
        with core.DeviceScope(device_opt):
            # specify the loss function
            nd = 1.0 if dlrm.ndevices <= 1 else 1.0 / dlrm.ndevices  # 1
            if args.loss_function == ""mse"":
                dlrm.MSEloss(scale=nd)
            elif args.loss_function == ""bce"":
                dlrm.BCEloss(scale=nd, threshold=args.loss_threshold)
            else:
                sys.exit(""ERROR: --loss-function="" + args.loss_function
                         + "" is not supported"")

            # define test net (as train net without gradients)
            dlrm.test_net = core.Net(copy.deepcopy(dlrm.model.net.Proto()))

            # specify the optimizer algorithm
            if args.optimizer == ""sgd"":
                dlrm.sgd_optimizer(
                    args.learning_rate, sync_dense_params=args.sync_dense_params
                )
            elif args.optimizer in [""adagrad"", ""rwsadagrad""]:
                dlrm.adagrad_optimizer(
                    args.learning_rate, sync_dense_params=args.sync_dense_params
                )
            else:
                sys.exit(""""""ERROR: Select an optimizer for
                                embedding tables : 'sgd', 'adagrad',
                                or 'rwsadagrad' """""")

    # init/create
    X, lS_l, lS_i, T = next(iter(train_ld)) # does not affect the enumerate(train_ld) in the main loop
    dlrm.create(X, lS_l, lS_i, T.int())

    ### main loop ###
    best_gA_test = 0
    best_auc_test = 0
    total_time = 0
    total_loss = 0
    total_accu = 0
    total_iter = 0
    total_samp = 0
    k = 0

    print(""time/loss/accuracy (if enabled):"")
    while k < args.nepochs:
        j = 0
        for j, inputBatch in enumerate(train_ld):
            # forward and backward pass, where the latter runs only
            # when gradients and loss have been added to the net
            time1 = time.time()
            lX_j, lS_l_j, lS_i_j, lT_j = inputBatch
            lT_j = lT_j.int() if args.loss_function == ""bce"" else lT_j
            dlrm.run(lX_j, lS_l_j, lS_i_j, lT_j)

            time2 = time.time()
            total_time += time2 - time1

            # compte loss and accuracy
            Z = dlrm.get_output()  # numpy array
            T = lT_j.numpy()
            '''
            # debug prints
            print(""output and loss"")
            print(Z)
            print(dlrm.get_loss())
            '''
            mbs = T.shape[0]  # = args.mini_batch_size except maybe for last
            A = np.sum((np.round(Z, 0) == T).astype(np.uint8))
            total_accu += 0 if args.inference_only else A
            total_loss += 0 if args.inference_only else dlrm.get_loss() * mbs
            total_iter += 1
            total_samp += mbs

            # print time, loss and accuracy
            should_print = ((j + 1) % args.print_freq == 0) or (j + 1 == nbatches)
            should_test = (
                (args.test_freq > 0)
                and (args.data_generation in [""dataset"", ""random""])
                and (((j + 1) % args.test_freq == 0) or (j + 1 == nbatches))
            )
            if should_print or should_test:
                gT = 1000. * total_time / total_iter if args.print_time else -1
                total_time = 0

                gA = total_accu / total_samp
                total_accu = 0

                gL = total_loss / total_samp
                total_loss = 0

                str_run_type = ""inference"" if args.inference_only else ""training""
                print(
                    ""Finished {} it {}/{} of epoch {}, {:.2f} ms/it,"".format(
                        str_run_type, j + 1, nbatches, k, gT
                    )
                    + "" loss {:.6f}"".format(gL)
                )
                total_iter = 0
                total_samp = 0
                # debug prints
                # print(Z)
                # print(T)

                # testing
                if should_test and not args.inference_only:
                    # don't measure training iter time in a test iteration
                    if args.mlperf_logging:
                        previous_iteration_time = None

                    test_accu = 0
                    test_loss = 0
                    test_samp = 0

                    if args.mlperf_logging:
                        scores = []
                        targets = []

                    for i, testBatch in enumerate(test_ld):
                        # early exit if nbatches was set by the user and was exceeded
                        if nbatches > 0 and i >= nbatches:
                            break

                        # forward pass

                        lX_test_i, lS_l_test_i, lS_i_test_i, lT_test_i = testBatch
                        lT_test_i = lT_test_i.int() if args.loss_function == ""bce"" else lT_test_i
                        dlrm.run(lX_test_i, lS_l_test_i, lS_i_test_i, lT_test_i, test_net=True)

                        Z_test = dlrm.get_output()
                        T_test = lT_test_i.numpy()

                        if args.mlperf_logging:
                            scores.append(Z_test)
                            targets.append(T_test)
                        else:
                            # compte loss and accuracy
                            L_test = dlrm.get_loss()
                            mbs_test = T_test.shape[0]  # = mini_batch_size except last
                            A_test = np.sum((np.round(Z_test, 0) == T_test).astype(np.uint8))
                            test_accu += A_test
                            test_loss += L_test * mbs_test
                            test_samp += mbs_test

                    # compute metrics (after test loop has finished)
                    if args.mlperf_logging:
                        validation_results = calculate_metrics(targets, scores)
                        gA_test = validation_results['accuracy']
                        gL_test = validation_results['loss']
                    else:
                        gA_test = test_accu / test_samp
                        gL_test = test_loss / test_samp

                    # print metrics
                    is_best = gA_test > best_gA_test
                    if is_best:
                        best_gA_test = gA_test

                    if args.mlperf_logging:
                        is_best = validation_results['roc_auc'] > best_auc_test
                        if is_best:
                            best_auc_test = validation_results['roc_auc']

                        print(
                            ""Testing at - {}/{} of epoch {},"".format(j + 1, nbatches, k)
                            + "" loss {:.6f}, recall {:.4f}, precision {:.4f},"".format(
                                validation_results['loss'],
                                validation_results['recall'],
                                validation_results['precision']
                            )
                            + "" f1 {:.4f}, ap {:.4f},"".format(
                                validation_results['f1'],
                                validation_results['ap'],
                            )
                            + "" auc {:.4f}, best auc {:.4f},"".format(
                                validation_results['roc_auc'],
                                best_auc_test
                            )
                            + "" accuracy {:3.3f} %, best accuracy {:3.3f} %"".format(
                                validation_results['accuracy'] * 100,
                                best_gA_test * 100
                            )
                        )
                    else:
                        print(
                            ""Testing at - {}/{} of epoch {},"".format(j + 1, nbatches, 0)
                            + "" loss {:.6f}, accuracy {:3.3f} %, best {:3.3f} %"".format(
                                gL_test, gA_test * 100, best_gA_test * 100
                            )
                        )

                    # check thresholds
                    if (args.mlperf_logging
                        and (args.mlperf_acc_threshold > 0)
                        and (best_gA_test > args.mlperf_acc_threshold)):
                        print(""MLPerf testing accuracy threshold ""
                              + str(args.mlperf_acc_threshold)
                              + "" reached, stop training"")
                        break

                    if (args.mlperf_logging
                        and (args.mlperf_auc_threshold > 0)
                        and (best_auc_test > args.mlperf_auc_threshold)):
                        print(""MLPerf testing auc threshold ""
                              + str(args.mlperf_auc_threshold)
                              + "" reached, stop training"")
                        break

            j += 1  # nbatches
        k += 1  # nepochs

    # test prints
    if not args.inference_only and args.debug_mode:
        print(""updated parameters (weights and bias):"")
        dlrm.print_weights()

    # build onnx model from caffe2
    if args.save_onnx:
        pnet = dlrm.parameters().net.Proto()
        inet = dlrm.parameters().param_init_net.Proto()
        value_info = dlrm.onnx_tsd  # None
        # debug prints
        # print(value_info)

        # WARNING: Why Caffe2 to ONNX net transformation currently does not work?
        # 1. ONNX does not support SparseLengthsSum operator directly. A workaround
        # could be for the Caffe2 ONNX frontend to indirectly map this operator to
        # Gather and ReducedSum ONNX operators, following the PyTorch approach.
        c2f = caffe2.python.onnx.frontend.Caffe2Frontend()
        dlrm_caffe2_onnx = c2f.caffe2_net_to_onnx_model(pnet, inet, value_info)
        # check the onnx model
        onnx.checker.check_model(dlrm_caffe2_onnx)

        # save model to a file
        with open(""dlrm_s_caffe2.onnx"", ""w+"") as dlrm_caffe2_onnx_file:
            dlrm_caffe2_onnx_file.write(str(dlrm_caffe2_onnx))

    # build protobuf with types and shapes
    if args.save_proto_types_shapes:
        # add types and shapes to protobuf
        __TYPE_MAPPING = {
            onnx.TensorProto.FLOAT: caffe2_pb2.TensorProto.FLOAT,
            onnx.TensorProto.UINT8: caffe2_pb2.TensorProto.UINT8,
            onnx.TensorProto.INT8: caffe2_pb2.TensorProto.INT8,
            onnx.TensorProto.UINT16: caffe2_pb2.TensorProto.UINT16,
            onnx.TensorProto.INT16: caffe2_pb2.TensorProto.INT16,
            onnx.TensorProto.INT32: caffe2_pb2.TensorProto.INT32,
            onnx.TensorProto.INT64: caffe2_pb2.TensorProto.INT64,
            onnx.TensorProto.STRING: caffe2_pb2.TensorProto.STRING,
            onnx.TensorProto.BOOL: caffe2_pb2.TensorProto.BOOL,
            onnx.TensorProto.FLOAT16: caffe2_pb2.TensorProto.FLOAT16,
            onnx.TensorProto.DOUBLE: caffe2_pb2.TensorProto.DOUBLE,
        }

        pnet = dlrm.parameters().net.Proto()
        arg = pnet.arg.add()
        arg.name = ""input_shape_info""
        for i in pnet.external_input:
            if i in dlrm.onnx_tsd:
                onnx_dtype, shape = dlrm.onnx_tsd[i]
                t = arg.tensors.add()
                t.name = i
                t.data_type = __TYPE_MAPPING[onnx_dtype]
                t.dims.extend(shape)
            else:
                print(""Warning: we don't have shape/type info for input: {}"".format(i))
        # debug print
        # print(pnet)

        # export the protobuf with types and shapes
        with open(""dlrm_s_caffe2.proto"", ""w+"") as dlrm_s_proto_file:
            dlrm_s_proto_file.write(str(pnet))

        """"""
        # export the protobuf with types and shapes as well as weights
        # see https://github.com/pytorch/pytorch/issues/9533
        #save
        net = dlrm.parameters().net
        params = dlrm.parameters().params
        init_net, predict_net = mobile_exporter.Export(workspace, net, params)
        with open(""dlrm_s_caffe2.predict"", ""wb"") as dlrm_s_predict_file:
            dlrm_s_predict_file.write(predict_net.SerializeToString())
        with open(""dlrm_s_caffe2.init"", ""wb"") as dlrm_s_init_file:
            dlrm_s_init_file.write(init_net.SerializeToString())
        #load
        net_def = caffe2_pb2.NetDef()
        init_def= caffe2_pb2.NetDef()
        with open(""dlrm_s_caffe2.predict"", ""rb"") as dlrm_s_predict_file:
            net_def.ParseFromString(dlrm_s_predict_file.read())
            print(net_def)
        with open(""dlrm_s_caffe2.init"", ""rb"") as dlrm_s_init_file:
            init_def.ParseFromString(dlrm_s_init_file.read())
            print(init_def)
        """"""",_3459.py,471,args.mlperf_logging and args.mlperf_auc_threshold > 0 and (best_auc_test > args.mlperf_auc_threshold),best_auc_test > args.mlperf_auc_threshold > 0 and args.mlperf_logging
https://github.com/facebookresearch/dlrm/tree/master//dlrm_s_caffe2.py,"if __name__ == ""__main__"":
    ### import packages ###
    import sys
    import argparse

    ### parse arguments ###
    parser = argparse.ArgumentParser(
        description=""Train Deep Learning Recommendation Model (DLRM)""
    )
    # model related parameters
    parser.add_argument(""--arch-sparse-feature-size"", type=int, default=2)
    parser.add_argument(""--arch-embedding-size"", type=str, default=""4-3-2"")
    parser.add_argument(""--arch-mlp-bot"", type=str, default=""4-3-2"")
    parser.add_argument(""--arch-mlp-top"", type=str, default=""4-2-1"")
    parser.add_argument(""--arch-interaction-op"", type=str, default=""dot"")
    parser.add_argument(""--arch-interaction-itself"", action=""store_true"", default=False)
    # activations and loss
    parser.add_argument(""--activation-function"", type=str, default=""relu"")
    parser.add_argument(""--loss-function"", type=str, default=""mse"")   # or bce
    parser.add_argument(""--loss-threshold"", type=float, default=0.0)  # 1.0e-7
    parser.add_argument(""--round-targets"", type=bool, default=False)
    parser.add_argument(""--weighted-pooling"", type=str, default=None)
    # data
    parser.add_argument(""--data-size"", type=int, default=1)
    parser.add_argument(""--num-batches"", type=int, default=0)
    parser.add_argument(""--data-generation"", type=str, default=""random"")  # or synthetic or dataset
    parser.add_argument(""--rand-data-dist"", type=str, default=""uniform"")  # uniform or gaussian
    parser.add_argument(""--rand-data-min"", type=float, default=0)
    parser.add_argument(""--rand-data-max"", type=float, default=1)
    parser.add_argument(""--rand-data-mu"", type=float, default=-1)
    parser.add_argument(""--rand-data-sigma"", type=float, default=1)
    parser.add_argument(""--data-trace-file"", type=str, default=""./input/dist_emb_j.log"")
    parser.add_argument(""--data-set"", type=str, default=""kaggle"")  # or terabyte
    parser.add_argument(""--raw-data-file"", type=str, default="""")
    parser.add_argument(""--processed-data-file"", type=str, default="""")
    parser.add_argument(""--data-randomize"", type=str, default=""total"")  # or day or none
    parser.add_argument(""--data-trace-enable-padding"", type=bool, default=False)
    parser.add_argument(""--max-ind-range"", type=int, default=-1)
    parser.add_argument(""--data-sub-sample-rate"", type=float, default=0.0)  # in [0, 1]
    parser.add_argument(""--num-indices-per-lookup"", type=int, default=10)
    parser.add_argument(""--num-indices-per-lookup-fixed"", type=bool, default=False)
    parser.add_argument(""--num-workers"", type=int, default=0)
    parser.add_argument(""--memory-map"", action=""store_true"", default=False)
    # training
    parser.add_argument(""--mini-batch-size"", type=int, default=1)
    parser.add_argument(""--nepochs"", type=int, default=1)
    parser.add_argument(""--learning-rate"", type=float, default=0.01)
    parser.add_argument(""--print-precision"", type=int, default=5)
    parser.add_argument(""--numpy-rand-seed"", type=int, default=123)
    parser.add_argument(""--sync-dense-params"", type=bool, default=True)
    parser.add_argument(""--caffe2-net-type"", type=str, default="""")
    parser.add_argument(""--optimizer"", type=str, default=""sgd"",
        help=""""""This is the optimizer for embedding tables."""""")
    parser.add_argument(
        ""--dataset-multiprocessing"",
        action=""store_true"",
        default=False,
        help=""The Kaggle dataset can be multiprocessed in an environment \
                        with more than 7 CPU cores and more than 20 GB of memory. \n \
                        The Terabyte dataset can be multiprocessed in an environment \
                        with more than 24 CPU cores and at least 1 TB of memory."",
    )
    # inference
    parser.add_argument(""--inference-only"", action=""store_true"", default=False)
    # onnx (or protobuf with shapes)
    parser.add_argument(""--save-onnx"", action=""store_true"", default=False)
    parser.add_argument(""--save-proto-types-shapes"", action=""store_true"", default=False)
    # gpu
    parser.add_argument(""--use-gpu"", action=""store_true"", default=False)
    # debugging and profiling
    parser.add_argument(""--print-freq"", type=int, default=1)
    parser.add_argument(""--test-freq"", type=int, default=-1)
    parser.add_argument(""--test-mini-batch-size"", type=int, default=-1)
    parser.add_argument(""--test-num-workers"", type=int, default=-1)
    parser.add_argument(""--print-time"", action=""store_true"", default=False)
    parser.add_argument(""--debug-mode"", action=""store_true"", default=False)
    parser.add_argument(""--enable-profiling"", action=""store_true"", default=False)
    parser.add_argument(""--plot-compute-graph"", action=""store_true"", default=False)
    # mlperf logging (disables other output and stops early)
    parser.add_argument(""--mlperf-logging"", action=""store_true"", default=False)
    # stop at target accuracy Kaggle 0.789, Terabyte (sub-sampled=0.875) 0.8107
    parser.add_argument(""--mlperf-acc-threshold"", type=float, default=0.0)
    # stop at target AUC Terabyte (no subsampling) 0.8025
    parser.add_argument(""--mlperf-auc-threshold"", type=float, default=0.0)
    args = parser.parse_args()

    if args.dataset_multiprocessing:
        assert float(sys.version[:3]) > 3.7, ""The dataset_multiprocessing "" + \
        ""flag is susceptible to a bug in Python 3.7 and under. "" + \
        ""https://github.com/facebookresearch/dlrm/issues/172""

    ### some basic setup ###
    # WARNING: to obtain exactly the same initialization for
    # the weights we need to start from the same random seed.
    np.random.seed(args.numpy_rand_seed)

    np.set_printoptions(precision=args.print_precision)
    if (args.test_mini_batch_size < 0):
        # if the parameter is not set, use the training batch size
        args.test_mini_batch_size = args.mini_batch_size
    if (args.test_num_workers < 0):
        # if the parameter is not set, use the same parameter for training
        args.test_num_workers = args.num_workers

    use_gpu = args.use_gpu
    if use_gpu:
        device_opt = core.DeviceOption(workspace.GpuDeviceType, 0)
        ngpus = workspace.NumGpuDevices()  # 1
        print(""Using {} GPU(s)..."".format(ngpus))
    else:
        device_opt = core.DeviceOption(caffe2_pb2.CPU)
        print(""Using CPU..."")

    ### prepare training data ###
    ln_bot = np.fromstring(args.arch_mlp_bot, dtype=int, sep=""-"")
    if args.data_generation == ""dataset"":
        if args.num_workers > 0 or args.test_num_workers > 0:
            print(""WARNING: non default --num-workers or --test-num-workers options""
                    + "" are not supported and will be ignored"")
        if args.mini_batch_size != args.test_mini_batch_size:
            print(""WARNING: non default ----test-mini-batch-size option""
                    + "" is not supported and will be ignored"")

        # input and target from dataset

        train_data, train_ld, test_data, test_ld = \
            dp.make_criteo_data_and_loaders(
                args,
                offset_to_length_converter=True,
            )

        nbatches = args.num_batches if args.num_batches > 0 \
            else len(train_ld)

        nbatches_test = len(test_ld)

        ln_emb = train_data.counts
        m_den = train_data.m_den

        # enforce maximum limit on number of vectors per embedding
        if args.max_ind_range > 0:
            ln_emb = np.array(list(map(
                lambda x: x if x < args.max_ind_range else args.max_ind_range,
                ln_emb
            )))
        ln_bot[0] = m_den

    else:
        if args.num_workers > 0 or args.test_num_workers > 0:
            print(""WARNING: non default --num-workers or --test-num-workers options""
                  + "" are not supported and will be ignored"")
        if args.mini_batch_size != args.test_mini_batch_size:
            print(""WARNING: non default ----test-mini-batch-size option""
                  + "" is not supported and will be ignored"")

        # input and target at random
        ln_emb = np.fromstring(args.arch_embedding_size, dtype=int, sep=""-"")
        m_den = ln_bot[0]
        train_data, train_ld, test_data, test_ld = dp.make_random_data_and_loader(args, ln_emb, m_den, \
            offset_to_length_converter=True,
        )
        nbatches = args.num_batches if args.num_batches > 0 else len(train_ld)
        nbatches_test = len(test_ld)
        # table_feature_map = {idx : idx for idx in range(len(ln_emb))}

    ### parse command line arguments ###
    m_spa = args.arch_sparse_feature_size
    ln_emb = np.asarray(ln_emb)
    num_fea = ln_emb.size + 1  # num sparse + num dense features
    m_den_out = ln_bot[ln_bot.size - 1]
    if args.arch_interaction_op == ""dot"":
        # approach 1: all
        # num_int = num_fea * num_fea + m_den_out
        # approach 2: unique
        if args.arch_interaction_itself:
            num_int = (num_fea * (num_fea + 1)) // 2 + m_den_out
        else:
            num_int = (num_fea * (num_fea - 1)) // 2 + m_den_out
    elif args.arch_interaction_op == ""cat"":
        num_int = num_fea * m_den_out
    else:
        sys.exit(""ERROR: --arch-interaction-op=""
                 + args.arch_interaction_op + "" is not supported"")
    arch_mlp_top_adjusted = str(num_int) + ""-"" + args.arch_mlp_top
    ln_top = np.fromstring(arch_mlp_top_adjusted, dtype=int, sep=""-"")
    # sanity check: feature sizes and mlp dimensions must match
    if m_den != ln_bot[0]:
        sys.exit(""ERROR: arch-dense-feature-size ""
            + str(m_den) + "" does not match first dim of bottom mlp "" + str(ln_bot[0]))
    if m_spa != m_den_out:
        sys.exit(""ERROR: arch-sparse-feature-size ""
            + str(m_spa) + "" does not match last dim of bottom mlp "" + str(m_den_out))
    if num_int != ln_top[0]:
        sys.exit(""ERROR: # of feature interactions ""
            + str(num_int) + "" does not match first dim of top mlp "" + str(ln_top[0]))

    # test prints (model arch)
    if args.debug_mode:
        print(""model arch:"")
        print(""mlp top arch "" + str(ln_top.size - 1)
              + "" layers, with input to output dimensions:"")
        print(ln_top)

        print(""# of interactions"")
        print(num_int)
        print(""mlp bot arch "" + str(ln_bot.size - 1)
              + "" layers, with input to output dimensions:"")
        print(ln_bot)
        print(""# of features (sparse and dense)"")
        print(num_fea)
        print(""dense feature size"")
        print(m_den)
        print(""sparse feature size"")
        print(m_spa)
        print(""# of embeddings (= # of sparse features) "" + str(ln_emb.size)
              + "", with dimensions "" + str(m_spa) + ""x:"")
        print(ln_emb)

        print(""data (inputs and targets):"")
        for j, inputBatch in enumerate(train_ld):
            lX_j, lS_l_j, lS_i_j, lT_j = inputBatch
            print(""mini-batch: %d"" % j)
            print(lX_j)
            print(lS_l_j)
            print(lS_i_j)
            print(lT_j)

    ### construct the neural network specified above ###
    # WARNING: to obtain exactly the same initialization for
    # the weights we need to start from the same random seed.
    # np.random.seed(args.numpy_rand_seed)
    ndevices = min(ngpus, args.mini_batch_size, num_fea - 1) if use_gpu else -1
    flag_types_shapes = args.save_onnx or args.save_proto_types_shapes
    flag_forward_ops = not (use_gpu and ndevices > 1)
    with core.DeviceScope(device_opt):
        dlrm = DLRM_Net(
            m_spa,
            ln_emb,
            ln_bot,
            ln_top,
            args.arch_interaction_op,
            arch_interaction_itself=args.arch_interaction_itself,
            sigmoid_bot=-1,
            sigmoid_top=ln_top.size - 1,
            save_onnx=flag_types_shapes,
            ndevices=ndevices,
            # forward_ops = flag_forward_ops
            enable_prof=args.enable_profiling,
            weighted_pooling=args.weighted_pooling,
            emb_optimizer=args.optimizer
        )
    # load nccl if using multiple devices
    if args.sync_dense_params and ndevices > 1:
        dyndep.InitOpsLibrary(""//caffe2/caffe2/contrib/nccl:nccl_ops"")
    # set the net type for better performance (dag, async_scheduling, etc)
    if args.caffe2_net_type:
        dlrm.parameters().net.Proto().type = args.caffe2_net_type
    # plot compute graph
    if args.plot_compute_graph:
        graph = net_drawer.GetPydotGraph(
            dlrm.parameters().net,
            ""dlrm_s_caffe2_graph"",
            ""BT""
        )
        graph.write_pdf(graph.get_name() + "".pdf"")
    # test prints
    if args.debug_mode:
        print(""initial parameters (weights and bias):"")
        dlrm.print_weights()

    # add training loss if needed
    if not args.inference_only:
        with core.DeviceScope(device_opt):
            # specify the loss function
            nd = 1.0 if dlrm.ndevices <= 1 else 1.0 / dlrm.ndevices  # 1
            if args.loss_function == ""mse"":
                dlrm.MSEloss(scale=nd)
            elif args.loss_function == ""bce"":
                dlrm.BCEloss(scale=nd, threshold=args.loss_threshold)
            else:
                sys.exit(""ERROR: --loss-function="" + args.loss_function
                         + "" is not supported"")

            # define test net (as train net without gradients)
            dlrm.test_net = core.Net(copy.deepcopy(dlrm.model.net.Proto()))

            # specify the optimizer algorithm
            if args.optimizer == ""sgd"":
                dlrm.sgd_optimizer(
                    args.learning_rate, sync_dense_params=args.sync_dense_params
                )
            elif args.optimizer in [""adagrad"", ""rwsadagrad""]:
                dlrm.adagrad_optimizer(
                    args.learning_rate, sync_dense_params=args.sync_dense_params
                )
            else:
                sys.exit(""""""ERROR: Select an optimizer for
                                embedding tables : 'sgd', 'adagrad',
                                or 'rwsadagrad' """""")

    # init/create
    X, lS_l, lS_i, T = next(iter(train_ld)) # does not affect the enumerate(train_ld) in the main loop
    dlrm.create(X, lS_l, lS_i, T.int())

    ### main loop ###
    best_gA_test = 0
    best_auc_test = 0
    total_time = 0
    total_loss = 0
    total_accu = 0
    total_iter = 0
    total_samp = 0
    k = 0

    print(""time/loss/accuracy (if enabled):"")
    while k < args.nepochs:
        j = 0
        for j, inputBatch in enumerate(train_ld):
            # forward and backward pass, where the latter runs only
            # when gradients and loss have been added to the net
            time1 = time.time()
            lX_j, lS_l_j, lS_i_j, lT_j = inputBatch
            lT_j = lT_j.int() if args.loss_function == ""bce"" else lT_j
            dlrm.run(lX_j, lS_l_j, lS_i_j, lT_j)

            time2 = time.time()
            total_time += time2 - time1

            # compte loss and accuracy
            Z = dlrm.get_output()  # numpy array
            T = lT_j.numpy()
            '''
            # debug prints
            print(""output and loss"")
            print(Z)
            print(dlrm.get_loss())
            '''
            mbs = T.shape[0]  # = args.mini_batch_size except maybe for last
            A = np.sum((np.round(Z, 0) == T).astype(np.uint8))
            total_accu += 0 if args.inference_only else A
            total_loss += 0 if args.inference_only else dlrm.get_loss() * mbs
            total_iter += 1
            total_samp += mbs

            # print time, loss and accuracy
            should_print = ((j + 1) % args.print_freq == 0) or (j + 1 == nbatches)
            should_test = (
                (args.test_freq > 0)
                and (args.data_generation in [""dataset"", ""random""])
                and (((j + 1) % args.test_freq == 0) or (j + 1 == nbatches))
            )
            if should_print or should_test:
                gT = 1000. * total_time / total_iter if args.print_time else -1
                total_time = 0

                gA = total_accu / total_samp
                total_accu = 0

                gL = total_loss / total_samp
                total_loss = 0

                str_run_type = ""inference"" if args.inference_only else ""training""
                print(
                    ""Finished {} it {}/{} of epoch {}, {:.2f} ms/it,"".format(
                        str_run_type, j + 1, nbatches, k, gT
                    )
                    + "" loss {:.6f}"".format(gL)
                )
                total_iter = 0
                total_samp = 0
                # debug prints
                # print(Z)
                # print(T)

                # testing
                if should_test and not args.inference_only:
                    # don't measure training iter time in a test iteration
                    if args.mlperf_logging:
                        previous_iteration_time = None

                    test_accu = 0
                    test_loss = 0
                    test_samp = 0

                    if args.mlperf_logging:
                        scores = []
                        targets = []

                    for i, testBatch in enumerate(test_ld):
                        # early exit if nbatches was set by the user and was exceeded
                        if nbatches > 0 and i >= nbatches:
                            break

                        # forward pass

                        lX_test_i, lS_l_test_i, lS_i_test_i, lT_test_i = testBatch
                        lT_test_i = lT_test_i.int() if args.loss_function == ""bce"" else lT_test_i
                        dlrm.run(lX_test_i, lS_l_test_i, lS_i_test_i, lT_test_i, test_net=True)

                        Z_test = dlrm.get_output()
                        T_test = lT_test_i.numpy()

                        if args.mlperf_logging:
                            scores.append(Z_test)
                            targets.append(T_test)
                        else:
                            # compte loss and accuracy
                            L_test = dlrm.get_loss()
                            mbs_test = T_test.shape[0]  # = mini_batch_size except last
                            A_test = np.sum((np.round(Z_test, 0) == T_test).astype(np.uint8))
                            test_accu += A_test
                            test_loss += L_test * mbs_test
                            test_samp += mbs_test

                    # compute metrics (after test loop has finished)
                    if args.mlperf_logging:
                        validation_results = calculate_metrics(targets, scores)
                        gA_test = validation_results['accuracy']
                        gL_test = validation_results['loss']
                    else:
                        gA_test = test_accu / test_samp
                        gL_test = test_loss / test_samp

                    # print metrics
                    is_best = gA_test > best_gA_test
                    if is_best:
                        best_gA_test = gA_test

                    if args.mlperf_logging:
                        is_best = validation_results['roc_auc'] > best_auc_test
                        if is_best:
                            best_auc_test = validation_results['roc_auc']

                        print(
                            ""Testing at - {}/{} of epoch {},"".format(j + 1, nbatches, k)
                            + "" loss {:.6f}, recall {:.4f}, precision {:.4f},"".format(
                                validation_results['loss'],
                                validation_results['recall'],
                                validation_results['precision']
                            )
                            + "" f1 {:.4f}, ap {:.4f},"".format(
                                validation_results['f1'],
                                validation_results['ap'],
                            )
                            + "" auc {:.4f}, best auc {:.4f},"".format(
                                validation_results['roc_auc'],
                                best_auc_test
                            )
                            + "" accuracy {:3.3f} %, best accuracy {:3.3f} %"".format(
                                validation_results['accuracy'] * 100,
                                best_gA_test * 100
                            )
                        )
                    else:
                        print(
                            ""Testing at - {}/{} of epoch {},"".format(j + 1, nbatches, 0)
                            + "" loss {:.6f}, accuracy {:3.3f} %, best {:3.3f} %"".format(
                                gL_test, gA_test * 100, best_gA_test * 100
                            )
                        )

                    # check thresholds
                    if (args.mlperf_logging
                        and (args.mlperf_acc_threshold > 0)
                        and (best_gA_test > args.mlperf_acc_threshold)):
                        print(""MLPerf testing accuracy threshold ""
                              + str(args.mlperf_acc_threshold)
                              + "" reached, stop training"")
                        break

                    if (args.mlperf_logging
                        and (args.mlperf_auc_threshold > 0)
                        and (best_auc_test > args.mlperf_auc_threshold)):
                        print(""MLPerf testing auc threshold ""
                              + str(args.mlperf_auc_threshold)
                              + "" reached, stop training"")
                        break

            j += 1  # nbatches
        k += 1  # nepochs

    # test prints
    if not args.inference_only and args.debug_mode:
        print(""updated parameters (weights and bias):"")
        dlrm.print_weights()

    # build onnx model from caffe2
    if args.save_onnx:
        pnet = dlrm.parameters().net.Proto()
        inet = dlrm.parameters().param_init_net.Proto()
        value_info = dlrm.onnx_tsd  # None
        # debug prints
        # print(value_info)

        # WARNING: Why Caffe2 to ONNX net transformation currently does not work?
        # 1. ONNX does not support SparseLengthsSum operator directly. A workaround
        # could be for the Caffe2 ONNX frontend to indirectly map this operator to
        # Gather and ReducedSum ONNX operators, following the PyTorch approach.
        c2f = caffe2.python.onnx.frontend.Caffe2Frontend()
        dlrm_caffe2_onnx = c2f.caffe2_net_to_onnx_model(pnet, inet, value_info)
        # check the onnx model
        onnx.checker.check_model(dlrm_caffe2_onnx)

        # save model to a file
        with open(""dlrm_s_caffe2.onnx"", ""w+"") as dlrm_caffe2_onnx_file:
            dlrm_caffe2_onnx_file.write(str(dlrm_caffe2_onnx))

    # build protobuf with types and shapes
    if args.save_proto_types_shapes:
        # add types and shapes to protobuf
        __TYPE_MAPPING = {
            onnx.TensorProto.FLOAT: caffe2_pb2.TensorProto.FLOAT,
            onnx.TensorProto.UINT8: caffe2_pb2.TensorProto.UINT8,
            onnx.TensorProto.INT8: caffe2_pb2.TensorProto.INT8,
            onnx.TensorProto.UINT16: caffe2_pb2.TensorProto.UINT16,
            onnx.TensorProto.INT16: caffe2_pb2.TensorProto.INT16,
            onnx.TensorProto.INT32: caffe2_pb2.TensorProto.INT32,
            onnx.TensorProto.INT64: caffe2_pb2.TensorProto.INT64,
            onnx.TensorProto.STRING: caffe2_pb2.TensorProto.STRING,
            onnx.TensorProto.BOOL: caffe2_pb2.TensorProto.BOOL,
            onnx.TensorProto.FLOAT16: caffe2_pb2.TensorProto.FLOAT16,
            onnx.TensorProto.DOUBLE: caffe2_pb2.TensorProto.DOUBLE,
        }

        pnet = dlrm.parameters().net.Proto()
        arg = pnet.arg.add()
        arg.name = ""input_shape_info""
        for i in pnet.external_input:
            if i in dlrm.onnx_tsd:
                onnx_dtype, shape = dlrm.onnx_tsd[i]
                t = arg.tensors.add()
                t.name = i
                t.data_type = __TYPE_MAPPING[onnx_dtype]
                t.dims.extend(shape)
            else:
                print(""Warning: we don't have shape/type info for input: {}"".format(i))
        # debug print
        # print(pnet)

        # export the protobuf with types and shapes
        with open(""dlrm_s_caffe2.proto"", ""w+"") as dlrm_s_proto_file:
            dlrm_s_proto_file.write(str(pnet))

        """"""
        # export the protobuf with types and shapes as well as weights
        # see https://github.com/pytorch/pytorch/issues/9533
        #save
        net = dlrm.parameters().net
        params = dlrm.parameters().params
        init_net, predict_net = mobile_exporter.Export(workspace, net, params)
        with open(""dlrm_s_caffe2.predict"", ""wb"") as dlrm_s_predict_file:
            dlrm_s_predict_file.write(predict_net.SerializeToString())
        with open(""dlrm_s_caffe2.init"", ""wb"") as dlrm_s_init_file:
            dlrm_s_init_file.write(init_net.SerializeToString())
        #load
        net_def = caffe2_pb2.NetDef()
        init_def= caffe2_pb2.NetDef()
        with open(""dlrm_s_caffe2.predict"", ""rb"") as dlrm_s_predict_file:
            net_def.ParseFromString(dlrm_s_predict_file.read())
            print(net_def)
        with open(""dlrm_s_caffe2.init"", ""rb"") as dlrm_s_init_file:
            init_def.ParseFromString(dlrm_s_init_file.read())
            print(init_def)
        """"""",_3459.py,391,nbatches > 0 and i >= nbatches,i >= nbatches > 0
https://github.com/saltstack/salt/tree/master/salt/modules/state.py,"def top(topfn, test=None, queue=False, **kwargs):
    """"""
    Execute a specific top file instead of the default. This is useful to apply
    configurations from a different environment (for example, dev or prod), without
    modifying the default top file.

    queue : False
        Instead of failing immediately when another state run is in progress,
        queue the new state run to begin running once the other has finished.

        This option starts a new thread for each queued state run, so use this
        option sparingly.

    saltenv
        Specify a salt fileserver environment to be used when applying states

    pillarenv
        Specify a Pillar environment to be used when applying states. This
        can also be set in the minion config file using the
        :conf_minion:`pillarenv` option. When neither the
        :conf_minion:`pillarenv` minion config option nor this CLI argument is
        used, all Pillar environments will be merged together.

        .. versionadded:: 2017.7.0

    CLI Example:

    .. code-block:: bash

        salt '*' state.top reverse_top.sls
        salt '*' state.top prod_top.sls exclude=sls_to_exclude
        salt '*' state.top dev_top.sls exclude=""[{'id': 'id_to_exclude'}, {'sls': 'sls_to_exclude'}]""
    """"""
    conflict = _check_queue(queue, kwargs)
    if conflict is not None:
        return conflict
    orig_test = __opts__.get(""test"", None)
    opts = salt.utils.state.get_sls_opts(__opts__, **kwargs)
    opts[""test""] = _get_test_value(test, **kwargs)

    pillar_override = kwargs.get(""pillar"")
    pillar_enc = kwargs.get(""pillar_enc"")
    if (
        pillar_enc is None
        and pillar_override is not None
        and not isinstance(pillar_override, dict)
    ):
        raise SaltInvocationError(
            ""Pillar data must be formatted as a dictionary, unless pillar_enc ""
            ""is specified.""
        )
    try:
        st_ = salt.state.HighState(
            opts,
            pillar_override,
            pillar_enc=pillar_enc,
            context=dict(__context__),
            proxy=dict(__proxy__),
            initial_pillar=_get_initial_pillar(opts),
        )
    except NameError:
        st_ = salt.state.HighState(
            opts,
            pillar_override,
            pillar_enc=pillar_enc,
            context=dict(__context__),
            initial_pillar=_get_initial_pillar(opts),
        )

    with st_:
        errors = _get_pillar_errors(kwargs, pillar=st_.opts[""pillar""])
        if errors:
            __context__[""retcode""] = salt.defaults.exitcodes.EX_PILLAR_FAILURE
            return [""Pillar failed to render with the following messages:""] + errors

        st_.push_active()
        st_.opts[""state_top""] = salt.utils.url.create(topfn)
        ret = {}
        orchestration_jid = kwargs.get(""orchestration_jid"")
        if ""saltenv"" in kwargs:
            st_.opts[""state_top_saltenv""] = kwargs[""saltenv""]
        try:
            snapper_pre = _snapper_pre(opts, kwargs.get(""__pub_jid"", ""called localy""))
            ret = st_.call_highstate(
                exclude=kwargs.get(""exclude"", []),
                cache=kwargs.get(""cache"", None),
                cache_name=kwargs.get(""cache_name"", ""highstate""),
                orchestration_jid=orchestration_jid,
            )
        finally:
            st_.pop_active()

        _set_retcode(ret, highstate=st_.building_highstate)
        # Work around Windows multiprocessing bug, set __opts__['test'] back to
        # value from before this function was run.
        _snapper_post(opts, kwargs.get(""__pub_jid"", ""called localy""), snapper_pre)
        __opts__[""test""] = orig_test
        return ret",_3730.py,44,"pillar_enc is None and pillar_override is not None and (not isinstance(pillar_override, dict))","pillar_enc is None is not pillar_override and (not isinstance(pillar_override, dict))"
https://github.com/ansible/ansible/tree/master/lib/ansible/plugins/lookup/sequence.py,"def sanity_check(self):
        if self.count is None and self.end is None:
            raise AnsibleError(""must specify count or end in with_sequence"")
        elif self.count is not None and self.end is not None:
            raise AnsibleError(""can't specify both count and end in with_sequence"")
        elif self.count is not None:
            # convert count to end
            if self.count != 0:
                self.end = self.start + self.count * self.stride - 1
            else:
                self.start = 0
                self.end = 0
                self.stride = 0
            del self.count
        if self.stride > 0 and self.end < self.start:
            raise AnsibleError(""to count backwards make stride negative"")
        if self.stride < 0 and self.end > self.start:
            raise AnsibleError(""to count forward don't make stride negative"")
        if self.format.count('%') != 1:
            raise AnsibleError(""bad formatting string: %s"" % self.format)",_3761.py,2,self.count is None and self.end is None,self.count is None is self.end
https://github.com/ansible/ansible/tree/master/lib/ansible/plugins/lookup/sequence.py,"def sanity_check(self):
        if self.count is None and self.end is None:
            raise AnsibleError(""must specify count or end in with_sequence"")
        elif self.count is not None and self.end is not None:
            raise AnsibleError(""can't specify both count and end in with_sequence"")
        elif self.count is not None:
            # convert count to end
            if self.count != 0:
                self.end = self.start + self.count * self.stride - 1
            else:
                self.start = 0
                self.end = 0
                self.stride = 0
            del self.count
        if self.stride > 0 and self.end < self.start:
            raise AnsibleError(""to count backwards make stride negative"")
        if self.stride < 0 and self.end > self.start:
            raise AnsibleError(""to count forward don't make stride negative"")
        if self.format.count('%') != 1:
            raise AnsibleError(""bad formatting string: %s"" % self.format)",_3761.py,4,self.count is not None and self.end is not None,self.count is not None is not self.end
https://github.com/NTMC-Community/MatchZoo-py/tree/master/matchzoo/utils/get_file.py,"def update(self, current):
        """"""Updates the progress bar.""""""
        self._seen_so_far = current

        now = time.time()
        info = ' - {0:.0f}s'.format(now - self._start)
        if self.verbose == 1:
            if (now - self._last_update < self.interval and self.target is not
               None and current < self.target):
                return

            prev_total_width = self._total_width
            if self._dynamic_display:
                sys.stdout.write('\b' * prev_total_width)
                sys.stdout.write('\r')
            else:
                sys.stdout.write('\n')

            if self.target is not None:
                numdigits = int(np.floor(np.log10(self.target))) + 1
                bar = '{2:{0:d}d}/{1} ['.format(
                    numdigits, self.target, current)
                prog = float(current) / self.target
                prog_width = int(self.width * prog)
                if prog_width > 0:
                    bar += ('=' * (prog_width - 1))
                    if current < self.target:
                        bar += '>'
                    else:
                        bar += '='
                bar += ('.' * (self.width - prog_width))
                bar += ']'
            else:
                bar = '{0:7d}/Unknown'.format(current)

            self._total_width = len(bar)
            sys.stdout.write(bar)

            if current:
                time_per_unit = (now - self._start) / current
            else:
                time_per_unit = 0
            if self.target is not None and current < self.target:
                eta = int(time_per_unit * (self.target - current))
                if eta > 3600:
                    eta_format = ('{0:d}:{1:02d}:{2:02d}'.format(
                        eta // 3600, (eta % 3600) // 60, eta % 60))
                elif eta > 60:
                    eta_format = '{0:d}:{1:02d}'.format(eta // 60, eta % 60)
                else:
                    eta_format = '{0:d}s'.format(eta)

                info = ' - ETA: {0}'.format(eta_format)
            else:
                if time_per_unit >= 1:
                    info += ' {0:.0f}s/step'.format(time_per_unit)
                elif time_per_unit >= 1e-3:
                    info += ' {0:.0f}ms/step'.format(time_per_unit * 1e3)
                else:
                    info += ' {0:.0f}us/step'.format(time_per_unit * 1e6)

            self._total_width += len(info)
            if prev_total_width > self._total_width:
                info += (' ' * (prev_total_width - self._total_width))

            if self.target is not None and current >= self.target:
                info += '\n'

            sys.stdout.write(info)
            sys.stdout.flush()

        elif self.verbose == 2:
            if self.target is None or current >= self.target:
                info += '\n'
                sys.stdout.write(info)
                sys.stdout.flush()

        self._last_update = now",_3817.py,8,now - self._last_update < self.interval and self.target is not None and (current < self.target),now - self._last_update < self.interval and None is not self.target > current
https://github.com/NTMC-Community/MatchZoo-py/tree/master/matchzoo/utils/get_file.py,"def update(self, current):
        """"""Updates the progress bar.""""""
        self._seen_so_far = current

        now = time.time()
        info = ' - {0:.0f}s'.format(now - self._start)
        if self.verbose == 1:
            if (now - self._last_update < self.interval and self.target is not
               None and current < self.target):
                return

            prev_total_width = self._total_width
            if self._dynamic_display:
                sys.stdout.write('\b' * prev_total_width)
                sys.stdout.write('\r')
            else:
                sys.stdout.write('\n')

            if self.target is not None:
                numdigits = int(np.floor(np.log10(self.target))) + 1
                bar = '{2:{0:d}d}/{1} ['.format(
                    numdigits, self.target, current)
                prog = float(current) / self.target
                prog_width = int(self.width * prog)
                if prog_width > 0:
                    bar += ('=' * (prog_width - 1))
                    if current < self.target:
                        bar += '>'
                    else:
                        bar += '='
                bar += ('.' * (self.width - prog_width))
                bar += ']'
            else:
                bar = '{0:7d}/Unknown'.format(current)

            self._total_width = len(bar)
            sys.stdout.write(bar)

            if current:
                time_per_unit = (now - self._start) / current
            else:
                time_per_unit = 0
            if self.target is not None and current < self.target:
                eta = int(time_per_unit * (self.target - current))
                if eta > 3600:
                    eta_format = ('{0:d}:{1:02d}:{2:02d}'.format(
                        eta // 3600, (eta % 3600) // 60, eta % 60))
                elif eta > 60:
                    eta_format = '{0:d}:{1:02d}'.format(eta // 60, eta % 60)
                else:
                    eta_format = '{0:d}s'.format(eta)

                info = ' - ETA: {0}'.format(eta_format)
            else:
                if time_per_unit >= 1:
                    info += ' {0:.0f}s/step'.format(time_per_unit)
                elif time_per_unit >= 1e-3:
                    info += ' {0:.0f}ms/step'.format(time_per_unit * 1e3)
                else:
                    info += ' {0:.0f}us/step'.format(time_per_unit * 1e6)

            self._total_width += len(info)
            if prev_total_width > self._total_width:
                info += (' ' * (prev_total_width - self._total_width))

            if self.target is not None and current >= self.target:
                info += '\n'

            sys.stdout.write(info)
            sys.stdout.flush()

        elif self.verbose == 2:
            if self.target is None or current >= self.target:
                info += '\n'
                sys.stdout.write(info)
                sys.stdout.flush()

        self._last_update = now",_3817.py,43,self.target is not None and current < self.target,None is not self.target > current
https://github.com/NTMC-Community/MatchZoo-py/tree/master/matchzoo/utils/get_file.py,"def update(self, current):
        """"""Updates the progress bar.""""""
        self._seen_so_far = current

        now = time.time()
        info = ' - {0:.0f}s'.format(now - self._start)
        if self.verbose == 1:
            if (now - self._last_update < self.interval and self.target is not
               None and current < self.target):
                return

            prev_total_width = self._total_width
            if self._dynamic_display:
                sys.stdout.write('\b' * prev_total_width)
                sys.stdout.write('\r')
            else:
                sys.stdout.write('\n')

            if self.target is not None:
                numdigits = int(np.floor(np.log10(self.target))) + 1
                bar = '{2:{0:d}d}/{1} ['.format(
                    numdigits, self.target, current)
                prog = float(current) / self.target
                prog_width = int(self.width * prog)
                if prog_width > 0:
                    bar += ('=' * (prog_width - 1))
                    if current < self.target:
                        bar += '>'
                    else:
                        bar += '='
                bar += ('.' * (self.width - prog_width))
                bar += ']'
            else:
                bar = '{0:7d}/Unknown'.format(current)

            self._total_width = len(bar)
            sys.stdout.write(bar)

            if current:
                time_per_unit = (now - self._start) / current
            else:
                time_per_unit = 0
            if self.target is not None and current < self.target:
                eta = int(time_per_unit * (self.target - current))
                if eta > 3600:
                    eta_format = ('{0:d}:{1:02d}:{2:02d}'.format(
                        eta // 3600, (eta % 3600) // 60, eta % 60))
                elif eta > 60:
                    eta_format = '{0:d}:{1:02d}'.format(eta // 60, eta % 60)
                else:
                    eta_format = '{0:d}s'.format(eta)

                info = ' - ETA: {0}'.format(eta_format)
            else:
                if time_per_unit >= 1:
                    info += ' {0:.0f}s/step'.format(time_per_unit)
                elif time_per_unit >= 1e-3:
                    info += ' {0:.0f}ms/step'.format(time_per_unit * 1e3)
                else:
                    info += ' {0:.0f}us/step'.format(time_per_unit * 1e6)

            self._total_width += len(info)
            if prev_total_width > self._total_width:
                info += (' ' * (prev_total_width - self._total_width))

            if self.target is not None and current >= self.target:
                info += '\n'

            sys.stdout.write(info)
            sys.stdout.flush()

        elif self.verbose == 2:
            if self.target is None or current >= self.target:
                info += '\n'
                sys.stdout.write(info)
                sys.stdout.flush()

        self._last_update = now",_3817.py,66,self.target is not None and current >= self.target,None is not self.target <= current
https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/model/post_processing/instance_post_processing.py,"def get_panoptic_segmentation(sem, ctr_hmp, offsets, thing_list, label_divisor, stuff_area, void_label,
                              threshold=0.1, nms_kernel=3, top_k=None, foreground_mask=None):
    """"""
    Post-processing for panoptic segmentation.
    Arguments:
        sem: A Tensor of shape [N, C, H, W] of raw semantic output, where N is the batch size, for consistent,
            we only support N=1. Or, a processed Tensor of shape [1, H, W].
        ctr_hmp: A Tensor of shape [N, 1, H, W] of raw center heatmap output, where N is the batch size,
            for consistent, we only support N=1.
        offsets: A Tensor of shape [N, 2, H, W] of raw offset output, where N is the batch size,
            for consistent, we only support N=1. The order of second dim is (offset_y, offset_x).
        thing_list: A List of thing class id.
        label_divisor: An Integer, used to convert panoptic id = semantic id * label_divisor + instance_id.
        stuff_area: An Integer, remove stuff whose area is less tan stuff_area.
        void_label: An Integer, indicates the region has no confident prediction.
        threshold: A Float, threshold applied to center heatmap score.
        nms_kernel: An Integer, NMS max pooling kernel size.
        top_k: An Integer, top k centers to keep.
        foreground_mask: A Tensor of shape [N, 2, H, W] of raw foreground mask, where N is the batch size,
            we only support N=1. Or, a processed Tensor of shape [1, H, W].
    Returns:
        A Tensor of shape [1, H, W] (to be gathered by distributed data parallel), int64.
    Raises:
        ValueError, if batch size is not 1.
    """"""
    if sem.dim() != 4 and sem.dim() != 3:
        raise ValueError('Semantic prediction with un-supported dimension: {}.'.format(sem.dim()))
    if sem.dim() == 4 and sem.size(0) != 1:
        raise ValueError('Only supports inference for batch size = 1')
    if ctr_hmp.size(0) != 1:
        raise ValueError('Only supports inference for batch size = 1')
    if offsets.size(0) != 1:
        raise ValueError('Only supports inference for batch size = 1')
    if foreground_mask is not None:
        if foreground_mask.dim() != 4 and foreground_mask.dim() != 3:
            raise ValueError('Foreground prediction with un-supported dimension: {}.'.format(sem.dim()))

    if sem.dim() == 4:
        semantic = get_semantic_segmentation(sem)
    else:
        semantic = sem

    if foreground_mask is not None:
        if foreground_mask.dim() == 4:
            thing_seg = get_semantic_segmentation(foreground_mask)
        else:
            thing_seg = foreground_mask
    else:
        thing_seg = None

    instance, center = get_instance_segmentation(semantic, ctr_hmp, offsets, thing_list,
                                                 threshold=threshold, nms_kernel=nms_kernel, top_k=top_k,
                                                 thing_seg=thing_seg)
    panoptic = merge_semantic_and_instance(semantic, instance, label_divisor, thing_list, stuff_area, void_label)

    return panoptic, center",_4087.py,26,sem.dim() != 4 and sem.dim() != 3,4 != sem.dim() != 3
https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/model/post_processing/instance_post_processing.py,"def get_panoptic_segmentation(sem, ctr_hmp, offsets, thing_list, label_divisor, stuff_area, void_label,
                              threshold=0.1, nms_kernel=3, top_k=None, foreground_mask=None):
    """"""
    Post-processing for panoptic segmentation.
    Arguments:
        sem: A Tensor of shape [N, C, H, W] of raw semantic output, where N is the batch size, for consistent,
            we only support N=1. Or, a processed Tensor of shape [1, H, W].
        ctr_hmp: A Tensor of shape [N, 1, H, W] of raw center heatmap output, where N is the batch size,
            for consistent, we only support N=1.
        offsets: A Tensor of shape [N, 2, H, W] of raw offset output, where N is the batch size,
            for consistent, we only support N=1. The order of second dim is (offset_y, offset_x).
        thing_list: A List of thing class id.
        label_divisor: An Integer, used to convert panoptic id = semantic id * label_divisor + instance_id.
        stuff_area: An Integer, remove stuff whose area is less tan stuff_area.
        void_label: An Integer, indicates the region has no confident prediction.
        threshold: A Float, threshold applied to center heatmap score.
        nms_kernel: An Integer, NMS max pooling kernel size.
        top_k: An Integer, top k centers to keep.
        foreground_mask: A Tensor of shape [N, 2, H, W] of raw foreground mask, where N is the batch size,
            we only support N=1. Or, a processed Tensor of shape [1, H, W].
    Returns:
        A Tensor of shape [1, H, W] (to be gathered by distributed data parallel), int64.
    Raises:
        ValueError, if batch size is not 1.
    """"""
    if sem.dim() != 4 and sem.dim() != 3:
        raise ValueError('Semantic prediction with un-supported dimension: {}.'.format(sem.dim()))
    if sem.dim() == 4 and sem.size(0) != 1:
        raise ValueError('Only supports inference for batch size = 1')
    if ctr_hmp.size(0) != 1:
        raise ValueError('Only supports inference for batch size = 1')
    if offsets.size(0) != 1:
        raise ValueError('Only supports inference for batch size = 1')
    if foreground_mask is not None:
        if foreground_mask.dim() != 4 and foreground_mask.dim() != 3:
            raise ValueError('Foreground prediction with un-supported dimension: {}.'.format(sem.dim()))

    if sem.dim() == 4:
        semantic = get_semantic_segmentation(sem)
    else:
        semantic = sem

    if foreground_mask is not None:
        if foreground_mask.dim() == 4:
            thing_seg = get_semantic_segmentation(foreground_mask)
        else:
            thing_seg = foreground_mask
    else:
        thing_seg = None

    instance, center = get_instance_segmentation(semantic, ctr_hmp, offsets, thing_list,
                                                 threshold=threshold, nms_kernel=nms_kernel, top_k=top_k,
                                                 thing_seg=thing_seg)
    panoptic = merge_semantic_and_instance(semantic, instance, label_divisor, thing_list, stuff_area, void_label)

    return panoptic, center",_4087.py,35,foreground_mask.dim() != 4 and foreground_mask.dim() != 3,4 != foreground_mask.dim() != 3
https://github.com/yenchenlin/DeepLearningFlappyBird/tree/master//deep_q_network.py,"def trainNetwork(s, readout, h_fc1, sess):
    # define the cost function
    a = tf.placeholder(""float"", [None, ACTIONS])
    y = tf.placeholder(""float"", [None])
    readout_action = tf.reduce_sum(tf.multiply(readout, a), reduction_indices=1)
    cost = tf.reduce_mean(tf.square(y - readout_action))
    train_step = tf.train.AdamOptimizer(1e-6).minimize(cost)

    # open up a game state to communicate with emulator
    game_state = game.GameState()

    # store the previous observations in replay memory
    D = deque()

    # printing
    a_file = open(""logs_"" + GAME + ""/readout.txt"", 'w')
    h_file = open(""logs_"" + GAME + ""/hidden.txt"", 'w')

    # get the first state by doing nothing and preprocess the image to 80x80x4
    do_nothing = np.zeros(ACTIONS)
    do_nothing[0] = 1
    x_t, r_0, terminal = game_state.frame_step(do_nothing)
    x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)
    ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)
    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)

    # saving and loading networks
    saver = tf.train.Saver()
    sess.run(tf.initialize_all_variables())
    checkpoint = tf.train.get_checkpoint_state(""saved_networks"")
    if checkpoint and checkpoint.model_checkpoint_path:
        saver.restore(sess, checkpoint.model_checkpoint_path)
        print(""Successfully loaded:"", checkpoint.model_checkpoint_path)
    else:
        print(""Could not find old network weights"")

    # start training
    epsilon = INITIAL_EPSILON
    t = 0
    while ""flappy bird"" != ""angry bird"":
        # choose an action epsilon greedily
        readout_t = readout.eval(feed_dict={s : [s_t]})[0]
        a_t = np.zeros([ACTIONS])
        action_index = 0
        if t % FRAME_PER_ACTION == 0:
            if random.random() <= epsilon:
                print(""----------Random Action----------"")
                action_index = random.randrange(ACTIONS)
                a_t[random.randrange(ACTIONS)] = 1
            else:
                action_index = np.argmax(readout_t)
                a_t[action_index] = 1
        else:
            a_t[0] = 1 # do nothing

        # scale down epsilon
        if epsilon > FINAL_EPSILON and t > OBSERVE:
            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE

        # run the selected action and observe next state and reward
        x_t1_colored, r_t, terminal = game_state.frame_step(a_t)
        x_t1 = cv2.cvtColor(cv2.resize(x_t1_colored, (80, 80)), cv2.COLOR_BGR2GRAY)
        ret, x_t1 = cv2.threshold(x_t1, 1, 255, cv2.THRESH_BINARY)
        x_t1 = np.reshape(x_t1, (80, 80, 1))
        #s_t1 = np.append(x_t1, s_t[:,:,1:], axis = 2)
        s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)

        # store the transition in D
        D.append((s_t, a_t, r_t, s_t1, terminal))
        if len(D) > REPLAY_MEMORY:
            D.popleft()

        # only train if done observing
        if t > OBSERVE:
            # sample a minibatch to train on
            minibatch = random.sample(D, BATCH)

            # get the batch variables
            s_j_batch = [d[0] for d in minibatch]
            a_batch = [d[1] for d in minibatch]
            r_batch = [d[2] for d in minibatch]
            s_j1_batch = [d[3] for d in minibatch]

            y_batch = []
            readout_j1_batch = readout.eval(feed_dict = {s : s_j1_batch})
            for i in range(0, len(minibatch)):
                terminal = minibatch[i][4]
                # if terminal, only equals reward
                if terminal:
                    y_batch.append(r_batch[i])
                else:
                    y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))

            # perform gradient step
            train_step.run(feed_dict = {
                y : y_batch,
                a : a_batch,
                s : s_j_batch}
            )

        # update the old values
        s_t = s_t1
        t += 1

        # save progress every 10000 iterations
        if t % 10000 == 0:
            saver.save(sess, 'saved_networks/' + GAME + '-dqn', global_step = t)

        # print info
        state = """"
        if t <= OBSERVE:
            state = ""observe""
        elif t > OBSERVE and t <= OBSERVE + EXPLORE:
            state = ""explore""
        else:
            state = ""train""

        print(""TIMESTEP"", t, ""/ STATE"", state, \
            ""/ EPSILON"", epsilon, ""/ ACTION"", action_index, ""/ REWARD"", r_t, \
            ""/ Q_MAX %e"" % np.max(readout_t))
        # write info to files
        '''
        if t % 10000 <= 100:
            a_file.write("","".join([str(x) for x in readout_t]) + '\n')
            h_file.write("","".join([str(x) for x in h_fc1.eval(feed_dict={s:[s_t]})[0]]) + '\n')
            cv2.imwrite(""logs_tetris/frame"" + str(t) + "".png"", x_t1)
        '''",_4107.py,113,t > OBSERVE and t <= OBSERVE + EXPLORE,OBSERVE < t <= OBSERVE + EXPLORE
https://github.com/ros/ros_comm/tree/master/clients/rospy/src/rospy/impl/tcpros_pubsub.py,"def write_data(self, data):
        with self._lock:
            # if there was previously an error within the dispatch thread raise it
            if self._error:
                error = self._error
                self._error = None
                raise error
            # pop oldest data if queue limit is reached
            if self._queue_size > 0 and len(self._queue) == self._queue_size:
                del self._queue[0]
            self._queue.append(data)
            self._cond_data_available.notify()
        # effectively yields the rest of the thread quantum
        time.sleep(0)
        return True",_4166.py,9,self._queue_size > 0 and len(self._queue) == self._queue_size,len(self._queue) == self._queue_size > 0
https://github.com/PaddlePaddle/PaddleSlim/tree/master/paddleslim/prune/pruner.py,"def _transform(self, items):
        ret = []
        for name, axis, pruned_idx, transforms in items:
            src = pruned_idx
            for trans in transforms:
                target = []
                if 'src_start' in trans:
                    src_start = trans['src_start']
                    src_end = trans['src_end']
                    src_len = src_end - src_start
                    target_start = trans['target_start']
                    target_end = trans['target_end']
                    starts = np.array(range(target_start, target_end, src_len))
                    for idx in src:
                        if idx >= src_start and idx < src_end:
                            idx -= src_start
                            target.extend(list(idx + starts))
                elif ""repeat"" in trans:
                    repeat = trans['repeat']
                    for idx in src:
                        idx = idx * repeat
                        target.extend(range(idx, idx + repeat))
                elif ""squeeze"" in trans:
                    repeat = trans['repeat']
                    targets_set = set()
                    for idx in src:
                        targets_set.add(idx / repeat)
                    target = list(targets_set)

                src = target

            ret.append((name, axis, src))
        return ret",_4299.py,15,idx >= src_start and idx < src_end,src_start <= idx < src_end
https://github.com/brian-team/brian2/tree/master/brian2/core/functions.py,"def declare_types(**types):
    """"""
    Decorator to declare argument and result types for a function

    Usage is similar to `check_units` except that types must be one of ``{VALID_ARG_TYPES}``
    and the result type must be one of ``{VALID_RETURN_TYPES}``. Unspecified argument
    types are assumed to be ``'all'`` (i.e. anything is permitted), and an unspecified
    result type is assumed to be ``'float'``. Note that the ``'highest'`` option for
    result type will give the highest type of its argument, e.g. if the arguments
    were boolean and integer then the result would be integer, if the arguments were
    integer and float it would be float.
    """"""

    def annotate_function_with_types(f):
        if hasattr(f, ""_orig_arg_names""):
            arg_names = f._orig_arg_names
        else:
            arg_names = f.__code__.co_varnames[0 : f.__code__.co_argcount]
        argtypes = []
        for name in arg_names:
            arg_type = types.get(name, ""any"")
            if arg_type not in VALID_ARG_TYPES:
                raise ValueError(
                    f""Argument type {arg_type} is not valid, must be one of""
                    f"" {VALID_ARG_TYPES}, for argument {name}""
                )
            argtypes.append(arg_type)
        for n in types:
            if n not in arg_names and n != ""result"":
                raise ValueError(f""Type specified for unknown argument {n}"")
        return_type = types.get(""result"", ""float"")
        if return_type not in VALID_RETURN_TYPES:
            raise ValueError(
                f""Result type {return_type} is not valid, must be one of""
                f"" {VALID_RETURN_TYPES}""
            )
        f._arg_types = argtypes
        f._return_type = return_type
        f._orig_arg_names = arg_names
        f._annotation_attributes = getattr(f, ""_annotation_attributes"", []) + [
            ""_arg_types"",
            ""_return_type"",
        ]
        return f

    return annotate_function_with_types",_4310.py,29,n not in arg_names and n != 'result','result' != n not in arg_names
https://github.com/nats-io/nats.py/tree/master/nats/aio/client.py,"def _connect_command(self):
        '''
        Generates a JSON string with the params to be used
        when sending CONNECT to the server.

          ->> CONNECT {""lang"": ""python3""}

        '''
        options = {
            ""verbose"": self.options[""verbose""],
            ""pedantic"": self.options[""pedantic""],
            ""lang"": __lang__,
            ""version"": __version__,
            ""protocol"": PROTOCOL
        }
        if ""headers"" in self._server_info:
            options[""headers""] = self._server_info[""headers""]
            options[""no_responders""] = self._server_info[""headers""]

        if ""auth_required"" in self._server_info:
            if self._server_info[""auth_required""]:
                if ""nonce"" in self._server_info and self._signature_cb is not None:
                    sig = self._signature_cb(self._server_info[""nonce""])
                    options[""sig""] = sig.decode()

                    if self._user_jwt_cb is not None:
                        jwt = self._user_jwt_cb()
                        options[""jwt""] = jwt.decode()
                    elif self._public_nkey is not None:
                        options[""nkey""] = self._public_nkey
                # In case there is no password, then consider handle
                # sending a token instead.
                elif self.options[""user""] is not None and self.options[
                        ""password""] is not None:
                    options[""user""] = self.options[""user""]
                    options[""pass""] = self.options[""password""]
                elif self.options[""token""] is not None:
                    options[""auth_token""] = self.options[""token""]
                elif self._current_server.uri.username is not None:
                    if self._current_server.uri.password is None:
                        options[""auth_token""
                                ] = self._current_server.uri.username
                    else:
                        options[""user""] = self._current_server.uri.username
                        options[""pass""] = self._current_server.uri.password

        if self.options[""name""] is not None:
            options[""name""] = self.options[""name""]
        if self.options[""no_echo""] is not None:
            options[""echo""] = not self.options[""no_echo""]

        connect_opts = json.dumps(options, sort_keys=True)
        return b''.join([CONNECT_OP + _SPC_ + connect_opts.encode() + _CRLF_])",_4331.py,33,self.options['user'] is not None and self.options['password'] is not None,self.options['user'] is not None is not self.options['password']
https://github.com/graph4ai/graph4nlp/tree/master/graph4nlp/pytorch/modules/prediction/generation/decoder_strategy.py,"def beam_search_for_tree_decoding(
        self,
        decoder_initial_state,
        decoder_initial_input,
        parent_state,
        graph_node_embedding,
        rnn_node_embedding=None,
        src_seq=None,
        oov_dict=None,
        sibling_state=None,
        device=None,
        topk=1,
        enc_batch=None,
    ):
        min_out_len = 1
        max_out_len = self.max_decoder_step
        batch_size = graph_node_embedding.size(0)
        assert batch_size == 1
        decoder_hidden = decoder_initial_state

        batch_results = []
        for _ in range(batch_size):
            single_graph_node_embedding = graph_node_embedding.expand(
                self.beam_size, -1, -1
            ).contiguous()
            single_parent_state = parent_state.expand(self.beam_size, -1).contiguous()

            step = 0
            results, backup_results = [], []
            hypos = [
                Hypothesis(
                    tokens=[decoder_initial_input],
                    log_probs=[],
                    dec_state=decoder_hidden,
                    input_feed=None,
                    num_non_words=1,
                    enc_attn_weights=[],
                    use_coverage=self.use_coverage,
                    states_for_tree=[decoder_hidden],
                )
            ]

            while len(hypos) > 0 and step <= self.max_decoder_step:
                n_hypos = len(hypos)
                if n_hypos < self.beam_size:
                    hypos.extend(
                        hypos[-1] for _ in range(self.beam_size - n_hypos)
                    )  # check deep copy
                decoder_input = torch.tensor([h.tokens[-1] for h in hypos]).to(
                    graph_node_embedding.device
                )
                decoder_hidden = (
                    torch.cat([h.dec_state[0] for h in hypos], 0),
                    torch.cat([h.dec_state[1] for h in hypos], 0),
                )

                prediction, decoder_hidden, dec_attn_scores = self.decoder.decode_step(
                    tgt_batch_size=self.beam_size,
                    dec_single_input=decoder_input,
                    dec_single_state=decoder_hidden,
                    memory=single_graph_node_embedding,
                    parent_state=single_parent_state,
                    oov_dict=oov_dict,
                    enc_batch=enc_batch,
                )
                prediction = torch.log(prediction + 1e-31)
                top_v, top_i = prediction.data.topk(self.beam_size)

                new_hypos = []
                for in_idx in range(n_hypos):
                    for out_idx in range(self.beam_size):
                        new_tok = top_i[in_idx][out_idx].item()
                        new_prob = top_v[in_idx][out_idx].item()
                        new_enc_attn_weights = dec_attn_scores[in_idx, :].unsqueeze(0).unsqueeze(0)

                        non_word = new_tok == self.vocab.get_symbol_idx(
                            self.vocab.end_token
                        )  # only SOS & EOS don't count
                        tmp_decoder_state = (
                            decoder_hidden[0][in_idx, :].unsqueeze(0),
                            decoder_hidden[1][in_idx, :].unsqueeze(0),
                        )

                        new_hypo = hypos[in_idx].create_next(
                            token=new_tok,
                            log_prob=new_prob,
                            dec_state=tmp_decoder_state,
                            input_feed=None,
                            non_word=non_word,
                            add_enc_attn_weights=new_enc_attn_weights,
                        )
                        new_hypos.append(new_hypo)

                new_hypos = sorted(new_hypos, key=lambda h: -h.avg_log_prob)[: self.beam_size]
                hypos = []
                new_complete_results, new_incomplete_results = [], []
                for nh in new_hypos:
                    length = len(nh)  # Does not count SOS and EOS
                    if nh.tokens[-1] == self.vocab.get_symbol_idx(
                        self.vocab.end_token
                    ):  # a complete hypothesis
                        if (
                            len(new_complete_results) < self.beam_size
                            and min_out_len <= length <= max_out_len
                        ):
                            new_complete_results.append(nh)
                    elif (
                        len(hypos) < self.beam_size and length < max_out_len
                    ):  # an incomplete hypothesis
                        hypos.append(nh)
                    elif length == max_out_len and len(new_incomplete_results) < self.beam_size:
                        new_incomplete_results.append(nh)
                if new_complete_results:
                    results.extend(new_complete_results)
                elif new_incomplete_results:
                    backup_results.extend(new_incomplete_results)
                step += 1

            if (
                not results
            ):  # if no sequence ends with EOS within desired length, fallback to sequences
                results = backup_results  # that are ""truncated"" at the end to max_out_len
            batch_results.append(sorted(results, key=lambda h: -h.avg_log_prob)[:topk])

        ret = torch.zeros(batch_size, topk, self.max_decoder_step).long()
        states = []
        for sent_id, each in enumerate(batch_results):
            for i in range(topk):
                ids = torch.Tensor(each[i].tokens[:])[: self.max_decoder_step]
                if ids.shape[0] < self.max_decoder_step:
                    pad = torch.zeros(self.max_decoder_step - ids.shape[0])
                    ids = torch.cat((ids, pad), dim=0)
                ret[sent_id, i, :] = ids
                states.append(each[i].states_for_tree[:][: self.max_decoder_step])
        assert batch_size == 1 and topk == 1
        # for id
        output_results = [[[]]]

        token_id_list = ret[0][0]
        states_emb_list = states[0]
        for index in range(self.max_decoder_step):
            if token_id_list[index] != self.vocab.get_symbol_idx(self.vocab.pad_token):
                output_results[0][0].append(
                    BeamSearchNode(
                        hiddenstate=states_emb_list[index],
                        enc_attn_weights_average=None,
                        previousNode=None,
                        wordId=token_id_list[index],
                        logProb=0,
                        length=-1,
                    )
                )

        return output_results",_4627.py,135,batch_size == 1 and topk == 1,batch_size == 1 == topk
https://github.com/PaddlePaddle/PaddleDetection/tree/master/deploy/pptracking/python/mot/mtmct/zone.py,"def do_intra_matching2(self, mot_list, sub_list):
        new_zone_dict = dict()

        def get_trac_info(tracklet1):
            t1_f = list(tracklet1)
            t1_f.sort()
            t1_fs = t1_f[0]
            t1_fe = t1_f[-1]
            t1_zs = tracklet1[t1_fs]['zone']
            t1_ze = tracklet1[t1_fe]['zone']
            t1_boxs = tracklet1[t1_fs]['bbox']
            t1_boxe = tracklet1[t1_fe]['bbox']
            t1_boxs = [(t1_boxs[2] + t1_boxs[0]) / 2,
                       (t1_boxs[3] + t1_boxs[1]) / 2]
            t1_boxe = [(t1_boxe[2] + t1_boxe[0]) / 2,
                       (t1_boxe[3] + t1_boxe[1]) / 2]
            return t1_fs, t1_fe, t1_zs, t1_ze, t1_boxs, t1_boxe

        for t1id in sub_list:
            tracklet1 = sub_list[t1id]
            if tracklet1 == -1:
                continue
            t1_fs, t1_fe, t1_zs, t1_ze, t1_boxs, t1_boxe = get_trac_info(
                tracklet1)
            sim_dict = dict()
            for t2id in mot_list:
                tracklet2 = mot_list[t2id]
                t2_fs, t2_fe, t2_zs, t2_ze, t2_boxs, t2_boxe = get_trac_info(
                    tracklet2)
                if t1_ze == t2_zs:
                    if abs(t2_fs - t1_fe) < 5 and abs(t2_boxe[0] - t1_boxs[
                            0]) < 50 and abs(t2_boxe[1] - t1_boxs[1]) < 50:
                        t1_feat = tracklet1[t1_fe]['feat']
                        t2_feat = tracklet2[t2_fs]['feat']
                        sim_dict[t2id] = np.matmul(t1_feat, t2_feat)
                if t1_zs == t2_ze:
                    if abs(t2_fe - t1_fs) < 5 and abs(t2_boxs[0] - t1_boxe[
                            0]) < 50 and abs(t2_boxs[1] - t1_boxe[1]) < 50:
                        t1_feat = tracklet1[t1_fs]['feat']
                        t2_feat = tracklet2[t2_fe]['feat']
                        sim_dict[t2id] = np.matmul(t1_feat, t2_feat)
            if len(sim_dict) > 0:
                max_sim = 0
                max_id = 0
                for t2id in sim_dict:
                    if sim_dict[t2id] > max_sim:
                        sim_dict[t2id] = max_sim
                        max_id = t2id
                if max_sim > 0.5:
                    t2 = mot_list[max_id]
                    for t1f in tracklet1:
                        if t1f not in t2:
                            tracklet1[t1f]['id'] = max_id
                            t2[t1f] = tracklet1[t1f]
                    mot_list[max_id] = t2
                    sub_list[t1id] = -1
        return mot_list, sub_list",_4647.py,31,abs(t2_fs - t1_fe) < 5 and abs(t2_boxe[0] - t1_boxs[0]) < 50 and (abs(t2_boxe[1] - t1_boxs[1]) < 50),abs(t2_fs - t1_fe) < 5 and abs(t2_boxe[0] - t1_boxs[0]) < 50 > abs(t2_boxe[1] - t1_boxs[1])
https://github.com/PaddlePaddle/PaddleDetection/tree/master/deploy/pptracking/python/mot/mtmct/zone.py,"def do_intra_matching2(self, mot_list, sub_list):
        new_zone_dict = dict()

        def get_trac_info(tracklet1):
            t1_f = list(tracklet1)
            t1_f.sort()
            t1_fs = t1_f[0]
            t1_fe = t1_f[-1]
            t1_zs = tracklet1[t1_fs]['zone']
            t1_ze = tracklet1[t1_fe]['zone']
            t1_boxs = tracklet1[t1_fs]['bbox']
            t1_boxe = tracklet1[t1_fe]['bbox']
            t1_boxs = [(t1_boxs[2] + t1_boxs[0]) / 2,
                       (t1_boxs[3] + t1_boxs[1]) / 2]
            t1_boxe = [(t1_boxe[2] + t1_boxe[0]) / 2,
                       (t1_boxe[3] + t1_boxe[1]) / 2]
            return t1_fs, t1_fe, t1_zs, t1_ze, t1_boxs, t1_boxe

        for t1id in sub_list:
            tracklet1 = sub_list[t1id]
            if tracklet1 == -1:
                continue
            t1_fs, t1_fe, t1_zs, t1_ze, t1_boxs, t1_boxe = get_trac_info(
                tracklet1)
            sim_dict = dict()
            for t2id in mot_list:
                tracklet2 = mot_list[t2id]
                t2_fs, t2_fe, t2_zs, t2_ze, t2_boxs, t2_boxe = get_trac_info(
                    tracklet2)
                if t1_ze == t2_zs:
                    if abs(t2_fs - t1_fe) < 5 and abs(t2_boxe[0] - t1_boxs[
                            0]) < 50 and abs(t2_boxe[1] - t1_boxs[1]) < 50:
                        t1_feat = tracklet1[t1_fe]['feat']
                        t2_feat = tracklet2[t2_fs]['feat']
                        sim_dict[t2id] = np.matmul(t1_feat, t2_feat)
                if t1_zs == t2_ze:
                    if abs(t2_fe - t1_fs) < 5 and abs(t2_boxs[0] - t1_boxe[
                            0]) < 50 and abs(t2_boxs[1] - t1_boxe[1]) < 50:
                        t1_feat = tracklet1[t1_fs]['feat']
                        t2_feat = tracklet2[t2_fe]['feat']
                        sim_dict[t2id] = np.matmul(t1_feat, t2_feat)
            if len(sim_dict) > 0:
                max_sim = 0
                max_id = 0
                for t2id in sim_dict:
                    if sim_dict[t2id] > max_sim:
                        sim_dict[t2id] = max_sim
                        max_id = t2id
                if max_sim > 0.5:
                    t2 = mot_list[max_id]
                    for t1f in tracklet1:
                        if t1f not in t2:
                            tracklet1[t1f]['id'] = max_id
                            t2[t1f] = tracklet1[t1f]
                    mot_list[max_id] = t2
                    sub_list[t1id] = -1
        return mot_list, sub_list",_4647.py,37,abs(t2_fe - t1_fs) < 5 and abs(t2_boxs[0] - t1_boxe[0]) < 50 and (abs(t2_boxs[1] - t1_boxe[1]) < 50),abs(t2_fe - t1_fs) < 5 and abs(t2_boxs[0] - t1_boxe[0]) < 50 > abs(t2_boxs[1] - t1_boxe[1])
https://github.com/sympy/sympy/tree/master/sympy/plotting/plot.py,"def sample(param_p, param_q, p, q, depth):
            """""" Samples recursively if three points are almost collinear.
            For depth < 6, points are added irrespective of whether they
            satisfy the collinearity condition or not. The maximum depth
            allowed is 12.
            """"""
            # Randomly sample to avoid aliasing.
            np = import_module('numpy')
            random = 0.45 + np.random.rand() * 0.1
            param_new = param_p + random * (param_q - param_p)
            xnew = f_x(param_new)
            ynew = f_y(param_new)
            new_point = np.array([xnew, ynew])

            # Maximum depth
            if depth > self.depth:
                x_coords.append(q[0])
                y_coords.append(q[1])

            # Sample irrespective of whether the line is flat till the
            # depth of 6. We are not using linspace to avoid aliasing.
            elif depth < 6:
                sample(param_p, param_new, p, new_point, depth + 1)
                sample(param_new, param_q, new_point, q, depth + 1)

            # Sample ten points if complex values are encountered
            # at both ends. If there is a real value in between, then
            # sample those points further.
            elif ((p[0] is None and q[1] is None) or
                    (p[1] is None and q[1] is None)):
                param_array = np.linspace(param_p, param_q, 10)
                x_array = list(map(f_x, param_array))
                y_array = list(map(f_y, param_array))
                if not all(x is None and y is None
                           for x, y in zip(x_array, y_array)):
                    for i in range(len(y_array) - 1):
                        if ((x_array[i] is not None and y_array[i] is not None) or
                                (x_array[i + 1] is not None and y_array[i + 1] is not None)):
                            point_a = [x_array[i], y_array[i]]
                            point_b = [x_array[i + 1], y_array[i + 1]]
                            sample(param_array[i], param_array[i], point_a,
                                   point_b, depth + 1)

            # Sample further if one of the end points in None (i.e. a complex
            # value) or the three points are not almost collinear.
            elif (p[0] is None or p[1] is None
                    or q[1] is None or q[0] is None
                    or not flat(p, new_point, q)):
                sample(param_p, param_new, p, new_point, depth + 1)
                sample(param_new, param_q, new_point, q, depth + 1)
            else:
                x_coords.append(q[0])
                y_coords.append(q[1])",_4707.py,29,p[0] is None and q[1] is None,p[0] is None is q[1]
https://github.com/sympy/sympy/tree/master/sympy/plotting/plot.py,"def sample(param_p, param_q, p, q, depth):
            """""" Samples recursively if three points are almost collinear.
            For depth < 6, points are added irrespective of whether they
            satisfy the collinearity condition or not. The maximum depth
            allowed is 12.
            """"""
            # Randomly sample to avoid aliasing.
            np = import_module('numpy')
            random = 0.45 + np.random.rand() * 0.1
            param_new = param_p + random * (param_q - param_p)
            xnew = f_x(param_new)
            ynew = f_y(param_new)
            new_point = np.array([xnew, ynew])

            # Maximum depth
            if depth > self.depth:
                x_coords.append(q[0])
                y_coords.append(q[1])

            # Sample irrespective of whether the line is flat till the
            # depth of 6. We are not using linspace to avoid aliasing.
            elif depth < 6:
                sample(param_p, param_new, p, new_point, depth + 1)
                sample(param_new, param_q, new_point, q, depth + 1)

            # Sample ten points if complex values are encountered
            # at both ends. If there is a real value in between, then
            # sample those points further.
            elif ((p[0] is None and q[1] is None) or
                    (p[1] is None and q[1] is None)):
                param_array = np.linspace(param_p, param_q, 10)
                x_array = list(map(f_x, param_array))
                y_array = list(map(f_y, param_array))
                if not all(x is None and y is None
                           for x, y in zip(x_array, y_array)):
                    for i in range(len(y_array) - 1):
                        if ((x_array[i] is not None and y_array[i] is not None) or
                                (x_array[i + 1] is not None and y_array[i + 1] is not None)):
                            point_a = [x_array[i], y_array[i]]
                            point_b = [x_array[i + 1], y_array[i + 1]]
                            sample(param_array[i], param_array[i], point_a,
                                   point_b, depth + 1)

            # Sample further if one of the end points in None (i.e. a complex
            # value) or the three points are not almost collinear.
            elif (p[0] is None or p[1] is None
                    or q[1] is None or q[0] is None
                    or not flat(p, new_point, q)):
                sample(param_p, param_new, p, new_point, depth + 1)
                sample(param_new, param_q, new_point, q, depth + 1)
            else:
                x_coords.append(q[0])
                y_coords.append(q[1])",_4707.py,30,p[1] is None and q[1] is None,p[1] is None is q[1]
https://github.com/sympy/sympy/tree/master/sympy/plotting/plot.py,"def sample(param_p, param_q, p, q, depth):
            """""" Samples recursively if three points are almost collinear.
            For depth < 6, points are added irrespective of whether they
            satisfy the collinearity condition or not. The maximum depth
            allowed is 12.
            """"""
            # Randomly sample to avoid aliasing.
            np = import_module('numpy')
            random = 0.45 + np.random.rand() * 0.1
            param_new = param_p + random * (param_q - param_p)
            xnew = f_x(param_new)
            ynew = f_y(param_new)
            new_point = np.array([xnew, ynew])

            # Maximum depth
            if depth > self.depth:
                x_coords.append(q[0])
                y_coords.append(q[1])

            # Sample irrespective of whether the line is flat till the
            # depth of 6. We are not using linspace to avoid aliasing.
            elif depth < 6:
                sample(param_p, param_new, p, new_point, depth + 1)
                sample(param_new, param_q, new_point, q, depth + 1)

            # Sample ten points if complex values are encountered
            # at both ends. If there is a real value in between, then
            # sample those points further.
            elif ((p[0] is None and q[1] is None) or
                    (p[1] is None and q[1] is None)):
                param_array = np.linspace(param_p, param_q, 10)
                x_array = list(map(f_x, param_array))
                y_array = list(map(f_y, param_array))
                if not all(x is None and y is None
                           for x, y in zip(x_array, y_array)):
                    for i in range(len(y_array) - 1):
                        if ((x_array[i] is not None and y_array[i] is not None) or
                                (x_array[i + 1] is not None and y_array[i + 1] is not None)):
                            point_a = [x_array[i], y_array[i]]
                            point_b = [x_array[i + 1], y_array[i + 1]]
                            sample(param_array[i], param_array[i], point_a,
                                   point_b, depth + 1)

            # Sample further if one of the end points in None (i.e. a complex
            # value) or the three points are not almost collinear.
            elif (p[0] is None or p[1] is None
                    or q[1] is None or q[0] is None
                    or not flat(p, new_point, q)):
                sample(param_p, param_new, p, new_point, depth + 1)
                sample(param_new, param_q, new_point, q, depth + 1)
            else:
                x_coords.append(q[0])
                y_coords.append(q[1])",_4707.py,34,x is None and y is None,x is None is y
https://github.com/sympy/sympy/tree/master/sympy/plotting/plot.py,"def sample(param_p, param_q, p, q, depth):
            """""" Samples recursively if three points are almost collinear.
            For depth < 6, points are added irrespective of whether they
            satisfy the collinearity condition or not. The maximum depth
            allowed is 12.
            """"""
            # Randomly sample to avoid aliasing.
            np = import_module('numpy')
            random = 0.45 + np.random.rand() * 0.1
            param_new = param_p + random * (param_q - param_p)
            xnew = f_x(param_new)
            ynew = f_y(param_new)
            new_point = np.array([xnew, ynew])

            # Maximum depth
            if depth > self.depth:
                x_coords.append(q[0])
                y_coords.append(q[1])

            # Sample irrespective of whether the line is flat till the
            # depth of 6. We are not using linspace to avoid aliasing.
            elif depth < 6:
                sample(param_p, param_new, p, new_point, depth + 1)
                sample(param_new, param_q, new_point, q, depth + 1)

            # Sample ten points if complex values are encountered
            # at both ends. If there is a real value in between, then
            # sample those points further.
            elif ((p[0] is None and q[1] is None) or
                    (p[1] is None and q[1] is None)):
                param_array = np.linspace(param_p, param_q, 10)
                x_array = list(map(f_x, param_array))
                y_array = list(map(f_y, param_array))
                if not all(x is None and y is None
                           for x, y in zip(x_array, y_array)):
                    for i in range(len(y_array) - 1):
                        if ((x_array[i] is not None and y_array[i] is not None) or
                                (x_array[i + 1] is not None and y_array[i + 1] is not None)):
                            point_a = [x_array[i], y_array[i]]
                            point_b = [x_array[i + 1], y_array[i + 1]]
                            sample(param_array[i], param_array[i], point_a,
                                   point_b, depth + 1)

            # Sample further if one of the end points in None (i.e. a complex
            # value) or the three points are not almost collinear.
            elif (p[0] is None or p[1] is None
                    or q[1] is None or q[0] is None
                    or not flat(p, new_point, q)):
                sample(param_p, param_new, p, new_point, depth + 1)
                sample(param_new, param_q, new_point, q, depth + 1)
            else:
                x_coords.append(q[0])
                y_coords.append(q[1])",_4707.py,37,x_array[i] is not None and y_array[i] is not None,x_array[i] is not None is not y_array[i]
https://github.com/sympy/sympy/tree/master/sympy/plotting/plot.py,"def sample(param_p, param_q, p, q, depth):
            """""" Samples recursively if three points are almost collinear.
            For depth < 6, points are added irrespective of whether they
            satisfy the collinearity condition or not. The maximum depth
            allowed is 12.
            """"""
            # Randomly sample to avoid aliasing.
            np = import_module('numpy')
            random = 0.45 + np.random.rand() * 0.1
            param_new = param_p + random * (param_q - param_p)
            xnew = f_x(param_new)
            ynew = f_y(param_new)
            new_point = np.array([xnew, ynew])

            # Maximum depth
            if depth > self.depth:
                x_coords.append(q[0])
                y_coords.append(q[1])

            # Sample irrespective of whether the line is flat till the
            # depth of 6. We are not using linspace to avoid aliasing.
            elif depth < 6:
                sample(param_p, param_new, p, new_point, depth + 1)
                sample(param_new, param_q, new_point, q, depth + 1)

            # Sample ten points if complex values are encountered
            # at both ends. If there is a real value in between, then
            # sample those points further.
            elif ((p[0] is None and q[1] is None) or
                    (p[1] is None and q[1] is None)):
                param_array = np.linspace(param_p, param_q, 10)
                x_array = list(map(f_x, param_array))
                y_array = list(map(f_y, param_array))
                if not all(x is None and y is None
                           for x, y in zip(x_array, y_array)):
                    for i in range(len(y_array) - 1):
                        if ((x_array[i] is not None and y_array[i] is not None) or
                                (x_array[i + 1] is not None and y_array[i + 1] is not None)):
                            point_a = [x_array[i], y_array[i]]
                            point_b = [x_array[i + 1], y_array[i + 1]]
                            sample(param_array[i], param_array[i], point_a,
                                   point_b, depth + 1)

            # Sample further if one of the end points in None (i.e. a complex
            # value) or the three points are not almost collinear.
            elif (p[0] is None or p[1] is None
                    or q[1] is None or q[0] is None
                    or not flat(p, new_point, q)):
                sample(param_p, param_new, p, new_point, depth + 1)
                sample(param_new, param_q, new_point, q, depth + 1)
            else:
                x_coords.append(q[0])
                y_coords.append(q[1])",_4707.py,38,x_array[i + 1] is not None and y_array[i + 1] is not None,x_array[i + 1] is not None is not y_array[i + 1]
https://github.com/osmr/imgclsmob/tree/master/pytorch/pytorchcv/models/contextnet.py,"def __init__(self,
                 aux=False,
                 fixed_size=False,
                 in_channels=3,
                 in_size=(1024, 2048),
                 num_classes=19):
        super(ContextNet, self).__init__()
        assert (aux is not None)
        assert (fixed_size is not None)
        assert ((in_size[0] % 8 == 0) and (in_size[1] % 8 == 0))
        self.in_size = in_size
        self.num_classes = num_classes
        self.aux = aux
        self.fixed_size = fixed_size

        self.features_high = CtxShallowNet(
            in_channels=in_channels,
            mid1_channels=32,
            mid2_channels=64,
            out_channels=128)
        self.down = InterpolationBlock(
            scale_factor=4,
            align_corners=True,
            up=False)
        self.features_low = CtxDeepNet(
            in_channels=in_channels,
            init_block_channels=32)
        self.fusion = FeatureFusion(
            in_channels_high=128,
            in_channels_low=128,
            out_channels=128)
        self.head = CtxHead(
            in_channels=128,
            num_classes=num_classes)
        self.up = InterpolationBlock(
            scale_factor=8,
            align_corners=True)

        if self.aux:
            self.aux_head = CtxAuxHead(
                in_channels=128,
                mid_channels=32,
                num_classes=num_classes)",_5052.py,10,in_size[0] % 8 == 0 and in_size[1] % 8 == 0,in_size[0] % 8 == 0 == in_size[1] % 8
https://github.com/HRNet/Lite-HRNet/tree/master/tools/torchstat/compute_flops.py,"def compute_ConvTranspose2d_flops(module, inp, out):
    # Can have multiple inputs, getting the first one
    assert isinstance(module, nn.ConvTranspose2d)
    assert len(inp.size()) == 4 and len(inp.size()) == len(out.size())

    batch_size = inp.size()[0]
    in_h, in_w = inp.size()[2:]

    k_h, k_w = module.kernel_size
    in_c = module.in_channels
    out_c = module.out_channels
    groups = module.groups

    filters_per_channel = out_c // groups
    conv_per_position_flops = k_h * k_w * in_c * filters_per_channel
    active_elements_count = batch_size * in_h * in_w

    total_conv_flops = conv_per_position_flops * active_elements_count

    bias_flops = 0
    if module.bias is not None:
        out_h, out_w = out.size()[2:]
        bias_flops = out_c * batch_size * out_h * out_w

    total_flops = total_conv_flops + bias_flops

    return total_flops",_5075.py,4,len(inp.size()) == 4 and len(inp.size()) == len(out.size()),4 == len(inp.size()) == len(out.size())
https://github.com/virt-manager/virt-manager/tree/master/virtManager/details/console.py,"def _enter_notify(self, ignore1, ignore2):
        x, y = self._ebox.get_pointer()
        alloc = self._ebox.get_allocation()
        entered = bool(x >= 0 and y >= 0 and
                       x < alloc.width and y < alloc.height)

        if not self._in_fullscreen:
            return

        # Pointer exited the toolbar, and toolbar is revealed. Schedule
        # a timeout to close it, if one isn't already scheduled
        if not entered and self._revealer.get_reveal_child():
            self._schedule_unreveal_timeout(1000)
            return

        self._unregister_timeout()
        if entered and not self._revealer.get_reveal_child():
            self._revealer.set_reveal_child(True)",_5199.py,4,x >= 0 and y >= 0 and (x < alloc.width) and (y < alloc.height),alloc.width > x >= 0 <= y < alloc.height
https://github.com/HaddyYang/django2.0-course/tree/master/33.å‘æŒ¥é‚®ç®±ä½œç”¨/user/forms.py,"def clean_verification_code(self):
        verification_code = self.cleaned_data.get('verification_code', '').strip()
        if verification_code == '':
            raise forms.ValidationError('éªŒè¯ç ä¸èƒ½ä¸ºç©º')

        # åˆ¤æ–­éªŒè¯ç 
        code = self.request.session.get('forgot_password_code', '')
        verification_code = self.cleaned_data.get('verification_code', '')
        if not (code != '' and code == verification_code):
            raise forms.ValidationError('éªŒè¯ç ä¸æ­£ç¡®')

        return verification_code",_5235.py,9,code != '' and code == verification_code,'' != code == verification_code
https://github.com/OpenNMT/OpenNMT-py/tree/master/onmt/utils/optimizers.py,"def step(self, closure=None, grads=None, output_params=None,
             scale=1., grad_norms=None):
        """"""Performs a single optimization step.

        Arguments:
            closure (callable, optional): A closure that reevaluates the model
                and returns the loss.
            grads (list of tensors, optional): weight gradient to use for the
                optimizer update. If gradients have type torch.half, parameters
                are expected to be in type torch.float. (default: None)
            output params (list of tensors, optional): A reduced precision copy
                of the updated weights written out in addition to the regular
                updated weights. Have to be of same type as gradients.
                (default: None)
            scale (float, optional): factor to divide gradient tensor values
                by before applying to weights. (default: 1)
        """"""
        loss = None
        if closure is not None:
            loss = closure()

        if grads is None:
            grads_group = [None]*len(self.param_groups)
        # backward compatibility
        # assuming a list/generator of parameter means single group
        elif isinstance(grads, types.GeneratorType):
            grads_group = [grads]
        elif type(grads[0]) != list:
            grads_group = [grads]
        else:
            grads_group = grads

        if output_params is None:
            output_params_group = [None]*len(self.param_groups)
        elif isinstance(output_params, types.GeneratorType):
            output_params_group = [output_params]
        elif type(output_params[0]) != list:
            output_params_group = [output_params]
        else:
            output_params_group = output_params

        if grad_norms is None:
            grad_norms = [None]*len(self.param_groups)

        for group, grads_this_group, output_params_this_group, \
            grad_norm in zip(self.param_groups, grads_group,
                             output_params_group, grad_norms):
            if grads_this_group is None:
                grads_this_group = [None]*len(group['params'])
            if output_params_this_group is None:
                output_params_this_group = [None]*len(group['params'])

            # compute combined scale factor for this group
            combined_scale = scale
            if group['max_grad_norm'] > 0:
                # norm is in fact norm*scale
                clip = ((grad_norm / scale) + 1e-6) / group['max_grad_norm']
                if clip > 1:
                    combined_scale = clip * scale

            bias_correction = 1 if group['bias_correction'] else 0

            for p, grad, output_param in zip(group['params'],
                                             grads_this_group,
                                             output_params_this_group):
                # note: p.grad should not ever be set for correct operation of
                # mixed precision optimizer that sometimes sends None gradients
                if p.grad is None and grad is None:
                    continue
                if grad is None:
                    grad = p.grad.data
                if grad.is_sparse:
                    raise RuntimeError('FusedAdam does not support sparse \
                                       gradients, please consider \
                                       SparseAdam instead')

                state = self.state[p]

                # State initialization
                if len(state) == 0:
                    state['step'] = 0
                    # Exponential moving average of gradient values
                    state['exp_avg'] = torch.zeros_like(p.data)
                    # Exponential moving average of squared gradient values
                    state['exp_avg_sq'] = torch.zeros_like(p.data)

                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']
                beta1, beta2 = group['betas']

                state['step'] += 1

                out_p = torch.tensor([], dtype=torch.float) if \
                    output_param is None else output_param
                fused_adam_cuda.adam(p.data,
                                     out_p,
                                     exp_avg,
                                     exp_avg_sq,
                                     grad,
                                     group['lr'],
                                     beta1,
                                     beta2,
                                     group['eps'],
                                     combined_scale,
                                     state['step'],
                                     self.eps_mode,
                                     bias_correction,
                                     group['weight_decay'])
        return loss",_5246.py,68,p.grad is None and grad is None,p.grad is None is grad
https://github.com/openstack/nova/tree/master/nova/tests/unit/scheduler/test_utils.py,"def _test_populate_filter_props(
        self, with_retry=True, force_hosts=None, force_nodes=None,
        no_limits=None,
    ):
        if force_hosts is None:
            force_hosts = []
        if force_nodes is None:
            force_nodes = []
        if with_retry:
            if (
                (len(force_hosts) == 1 and len(force_nodes) <= 1) or
                (len(force_nodes) == 1 and len(force_hosts) <= 1)
            ):
                filter_properties = dict(force_hosts=force_hosts,
                                         force_nodes=force_nodes)
            elif len(force_hosts) > 1 or len(force_nodes) > 1:
                filter_properties = dict(retry=dict(hosts=[]),
                                         force_hosts=force_hosts,
                                         force_nodes=force_nodes)
            else:
                filter_properties = dict(retry=dict(hosts=[]))
        else:
            filter_properties = dict()

        if no_limits:
            fake_limits = None
        else:
            fake_limits = objects.SchedulerLimits(
                vcpu=1, disk_gb=2, memory_mb=3, numa_topology=None)

        selection = objects.Selection(
            service_host=""fake-host"", nodename=""fake-node"", limits=fake_limits)

        utils.populate_filter_properties(filter_properties, selection)

        enable_retry_force_hosts = not force_hosts or len(force_hosts) > 1
        enable_retry_force_nodes = not force_nodes or len(force_nodes) > 1
        if with_retry or enable_retry_force_hosts or enable_retry_force_nodes:
            # So we can check for 2 hosts
            utils.populate_filter_properties(filter_properties, selection)

        if force_hosts:
            expected_limits = None
        elif no_limits:
            expected_limits = {}
        elif isinstance(fake_limits, objects.SchedulerLimits):
            expected_limits = fake_limits.to_dict()
        else:
            expected_limits = fake_limits
        self.assertEqual(expected_limits, filter_properties.get('limits'))

        if (
            with_retry and enable_retry_force_hosts and
            enable_retry_force_nodes
        ):
            self.assertEqual(
                [['fake-host', 'fake-node'], ['fake-host', 'fake-node']],
                filter_properties['retry']['hosts'])
        else:
            self.assertNotIn('retry', filter_properties)",_5524.py,11,len(force_hosts) == 1 and len(force_nodes) <= 1,len(force_hosts) == 1 >= len(force_nodes)
https://github.com/openstack/nova/tree/master/nova/tests/unit/scheduler/test_utils.py,"def _test_populate_filter_props(
        self, with_retry=True, force_hosts=None, force_nodes=None,
        no_limits=None,
    ):
        if force_hosts is None:
            force_hosts = []
        if force_nodes is None:
            force_nodes = []
        if with_retry:
            if (
                (len(force_hosts) == 1 and len(force_nodes) <= 1) or
                (len(force_nodes) == 1 and len(force_hosts) <= 1)
            ):
                filter_properties = dict(force_hosts=force_hosts,
                                         force_nodes=force_nodes)
            elif len(force_hosts) > 1 or len(force_nodes) > 1:
                filter_properties = dict(retry=dict(hosts=[]),
                                         force_hosts=force_hosts,
                                         force_nodes=force_nodes)
            else:
                filter_properties = dict(retry=dict(hosts=[]))
        else:
            filter_properties = dict()

        if no_limits:
            fake_limits = None
        else:
            fake_limits = objects.SchedulerLimits(
                vcpu=1, disk_gb=2, memory_mb=3, numa_topology=None)

        selection = objects.Selection(
            service_host=""fake-host"", nodename=""fake-node"", limits=fake_limits)

        utils.populate_filter_properties(filter_properties, selection)

        enable_retry_force_hosts = not force_hosts or len(force_hosts) > 1
        enable_retry_force_nodes = not force_nodes or len(force_nodes) > 1
        if with_retry or enable_retry_force_hosts or enable_retry_force_nodes:
            # So we can check for 2 hosts
            utils.populate_filter_properties(filter_properties, selection)

        if force_hosts:
            expected_limits = None
        elif no_limits:
            expected_limits = {}
        elif isinstance(fake_limits, objects.SchedulerLimits):
            expected_limits = fake_limits.to_dict()
        else:
            expected_limits = fake_limits
        self.assertEqual(expected_limits, filter_properties.get('limits'))

        if (
            with_retry and enable_retry_force_hosts and
            enable_retry_force_nodes
        ):
            self.assertEqual(
                [['fake-host', 'fake-node'], ['fake-host', 'fake-node']],
                filter_properties['retry']['hosts'])
        else:
            self.assertNotIn('retry', filter_properties)",_5524.py,12,len(force_nodes) == 1 and len(force_hosts) <= 1,len(force_nodes) == 1 >= len(force_hosts)
https://github.com/microsoft/MASS/tree/master/MASS-unsupNMT/src/trainer.py,"def word_dropout(self, x, l):
        """"""
        Randomly drop input words.
        """"""
        if self.params.word_dropout == 0:
            return x, l
        assert 0 < self.params.word_dropout < 1

        # define words to drop
        eos = self.params.eos_index
        assert (x[0] == eos).sum() == l.size(0)
        keep = np.random.rand(x.size(0) - 1, x.size(1)) >= self.params.word_dropout
        keep[0] = 1  # do not drop the start sentence symbol

        sentences = []
        lengths = []
        for i in range(l.size(0)):
            assert x[l[i] - 1, i] == eos
            words = x[:l[i] - 1, i].tolist()
            # randomly drop words from the input
            new_s = [w for j, w in enumerate(words) if keep[j, i]]
            # we need to have at least one word in the sentence (more than the start / end sentence symbols)
            if len(new_s) == 1:
                new_s.append(words[np.random.randint(1, len(words))])
            new_s.append(eos)
            assert len(new_s) >= 3 and new_s[0] == eos and new_s[-1] == eos
            sentences.append(new_s)
            lengths.append(len(new_s))
        # re-construct input
        l2 = torch.LongTensor(lengths)
        x2 = torch.LongTensor(l2.max(), l2.size(0)).fill_(self.params.pad_index)
        for i in range(l2.size(0)):
            x2[:l2[i], i].copy_(torch.LongTensor(sentences[i]))
        return x2, l2",_5655.py,26,len(new_s) >= 3 and new_s[0] == eos and (new_s[-1] == eos),len(new_s) >= 3 and new_s[0] == eos == new_s[-1]
https://github.com/maxpumperla/deep_learning_and_the_game_of_go/tree/master/code/dlgo/corpora/index.py,"def _sequence(game_record):
    """"""Extract game moves from a game record.

    The main sequence includes lots of stuff that is not actual game
    moves.
    """"""
    seq = []
    for item in game_record.get_main_sequence():
        color, move = item.get_move()
        # color == None is entries that are not actual game play
        # move == None is a pass, which in theory we could try to
        # predict, but not yet
        if color is not None and move is not None:
            seq.append((color, move))
    return seq",_5727.py,13,color is not None and move is not None,color is not None is not move
https://github.com/TianhongDai/reinforcement-learning-algorithms/tree/master/rl_utils/env_wrapper/atari_wrapper.py,"def step(self, action):
        obs, reward, done, info = self.env.step(action)
        self.was_real_done = done
        # check current lives, make loss of life terminal,
        # then update lives to handle bonus lives
        lives = self.env.unwrapped.ale.lives()
        if lives < self.lives and lives > 0:
            # for Qbert sometimes we stay in lives == 0 condition for a few frames
            # so it's important to keep lives > 0, so that we only reset once
            # the environment advertises done.
            done = True
        self.lives = lives
        return obs, reward, done, info",_5753.py,7,lives < self.lives and lives > 0,self.lives > lives > 0
https://github.com/saturday06/VRM_Addon_for_Blender/tree/master/io_scene_vrm/editor/glsl_drawer.py,"def draw_func_add(invisibles: bool, only_selections: bool) -> None:
        GlslDrawObj.draw_func_remove()
        GlslDrawObj.draw_objs = [
            obj
            for obj in search.export_objects(invisibles, only_selections)
            if obj.type == ""MESH""
        ]
        if GlslDrawObj.myinstance is None or GlslDrawObj.draw_func is None:
            GlslDrawObj.myinstance = GlslDrawObj()
        GlslDrawObj.build_scene()
        if GlslDrawObj.draw_func is not None:
            GlslDrawObj.draw_func_remove()
        GlslDrawObj.draw_func = bpy.types.SpaceView3D.draw_handler_add(
            GlslDrawObj.myinstance.glsl_draw, (), ""WINDOW"", ""POST_PIXEL""
        )

        if (
            GlslDrawObj.build_mesh_func is not None
            and GlslDrawObj.build_mesh_func in bpy.app.handlers.depsgraph_update_post
        ):
            bpy.app.handlers.depsgraph_update_post.remove(GlslDrawObj.build_mesh_func)
        bpy.app.handlers.depsgraph_update_post.append(GlslDrawObj.build_scene)
        GlslDrawObj.build_mesh_func = bpy.app.handlers.depsgraph_update_post[-1]
        # bpy.app.handlers.frame_change_post.append(build_sub_index)
        bpy.ops.wm.redraw_timer(type=""DRAW"", iterations=1)",_5789.py,18,GlslDrawObj.build_mesh_func is not None and GlslDrawObj.build_mesh_func in bpy.app.handlers.depsgraph_update_post,None is not GlslDrawObj.build_mesh_func in bpy.app.handlers.depsgraph_update_post
https://github.com/oegedijk/explainerdashboard/tree/master/explainerdashboard/dashboard_components/shap_components.py,"def component_callbacks(self, app):
        @app.callback(
            [Output('interaction-dependence-col-'+self.dep_name, 'value'),
             Output('interaction-dependence-index-'+self.dep_name, 'value'),
             Output('interaction-dependence-interact-col-'+self.dep_name, 'value')],
            [Input('interaction-summary-col-'+self.sum_name, 'value'),
             Input('interaction-summary-graph-'+self.sum_name, 'clickData')])
        def update_interact_col_highlight(col, clickdata):
            if clickdata is not None and clickdata['points'][0] is not None:
                if isinstance(clickdata['points'][0]['y'], float): # detailed
                    index = clickdata['points'][0]['text'].split('=')[1].split('<br>')[0]
                    interact_col = clickdata['points'][0]['text'].split('=')[1].split('<br>')[1]                          
                    return (col, index, interact_col)
                elif isinstance(clickdata['points'][0]['y'], str): # aggregate
                    # in aggregate clickdata returns col name -> type==str
                    interact_col = clickdata['points'][0]['y'].split(' ')[1]
                    return (col, dash.no_update, interact_col)
            else:
                return (col, dash.no_update, dash.no_update)
            raise PreventUpdate",_5958.py,9,clickdata is not None and clickdata['points'][0] is not None,clickdata is not None is not clickdata['points'][0]
https://github.com/EasyIME/PIME/tree/master/python/python3/tornado/template.py,"def _parse(
    reader: _TemplateReader,
    template: Template,
    in_block: Optional[str] = None,
    in_loop: Optional[str] = None,
) -> _ChunkList:
    body = _ChunkList([])
    while True:
        # Find next template directive
        curly = 0
        while True:
            curly = reader.find(""{"", curly)
            if curly == -1 or curly + 1 == reader.remaining():
                # EOF
                if in_block:
                    reader.raise_parse_error(
                        ""Missing {%% end %%} block for %s"" % in_block
                    )
                body.chunks.append(
                    _Text(reader.consume(), reader.line, reader.whitespace)
                )
                return body
            # If the first curly brace is not the start of a special token,
            # start searching from the character after it
            if reader[curly + 1] not in (""{"", ""%"", ""#""):
                curly += 1
                continue
            # When there are more than 2 curlies in a row, use the
            # innermost ones.  This is useful when generating languages
            # like latex where curlies are also meaningful
            if (
                curly + 2 < reader.remaining()
                and reader[curly + 1] == ""{""
                and reader[curly + 2] == ""{""
            ):
                curly += 1
                continue
            break

        # Append any text before the special token
        if curly > 0:
            cons = reader.consume(curly)
            body.chunks.append(_Text(cons, reader.line, reader.whitespace))

        start_brace = reader.consume(2)
        line = reader.line

        # Template directives may be escaped as ""{{!"" or ""{%!"".
        # In this case output the braces and consume the ""!"".
        # This is especially useful in conjunction with jquery templates,
        # which also use double braces.
        if reader.remaining() and reader[0] == ""!"":
            reader.consume(1)
            body.chunks.append(_Text(start_brace, line, reader.whitespace))
            continue

        # Comment
        if start_brace == ""{#"":
            end = reader.find(""#}"")
            if end == -1:
                reader.raise_parse_error(""Missing end comment #}"")
            contents = reader.consume(end).strip()
            reader.consume(2)
            continue

        # Expression
        if start_brace == ""{{"":
            end = reader.find(""}}"")
            if end == -1:
                reader.raise_parse_error(""Missing end expression }}"")
            contents = reader.consume(end).strip()
            reader.consume(2)
            if not contents:
                reader.raise_parse_error(""Empty expression"")
            body.chunks.append(_Expression(contents, line))
            continue

        # Block
        assert start_brace == ""{%"", start_brace
        end = reader.find(""%}"")
        if end == -1:
            reader.raise_parse_error(""Missing end block %}"")
        contents = reader.consume(end).strip()
        reader.consume(2)
        if not contents:
            reader.raise_parse_error(""Empty block tag ({% %})"")

        operator, space, suffix = contents.partition("" "")
        suffix = suffix.strip()

        # Intermediate (""else"", ""elif"", etc) blocks
        intermediate_blocks = {
            ""else"": set([""if"", ""for"", ""while"", ""try""]),
            ""elif"": set([""if""]),
            ""except"": set([""try""]),
            ""finally"": set([""try""]),
        }
        allowed_parents = intermediate_blocks.get(operator)
        if allowed_parents is not None:
            if not in_block:
                reader.raise_parse_error(
                    ""%s outside %s block"" % (operator, allowed_parents)
                )
            if in_block not in allowed_parents:
                reader.raise_parse_error(
                    ""%s block cannot be attached to %s block"" % (operator, in_block)
                )
            body.chunks.append(_IntermediateControlBlock(contents, line))
            continue

        # End tag
        elif operator == ""end"":
            if not in_block:
                reader.raise_parse_error(""Extra {% end %} block"")
            return body

        elif operator in (
            ""extends"",
            ""include"",
            ""set"",
            ""import"",
            ""from"",
            ""comment"",
            ""autoescape"",
            ""whitespace"",
            ""raw"",
            ""module"",
        ):
            if operator == ""comment"":
                continue
            if operator == ""extends"":
                suffix = suffix.strip('""').strip(""'"")
                if not suffix:
                    reader.raise_parse_error(""extends missing file path"")
                block = _ExtendsBlock(suffix)  # type: _Node
            elif operator in (""import"", ""from""):
                if not suffix:
                    reader.raise_parse_error(""import missing statement"")
                block = _Statement(contents, line)
            elif operator == ""include"":
                suffix = suffix.strip('""').strip(""'"")
                if not suffix:
                    reader.raise_parse_error(""include missing file path"")
                block = _IncludeBlock(suffix, reader, line)
            elif operator == ""set"":
                if not suffix:
                    reader.raise_parse_error(""set missing statement"")
                block = _Statement(suffix, line)
            elif operator == ""autoescape"":
                fn = suffix.strip()  # type: Optional[str]
                if fn == ""None"":
                    fn = None
                template.autoescape = fn
                continue
            elif operator == ""whitespace"":
                mode = suffix.strip()
                # Validate the selected mode
                filter_whitespace(mode, """")
                reader.whitespace = mode
                continue
            elif operator == ""raw"":
                block = _Expression(suffix, line, raw=True)
            elif operator == ""module"":
                block = _Module(suffix, line)
            body.chunks.append(block)
            continue

        elif operator in (""apply"", ""block"", ""try"", ""if"", ""for"", ""while""):
            # parse inner body recursively
            if operator in (""for"", ""while""):
                block_body = _parse(reader, template, operator, operator)
            elif operator == ""apply"":
                # apply creates a nested function so syntactically it's not
                # in the loop.
                block_body = _parse(reader, template, operator, None)
            else:
                block_body = _parse(reader, template, operator, in_loop)

            if operator == ""apply"":
                if not suffix:
                    reader.raise_parse_error(""apply missing method name"")
                block = _ApplyBlock(suffix, line, block_body)
            elif operator == ""block"":
                if not suffix:
                    reader.raise_parse_error(""block missing name"")
                block = _NamedBlock(suffix, block_body, template, line)
            else:
                block = _ControlBlock(contents, line, block_body)
            body.chunks.append(block)
            continue

        elif operator in (""break"", ""continue""):
            if not in_loop:
                reader.raise_parse_error(
                    ""%s outside %s block"" % (operator, set([""for"", ""while""]))
                )
            body.chunks.append(_Statement(contents, line))
            continue

        else:
            reader.raise_parse_error(""unknown operator: %r"" % operator)",_6358.py,32,curly + 2 < reader.remaining() and reader[curly + 1] == '{' and (reader[curly + 2] == '{'),curly + 2 < reader.remaining() and reader[curly + 1] == '{' == reader[curly + 2]
https://github.com/ChenRocks/UNITER/tree/master/data/vcr.py,"def __init__(self, txt_db, img_db_gt=None, img_db=None):
        assert not (img_db_gt is None and img_db is None),\
            ""img_db_gt and img_db cannot all be None""
        assert isinstance(txt_db, VcrTxtTokLmdb)
        assert img_db_gt is None or isinstance(img_db_gt, DetectFeatLmdb)
        assert img_db is None or isinstance(img_db, DetectFeatLmdb)
        self.txt_db = txt_db
        self.img_db = img_db
        self.img_db_gt = img_db_gt
        self.task = self.txt_db.task
        txt_lens, self.ids = get_ids_and_lens(txt_db)

        txt2img = txt_db.txt2img

        if self.img_db and self.img_db_gt:
            self.lens = [tl+self.img_db_gt.name2nbb[txt2img[id_][0]] +
                         self.img_db.name2nbb[txt2img[id_][1]]
                         for tl, id_ in zip(txt_lens, self.ids)]
        elif self.img_db:
            self.lens = [tl+self.img_db.name2nbb[txt2img[id_][1]]
                         for tl, id_ in zip(txt_lens, self.ids)]
        else:
            self.lens = [tl+self.img_db_gt.name2nbb[txt2img[id_][0]]
                         for tl, id_ in zip(txt_lens, self.ids)]",_6632.py,2,img_db_gt is None and img_db is None,img_db_gt is None is img_db
https://github.com/zenodo/zenodo/tree/master/zenodo/modules/records/views.py,"def is_embargoed(embargo_date, accessright=None):
    """"""Test if date is still embargoed (according to UTC date.""""""
    if accessright is not None and accessright != AccessRight.EMBARGOED:
        return False
    if embargo_date is not None:
        return AccessRight.is_embargoed(embargo_date)
    return False",_6633.py,3,accessright is not None and accessright != AccessRight.EMBARGOED,None is not accessright != AccessRight.EMBARGOED
https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_generate_proposals_v2_op.py,"def proposal_for_one_image(
    im_shape,
    all_anchors,
    variances,
    bbox_deltas,
    scores,
    pre_nms_topN,
    post_nms_topN,
    nms_thresh,
    min_size,
    eta,
    pixel_offset,
):
    # Transpose and reshape predicted bbox transformations to get them
    # into the same order as the anchors:
    #   - bbox deltas will be (4 * A, H, W) format from conv output
    #   - transpose to (H, W, 4 * A)
    #   - reshape to (H * W * A, 4) where rows are ordered by (H, W, A)
    #     in slowest to fastest order to match the enumerated anchors
    bbox_deltas = bbox_deltas.transpose((1, 2, 0)).reshape(-1, 4)
    all_anchors = all_anchors.reshape(-1, 4)
    variances = variances.reshape(-1, 4)
    # Same story for the scores:
    #   - scores are (A, H, W) format from conv output
    #   - transpose to (H, W, A)
    #   - reshape to (H * W * A, 1) where rows are ordered by (H, W, A)
    #     to match the order of anchors and bbox_deltas
    scores = scores.transpose((1, 2, 0)).reshape(-1, 1)

    # sort all (proposal, score) pairs by score from highest to lowest
    # take top pre_nms_topN (e.g. 6000)
    if pre_nms_topN <= 0 or pre_nms_topN >= len(scores):
        order = np.argsort(-scores.squeeze())
    else:
        # Avoid sorting possibly large arrays;
        # First partition to get top K unsorted
        # and then sort just those
        inds = np.argpartition(-scores.squeeze(), pre_nms_topN)[:pre_nms_topN]
        order = np.argsort(-scores[inds].squeeze())
        order = inds[order]
    scores = scores[order, :]
    bbox_deltas = bbox_deltas[order, :]
    all_anchors = all_anchors[order, :]
    proposals = box_coder(all_anchors, bbox_deltas, variances, pixel_offset)
    # clip proposals to image (may result in proposals with zero area
    # that will be removed in the next step)
    proposals = clip_tiled_boxes(proposals, im_shape, pixel_offset)
    # remove predicted boxes with height or width < min_size
    keep = filter_boxes(proposals, min_size, im_shape, pixel_offset)
    if len(keep) == 0:
        proposals = np.zeros((1, 4)).astype('float32')
        scores = np.zeros((1, 1)).astype('float32')
        return proposals, scores
    proposals = proposals[keep, :]
    scores = scores[keep, :]

    # apply loose nms (e.g. threshold = 0.7)
    # take post_nms_topN (e.g. 1000)
    # return the top proposals
    if nms_thresh > 0:
        keep = nms(
            boxes=proposals,
            scores=scores,
            nms_threshold=nms_thresh,
            eta=eta,
            pixel_offset=pixel_offset,
        )
        if post_nms_topN > 0 and post_nms_topN < len(keep):
            keep = keep[:post_nms_topN]
        proposals = proposals[keep, :]
        scores = scores[keep, :]

    return proposals, scores",_6634.py,68,post_nms_topN > 0 and post_nms_topN < len(keep),0 < post_nms_topN < len(keep)
https://github.com/dmlc/gluon-nlp/tree/master/scripts/machine_translation/train_transformer.py,"def load_dataset_with_cache(src_corpus_path: str,
                            tgt_corpus_path: str,
                            src_tokenizer: BaseTokenizerWithVocab,
                            tgt_tokenizer: BaseTokenizerWithVocab,
                            overwrite_cache: bool,
                            local_rank: int,
                            max_src_length: int = None,
                            max_tgt_length: int = None,
                            pretokenized=True):
    src_md5sum = md5sum(src_corpus_path)
    tgt_md5sum = md5sum(tgt_corpus_path)
    cache_filepath = os.path.join(CACHE_PATH,
                                  '{}_{}_{}_{}.cache.npz'.format(src_md5sum[:6],
                                                                 tgt_md5sum[:6],
                                                                 max_src_length,
                                                                 max_tgt_length))
    if os.path.exists(cache_filepath) and not overwrite_cache:
        if local_rank == 0:
            logging.info('Load cache from {}'.format(cache_filepath))
        npz_data = np.load(cache_filepath, allow_pickle=True)
        src_data, tgt_data = npz_data['src_data'][:], npz_data['tgt_data'][:]
    else:
        assert src_tokenizer.vocab.eos_id is not None,\
            'You will need to add the EOS token to the vocabulary used in the tokenizer of ' \
            'the source language.'
        assert tgt_tokenizer.vocab.bos_id is not None and tgt_tokenizer.vocab.eos_id is not None, \
            'You will need to add both the BOS token and the EOS tokens to the vocabulary used ' \
            'in the tokenizer of the target language.'
        src_data = []
        tgt_data = []
        # TODO(sxjscience) Optimize the speed of converting to cache
        with open(src_corpus_path) as f:
            for line in f:
                if pretokenized:
                    src_tokens = src_tokenizer.vocab[line.strip().split()]
                else:
                    src_tokens = src_tokenizer.encode(line.strip(), output_type=int)
                if max_src_length is not None:
                    src_tokens = src_tokens[:max_src_length]
                sample = np.array(src_tokens + [src_tokenizer.vocab.eos_id], dtype=np.int32)
                src_data.append(sample)
        with open(tgt_corpus_path) as f:
            for line in f:
                if pretokenized:
                    tgt_tokens = tgt_tokenizer.vocab[line.strip().split()]
                else:
                    tgt_tokens = tgt_tokenizer.encode(line.strip(), output_type=int)
                if max_tgt_length is not None:
                    tgt_tokens = tgt_tokens[:max_tgt_length]
                sample = np.array([tgt_tokenizer.vocab.bos_id] +
                                  tgt_tokens +
                                  [tgt_tokenizer.vocab.eos_id], dtype=np.int32)
                tgt_data.append(sample)
        src_data = np.array(src_data)
        tgt_data = np.array(tgt_data)
        np.savez(cache_filepath, src_data=src_data, tgt_data=tgt_data)
    return src_data, tgt_data",_6661.py,26,tgt_tokenizer.vocab.bos_id is not None and tgt_tokenizer.vocab.eos_id is not None,tgt_tokenizer.vocab.bos_id is not None is not tgt_tokenizer.vocab.eos_id
https://github.com/tensorflow/transform/tree/master/tensorflow_transform/beam/analyzer_impls.py,"def expand(self, inputs):
    if self._top_k is not None and self._top_k < 0:
      raise ValueError('top_k for VocabularyImpl should be >= 0 or None, got '
                       '{}.'.format(self._top_k))
    if self._frequency_threshold is not None and self._frequency_threshold < 0:
      raise ValueError(
          'frequency_threshold for VocabularyImpl should be >= 0 or None, '
          'got {}.'.format(self._frequency_threshold))
    if self._coverage_top_k is not None and self._coverage_top_k < 0:
      raise ValueError('coverage_top_k for VocabularyImpl should be >= 0 or '
                       'None, got {}.'.format(self._coverage_top_k))
    if (self._coverage_frequency_threshold is not None and
        self._coverage_frequency_threshold < 0):
      raise ValueError(
          'coverage_frequency_threshold for VocabularyImpl should be >= 0 or '
          'None, got {}.'.format(self._coverage_frequency_threshold))
    pcoll, = inputs

    result = (
        pcoll
        | 'ApplyThresholdsAndTopK' >> (
            _ApplyThresholdsAndTopK(  # pylint: disable=no-value-for-parameter
                self._frequency_threshold, self._top_k, self._input_dtype,
                self._informativeness_threshold, None)))

    if self._key_fn:
      # Note: current APIs do not allow for specifying a coverage
      # informativeness threshold.
      coverage_counts = (
          pcoll | 'ApplyCoverageThresholdAndTopK' >> (
              _ApplyThresholdsAndTopK(  # pylint: disable=no-value-for-parameter
                  self._coverage_frequency_threshold, self._coverage_top_k,
                  self._input_dtype, self._coverage_informativeness_threshold,
                  self._key_fn)))

      result = ((result, coverage_counts)
                | 'MergeStandardAndCoverageArms' >> beam.Flatten()
                | 'RemoveDuplicates' >> beam.Distinct())

    return result",_6690.py,2,self._top_k is not None and self._top_k < 0,None is not self._top_k < 0
https://github.com/tensorflow/transform/tree/master/tensorflow_transform/beam/analyzer_impls.py,"def expand(self, inputs):
    if self._top_k is not None and self._top_k < 0:
      raise ValueError('top_k for VocabularyImpl should be >= 0 or None, got '
                       '{}.'.format(self._top_k))
    if self._frequency_threshold is not None and self._frequency_threshold < 0:
      raise ValueError(
          'frequency_threshold for VocabularyImpl should be >= 0 or None, '
          'got {}.'.format(self._frequency_threshold))
    if self._coverage_top_k is not None and self._coverage_top_k < 0:
      raise ValueError('coverage_top_k for VocabularyImpl should be >= 0 or '
                       'None, got {}.'.format(self._coverage_top_k))
    if (self._coverage_frequency_threshold is not None and
        self._coverage_frequency_threshold < 0):
      raise ValueError(
          'coverage_frequency_threshold for VocabularyImpl should be >= 0 or '
          'None, got {}.'.format(self._coverage_frequency_threshold))
    pcoll, = inputs

    result = (
        pcoll
        | 'ApplyThresholdsAndTopK' >> (
            _ApplyThresholdsAndTopK(  # pylint: disable=no-value-for-parameter
                self._frequency_threshold, self._top_k, self._input_dtype,
                self._informativeness_threshold, None)))

    if self._key_fn:
      # Note: current APIs do not allow for specifying a coverage
      # informativeness threshold.
      coverage_counts = (
          pcoll | 'ApplyCoverageThresholdAndTopK' >> (
              _ApplyThresholdsAndTopK(  # pylint: disable=no-value-for-parameter
                  self._coverage_frequency_threshold, self._coverage_top_k,
                  self._input_dtype, self._coverage_informativeness_threshold,
                  self._key_fn)))

      result = ((result, coverage_counts)
                | 'MergeStandardAndCoverageArms' >> beam.Flatten()
                | 'RemoveDuplicates' >> beam.Distinct())

    return result",_6690.py,5,self._frequency_threshold is not None and self._frequency_threshold < 0,None is not self._frequency_threshold < 0
https://github.com/tensorflow/transform/tree/master/tensorflow_transform/beam/analyzer_impls.py,"def expand(self, inputs):
    if self._top_k is not None and self._top_k < 0:
      raise ValueError('top_k for VocabularyImpl should be >= 0 or None, got '
                       '{}.'.format(self._top_k))
    if self._frequency_threshold is not None and self._frequency_threshold < 0:
      raise ValueError(
          'frequency_threshold for VocabularyImpl should be >= 0 or None, '
          'got {}.'.format(self._frequency_threshold))
    if self._coverage_top_k is not None and self._coverage_top_k < 0:
      raise ValueError('coverage_top_k for VocabularyImpl should be >= 0 or '
                       'None, got {}.'.format(self._coverage_top_k))
    if (self._coverage_frequency_threshold is not None and
        self._coverage_frequency_threshold < 0):
      raise ValueError(
          'coverage_frequency_threshold for VocabularyImpl should be >= 0 or '
          'None, got {}.'.format(self._coverage_frequency_threshold))
    pcoll, = inputs

    result = (
        pcoll
        | 'ApplyThresholdsAndTopK' >> (
            _ApplyThresholdsAndTopK(  # pylint: disable=no-value-for-parameter
                self._frequency_threshold, self._top_k, self._input_dtype,
                self._informativeness_threshold, None)))

    if self._key_fn:
      # Note: current APIs do not allow for specifying a coverage
      # informativeness threshold.
      coverage_counts = (
          pcoll | 'ApplyCoverageThresholdAndTopK' >> (
              _ApplyThresholdsAndTopK(  # pylint: disable=no-value-for-parameter
                  self._coverage_frequency_threshold, self._coverage_top_k,
                  self._input_dtype, self._coverage_informativeness_threshold,
                  self._key_fn)))

      result = ((result, coverage_counts)
                | 'MergeStandardAndCoverageArms' >> beam.Flatten()
                | 'RemoveDuplicates' >> beam.Distinct())

    return result",_6690.py,9,self._coverage_top_k is not None and self._coverage_top_k < 0,None is not self._coverage_top_k < 0
https://github.com/tensorflow/transform/tree/master/tensorflow_transform/beam/analyzer_impls.py,"def expand(self, inputs):
    if self._top_k is not None and self._top_k < 0:
      raise ValueError('top_k for VocabularyImpl should be >= 0 or None, got '
                       '{}.'.format(self._top_k))
    if self._frequency_threshold is not None and self._frequency_threshold < 0:
      raise ValueError(
          'frequency_threshold for VocabularyImpl should be >= 0 or None, '
          'got {}.'.format(self._frequency_threshold))
    if self._coverage_top_k is not None and self._coverage_top_k < 0:
      raise ValueError('coverage_top_k for VocabularyImpl should be >= 0 or '
                       'None, got {}.'.format(self._coverage_top_k))
    if (self._coverage_frequency_threshold is not None and
        self._coverage_frequency_threshold < 0):
      raise ValueError(
          'coverage_frequency_threshold for VocabularyImpl should be >= 0 or '
          'None, got {}.'.format(self._coverage_frequency_threshold))
    pcoll, = inputs

    result = (
        pcoll
        | 'ApplyThresholdsAndTopK' >> (
            _ApplyThresholdsAndTopK(  # pylint: disable=no-value-for-parameter
                self._frequency_threshold, self._top_k, self._input_dtype,
                self._informativeness_threshold, None)))

    if self._key_fn:
      # Note: current APIs do not allow for specifying a coverage
      # informativeness threshold.
      coverage_counts = (
          pcoll | 'ApplyCoverageThresholdAndTopK' >> (
              _ApplyThresholdsAndTopK(  # pylint: disable=no-value-for-parameter
                  self._coverage_frequency_threshold, self._coverage_top_k,
                  self._input_dtype, self._coverage_informativeness_threshold,
                  self._key_fn)))

      result = ((result, coverage_counts)
                | 'MergeStandardAndCoverageArms' >> beam.Flatten()
                | 'RemoveDuplicates' >> beam.Distinct())

    return result",_6690.py,12,self._coverage_frequency_threshold is not None and self._coverage_frequency_threshold < 0,None is not self._coverage_frequency_threshold < 0
https://github.com/great-expectations/great_expectations/tree/master/great_expectations/cli/toolkit.py,"def confirm_proceed_or_exit(
    confirm_prompt: str = ""Would you like to proceed?"",
    continuation_message: str = ""Ok, exiting now. You can always read more at https://docs.greatexpectations.io/ !"",
    exit_on_no: bool = True,
    exit_code: int = 0,
    data_context: Optional[DataContext] = None,
    usage_stats_event: Optional[str] = None,
) -> bool:
    """"""
    Every CLI command that starts a potentially lengthy (>1 sec) computation
    or modifies some resources (e.g., edits the config file, adds objects
    to the stores) must follow this pattern:
    1. Explain which resources will be created/modified/deleted
    2. Use this method to ask for user's confirmation

    The goal of this standardization is for the users to expect consistency -
    if you saw one command, you know what to expect from all others.

    If the user does not confirm, the program should exit. The purpose of the exit_on_no parameter is to provide
    the option to perform cleanup actions before exiting outside of the function.
    """"""
    confirm_prompt_colorized = cli_colorize_string(confirm_prompt)
    continuation_message_colorized = cli_colorize_string(continuation_message)
    if not click.confirm(confirm_prompt_colorized, default=True):
        if exit_on_no:
            cli_message(string=continuation_message_colorized)
            cli_message(string=continuation_message_colorized)
            if (usage_stats_event is not None) and (data_context is not None):
                # noinspection PyBroadException
                try:
                    send_usage_message(
                        data_context=data_context,
                        event=usage_stats_event,
                        event_payload={""cancelled"": True},
                        success=True,
                    )
                except Exception as e:
                    # Don't fail on usage stats
                    logger.debug(f""Something went wrong when sending usage stats: {e}"")
                    pass
            sys.exit(exit_code)
        else:
            return False
    return True",_6809.py,28,usage_stats_event is not None and data_context is not None,usage_stats_event is not None is not data_context
https://github.com/Ultimaker/Cura/tree/master/cura/Scene/ConvexHullDecorator.py,"def setNode(self, node: ""SceneNode"") -> None:
        previous_node = self._node
        # Disconnect from previous node signals
        if previous_node is not None and node is not previous_node:
            previous_node.boundingBoxChanged.disconnect(self._onChanged)

        super().setNode(node)

        node.boundingBoxChanged.connect(self._onChanged)

        per_object_stack = node.callDecoration(""getStack"")
        if per_object_stack:
            per_object_stack.propertyChanged.connect(self._onSettingValueChanged)

        self._onChanged()",_6961.py,4,previous_node is not None and node is not previous_node,None is not previous_node is not node
https://github.com/Qianlitp/WatchAD/tree/master/modules/detect/DetectBase.py,"def init(self, log=None, krb=None):
        if log is None and krb is None:
            raise NoDataInitEvent()
        if log and krb:
            raise NoDataInitEvent()
        if log:
            assert isinstance(log, Log)
            self.domain = ""."".join(log.dc_computer_name.split(""."")[1:])
        else:
            assert isinstance(krb, Kerberos)
            self.domain = krb.req.req_body.realm
        self.log = log
        self.krb = krb
        self._format_domain()",_6972.py,2,log is None and krb is None,log is None is krb
https://github.com/microsoft/fastformers/tree/master/src/transformers/modeling_tf_openai.py,"def call(
        self,
        inputs,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        output_attentions=None,
        output_hidden_states=None,
        training=False,
    ):
        if isinstance(inputs, (tuple, list)):
            input_ids = inputs[0]
            attention_mask = inputs[1] if len(inputs) > 1 else attention_mask
            token_type_ids = inputs[2] if len(inputs) > 2 else token_type_ids
            position_ids = inputs[3] if len(inputs) > 3 else position_ids
            head_mask = inputs[4] if len(inputs) > 4 else head_mask
            inputs_embeds = inputs[5] if len(inputs) > 5 else inputs_embeds
            output_attentions = inputs[6] if len(inputs) > 6 else output_attentions
            output_hidden_states = inputs[7] if len(inputs) > 7 else output_hidden_states
            assert len(inputs) <= 8, ""Too many inputs.""
        elif isinstance(inputs, (dict, BatchEncoding)):
            input_ids = inputs.get(""input_ids"")
            attention_mask = inputs.get(""attention_mask"", attention_mask)
            token_type_ids = inputs.get(""token_type_ids"", token_type_ids)
            position_ids = inputs.get(""position_ids"", position_ids)
            head_mask = inputs.get(""head_mask"", head_mask)
            inputs_embeds = inputs.get(""inputs_embeds"", inputs_embeds)
            output_attentions = inputs.get(""output_attentions"", output_attentions)
            output_hidden_states = inputs.get(""output_hidden_states"", output_hidden_states)
            assert len(inputs) <= 8, ""Too many inputs.""
        else:
            input_ids = inputs

        output_attentions = output_attentions if output_attentions is not None else self.output_attentions
        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.output_hidden_states

        if input_ids is not None and inputs_embeds is not None:
            raise ValueError(""You cannot specify both input_ids and inputs_embeds at the same time"")
        elif input_ids is not None:
            input_shape = shape_list(input_ids)
            input_ids = tf.reshape(input_ids, [-1, input_shape[-1]])
        elif inputs_embeds is not None:
            input_shape = shape_list(inputs_embeds)[:-1]
        else:
            raise ValueError(""You have to specify either input_ids or inputs_embeds"")

        if position_ids is None:
            position_ids = tf.range(input_shape[-1], dtype=tf.int32)[tf.newaxis, :]

        if attention_mask is not None:
            # We create a 3D attention mask from a 2D tensor mask.
            # Sizes are [batch_size, 1, 1, to_seq_length]
            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]
            # this attention mask is more simple than the triangular masking of causal attention
            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.
            attention_mask = attention_mask[:, tf.newaxis, tf.newaxis, :]

            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
            # masked positions, this operation will create a tensor which is 0.0 for
            # positions we want to attend and -10000.0 for masked positions.
            # Since we are adding it to the raw scores before the softmax, this is
            # effectively the same as removing these entirely.

            attention_mask = tf.cast(attention_mask, tf.float32)
            attention_mask = (1.0 - attention_mask) * -10000.0
        else:
            attention_mask = None

        # Prepare head mask if needed
        # 1.0 in head_mask indicate we keep the head
        # attention_probs has shape bsz x n_heads x N x N
        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]
        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]
        if head_mask is not None:
            raise NotImplementedError
        else:
            head_mask = [None] * self.num_hidden_layers
            # head_mask = tf.constant([0] * self.num_hidden_layers)

        position_ids = tf.reshape(position_ids, [-1, shape_list(position_ids)[-1]])

        if inputs_embeds is None:
            inputs_embeds = self.tokens_embed(input_ids, mode=""embedding"")
        position_embeds = self.positions_embed(position_ids)
        if token_type_ids is not None:
            token_type_ids = tf.reshape(token_type_ids, [-1, shape_list(token_type_ids)[-1]])
            token_type_embeds = self.tokens_embed(token_type_ids, mode=""embedding"")
        else:
            token_type_embeds = 0
        hidden_states = inputs_embeds + position_embeds + token_type_embeds
        hidden_states = self.drop(hidden_states, training=training)

        output_shape = input_shape + [shape_list(hidden_states)[-1]]

        all_attentions = []
        all_hidden_states = ()
        for i, block in enumerate(self.h):
            if cast_bool_to_primitive(output_hidden_states) is True:
                all_hidden_states = all_hidden_states + (tf.reshape(hidden_states, output_shape),)

            outputs = block([hidden_states, attention_mask, head_mask[i], output_attentions], training=training)
            hidden_states = outputs[0]
            if cast_bool_to_primitive(output_attentions) is True:
                all_attentions.append(outputs[1])

        hidden_states = tf.reshape(hidden_states, output_shape)
        # Add last hidden state
        if cast_bool_to_primitive(output_hidden_states) is True:
            all_hidden_states = all_hidden_states + (hidden_states,)

        outputs = (hidden_states,)
        if cast_bool_to_primitive(output_hidden_states) is True:
            outputs = outputs + (all_hidden_states,)
        if cast_bool_to_primitive(output_attentions) is True:
            # let the number of heads free (-1) so we can extract attention even after head pruning
            attention_output_shape = input_shape[:-1] + [-1] + shape_list(all_attentions[0])[-2:]
            all_attentions = tuple(tf.reshape(t, attention_output_shape) for t in all_attentions)
            outputs = outputs + (all_attentions,)
        return outputs",_7021.py,39,input_ids is not None and inputs_embeds is not None,input_ids is not None is not inputs_embeds
https://github.com/vmware/pyvmomi/tree/master/pyVim/sso.py,"def __init__(self, *args, **kwargs):
        '''
        Initializer.  See httplib.HTTPConnection for other arguments
        '''
        tmpKwargs = {}
        httpConn = six.moves.http_client.HTTPConnection
        for key in httpConn.__init__.__code__.co_varnames:
            if key in kwargs and key != 'self':
                tmpKwargs[key] = kwargs[key]
        self.host = kwargs.pop('host')
        six.moves.http_client.HTTPConnection.__init__(self, *args, **tmpKwargs)",_7045.py,8,key in kwargs and key != 'self','self' != key in kwargs
https://github.com/ring04h/weakfilescan/tree/master/libs/requests/packages/chardet/chardistribution.py,"def get_order(self, aBuf):
        # for sjis encoding, we are interested
        #   first  byte range: 0x81 -- 0x9f , 0xe0 -- 0xfe
        #   second byte range: 0x40 -- 0x7e,  0x81 -- oxfe
        # no validation needed here. State machine has done that
        first_char, second_char = wrap_ord(aBuf[0]), wrap_ord(aBuf[1])
        if (first_char >= 0x81) and (first_char <= 0x9F):
            order = 188 * (first_char - 0x81)
        elif (first_char >= 0xE0) and (first_char <= 0xEF):
            order = 188 * (first_char - 0xE0 + 31)
        else:
            return -1
        order = order + second_char - 0x40
        if second_char > 0x7F:
            order = -1
        return order",_7162.py,7,first_char >= 129 and first_char <= 159,129 <= first_char <= 159
https://github.com/ring04h/weakfilescan/tree/master/libs/requests/packages/chardet/chardistribution.py,"def get_order(self, aBuf):
        # for sjis encoding, we are interested
        #   first  byte range: 0x81 -- 0x9f , 0xe0 -- 0xfe
        #   second byte range: 0x40 -- 0x7e,  0x81 -- oxfe
        # no validation needed here. State machine has done that
        first_char, second_char = wrap_ord(aBuf[0]), wrap_ord(aBuf[1])
        if (first_char >= 0x81) and (first_char <= 0x9F):
            order = 188 * (first_char - 0x81)
        elif (first_char >= 0xE0) and (first_char <= 0xEF):
            order = 188 * (first_char - 0xE0 + 31)
        else:
            return -1
        order = order + second_char - 0x40
        if second_char > 0x7F:
            order = -1
        return order",_7162.py,9,first_char >= 224 and first_char <= 239,224 <= first_char <= 239
https://github.com/polakowo/vectorbt/tree/master/vectorbt/generic/accessors.py,"def sum(self, group_by: tp.GroupByLike = None, wrap_kwargs: tp.KwargsLike = None) -> tp.MaybeSeries:
        """"""Return sum of non-NaN elements.""""""
        wrap_kwargs = merge_dicts(dict(name_or_index='sum'), wrap_kwargs)
        if self.wrapper.grouper.is_grouped(group_by=group_by):
            return self.reduce(nb.sum_reduce_nb, group_by=group_by, flatten=True, wrap_kwargs=wrap_kwargs)

        arr = self.to_2d_array()
        if arr.dtype != int and arr.dtype != float:
            # bottleneck can't consume other than that
            _nansum = np.nansum
        else:
            _nansum = nansum
        return self.wrapper.wrap_reduced(_nansum(arr, axis=0), group_by=False, **wrap_kwargs)",_7230.py,8,arr.dtype != int and arr.dtype != float,int != arr.dtype != float
https://github.com/lyft/l5kit/tree/master/l5kit/l5kit/visualization/gif.py,"def write_gif(
        output_filepath: str,
        images: Iterable[np.ndarray],
        resolution: Tuple[int, int],
        fps: float = 24.0,
        loop: int = 0,
        interpolation: int = cv2.INTER_CUBIC,
) -> None:
    """"""Writes input RGB images to given output gif filepath using imageio. It resizes images
    if necessary using given interpolation (default=``cv2.INTER_CUBIC``).

    Arguments:
        output_filepath (str): output filepath, should end in .gif
        images (Iterable[np.ndarray]): a list or other iterable of images.
        resolution (Tuple[int, int]): desired resolution, e.g. (512, 512)

    Keyword Arguments:
        fps (float): Frames per second (default: {24.0})
        loop (int): 0 means loop forever, any other number loops the GIF that many times (default: {0})
        interpolation (int): Interpolation to be used when resizing (default: {cv2.INTER_CUBIC})
    """"""

    duration = 1 / fps
    resized_images = []

    for img in images:
        # Go from C,0,1 to 0,1,C ordering
        if len(img.shape) == 3 and img.shape[0] == 3:
            img = img.transpose(1, 2, 0)

        if img.shape[:2] != resolution:
            img = cv2.resize(img, resolution, interpolation=interpolation)
        resized_images.append(img)

    imageio.mimwrite(output_filepath, resized_images, duration=duration, loop=loop)",_7344.py,28,len(img.shape) == 3 and img.shape[0] == 3,len(img.shape) == 3 == img.shape[0]
https://github.com/openstack/neutron/tree/master/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovsdb_monitor.py,"def match_fn(self, event, row, old):
        # This event should catch only those events from ports that are
        # ""virtual"" or have been ""virtual"". The second happens when all virtual
        # parent are disassociated; in the same transaction the
        # ""virtual-parents"" list is removed from ""options"" and the type is set
        # to """".
        if (row.type != ovn_const.PB_TYPE_VIRTUAL and
                getattr(old, 'type', None) != ovn_const.PB_TYPE_VIRTUAL):
            return False

        virtual_parents = (row.options or {}).get(
            ovn_const.LSP_OPTIONS_VIRTUAL_PARENTS_KEY)
        old_virtual_parents = getattr(old, 'options', {}).get(
            ovn_const.LSP_OPTIONS_VIRTUAL_PARENTS_KEY)
        chassis = row.chassis
        old_chassis = getattr(old, 'chassis', [])

        if virtual_parents and chassis != old_chassis:
            # That happens when the chassis is assigned (VIP is first detected
            # in a port) or changed (the VIP changes of assigned port and
            # host).
            return True

        if not virtual_parents and old_virtual_parents:
            # All virtual parent ports are removed, the VIP is unbound.
            return True
        return False",_7398.py,7,"row.type != ovn_const.PB_TYPE_VIRTUAL and getattr(old, 'type', None) != ovn_const.PB_TYPE_VIRTUAL","row.type != ovn_const.PB_TYPE_VIRTUAL != getattr(old, 'type', None)"
https://github.com/emilianavt/OpenSeeFace/tree/master//tracker.py,"def adjust_3d(self):
        if self.conf < 0.4 or self.pnp_error > 300:
            return

        if self.tracker.model_type != -1 and not self.tracker.static_model:
            max_runs = 1
            eligible = np.delete(np.arange(0, 66), [30])
            changed_any = False
            update_type = -1
            d_o = np.ones((66,))
            d_c = np.ones((66,))
            for runs in range(max_runs):
                r = 1.0 + np.random.random_sample((66,3)) * 0.02 - 0.01
                r[30, :] = 1.0
                if self.euler[0] > -165 and self.euler[0] < 145:
                    continue
                elif self.euler[1] > -10 and self.euler[1] < 20:
                    r[:, 2] = 1.0
                    update_type = 0
                else:
                    r[:, 0:2] = 1.0
                    if self.euler[2] > 120 or self.euler[2] < 60:
                        continue
                    # Enable only one side of the points, depending on direction
                    elif self.euler[1] < -10:
                        update_type = 1
                        r[[0, 1, 2, 3, 4, 5, 6, 7, 17, 18, 19, 20, 21, 31, 32, 36, 37, 38, 39, 40, 41, 48, 49, 56, 57, 58, 59, 65], 2] = 1.0
                        eligible = [8, 9, 10, 11, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 33, 34, 35, 42, 43, 44, 45, 46, 47, 50, 51, 52, 53, 54, 55, 60, 61, 62, 63, 64]
                    else:
                        update_type = 1
                        r[[9, 10, 11, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 34, 35, 42, 43, 44, 45, 46, 47, 51, 52, 53, 54, 61, 62, 63], 2] = 1.0
                        eligible = [0, 1, 2, 3, 4, 5, 6, 7, 8, 17, 18, 19, 20, 21, 27, 28, 29, 31, 32, 33, 36, 37, 38, 39, 40, 41, 48, 49, 50, 55, 56, 57, 58, 59, 60, 64, 65]

                if self.limit_3d_adjustment:
                    eligible = np.nonzero(self.update_counts[:, update_type] < self.update_counts[:, abs(update_type - 1)] + self.update_count_delta)[0]
                    if eligible.shape[0] <= 0:
                        break

                if runs == 0:
                    updated = copy.copy(self.face_3d[0:66])
                    o_projected = np.ones((66,2))
                    o_projected[eligible] = np.squeeze(np.array(cv2.projectPoints(self.face_3d[eligible], self.rotation, self.translation, self.tracker.camera, self.tracker.dist_coeffs)[0]), 1)
                c = updated * r
                c_projected = np.zeros((66,2))
                c_projected[eligible] = np.squeeze(np.array(cv2.projectPoints(c[eligible], self.rotation, self.translation, self.tracker.camera, self.tracker.dist_coeffs)[0]), 1)
                changed = False

                d_o[eligible] = np.linalg.norm(o_projected[eligible] - self.lms[eligible, 0:2], axis=1)
                d_c[eligible] = np.linalg.norm(c_projected[eligible] - self.lms[eligible, 0:2], axis=1)
                indices = np.nonzero(d_c < d_o)[0]
                if indices.shape[0] > 0:
                    if self.limit_3d_adjustment:
                        indices = np.intersect1d(indices, eligible)
                    if indices.shape[0] > 0:
                        self.update_counts[indices, update_type] += 1
                        updated[indices] = c[indices]
                        o_projected[indices] = c_projected[indices]
                        changed = True
                changed_any = changed_any or changed

                if not changed:
                    break

            if changed_any:
                # Update weighted by point confidence
                weights = np.zeros((66,3))
                weights[:, :] = self.lms[0:66, 2:3]
                weights[weights > 0.7] = 1.0
                weights = 1.0 - weights
                update_indices = np.arange(0, 66)
                if self.limit_3d_adjustment:
                    update_indices = np.nonzero(self.update_counts[:, update_type] <= self.update_count_max)[0]
                self.face_3d[update_indices] = self.face_3d[update_indices] * weights[update_indices] + updated[update_indices] * (1. - weights[update_indices])
                self.update_contour()

        self.pts_3d = self.normalize_pts3d(self.pts_3d)
        if self.tracker.feature_level == 2:
            self.current_features = self.features.update(self.pts_3d[:, 0:2])
            self.eye_blink = []
            self.eye_blink.append(1 - min(max(0, -self.current_features[""eye_r""]), 1))
            self.eye_blink.append(1 - min(max(0, -self.current_features[""eye_l""]), 1))
        elif self.tracker.feature_level == 1:
            self.current_features = self.features.update(self.pts_3d[:, 0:2], False)
            self.eye_blink = []
            self.eye_blink.append(1 - min(max(0, -self.current_features[""eye_r""]), 1))
            self.eye_blink.append(1 - min(max(0, -self.current_features[""eye_l""]), 1))",_7462.py,15,self.euler[0] > -165 and self.euler[0] < 145,-165 < self.euler[0] < 145
https://github.com/emilianavt/OpenSeeFace/tree/master//tracker.py,"def adjust_3d(self):
        if self.conf < 0.4 or self.pnp_error > 300:
            return

        if self.tracker.model_type != -1 and not self.tracker.static_model:
            max_runs = 1
            eligible = np.delete(np.arange(0, 66), [30])
            changed_any = False
            update_type = -1
            d_o = np.ones((66,))
            d_c = np.ones((66,))
            for runs in range(max_runs):
                r = 1.0 + np.random.random_sample((66,3)) * 0.02 - 0.01
                r[30, :] = 1.0
                if self.euler[0] > -165 and self.euler[0] < 145:
                    continue
                elif self.euler[1] > -10 and self.euler[1] < 20:
                    r[:, 2] = 1.0
                    update_type = 0
                else:
                    r[:, 0:2] = 1.0
                    if self.euler[2] > 120 or self.euler[2] < 60:
                        continue
                    # Enable only one side of the points, depending on direction
                    elif self.euler[1] < -10:
                        update_type = 1
                        r[[0, 1, 2, 3, 4, 5, 6, 7, 17, 18, 19, 20, 21, 31, 32, 36, 37, 38, 39, 40, 41, 48, 49, 56, 57, 58, 59, 65], 2] = 1.0
                        eligible = [8, 9, 10, 11, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 33, 34, 35, 42, 43, 44, 45, 46, 47, 50, 51, 52, 53, 54, 55, 60, 61, 62, 63, 64]
                    else:
                        update_type = 1
                        r[[9, 10, 11, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 34, 35, 42, 43, 44, 45, 46, 47, 51, 52, 53, 54, 61, 62, 63], 2] = 1.0
                        eligible = [0, 1, 2, 3, 4, 5, 6, 7, 8, 17, 18, 19, 20, 21, 27, 28, 29, 31, 32, 33, 36, 37, 38, 39, 40, 41, 48, 49, 50, 55, 56, 57, 58, 59, 60, 64, 65]

                if self.limit_3d_adjustment:
                    eligible = np.nonzero(self.update_counts[:, update_type] < self.update_counts[:, abs(update_type - 1)] + self.update_count_delta)[0]
                    if eligible.shape[0] <= 0:
                        break

                if runs == 0:
                    updated = copy.copy(self.face_3d[0:66])
                    o_projected = np.ones((66,2))
                    o_projected[eligible] = np.squeeze(np.array(cv2.projectPoints(self.face_3d[eligible], self.rotation, self.translation, self.tracker.camera, self.tracker.dist_coeffs)[0]), 1)
                c = updated * r
                c_projected = np.zeros((66,2))
                c_projected[eligible] = np.squeeze(np.array(cv2.projectPoints(c[eligible], self.rotation, self.translation, self.tracker.camera, self.tracker.dist_coeffs)[0]), 1)
                changed = False

                d_o[eligible] = np.linalg.norm(o_projected[eligible] - self.lms[eligible, 0:2], axis=1)
                d_c[eligible] = np.linalg.norm(c_projected[eligible] - self.lms[eligible, 0:2], axis=1)
                indices = np.nonzero(d_c < d_o)[0]
                if indices.shape[0] > 0:
                    if self.limit_3d_adjustment:
                        indices = np.intersect1d(indices, eligible)
                    if indices.shape[0] > 0:
                        self.update_counts[indices, update_type] += 1
                        updated[indices] = c[indices]
                        o_projected[indices] = c_projected[indices]
                        changed = True
                changed_any = changed_any or changed

                if not changed:
                    break

            if changed_any:
                # Update weighted by point confidence
                weights = np.zeros((66,3))
                weights[:, :] = self.lms[0:66, 2:3]
                weights[weights > 0.7] = 1.0
                weights = 1.0 - weights
                update_indices = np.arange(0, 66)
                if self.limit_3d_adjustment:
                    update_indices = np.nonzero(self.update_counts[:, update_type] <= self.update_count_max)[0]
                self.face_3d[update_indices] = self.face_3d[update_indices] * weights[update_indices] + updated[update_indices] * (1. - weights[update_indices])
                self.update_contour()

        self.pts_3d = self.normalize_pts3d(self.pts_3d)
        if self.tracker.feature_level == 2:
            self.current_features = self.features.update(self.pts_3d[:, 0:2])
            self.eye_blink = []
            self.eye_blink.append(1 - min(max(0, -self.current_features[""eye_r""]), 1))
            self.eye_blink.append(1 - min(max(0, -self.current_features[""eye_l""]), 1))
        elif self.tracker.feature_level == 1:
            self.current_features = self.features.update(self.pts_3d[:, 0:2], False)
            self.eye_blink = []
            self.eye_blink.append(1 - min(max(0, -self.current_features[""eye_r""]), 1))
            self.eye_blink.append(1 - min(max(0, -self.current_features[""eye_l""]), 1))",_7462.py,17,self.euler[1] > -10 and self.euler[1] < 20,-10 < self.euler[1] < 20
https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/lxmert/modeling_tf_lxmert.py,"def call(self, input_ids=None, token_type_ids=None, inputs_embeds=None, training=False):
        """"""
        Applies embedding based on inputs tensor.

        Returns:
            final_embeddings (`tf.Tensor`): output embedding tensor.
        """"""
        assert not (input_ids is None and inputs_embeds is None)

        if input_ids is not None:
            # Note: tf.gather, on which the embedding layer is based, won't check positive out of bound
            # indices on GPU, returning zeros instead. This is a dangerous silent behavior.
            tf.debugging.assert_less(
                input_ids,
                tf.cast(self.vocab_size, dtype=input_ids.dtype),
                message=(
                    ""input_ids must be smaller than the embedding layer's input dimension (got""
                    f"" {tf.math.reduce_max(input_ids)} >= {self.vocab_size})""
                ),
            )
            inputs_embeds = tf.gather(params=self.weight, indices=input_ids)

        input_shape = shape_list(inputs_embeds)[:-1]

        if token_type_ids is None:
            token_type_ids = tf.fill(dims=input_shape, value=0)

        position_ids = tf.expand_dims(tf.range(start=0, limit=input_shape[-1]), axis=0)
        position_embeds = tf.gather(params=self.position_embeddings, indices=position_ids)
        token_type_embeds = tf.gather(params=self.token_type_embeddings, indices=token_type_ids)
        final_embeddings = inputs_embeds + position_embeds + token_type_embeds
        final_embeddings = self.LayerNorm(inputs=final_embeddings)
        final_embeddings = self.dropout(inputs=final_embeddings, training=training)

        return final_embeddings",_7583.py,8,input_ids is None and inputs_embeds is None,input_ids is None is inputs_embeds
https://github.com/cpfair/tapiriik/tree/master/tapiriik/services/pwx.py,"def Dump(activity):
        xroot = etree.Element(""pwx"", nsmap=PWXIO.Namespaces)

        xroot.attrib[""creator""] = ""tapiriik""
        xroot.attrib[""version""] = ""1.0""

        xworkout = etree.SubElement(xroot, ""workout"")

        if activity.Type in PWXIO._reverseSportTypeMappings:
            etree.SubElement(xworkout, ""sportType"").text = PWXIO._reverseSportTypeMappings[activity.Type]

        if activity.Name:
            etree.SubElement(xworkout, ""title"").text = activity.Name

        if activity.Notes:
            etree.SubElement(xworkout, ""cmt"").text = activity.Notes

        xdevice = etree.SubElement(xworkout, ""device"")

        # By Ben's request
        etree.SubElement(xdevice, ""make"").text = ""tapiriik""
        if hasattr(activity, ""SourceConnection""):
            etree.SubElement(xdevice, ""model"").text = activity.SourceConnection.Service.ID

        etree.SubElement(xworkout, ""time"").text = activity.StartTime.replace(tzinfo=None).isoformat()

        def _writeMinMaxAvg(xparent, name, stat, naturalValue=False):
            if stat.Min is None and stat.Max is None and stat.Average is None:
                return
            xstat = etree.SubElement(xparent, name)
            if stat.Min is not None:
                xstat.attrib[""min""] = str(stat.Min)
            if stat.Max is not None:
                xstat.attrib[""max""] = str(stat.Max)
            if stat.Average is not None:
                xstat.attrib[""avg""] = str(stat.Average)

        def _writeSummaryData(xparent, obj, time_ref):
            xsummary = etree.SubElement(xparent, ""summarydata"")
            etree.SubElement(xsummary, ""beginning"").text = str((obj.StartTime - time_ref).total_seconds())
            etree.SubElement(xsummary, ""duration"").text = str((obj.EndTime - obj.StartTime).total_seconds())

            if obj.Stats.TimerTime.Value is not None:
                etree.SubElement(xsummary, ""durationstopped"").text = str((obj.EndTime - obj.StartTime).total_seconds() - obj.Stats.TimerTime.asUnits(ActivityStatisticUnit.Seconds).Value)

            altStat = obj.Stats.Elevation.asUnits(ActivityStatisticUnit.Meters)

            _writeMinMaxAvg(xsummary, ""hr"", obj.Stats.HR.asUnits(ActivityStatisticUnit.BeatsPerMinute))
            _writeMinMaxAvg(xsummary, ""spd"", obj.Stats.Speed.asUnits(ActivityStatisticUnit.MetersPerSecond))
            _writeMinMaxAvg(xsummary, ""pwr"", obj.Stats.Power.asUnits(ActivityStatisticUnit.Watts))
            if obj.Stats.Cadence.Min is not None or obj.Stats.Cadence.Max is not None or obj.Stats.Cadence.Average is not None:
                _writeMinMaxAvg(xsummary, ""cad"", obj.Stats.Cadence.asUnits(ActivityStatisticUnit.RevolutionsPerMinute))
            else:
                _writeMinMaxAvg(xsummary, ""cad"", obj.Stats.RunCadence.asUnits(ActivityStatisticUnit.StepsPerMinute))
            if obj.Stats.Distance.Value:
                etree.SubElement(xsummary, ""dist"").text = str(obj.Stats.Distance.asUnits(ActivityStatisticUnit.Meters).Value)
            _writeMinMaxAvg(xsummary, ""alt"", altStat)
            _writeMinMaxAvg(xsummary, ""temp"", obj.Stats.Temperature.asUnits(ActivityStatisticUnit.DegreesCelcius))

            if altStat.Gain is not None:
                etree.SubElement(xsummary, ""climbingelevation"").text = str(altStat.Gain)
            if altStat.Loss is not None:
                etree.SubElement(xsummary, ""descendingelevation"").text = str(altStat.Loss)

        _writeSummaryData(xworkout, activity, time_ref=activity.StartTime)

        for lap in activity.Laps:
            xsegment = etree.SubElement(xworkout, ""segment"")
            _writeSummaryData(xsegment, lap, time_ref=activity.StartTime)

        for wp in activity.GetFlatWaypoints():
            xsample = etree.SubElement(xworkout, ""sample"")
            etree.SubElement(xsample, ""timeoffset"").text = str((wp.Timestamp - activity.StartTime).total_seconds())

            if wp.HR is not None:
                etree.SubElement(xsample, ""hr"").text = str(round(wp.HR))

            if wp.Speed is not None:
                etree.SubElement(xsample, ""spd"").text = str(wp.Speed)

            if wp.Power is not None:
                etree.SubElement(xsample, ""pwr"").text = str(round(wp.Power))

            if wp.Cadence is not None:
                etree.SubElement(xsample, ""cad"").text = str(round(wp.Cadence))
            else:
                if wp.RunCadence is not None:
                    etree.SubElement(xsample, ""cad"").text = str(round(wp.RunCadence))

            if wp.Distance is not None:
                etree.SubElement(xsample, ""dist"").text = str(wp.Distance)

            if wp.Location is not None:
                if wp.Location.Longitude is not None:
                    etree.SubElement(xsample, ""lat"").text = str(wp.Location.Latitude)
                    etree.SubElement(xsample, ""lon"").text = str(wp.Location.Longitude)
                if wp.Location.Altitude is not None:
                    etree.SubElement(xsample, ""alt"").text = str(wp.Location.Altitude)

            if wp.Temp is not None:
                etree.SubElement(xsample, ""temp"").text = str(wp.Temp)


        return etree.tostring(xroot, pretty_print=True, xml_declaration=True, encoding=""UTF-8"").decode(""UTF-8"")",_8069.py,28,stat.Min is None and stat.Max is None and (stat.Average is None),stat.Min is None is stat.Max and stat.Average is None
https://github.com/blmoistawinde/HarvestText/tree/master/harvesttext/harvesttext.py,"def add_entities(self, entity_mention_dict=None, entity_type_dict=None, override=False, load_path=None):
        '''ç™»å½•çš„å®žä½“ä¿¡æ¯åˆ°htï¼Œæˆ–è€…ä»Žsave_entitiesä¿å­˜çš„æ–‡ä»¶ä¸­è¯»å–ï¼ˆå¦‚æžœæŒ‡å®šäº†load_pathï¼‰

        :param entity_mention_dict: dict, {entity:[mentions]}æ ¼å¼ï¼Œ
        :param entity_type_dict: dict, {entity:entity_type}æ ¼å¼ï¼Œ
        :param override: bool, æ˜¯å¦è¦†ç›–å·²ç™»å½•å®žä½“ï¼Œé»˜è®¤False
        :param load_path: str, è¦è¯»å–çš„æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ä¸ä½¿ç”¨ï¼‰
        :return: None
        '''
        if load_path:
            self.load_entities(load_path, override)

        if override:
            self.clear()

        if entity_mention_dict is None and entity_type_dict is None:
            return

        if entity_mention_dict is None:         # ç”¨å®žä½“åç›´æŽ¥ä½œä¸ºé»˜è®¤æŒ‡ç§°
            entity_mention_dict = dict(
                (entity0, {entity0}) for entity0 in entity_type_dict)
        else:
            entity_mention_dict = dict(
                (entity0, set(mentions0)) for (entity0, mentions0) in entity_mention_dict.items())
        if len(self.entity_mention_dict) == 0:
            self.entity_mention_dict = entity_mention_dict
        else:
            for entity, mentions in entity_type_dict.items():
                if entity in self.entity_mention_dict:
                    self.entity_mention_dict[entity] |= entity_mention_dict[entity]
                else:
                    self.entity_mention_dict[entity] = entity_mention_dict[entity]


        if entity_type_dict is None:
            entity_type_dict = {entity: ""æ·»åŠ è¯"" for entity in self.entity_mention_dict}
        if len(self.entity_type_dict) == 0:
            self.entity_type_dict = entity_type_dict
        else:
            for entity, type0 in entity_type_dict.items():
                if entity in self.entity_type_dict and type0 != self.entity_type_dict[entity]:
                    # ä¸å…è®¸åŒä¸€å®žä½“æœ‰ä¸åŒç±»åž‹
                    warnings.warn(""You've added an entity twice with different types, the later type will be used."")
                self.entity_type_dict[entity] = type0

        # ä¸¤ä¸ªdictä¸å¯¹é½çš„æƒ…å†µä¸‹ï¼Œä»¥æ·»åŠ è¯ä½œä¸ºé»˜è®¤è¯æ€§
        for entity in self.entity_mention_dict:
            if entity not in self.entity_type_dict:
                self.entity_type_dict[entity] = ""æ·»åŠ è¯""


        type_entity_mention_dict = defaultdict(dict)
        for entity0, type0 in self.entity_type_dict.items():
            if entity0 in self.entity_mention_dict:
                type_entity_mention_dict[type0][entity0] = self.entity_mention_dict[entity0]
        self.type_entity_mention_dict = type_entity_mention_dict
        self._add_entities(type_entity_mention_dict)",_8092.py,16,entity_mention_dict is None and entity_type_dict is None,entity_mention_dict is None is entity_type_dict
https://github.com/SymbiFlow/prjxray/tree/master/fuzzers/032-cmt-pll/top.py,"def main():
    sites = sorted(list(gen_sites()))
    max_sites = len(sites)

    f = open('params.jl', 'w')
    f.write('module,loc,params\n')

    routes_file = open('routes.txt', 'w')

    print(
        """"""
module top(
    input [{N}:0] clkin1,
    input [{N}:0] clkin2,
    input [{N}:0] clkfb,
    input [{N}:0] dclk
);

    (* KEEP, DONT_TOUCH *)
    LUT1 dummy();
"""""".format(N=max_sites - 1))

    for i, (
            tile_name,
            tile_type,
            site,
    ) in enumerate(sorted(gen_sites())):
        params = {
            ""site"":
            site,
            'active':
            random.random() > .2,
            ""clkin1_conn"":
            random.choice(
                (""clkfbout_mult_BUFG_"" + site, ""clkin1[{}]"".format(i), """")),
            ""clkin2_conn"":
            random.choice(
                (""clkfbout_mult_BUFG_"" + site, ""clkin2[{}]"".format(i), """")),
            ""dclk_conn"":
            random.choice((
                ""0"",
                ""dclk[{}]"".format(i),
            )),
            ""dwe_conn"":
            random.choice((
                """",
                ""1"",
                ""0"",
                ""dwe_"" + site,
                ""den_"" + site,
            )),
            ""den_conn"":
            random.choice((
                """",
                ""1"",
                ""0"",
                ""den_"" + site,
            )),
            ""daddr4_conn"":
            random.choice((
                ""0"",
                ""dwe_"" + site,
            )),
            ""IS_RST_INVERTED"":
            random.randint(0, 1),
            ""IS_PWRDWN_INVERTED"":
            random.randint(0, 1),
            ""IS_CLKINSEL_INVERTED"":
            random.randint(0, 1),
            ""CLKFBOUT_MULT"":
            random.randint(2, 4),
            ""CLKOUT0_DIVIDE"":
            random.randint(1, 128),
            ""CLKOUT1_DIVIDE"":
            random.randint(1, 128),
            ""CLKOUT2_DIVIDE"":
            random.randint(1, 128),
            ""CLKOUT3_DIVIDE"":
            random.randint(1, 128),
            ""CLKOUT4_DIVIDE"":
            random.randint(1, 128),
            ""CLKOUT5_DIVIDE"":
            random.randint(1, 128),
            ""DIVCLK_DIVIDE"":
            random.randint(1, 5),
            ""CLKOUT0_DUTY_CYCLE"":
            ""0.500"",
            ""STARTUP_WAIT"":
            verilog.quote('TRUE' if random.randint(0, 1) else 'FALSE'),
            ""COMPENSATION"":
            verilog.quote(
                random.choice((
                    'ZHOLD',
                    'BUF_IN',
                    'EXTERNAL',
                    'INTERNAL',
                ))),
            ""BANDWIDTH"":
            verilog.quote(random.choice((
                'OPTIMIZED',
                'HIGH',
                'LOW',
            ))),
        }

        if verilog.unquote(params['COMPENSATION']) == 'ZHOLD':
            params['clkfbin_conn'] = random.choice(
                (
                    """",
                    ""clkfbout_mult_BUFG_"" + site,
                ))
        elif verilog.unquote(params['COMPENSATION']) == 'INTERNAL':
            params['clkfbin_conn'] = random.choice(
                (
                    """",
                    ""clkfbout_mult_"" + site,
                ))
        else:
            params['clkfbin_conn'] = random.choice(
                ("""", ""clkfb[{}]"".format(i), ""clkfbout_mult_BUFG_"" + site))

        params['clkin1_route'] = random.choice(
            (
                ""{}_CLKIN1"",
                ""{}_FREQ_BB0"",
                ""{}_FREQ_BB1"",
                ""{}_FREQ_BB2"",
                ""{}_FREQ_BB3"",
                ""{}_PLLE2_CLK_IN1_INT"",
            )).format(tile_type)

        params['clkin2_route'] = random.choice(
            (
                ""{}_CLKIN2"",
                ""{}_FREQ_BB0"",
                ""{}_FREQ_BB1"",
                ""{}_FREQ_BB2"",
                ""{}_FREQ_BB3"",
                ""{}_PLLE2_CLK_IN2_INT"",
            )).format(tile_type)

        params['clkfbin_route'] = random.choice(
            (
                ""{}_CLKFBOUT2IN"",
                ""{}_UPPER_T_FREQ_BB0"",
                ""{}_UPPER_T_FREQ_BB1"",
                ""{}_UPPER_T_FREQ_BB2"",
                ""{}_UPPER_T_FREQ_BB3"",
                ""{}_UPPER_T_PLLE2_CLK_FB_INT"",
            )).format(tile_type.replace(""_UPPER_T"", """"))

        f.write('%s\n' % (json.dumps(params)))

        def make_ibuf_net(net):
            p = net.find('[')
            return net[:p] + '_IBUF' + net[p:]

        if params['clkin1_conn'] != """":
            net = make_ibuf_net(params['clkin1_conn'])
            wire = '{}/{}'.format(tile_name, params['clkin1_route'])
            routes_file.write('{} {}\n'.format(net, wire))

        if params['clkin2_conn'] != """":
            net = make_ibuf_net(params['clkin2_conn'])
            wire = '{}/{}'.format(tile_name, params['clkin2_route'])
            routes_file.write('{} {}\n'.format(net, wire))

        if params['clkfbin_conn'] != """" and\
           params['clkfbin_conn'] != (""clkfbout_mult_BUFG_"" + site):
            net = params['clkfbin_conn']
            if ""["" in net and ""]"" in net:
                net = make_ibuf_net(net)
            wire = '{}/{}'.format(tile_name, params['clkfbin_route'])
            routes_file.write('{} {}\n'.format(net, wire))

        if not params['active']:
            continue

        print(
            """"""

    wire den_{site};
    wire dwe_{site};

    LUT1 den_lut_{site} (
        .O(den_{site})
    );

    LUT1 dwe_lut_{site} (
        .O(dwe_{site})
    );

    wire clkfbout_mult_{site};
    wire clkfbout_mult_BUFG_{site};
    wire clkout0_{site};
    wire clkout1_{site};
    wire clkout2_{site};
    wire clkout3_{site};
    wire clkout4_{site};
    wire clkout5_{site};
    (* KEEP, DONT_TOUCH, LOC = ""{site}"" *)
    PLLE2_ADV #(
            .IS_RST_INVERTED({IS_RST_INVERTED}),
            .IS_PWRDWN_INVERTED({IS_PWRDWN_INVERTED}),
            .IS_CLKINSEL_INVERTED({IS_CLKINSEL_INVERTED}),
            .CLKOUT0_DIVIDE({CLKOUT0_DIVIDE}),
            .CLKOUT1_DIVIDE({CLKOUT1_DIVIDE}),
            .CLKOUT2_DIVIDE({CLKOUT2_DIVIDE}),
            .CLKOUT3_DIVIDE({CLKOUT3_DIVIDE}),
            .CLKOUT4_DIVIDE({CLKOUT4_DIVIDE}),
            .CLKOUT5_DIVIDE({CLKOUT5_DIVIDE}),
            .CLKFBOUT_MULT({CLKFBOUT_MULT}),
            .DIVCLK_DIVIDE({DIVCLK_DIVIDE}),
            .STARTUP_WAIT({STARTUP_WAIT}),
            .CLKOUT0_DUTY_CYCLE({CLKOUT0_DUTY_CYCLE}),
            .COMPENSATION({COMPENSATION}),
            .BANDWIDTH({BANDWIDTH}),
            .CLKIN1_PERIOD(10.0),
            .CLKIN2_PERIOD(10.0)
    ) pll_{site} (
            .CLKFBOUT(clkfbout_mult_{site}),
            .CLKOUT0(clkout0_{site}),
            .CLKOUT1(clkout1_{site}),
            .CLKOUT2(clkout2_{site}),
            .CLKOUT3(clkout3_{site}),
            .CLKOUT4(clkout4_{site}),
            .CLKOUT5(clkout5_{site}),
            .DRDY(),
            .LOCKED(),
            .DO(),
            .CLKFBIN({clkfbin_conn}),
            .CLKIN1({clkin1_conn}),
            .CLKIN2({clkin2_conn}),
            .CLKINSEL(),
            .DCLK({dclk_conn}),
            .DEN({den_conn}),
            .DWE({dwe_conn}),
            .PWRDWN(),
            .RST(),
            .DI(),
            .DADDR({{7{{ {daddr4_conn} }} }}));

    (* KEEP, DONT_TOUCH *)
    BUFG bufg_{site} (
        .I(clkfbout_mult_{site}),
        .O(clkfbout_mult_BUFG_{site})
    );

    (* KEEP, DONT_TOUCH *)
    FDRE reg_clkfbout_mult_{site} (
        .C(clkfbout_mult_{site})
    );

    (* KEEP, DONT_TOUCH *)
    FDRE reg_clkout0_{site} (
        .C(clkout0_{site})
    );

    (* KEEP, DONT_TOUCH *)
    FDRE reg_clkout1_{site} (
        .C(clkout1_{site})
    );

    (* KEEP, DONT_TOUCH *)
    FDRE reg_clkout2_{site} (
        .C(clkout2_{site})
    );

    (* KEEP, DONT_TOUCH *)
    FDRE reg_clkout3_{site} (
        .C(clkout3_{site})
    );

    (* KEEP, DONT_TOUCH *)
    FDRE reg_clkout4_{site} (
        .C(clkout4_{site})
    );

    (* KEEP, DONT_TOUCH *)
    FDRE reg_clkout5_{site} (
        .C(clkout5_{site})
    );
            """""".format(**params))

    print('endmodule')

    f.close()",_8331.py,168,params['clkfbin_conn'] != '' and params['clkfbin_conn'] != 'clkfbout_mult_BUFG_' + site,'' != params['clkfbin_conn'] != 'clkfbout_mult_BUFG_' + site
https://github.com/edx/edx-platform/tree/master/openedx/core/djangoapps/discussions/views.py,"def update_configuration_data(request, course_key_string):
        """"""
        Update discussion configuration for the course based on data in the request.
        Args:
            request (Request): a DRF request
            course_key_string (str): a course key string

        Returns:
            Dict: modified course configuration data
        """"""
        course_key = validate_course_key(course_key_string)
        configuration = DiscussionsConfiguration.get(course_key)
        course = CourseOverview.get_from_id(course_key)
        serializer = DiscussionsConfigurationSerializer(
            configuration,
            context={
                'user_id': request.user.id,
            },
            data=request.data,
            partial=True,
        )
        if serializer.is_valid(raise_exception=True):
            new_provider_type = serializer.validated_data.get('provider_type', None)
            if new_provider_type is not None and new_provider_type != configuration.provider_type:
                check_course_permissions(course, request.user, 'change_provider')

            serializer.save()
        return serializer.data",_8367.py,24,new_provider_type is not None and new_provider_type != configuration.provider_type,None is not new_provider_type != configuration.provider_type
https://github.com/ahmedkhlief/APT-Hunter/tree/master/lib/EvtxDetection.py,"def detect_events_powershell_log(file_name,input_timzone):

    for file in file_name:
        parser = PyEvtxParser(file)
        for record in parser.records():
            EventID = EventID_rex.findall(record['data'])
            Computer = Computer_rex.findall(record['data'])
            Channel = Channel_rex.findall(record['data'])

            if len(EventID) > 0:
                Host_Application = HostApplication_rex.findall(record['data'])
                User =UserId_rex.findall(record['data'])
                Engine_Version = EngineVersion_rex.findall(record['data'])
                ScriptName = ScriptName_rex.findall(record['data'])
                CommandLine= CommandLine_rex.findall(record['data'])
                Error_Message = ErrorMessage_rex.findall(record['data'])
                Suspicious=[]
                #Powershell Pipeline Execution details
                host_app=""""

                if record['data'].strip().find(""\\temp\\"") > -1 or record['data'].strip().find(
                        ""\\tmp\\"") > -1:
                    Event_desc=""Powershell Operation including TEMP Folder""
                    Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                    Powershell_events[0]['Computer Name'].append(Computer[0])
                    Powershell_events[0]['Channel'].append(Channel[0])
                    Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                    Powershell_events[0]['Detection Rule'].append(
                        ""Powershell Executing Pipeline - Operation including TEMP folder "")
                    Powershell_events[0]['Detection Domain'].append(""Threat"")
                    Powershell_events[0]['Severity'].append(""High"")
                    Powershell_events[0]['Event Description'].append(Event_desc)
                    Powershell_events[0]['Event ID'].append(EventID[0])
                    Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"", "" ""))


                #Detect any log that contain suspicious process name or argument
                for i in Suspicious_executables:

                    if record['data'].lower().find(i.lower())>-1:

                        #print(""##### "" + record[""timestamp""] + "" ####  "", end='')
                        #print(""## Found Suspicios Process "", end='')
                        #print(""User Name : ( %s ) "" % Account_Name[0][0].strip(), end='')
                        #print(""with Command Line : ( "" + Process_Command_Line[0][0].strip() + "" )"")
                        # print(""###########"")

                        Event_desc =""Found a log contain suspicious powershell command ( %s)""%i
                        Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                        Powershell_events[0]['Computer Name'].append(Computer[0])
                        Powershell_events[0]['Channel'].append(Channel[0])
                        Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                        Powershell_events[0]['Detection Rule'].append(""Suspicious Command or process found in the log"")
                        Powershell_events[0]['Detection Domain'].append(""Threat"")
                        Powershell_events[0]['Severity'].append(""Critical"")
                        Powershell_events[0]['Event Description'].append(Event_desc)
                        Powershell_events[0]['Event ID'].append(EventID[0])
                        Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"", "" ""))
                        break

                if EventID[0]==""800"" :
                    if len(Host_Application) == 0:
                        host_app = """"
                    else:
                        host_app = Host_Application[0].strip()
                    for i in Suspicious_powershell_commands:
                        if i in record['data']:
                            Suspicious.append(i)

                    if len(Suspicious)>0:
                        #print(""##### "" + record[""timestamp""] + "" #### EventID=800 ### Powershell Pipeline Execution details #### "", end='')
                        #print(""Found User (""+User[0].strip()+"") run Suspicious PowerShell commands that include (""+"","".join(Suspicious)+"") in event with Command Line (""+CommandLine[0].strip()+"") and full command (""+Host_Application[0].strip()+"") "", end='')#, check event details ""+record['data'])
                        Event_desc =""Found User (""+User[0].strip()+"") run Suspicious PowerShell commands that include (""+"","".join(Suspicious)+"") in event with Command Line (""+CommandLine[0].strip()+"") and full command (""+host_app+"") ""
                        if len(Error_Message)>0:
                            Event_desc = Event_desc +""Error Message (""+Error_Message[0].strip()+"")""
                            #print(""Error Message (""+Error_Message[0].strip()+"")"")
                        #else:
                        #    print("""")

                        Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                        Powershell_events[0]['Computer Name'].append(Computer[0])
                        Powershell_events[0]['Channel'].append(Channel[0])
                        Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                        Powershell_events[0]['Detection Rule'].append(""Powershell Executing Pipeline - Suspicious Powershell Commands detected"")
                        Powershell_events[0]['Detection Domain'].append(""Threat"")
                        Powershell_events[0]['Severity'].append(""Critical"")
                        Powershell_events[0]['Event Description'].append(Event_desc)
                        Powershell_events[0]['Event ID'].append(EventID[0])
                        Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"", "" ""))

                Suspicious = []

                if EventID[0]==""600"" or EventID[0]==""400"" or EventID[0]==""403"" :
                    if len(Host_Application) == 0:
                        host_app = """"
                    else:
                        host_app = Host_Application[0].strip()
                    for i in Suspicious_powershell_commands:
                        if i in record['data']:
                            Suspicious.append(i)

                    if len(Suspicious)>0:
                        #print(""##### "" + record[""timestamp""] + "" #### EventID=""+EventID[0].strip()+"" ### Engine state is changed #### "", end='')
                        #print(""Found  Suspicious PowerShell commands that include (""+"","".join(Suspicious)+"") in event with Command Line (""+CommandLine[0].strip()+"") and full command (""+Host_Application[0].strip()+"") "", end='')#, check event details ""+record['data'])
                        Event_desc =""Found  Suspicious PowerShell commands that include ("" + "","".join(
                            Suspicious) + "") in event with Command Line ("" + CommandLine[
                            0].strip() + "") and full command ("" + host_app + "") ""

                        if len(Error_Message)>0:
                            Event_desc = Event_desc + ""Error Message ("" + Error_Message[0].strip() + "")""
                            #print(""Error Message (""+Error_Message[0].strip()+"")"")
                        #else:
                        #    print("""")
                        Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                        Powershell_events[0]['Computer Name'].append(Computer[0])
                        Powershell_events[0]['Channel'].append(Channel[0])
                        Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                        Powershell_events[0]['Detection Rule'].append(""Suspicious PowerShell commands Detected"")
                        Powershell_events[0]['Detection Domain'].append(""Threat"")
                        Powershell_events[0]['Severity'].append(""Critical"")
                        Powershell_events[0]['Event Description'].append(Event_desc)
                        Powershell_events[0]['Event ID'].append(EventID[0])
                        Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"","" ""))


                Suspicious = []
                if EventID[0]!=""600"" and EventID[0]!=""400"" or EventID[0]!=""403"" or EventID[0]!=""800"":
                    for i in Suspicious_powershell_commands:
                        if i in record['data']:
                            Suspicious.append(i)

                    if len(Suspicious)>0:
                        Event_desc =""Found  Suspicious PowerShell commands that include ("" + "","".join(Suspicious) + "") in event ""
                        Powershell_events[0]['Date and Time'].append(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())
                        Powershell_events[0]['Computer Name'].append(Computer[0])
                        Powershell_events[0]['Channel'].append(Channel[0])
                        Powershell_events[0]['timestamp'].append(datetime.timestamp(isoparse(parse(record[""timestamp""]).astimezone(input_timzone).isoformat())))
                        Powershell_events[0]['Detection Rule'].append(""Suspicious PowerShell commands Detected"")
                        Powershell_events[0]['Detection Domain'].append(""Threat"")
                        Powershell_events[0]['Severity'].append(""Critical"")
                        Powershell_events[0]['Event Description'].append(Event_desc)
                        Powershell_events[0]['Event ID'].append(EventID[0])
                        Powershell_events[0]['Original Event Log'].append(str(record['data']).replace(""\r"","" ""))
                Suspicious = []
            else:
                print(record['data'])",_8471.py,127,EventID[0] != '600' and EventID[0] != '400','600' != EventID[0] != '400'
https://github.com/tensorpack/tensorpack/tree/master/tensorpack/predict/config.py,"def __init__(self,
                 model=None,
                 tower_func=None,
                 input_signature=None,

                 input_names=None,
                 output_names=None,

                 session_creator=None,
                 session_init=None,
                 return_input=False,
                 create_graph=True,
                 ):
        """"""
        Users need to provide enough arguments to create a tower function,
        which will be used to construct the graph.
        This can be provided in the following ways:

        1. `model`: a :class:`ModelDesc` instance. It will contain a tower function by itself.
        2. `tower_func`: a :class:`tfutils.TowerFunc` instance.
            Provide a tower function instance directly.
        3. `tower_func`: a symbolic function and `input_signature`: the signature of the function.
            Provide both a function and its signature.

        Example:

        .. code-block:: python

            config = PredictConfig(model=my_model,
                                   inputs_names=['image'],
                                   output_names=['linear/output', 'prediction'])

        Args:
            model (ModelDescBase): to be used to construct a tower function.
            tower_func: a callable which takes input tensors (by positional args) and construct a tower.
                or a :class:`tfutils.TowerFunc` instance.
            input_signature ([tf.TensorSpec]): if tower_func is a plain function (instead of a TowerFunc),
                this describes the list of inputs it takes.

            input_names (list): a list of input tensor names. Defaults to match input_signature.
                The name can be either the name of a tensor, or the name of one input of the tower.
            output_names (list): a list of names of the output tensors to predict, the
                tensors can be any tensor in the graph that's computable from the tensors correponding to `input_names`.

            session_creator (tf.train.SessionCreator): how to create the
                session. Defaults to :class:`NewSessionCreator()`.
            session_init (SessionInit): how to initialize variables of the session.
                Defaults to do nothing.

            return_input (bool): same as in :attr:`PredictorBase.return_input`.
            create_graph (bool): create a new graph, or use the default graph
                when predictor is first initialized.
        """"""
        def assert_type(v, tp, name):
            assert isinstance(v, tp), \
                ""Argument '{}' has to be type '{}', but an object of type '{}' found."".format(
                    name, tp.__name__, v.__class__.__name__)

        if model is not None:
            assert_type(model, ModelDescBase, 'model')
            assert input_signature is None and tower_func is None
            self.input_signature = model.get_input_signature()
            self.tower_func = TowerFunc(model.build_graph, self.input_signature)
        else:
            if isinstance(tower_func, TowerFunc):
                input_signature = tower_func.input_signature
            assert input_signature is not None and tower_func is not None
            self.input_signature = input_signature
            self.tower_func = TowerFunc(tower_func, input_signature)

        if session_init is None:
            session_init = JustCurrentSession()
        self.session_init = session_init
        assert_type(self.session_init, SessionInit, 'session_init')

        if session_creator is None:
            self.session_creator = NewSessionCreator()
        else:
            self.session_creator = session_creator

        # inputs & outputs
        self.input_names = input_names
        if self.input_names is None:
            self.input_names = [k.name for k in self.input_signature]
        assert output_names is not None, ""Argument 'output_names' is not provided!""
        self.output_names = output_names
        assert_type(self.output_names, list, 'output_names')
        assert_type(self.input_names, list, 'input_names')
        if len(self.input_names) == 0:
            logger.warn('PredictConfig receives empty ""input_names"".')
        for v in self.input_names:
            assert_type(v, six.string_types, 'Each item in input_names')
        assert len(self.output_names), ""Argument 'output_names' cannot be empty!""

        self.return_input = bool(return_input)
        self.create_graph = bool(create_graph)",_8648.py,61,input_signature is None and tower_func is None,input_signature is None is tower_func
https://github.com/tensorpack/tensorpack/tree/master/tensorpack/predict/config.py,"def __init__(self,
                 model=None,
                 tower_func=None,
                 input_signature=None,

                 input_names=None,
                 output_names=None,

                 session_creator=None,
                 session_init=None,
                 return_input=False,
                 create_graph=True,
                 ):
        """"""
        Users need to provide enough arguments to create a tower function,
        which will be used to construct the graph.
        This can be provided in the following ways:

        1. `model`: a :class:`ModelDesc` instance. It will contain a tower function by itself.
        2. `tower_func`: a :class:`tfutils.TowerFunc` instance.
            Provide a tower function instance directly.
        3. `tower_func`: a symbolic function and `input_signature`: the signature of the function.
            Provide both a function and its signature.

        Example:

        .. code-block:: python

            config = PredictConfig(model=my_model,
                                   inputs_names=['image'],
                                   output_names=['linear/output', 'prediction'])

        Args:
            model (ModelDescBase): to be used to construct a tower function.
            tower_func: a callable which takes input tensors (by positional args) and construct a tower.
                or a :class:`tfutils.TowerFunc` instance.
            input_signature ([tf.TensorSpec]): if tower_func is a plain function (instead of a TowerFunc),
                this describes the list of inputs it takes.

            input_names (list): a list of input tensor names. Defaults to match input_signature.
                The name can be either the name of a tensor, or the name of one input of the tower.
            output_names (list): a list of names of the output tensors to predict, the
                tensors can be any tensor in the graph that's computable from the tensors correponding to `input_names`.

            session_creator (tf.train.SessionCreator): how to create the
                session. Defaults to :class:`NewSessionCreator()`.
            session_init (SessionInit): how to initialize variables of the session.
                Defaults to do nothing.

            return_input (bool): same as in :attr:`PredictorBase.return_input`.
            create_graph (bool): create a new graph, or use the default graph
                when predictor is first initialized.
        """"""
        def assert_type(v, tp, name):
            assert isinstance(v, tp), \
                ""Argument '{}' has to be type '{}', but an object of type '{}' found."".format(
                    name, tp.__name__, v.__class__.__name__)

        if model is not None:
            assert_type(model, ModelDescBase, 'model')
            assert input_signature is None and tower_func is None
            self.input_signature = model.get_input_signature()
            self.tower_func = TowerFunc(model.build_graph, self.input_signature)
        else:
            if isinstance(tower_func, TowerFunc):
                input_signature = tower_func.input_signature
            assert input_signature is not None and tower_func is not None
            self.input_signature = input_signature
            self.tower_func = TowerFunc(tower_func, input_signature)

        if session_init is None:
            session_init = JustCurrentSession()
        self.session_init = session_init
        assert_type(self.session_init, SessionInit, 'session_init')

        if session_creator is None:
            self.session_creator = NewSessionCreator()
        else:
            self.session_creator = session_creator

        # inputs & outputs
        self.input_names = input_names
        if self.input_names is None:
            self.input_names = [k.name for k in self.input_signature]
        assert output_names is not None, ""Argument 'output_names' is not provided!""
        self.output_names = output_names
        assert_type(self.output_names, list, 'output_names')
        assert_type(self.input_names, list, 'input_names')
        if len(self.input_names) == 0:
            logger.warn('PredictConfig receives empty ""input_names"".')
        for v in self.input_names:
            assert_type(v, six.string_types, 'Each item in input_names')
        assert len(self.output_names), ""Argument 'output_names' cannot be empty!""

        self.return_input = bool(return_input)
        self.create_graph = bool(create_graph)",_8648.py,67,input_signature is not None and tower_func is not None,input_signature is not None is not tower_func
https://github.com/Jittor/jittor/tree/master/python/jittor/misc.py,"def cumsum(x, dim=None):
    '''
    Parameters:
    -----------
    x: jt.var
    dim: int

    Returns:
    --------
    the cumulative sum in dim of x
    '''
    if (dim == None):
        dim = -1
    assert(dim >= -1 and dim < len(x.shape))
    if jt.flags.use_cuda:
        return cub_cumsum(x, dim)
    else:
        return numpy_cumsum(x, dim)",_8826.py,14,dim >= -1 and dim < len(x.shape),-1 <= dim < len(x.shape)
https://github.com/petl-developers/petl/tree/master/petl/io/xlsx.py,"def _insert_sheet_on_workbook(mode, sheet, wb):
    if mode == ""replace"":
        try:
            ws = wb[str(sheet)]
            ws.delete_rows(1, ws.max_row)
        except KeyError:
            ws = wb.create_sheet(title=sheet)
    elif mode == ""add"":
        ws = wb.create_sheet(title=sheet)
        # it creates a sheet named ""foo1"" if ""foo"" exists.
        if sheet is not None and ws.title != sheet:
            raise ValueError(""Sheet %s already exists in file"" % sheet)
    elif mode == ""overwrite"":
        ws = wb.create_sheet(title=sheet)
    else:
        raise ValueError(""Unknown mode '%s'"" % mode)
    return ws",_8913.py,11,sheet is not None and ws.title != sheet,None is not sheet != ws.title
https://github.com/openai/universe-starter-agent/tree/master//a3c.py,"def process(self, sess):
        """"""
process grabs a rollout that's been produced by the thread runner,
and updates the parameters.  The update is then sent to the parameter
server.
""""""

        sess.run(self.sync)  # copy weights from shared to local
        rollout = self.pull_batch_from_queue()
        batch = process_rollout(rollout, gamma=0.99, lambda_=1.0)

        should_compute_summary = self.task == 0 and self.local_steps % 11 == 0

        if should_compute_summary:
            fetches = [self.summary_op, self.train_op, self.global_step]
        else:
            fetches = [self.train_op, self.global_step]

        feed_dict = {
            self.local_network.x: batch.si,
            self.ac: batch.a,
            self.adv: batch.adv,
            self.r: batch.r,
            self.local_network.state_in[0]: batch.features[0],
            self.local_network.state_in[1]: batch.features[1],
        }

        fetched = sess.run(fetches, feed_dict=feed_dict)

        if should_compute_summary:
            self.summary_writer.add_summary(tf.Summary.FromString(fetched[0]), fetched[-1])
            self.summary_writer.flush()
        self.local_steps += 1",_8981.py,12,self.task == 0 and self.local_steps % 11 == 0,self.task == 0 == self.local_steps % 11
https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/squad/agents.py,"def get(self, episode_idx, entry_idx=None):
        action = {}
        episode = self.episodes[episode_idx][entry_idx]
        context = ' '.join(episode['text'].split('\n')[:-1]).replace(
            '\xa0', ' '
        )  # get rid of non breaking space characters
        question = episode['text'].split('\n')[-1]
        label_field = 'labels' if 'labels' in episode else 'eval_labels'
        answers = []
        for answer in episode[label_field]:
            new_answer = answer.replace('.', '').replace('?', '').replace('!', '')
            context = context.replace(answer, new_answer)
            answers.append(new_answer)
        sentences = self.sent_tok.tokenize(context)
        labels = []
        label_starts = []
        for sentence in sentences:
            for answer in answers:
                if answer in sentence and sentence not in labels:
                    labels.append(sentence)
                    label_starts.append(context.index(sentence))

        action = {
            'context': context,
            'text': question,
            label_field: labels,
            'answer_starts': label_starts,
            'label_candidates': sentences,
            'episode_done': episode['episode_done'],
        }

        if self.include_context:
            action['text'] = action['context'] + '\n' + action['text']
            del action['context']

        return action",_9032.py,19,answer in sentence and sentence not in labels,answer in sentence not in labels
https://github.com/sympy/sympy/tree/master/sympy/matrices/reductions.py,"def _rank(M, iszerofunc=_iszero, simplify=False):
    """"""Returns the rank of a matrix.

    Examples
    ========

    >>> from sympy import Matrix
    >>> from sympy.abc import x
    >>> m = Matrix([[1, 2], [x, 1 - 1/x]])
    >>> m.rank()
    2
    >>> n = Matrix(3, 3, range(1, 10))
    >>> n.rank()
    2
    """"""

    def _permute_complexity_right(M, iszerofunc):
        """"""Permute columns with complicated elements as
        far right as they can go.  Since the ``sympy`` row reduction
        algorithms start on the left, having complexity right-shifted
        speeds things up.

        Returns a tuple (mat, perm) where perm is a permutation
        of the columns to perform to shift the complex columns right, and mat
        is the permuted matrix.""""""

        def complexity(i):
            # the complexity of a column will be judged by how many
            # element's zero-ness cannot be determined
            return sum(1 if iszerofunc(e) is None else 0 for e in M[:, i])

        complex = [(complexity(i), i) for i in range(M.cols)]
        perm    = [j for (i, j) in sorted(complex)]

        return (M.permute(perm, orientation='cols'), perm)

    simpfunc = simplify if isinstance(simplify, FunctionType) else _simplify

    # for small matrices, we compute the rank explicitly
    # if is_zero on elements doesn't answer the question
    # for small matrices, we fall back to the full routine.
    if M.rows <= 0 or M.cols <= 0:
        return 0

    if M.rows <= 1 or M.cols <= 1:
        zeros = [iszerofunc(x) for x in M]

        if False in zeros:
            return 1

    if M.rows == 2 and M.cols == 2:
        zeros = [iszerofunc(x) for x in M]

        if not False in zeros and not None in zeros:
            return 0

        d = M.det()

        if iszerofunc(d) and False in zeros:
            return 1
        if iszerofunc(d) is False:
            return 2

    mat, _       = _permute_complexity_right(M, iszerofunc=iszerofunc)
    _, pivots, _ = _row_reduce(mat, iszerofunc, simpfunc, normalize_last=True,
            normalize=False, zero_above=False)

    return len(pivots)",_9033.py,51,M.rows == 2 and M.cols == 2,M.rows == 2 == M.cols
https://github.com/Chia-Network/chia-blockchain/tree/master/chia/server/address_manager_sqlite_store.py,"def create_address_manager(
    metadata: Dict[str, str], nodes: List[Node], new_table_entries: List[Table]
) -> AddressManager:
    address_manager: AddressManager = AddressManager()

    # ----- NOTICE -----
    # The following code was taken from the original implementation of
    # AddressManagerStore.deserialize(). The code is duplicated/preserved
    # here to support migration from older versions.
    # ------------------
    address_manager.key = int(metadata[""key""])
    address_manager.new_count = int(metadata[""new_count""])
    # address_manager.tried_count = int(metadata[""tried_count""])
    address_manager.tried_count = 0

    new_table_nodes = [(node_id, info) for node_id, info in nodes if node_id < address_manager.new_count]
    for n, info in new_table_nodes:
        address_manager.map_addr[info.peer_info.host] = n
        address_manager.map_info[n] = info
        info.random_pos = len(address_manager.random_pos)
        address_manager.random_pos.append(n)
    address_manager.id_count = len(new_table_nodes)
    tried_table_nodes = [(node_id, info) for node_id, info in nodes if node_id >= address_manager.new_count]
    # lost_count = 0
    for node_id, info in tried_table_nodes:
        tried_bucket = info.get_tried_bucket(address_manager.key)
        tried_bucket_pos = info.get_bucket_position(address_manager.key, False, tried_bucket)
        if address_manager.tried_matrix[tried_bucket][tried_bucket_pos] == -1:
            info.random_pos = len(address_manager.random_pos)
            info.is_tried = True
            id_count = address_manager.id_count
            address_manager.random_pos.append(id_count)
            address_manager.map_info[id_count] = info
            address_manager.map_addr[info.peer_info.host] = id_count
            address_manager.tried_matrix[tried_bucket][tried_bucket_pos] = id_count
            address_manager.id_count += 1
            address_manager.tried_count += 1
        # else:
        #    lost_count += 1

    # address_manager.tried_count -= lost_count
    for node_id, bucket in new_table_entries:
        if node_id >= 0 and node_id < address_manager.new_count:
            info = address_manager.map_info[node_id]
            bucket_pos = info.get_bucket_position(address_manager.key, True, bucket)
            if address_manager.new_matrix[bucket][bucket_pos] == -1 and info.ref_count < NEW_BUCKETS_PER_ADDRESS:
                info.ref_count += 1
                address_manager.new_matrix[bucket][bucket_pos] = node_id

    for node_id, info in list(address_manager.map_info.items()):
        if not info.is_tried and info.ref_count == 0:
            address_manager.delete_new_entry_(node_id)
    address_manager.load_used_table_positions()

    return address_manager",_9271.py,43,node_id >= 0 and node_id < address_manager.new_count,0 <= node_id < address_manager.new_count
https://github.com/facebookresearch/ParlAI/tree/master/parlai/crowdsourcing/tasks/model_chat/worlds.py,"def parley(self):
        print(
            f'{self.__class__.__name__}:{self.tag}: is at turn {self.task_turn_idx}, with {self.num_turns} pairs of turns needed...'
        )

        if self.task_turn_idx == 0:
            self._run_initial_turn()
            self.task_turn_idx += 1
            return

        """"""Otherwise, we proceed accordingly""""""
        print(
            f'{self.__class__.__name__}:{self.tag}: About to act with task turn idx: {self.task_turn_idx}'
        )
        acts = [None, None]
        for idx, agent in enumerate([self.agent, self.bot]):
            if not self.chat_done:
                acts[idx] = agent.act(timeout=self.max_resp_time)
                if (
                    agent == self.bot
                    and hasattr(self.bot, 'agent_id')
                    and self.bot.agent_id
                ):
                    # Set speaker name as self.bot_agent_id otherwise, at frontend bot name such as ""TransformerGenerator"" would appear
                    Compatibility.backward_compatible_force_set(
                        acts[idx], 'id', self.bot.agent_id
                    )
                acts[idx] = Message(
                    Compatibility.maybe_fix_act(acts[idx])
                ).json_safe_payload()
                print(
                    f'Got act for agent idx {idx}, act was: {acts[idx]} and self.task_turn_idx: {self.task_turn_idx}.'
                )

            if acts[idx].get('task_data', {}).get('final_rating') is not None:

                self.chat_done = True
                # agent ends chat after exceeding minimum number of turns

                # Human has just responded. Any problem data received now will be
                # regarding the bot's prior utterance
                turn_idx = -1
                # Attach the problem data and final rating to the last utterance, since
                # the human hasn't said anything since then
                p = acts[idx]['task_data'].get('problem_data_for_prior_message')
                if p is not None:
                    self.__add_problem_data_to_utterance(p, turn_idx=turn_idx)
                self.dialog[turn_idx]['final_rating'] = acts[idx]['task_data'][
                    'final_rating'
                ]

                # Save the final chat data
                date_folder = time.strftime('%Y_%m_%d')
                time_string = time.strftime('%Y%m%d_%H%M%S')
                chat_data_subfolder = os.path.join(
                    self.opt['chat_data_folder'], date_folder
                )
                os.makedirs(chat_data_subfolder, exist_ok=True)
                chat_data_path = os.path.join(
                    chat_data_subfolder,
                    f'{time_string}_{np.random.randint(0, 1000)}_{self.task_type}.json',
                )
                self.final_chat_data = self.get_final_chat_data()
                self.agent.mephisto_agent.state.messages.append(
                    {
                        'final_chat_data': self.final_chat_data,
                        'data': {},
                        'packet_type': None,
                        'timestamp': None,
                    }
                )
                # Append the chat data directly to the agent state's message list in
                # order to prevent the worker from seeing a new text response in the UI.
                # Add some dummy keys for compatibility with all agent state messages
                # TODO: remove this when no longer saving data to disk manually
                with open(chat_data_path, 'w+') as f_json:
                    data_str = json.dumps(self.final_chat_data)
                    f_json.write(data_str)
                print(
                    f'{self.__class__.__name__}:{self.tag}: Data saved at '
                    f'{chat_data_path} for model: {self.bot.worker_id}.'
                )

                # Soft-block the worker if there were acceptability violations
                acceptability_violations = self.final_chat_data[
                    'acceptability_violations'
                ][0]
                if (
                    acceptability_violations is not None
                    and acceptability_violations != ''
                ):
                    print(
                        f'**NOTE** Acceptability violations detected: {acceptability_violations}'
                    )
                    # Grant the failed qualification
                    self.agent.mephisto_agent.get_worker().grant_qualification(
                        self.block_qualification, 1
                    )

                return

            else:
                utterance_data = {
                    'agent_idx': idx,
                    # Get rid of annotations HTML if it's the bot response
                    'text': acts[idx]['text'].split('<br>')[0],
                    'id': acts[idx].get(
                        'id', 'NULL_ID'
                    ),  # In case model doesn't set id
                }
                self.dialog.append(utterance_data)
                if idx == 0:
                    # Human has just responded. Any problem data received now will be
                    # regarding the bot's prior utterance
                    p = acts[idx]['task_data'].get('problem_data_for_prior_message')
                    if p is not None:
                        turn_idx = -2
                        # Attach the problem data to the second-to-last utterance, since
                        # the last utterance is what the human just said
                        self.__add_problem_data_to_utterance(p, turn_idx=turn_idx)

                self._postprocess_acts(acts=acts, agent_idx=idx)
                for other_agent in [self.agent, self.bot]:
                    if other_agent != agent:
                        other_agent.observe(validate(acts[idx]))

                print(
                    f'[agent {idx}] self.task_turn_idx: {self.task_turn_idx}, self.dialog is: {self.dialog}'
                )
                self.task_turn_idx += 1",_9511.py,89,acceptability_violations is not None and acceptability_violations != '',None is not acceptability_violations != ''
https://github.com/microsoft/DeepSpeed/tree/master/deepspeed/ops/transformer/transformer.py,"def __init__(self, config, initial_weights=None, initial_biases=None):
        super(DeepSpeedTransformerLayer, self).__init__()

        self.config = config
        self.config.layer_id = DeepSpeedTransformerLayer.layer_id
        DeepSpeedTransformerLayer.layer_id = DeepSpeedTransformerLayer.layer_id + 1

        print(""DeepSpeed Transformer config is "", self.config.__dict__)

        if self.config.local_rank >= 0:
            torch.cuda.set_device(self.config.local_rank)

        if initial_weights is None and initial_biases is None:
            self.attn_qkvw = nn.Parameter(
                torch.Tensor(self.config.hidden_size * 3,
                             self.config.hidden_size))
            self.attn_qkvb = nn.Parameter(torch.Tensor(self.config.hidden_size * 3))
            self.attn_ow = nn.Parameter(
                torch.Tensor(self.config.hidden_size,
                             self.config.hidden_size))
            self.attn_ob = nn.Parameter(torch.Tensor(self.config.hidden_size))
            self.attn_nw = nn.Parameter(torch.Tensor(self.config.hidden_size))
            self.attn_nb = nn.Parameter(torch.Tensor(self.config.hidden_size))
            self.inter_w = nn.Parameter(
                torch.Tensor(self.config.intermediate_size,
                             self.config.hidden_size))
            self.inter_b = nn.Parameter(torch.Tensor(self.config.intermediate_size))
            self.output_w = nn.Parameter(
                torch.Tensor(self.config.hidden_size,
                             self.config.intermediate_size))
            self.output_b = nn.Parameter(torch.Tensor(self.config.hidden_size))
            self.norm_w = nn.Parameter(torch.Tensor(self.config.hidden_size))
            self.norm_b = nn.Parameter(torch.Tensor(self.config.hidden_size))
            self.init_transformer_weights(self.config.adjust_init_range)
        else:
            # For testing only.
            q = initial_weights[0].data
            k = initial_weights[1].data
            v = initial_weights[2].data

            self.attn_qkvw = nn.Parameter(torch.cat((q, k, v)))
            #self.attn_qkvw[i * self.config.hidden_size:(i + 1) * self.config.hidden_size] = \
            #    initial_weights[i].clone()
            #torch.empty_like(initial_weights[i]).data.copy_(initial_weights[i].data)
            self.attn_qkvb = nn.Parameter(torch.Tensor(self.config.hidden_size * 3))
            self.attn_qkvb.data.zero_()
            self.attn_ow = initial_weights[3]
            self.attn_ob = initial_biases[3]
            self.attn_nw = initial_weights[4]
            self.attn_nb = initial_biases[4]
            self.inter_w = initial_weights[5]
            self.inter_b = initial_biases[5]
            self.output_w = initial_weights[6]
            self.output_b = initial_biases[6]
            self.norm_w = initial_weights[7]
            self.norm_b = initial_biases[7]

        # Load cuda modules if needed
        global transformer_cuda_module, stochastic_transformer_cuda_module
        if transformer_cuda_module is None and not self.config.stochastic_mode:
            transformer_cuda_module = TransformerBuilder().load()
        if stochastic_transformer_cuda_module is None and self.config.stochastic_mode:
            stochastic_transformer_cuda_module = StochasticTransformerBuilder().load()

        # create the layer in cuda kernels.
        cuda_module = stochastic_transformer_cuda_module if self.config.stochastic_mode else transformer_cuda_module
        create_layer_func = cuda_module.create_transformer_layer_fp16 if self.config.fp16 else cuda_module.create_transformer_layer_fp32

        create_layer_func(self.config.layer_id,
                          self.config.batch_size,
                          self.config.hidden_size,
                          self.config.heads,
                          self.config.intermediate_size,
                          self.config.attn_dropout_ratio,
                          self.config.hidden_dropout_ratio,
                          self.config.layer_norm_eps,
                          self.config.seed,
                          self.config.pre_layer_norm,
                          self.config.test_gemm,
                          self.config.attn_dropout_checkpoint,
                          self.config.normalize_invertible,
                          self.config.gelu_checkpoint,
                          self.config.stochastic_mode)",_9517.py,13,initial_weights is None and initial_biases is None,initial_weights is None is initial_biases
https://github.com/dmlc/gluon-nlp/tree/master/scripts/classification/train_classification.py,"def train(args):
    store, num_workers, rank, local_rank, is_master_node, ctx_l = init_comm(
        args.comm_backend, args.gpus)
    task = get_task(args.task_name, args.train_dir, args.eval_dir)
    #setup_logging(args, local_rank)
    #random seed
    set_seed(args.seed)
    level = logging.INFO
    detail_dir = os.path.join(args.output_dir, args.task_name)
    if not os.path.exists(detail_dir):
        os.mkdir(detail_dir)
    logging_config(detail_dir,
                   name='train_{}_{}_'.format(args.task_name, args.model_name) + str(rank),  # avoid race
                   level=level,
                   console=(local_rank == 0))
    logging.info(args)
    cfg, tokenizer, classify_net, use_segmentation = \
        get_network(args.model_name, ctx_l,
                    args.param_checkpoint,
                    args.backbone_path,
                    task)

    logging.info('Prepare training data')
    train_data, _ = get_task_data(args, task, tokenizer, segment='train')
    train_batchify = bf.Group(bf.Group(bf.Pad(), bf.Pad(), bf.Stack()),
                              bf.Stack())

    rs = np.random.RandomState(100)
    rs.shuffle(train_data)
    sampler = SplitSampler(
        len(train_data),
        num_parts=num_workers,
        part_index=rank,
        even_size=True)

    dataloader = DataLoader(train_data,
                            batch_size=args.batch_size,
                            batchify_fn=train_batchify,
                            num_workers=0,
                            sampler=sampler)



    param_dict = classify_net.collect_params()
    # Do not apply weight decay to all the LayerNorm and bias
    for _, v in classify_net.collect_params('.*beta|.*gamma|.*bias').items():
        v.wd_mult = 0.0
    # Set grad_req if gradient accumulation is required
    params = [p for p in param_dict.values() if p.grad_req != 'null']
    num_accumulated = args.num_accumulated
    if num_accumulated > 1:
        logging.info('Using gradient accumulation. Effective global batch size = {}'
                     .format(num_accumulated * args.batch_size * len(ctx_l) * num_workers))
        for p in params:
            p.grad_req = 'add'
    if local_rank == 0:
        writer = SummaryWriter(logdir=os.path.join(args.output_dir,
                                                   args.task_name + '_tensorboard_' +
                                                   str(args.lr) + '_' + str(args.epochs)))
    if args.comm_backend == 'horovod':
        # Horovod: fetch and broadcast parameters
        hvd.broadcast_parameters(param_dict, root_rank=0)

    epoch_size = (len(dataloader) + len(ctx_l) - 1) // len(ctx_l)
    max_update = epoch_size * args.epochs
    warmup_steps = int(np.ceil(max_update * args.warmup_ratio))

    dataloader = grouper(repeat(dataloader), len(ctx_l))

    lr_scheduler = PolyScheduler(max_update=max_update,
                                 base_lr=args.lr,
                                 warmup_begin_lr=0.0,
                                 pwr=1,
                                 final_lr=0.0,
                                 warmup_steps=warmup_steps,
                                 warmup_mode='linear')
    optimizer_params = {'learning_rate': args.lr,
                        'wd': args.wd,
                        'lr_scheduler': lr_scheduler}
    if args.comm_backend == 'horovod':
        trainer = hvd.DistributedTrainer(param_dict, args.optimizer, optimizer_params)
    else:
        trainer = mx.gluon.Trainer(classify_net.collect_params(),
                                   'adamw',
                                   optimizer_params)

    if args.task_name == 'sts':
        loss_function = gluon.loss.L2Loss()
    else:
        loss_function = gluon.loss.SoftmaxCELoss()

    metrics = task.metric
    #prepare loss function
    log_loss = 0
    log_gnorm = 0
    log_step = 0
    if args.log_interval > 0:
        log_interval = args.log_interval
    else:
        log_interval = int(epoch_size * 0.5)

    start_time = time.time()
    total_loss = 0
    total_grad = 0
    total_step = 0
    for i in range(max_update):
        sample_l = next(dataloader)
        loss_l = []
        for sample, ctx in zip(sample_l, ctx_l):
            (token_ids, token_types, valid_length), label = sample
            # Move to the corresponding context
            token_ids = mx.np.array(token_ids, ctx=ctx)
            token_types = mx.np.array(token_types, ctx=ctx)
            valid_length = mx.np.array(valid_length, ctx=ctx)
            label = mx.np.array(label, ctx=ctx)
            with mx.autograd.record():
                scores = classify_net(token_ids, token_types, valid_length)
                loss = loss_function(scores, label).mean() / len(ctx_l)
                loss_l.append(loss)
            if task.task_name == 'sts':
                label = label.reshape((-1, 1))
            for metric in metrics:
                metric.update([label], [scores])

        for loss in loss_l:
            loss.backward()
        trainer.allreduce_grads()
        # Begin Norm Clipping
        total_norm, ratio, is_finite = clip_grad_global_norm(params, args.max_grad_norm)
        trainer.update(1.0)
        step_loss = sum([loss.asnumpy() for loss in loss_l])
        log_loss += step_loss
        log_gnorm += total_norm
        log_step += 1
        total_step += 1
        total_loss += step_loss
        total_grad += total_norm
        if local_rank == 0:
            writer.add_scalar('train_loss_avg', total_loss * 1.0 / total_step, i)
            writer.add_scalar('lr', trainer.learning_rate, i)
            writer.add_scalar('train_loss', step_loss, i)
            writer.add_scalar('grad_norm_avg', total_grad * 1.0 / total_step, i)
            writer.add_scalar('grad_norm', total_norm, i)
            for metric in metrics:
                metric_name, result = metric.get()
                writer.add_scalar(metric_name, result, i)
        if log_step >= log_interval or i == max_update - 1:
            curr_time = time.time()
            metric_log = ''
            for metric in metrics:
                metric_nm, val = metric.get()
                metric_log += ', {}: = {}'.format(metric_nm, val)
            logging.info('[Iter {} / {}] avg {} = {:.2f}, avg gradient norm = {:.2f}, lr = {}, ETA={:.2f}h'.format(i + 1,
                                                                                      max_update,
                                                                                      'loss',
                                                                                      log_loss / log_step,
                                                                                      log_gnorm / log_step,
                                                                                      trainer.learning_rate,

                                                                         (max_update-i)*((curr_time - start_time)/i)/3600)
                                                                                + metric_log)
            log_loss = 0
            log_gnorm = 0
            log_step = 0
        if local_rank == 0 and (i == max_update - 1 or i%(max_update//args.epochs) == 0 and i>0):
            ckpt_name = '{}_{}_{}.params'.format(args.model_name,
                                                 args.task_name,
                                                 (i + 1))

            params_saved = os.path.join(detail_dir, ckpt_name)
            classify_net.save_parameters(params_saved)
            logging.info('Params saved in: {}'.format(params_saved))
            for metric in metrics:
                metric.reset()",_9685.py,165,i % (max_update // args.epochs) == 0 and i > 0,i % (max_update // args.epochs) == 0 < i
https://github.com/blackjack4494/yt-dlc/tree/master/youtube_dlc/downloader/fragment.py,"def _prepare_frag_download(self, ctx):
        if 'live' not in ctx:
            ctx['live'] = False
        if not ctx['live']:
            total_frags_str = '%d' % ctx['total_frags']
            ad_frags = ctx.get('ad_frags', 0)
            if ad_frags:
                total_frags_str += ' (not including %d ad)' % ad_frags
        else:
            total_frags_str = 'unknown (live)'
        self.to_screen(
            '[%s] Total fragments: %s' % (self.FD_NAME, total_frags_str))
        self.report_destination(ctx['filename'])
        dl = HttpQuietDownloader(
            self.ydl,
            {
                'continuedl': True,
                'quiet': True,
                'noprogress': True,
                'ratelimit': self.params.get('ratelimit'),
                'retries': self.params.get('retries', 0),
                'nopart': self.params.get('nopart', False),
                'test': self.params.get('test', False),
            }
        )
        tmpfilename = self.temp_name(ctx['filename'])
        open_mode = 'wb'
        resume_len = 0

        # Establish possible resume length
        if os.path.isfile(encodeFilename(tmpfilename)):
            open_mode = 'ab'
            resume_len = os.path.getsize(encodeFilename(tmpfilename))

        # Should be initialized before ytdl file check
        ctx.update({
            'tmpfilename': tmpfilename,
            'fragment_index': 0,
        })

        if self.__do_ytdl_file(ctx):
            if os.path.isfile(encodeFilename(self.ytdl_filename(ctx['filename']))):
                self._read_ytdl_file(ctx)
                is_corrupt = ctx.get('ytdl_corrupt') is True
                is_inconsistent = ctx['fragment_index'] > 0 and resume_len == 0
                if is_corrupt or is_inconsistent:
                    message = (
                        '.ytdl file is corrupt' if is_corrupt else
                        'Inconsistent state of incomplete fragment download')
                    self.report_warning(
                        '%s. Restarting from the beginning...' % message)
                    ctx['fragment_index'] = resume_len = 0
                    if 'ytdl_corrupt' in ctx:
                        del ctx['ytdl_corrupt']
                    self._write_ytdl_file(ctx)
            else:
                self._write_ytdl_file(ctx)
                assert ctx['fragment_index'] == 0

        dest_stream, tmpfilename = sanitize_open(tmpfilename, open_mode)

        ctx.update({
            'dl': dl,
            'dest_stream': dest_stream,
            'tmpfilename': tmpfilename,
            # Total complete fragments downloaded so far in bytes
            'complete_frags_downloaded_bytes': resume_len,
        })",_9848.py,45,ctx['fragment_index'] > 0 and resume_len == 0,ctx['fragment_index'] > 0 == resume_len
https://github.com/Blazemeter/taurus/tree/master/bzt/modules/blazemeter/cloud_provisioning.py,"def __get_kpiset(self, aggr, kpi, label):
        kpiset = KPISet()
        kpiset[KPISet.FAILURES] = kpi['ec']
        kpiset[KPISet.CONCURRENCY] = kpi['na']
        kpiset[KPISet.SAMPLE_COUNT] = kpi['n']
        assert kpi['n'] > 0 and kpi['n'] >= kpi['ec']
        kpiset[KPISet.SUCCESSES] = kpi['n'] - kpi['ec']
        kpiset.sum_rt += kpi['t_avg'] * kpi['n'] / 1000.0
        kpiset.sum_lt += kpi['lt_avg'] * kpi['n'] / 1000.0
        perc_map = {'90line': 90.0, ""95line"": 95.0, ""99line"": 99.0}
        for field, level in iteritems(perc_map):
            kpiset[KPISet.PERCENTILES][str(level)] = aggr[label][field] / 1000.0
        return kpiset",_9856.py,6,kpi['n'] > 0 and kpi['n'] >= kpi['ec'],0 < kpi['n'] >= kpi['ec']
https://github.com/ecederstrand/exchangelib/tree/master/exchangelib/fields.py,"def clean(self, value, version=None):
        if self.min is not None and value < self.min:
            raise ValueError(f""Value {value!r} on field {self.name!r} must be greater than {self.min}"")
        if self.max is not None and value > self.max:
            raise ValueError(f""Value {value!r} on field {self.name!r} must be less than {self.max}"")
        return super().clean(value, version=version)",_10048.py,2,self.min is not None and value < self.min,None is not self.min > value
https://github.com/ecederstrand/exchangelib/tree/master/exchangelib/fields.py,"def clean(self, value, version=None):
        if self.min is not None and value < self.min:
            raise ValueError(f""Value {value!r} on field {self.name!r} must be greater than {self.min}"")
        if self.max is not None and value > self.max:
            raise ValueError(f""Value {value!r} on field {self.name!r} must be less than {self.max}"")
        return super().clean(value, version=version)",_10048.py,4,self.max is not None and value > self.max,None is not self.max < value
https://github.com/PaddlePaddle/PaddleHub/tree/master/modules/text/text_generation/plato2_en_large/tasks/dialog_generation.py,"def _post_process_generation_output(self, predictions):
        """"""
        Post process generation output.

        Calculate repetion, reranking.
        """"""
        for info in predictions:
            tokens = post_process_context(info[""context_token_ids""], self.reader)
            pred_token_ids, pred_tokens = post_process_response(info[""response_token_ids""], self.reader)
            info[""context""] = "" [SEP] "".join("" "".join(u) for u in tokens)
            info[""response""] = "" "".join(pred_tokens)
            info[""num_token""] = len(pred_token_ids)
            info[""cross_turn_repetition""] = get_cross_turn_repetition(tokens, pred_tokens, self.reader.eos_id,
                                                                      self.is_cn)
            info[""in_turn_repetition""] = max(
                get_in_turn_repetition(pred_tokens, self.is_cn), get_in_turn_repetition(pred_token_ids))
        if self.nsp_predictor is not None:
            get_nsp_score_batch(self.nsp_predictor, predictions)

        group = defaultdict(list)
        for info in predictions:
            group[info[""data_id""]].append(info)

        predictions = []
        for data_id in group:
            infos = group[data_id]
            for info in infos:
                info[""score""] = info[self.ranking_score]
                if self.max_dec_len is not None and info[""num_token""] >= self.max_dec_len:  # not ending
                    info[""score""] -= 1e3
                elif info[""cross_turn_repetition""] > 0:
                    info[""score""] -= 1e3
                elif info[""in_turn_repetition""] > 0:
                    info[""score""] -= 1e3
            infos = sorted(infos, key=lambda info: -info[""score""])
            pred = infos[0]
            keep_attr = [""data_id"", ""score"", ""response""]
            pred = {k: pred[k] for k in keep_attr}
            predictions.append(pred)
        return predictions",_10530.py,29,self.max_dec_len is not None and info['num_token'] >= self.max_dec_len,None is not self.max_dec_len <= info['num_token']
https://github.com/zentralopensource/zentral/tree/master/zentral/contrib/mdm/models.py,"def can_be_deleted(self):
        return (
            self.depenrollment_set.count() == 0
            and self.otaenrollment_set.count() == 0
            and self.userenrollment_set.count() == 0
        )",_10583.py,3,self.depenrollment_set.count() == 0 and self.otaenrollment_set.count() == 0 and (self.userenrollment_set.count() == 0),self.depenrollment_set.count() == 0 == self.otaenrollment_set.count() and self.userenrollment_set.count() == 0
https://github.com/Qirky/FoxDot/tree/master/FoxDot/lib/OSC3.py,"def _unbundle(self, decoded):
        """"""Recursive bundle-unpacking function""""""
        
        if decoded[0] != ""#bundle"":
            self.replies += self.server.dispatchMessage(decoded[0], decoded[1][1:], decoded[2:], self.client_address)
            return
        
        now = time.time()
        timetag = decoded[1]
        if (timetag > 0.) and (timetag > now):
            time.sleep(timetag - now)
        
        for msg in decoded[2:]:
            self._unbundle(msg)",_10611.py,10,timetag > 0.0 and timetag > now,0.0 < timetag > now
https://github.com/kidscancode/pygame_tutorials/tree/master/examples/quad example.py,"def move_8way(self):
        keystate = pg.key.get_pressed()
        if keystate[pg.K_UP]:
            self.vy = -5
        if keystate[pg.K_DOWN]:
            self.vy = 5
        if keystate[pg.K_LEFT]:
            self.vx = -5
        if keystate[pg.K_RIGHT]:
            self.vx = 5
        if self.vx != 0 and self.vy != 0:
            self.vx *= 0.7071
            self.vy *= 0.7071",_10691.py,11,self.vx != 0 and self.vy != 0,self.vx != 0 != self.vy