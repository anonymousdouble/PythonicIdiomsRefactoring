file_html,method_content,file_name,lineno,old_code,new_code,,,
https://github.com/speechbrain/speechbrain/tree/master/recipes/Fisher-Callhome-Spanish/Tokenizer/fisher_callhome_prepare.py,"def make_data_splits(
    mapping_folder: str = ""../data/fisher-callhome-corpus/mapping"",
):
    """"""make data split from mapping file""""""
    fisher_splits = [""dev"", ""dev2"", ""test"", ""train""]

    if not os.path.exists(""splits""):
        os.mkdir(""splits"")

        for fisher_split in fisher_splits:
            split = set()
            with open(
                f""{mapping_folder}/fisher_{fisher_split}"", ""r"", encoding=""utf-8""
            ) as fisher_file, open(
                f""./splits/{fisher_split}"", ""a+"", encoding=""utf-8""
            ) as split_file:
                fisher_file_lines = fisher_file.readlines()

                for fisher_file_line in fisher_file_lines:
                    fisher_file_line = fisher_file_line.strip()
                    fisher_file_id = fisher_file_line.split("" "")[0]
                    split.add(fisher_file_id)

                split = sorted(list(split))
                for file_id in split:
                    split_file.write(f""{file_id}\n"")",_17302.py,19,"for fisher_file_line in fisher_file_lines:
    fisher_file_line = fisher_file_line.strip()
    fisher_file_id = fisher_file_line.split(' ')[0]
    split.add(fisher_file_id)","split = {fisher_file_line.strip().split(' ')[0]
for fisher_file_line in fisher_file_lines}",数据依赖,,
https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/landmanager.py,"def _get_coastline(self):
		result = set()
		for coords in self.island.ground_map:
			tile = self.island.ground_map[coords]
			if 'coastline' not in tile.classes:
				continue
			if tile.object is not None and not tile.object.buildable_upon:
				continue
			if tile.settlement is not None and tile.settlement.owner is not self.owner:
				continue
			result.add(coords)
		return result",_20553.py,3,"for coords in self.island.ground_map:
    tile = self.island.ground_map[coords]
    if 'coastline' not in tile.classes:
        continue
    if tile.object is not None and (not tile.object.buildable_upon):
        continue
    if tile.settlement is not None and tile.settlement.owner is not self.owner:
        continue
    result.add(coords)",result = {coords for coords in self.island.ground_map if 'coastline' in self.island.ground_map[coords.classes] and  (self.island.ground_map[coords.object is None or self.island.ground_map[coords.object.buildable_upon) and (self.island.ground_map[coords.settlement is None or self.island.ground_map[coords.settlement.owner is self.owner)},continue,,
https://github.com/beancount/beancount/tree/master/beancount/core/getters.py,"def get_all_payees(entries):
    """"""Return a list of all the unique payees seen in the given entries.

    Args:
      entries: A list of directive instances.
    Returns:
      A set of payee strings.
    """"""
    all_payees = set()
    for entry in entries:
        if not isinstance(entry, Transaction):
            continue
        all_payees.add(entry.payee)
    all_payees.discard(None)
    return sorted(all_payees)",_28776.py,10,"for entry in entries:
    if not isinstance(entry, Transaction):
        continue
    all_payees.add(entry.payee)","all_payees = {entry.payee for entry in entries if isinstance(entry, Transaction)}",continue,,
https://github.com/LinOTP/LinOTP/tree/master/linotp/lib/token.py,"def removeToken(self, user=None, serial=None):
        """"""
        delete a token from database

        :param user: the tokens of the user
        :param serial: the token with this serial number

        :return: the number of deleted tokens
        """"""
        if (user is None or user.is_empty) and (serial is None):
            raise ParameterError(""Parameter user or serial required!"", id=1212)

        tokenList = getTokens4UserOrSerial(user, serial, _class=False)

        serials = set()
        tokens = set()
        token_ids = set()
        try:

            for token in tokenList:
                ser = token.getSerial()
                serials.add(ser)
                token_ids.add(token.LinOtpTokenId)
                tokens.add(token)

            #  we cleanup the challenges
            challenges = set()
            for serial in serials:
                challenges.update(Challenges.lookup_challenges(serial=serial))

            for chall in challenges:
                db.session.delete(chall)

            #  due to legacy SQLAlchemy it could happen that the
            #  foreign key relation could not be deleted
            #  so we do this manualy

            for t_id in set(token_ids):
                TokenRealm.query.filter(TokenRealm.token_id == t_id).delete()

            db.session.commit()

            for token in tokens:
                db.session.delete(token)

        except Exception as exx:
            raise TokenAdminError(
                ""removeToken: Token update failed: %r"" % exx, id=1132
            )

        return len(serials)",_31493.py,20,"for token in tokenList:
    ser = token.getSerial()
    serials.add(ser)
    token_ids.add(token.LinOtpTokenId)
    tokens.add(token)","serials = {token.getSerial()  for token in tokenList}
token_ids = {token.LinOtpTokenId  for token in tokenList}
tokens = {token for token in tokenList}",multiple sets,,
https://github.com/rackerlabs/scantron/tree/master/console/extract_targets.py,"def retrieve_amazon_cloudfront_ip_ranges(
    retrieve_new_data=False, aws_cloudfront_filename=""aws_cloudfront_ip_networks.txt"", write_to_disk=True
):
    """"""Retrieve the IPv4 and IPv6 ranges for AWS' CloudFront servers.

    https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/LocationsOfEdgeServers.html
    """"""

    cloudfront_dict = {
        ""list_of_strings"": set(),
        ""list_of_ipaddress_objects"": set(),
    }

    # If aws_cloudfront_filename already exists and fresh data isn't requested.
    if os.path.exists(aws_cloudfront_filename) and not retrieve_new_data:

        print(f""File already exists: {aws_cloudfront_filename}"")

        with open(aws_cloudfront_filename, ""r"") as fh:
            for ip_network in fh.readlines():
                cloudfront_dict[""list_of_ipaddress_objects""].add(ipaddress.ip_network(ip_network.strip()))

    else:

        print(""Retrieving IPv4 and IPv6 network ranges for AWS' CloudFront servers."")

        url = ""https://ip-ranges.amazonaws.com/ip-ranges.json""
        response = requests.get(url, verify=True)

        if response.status_code == 200:
            json_data = response.json()

            for service in json_data[""prefixes""]:
                if service[""service""] == ""CLOUDFRONT"":
                    cloudfront_dict[""list_of_ipaddress_objects""].add(ipaddress.ip_network(service[""ip_prefix""]))

            for service in json_data[""ipv6_prefixes""]:
                if service[""service""] == ""CLOUDFRONT"":
                    cloudfront_dict[""list_of_ipaddress_objects""].add(ipaddress.ip_network(service[""ipv6_prefix""]))

        else:
            print(""CloudFront IP networks could not be retrieved."")

    # Return a list of sorted IPv4 and IPv6 networks.
    # See https://docs.python.org/3/library/ipaddress.html#ipaddress.get_mixed_type_key
    cloudfront_dict[""list_of_ipaddress_objects""] = sorted(
        cloudfront_dict[""list_of_ipaddress_objects""], key=lambda obj: ipaddress.get_mixed_type_key(obj)
    )

    # Convert ipaddress objects to strings.
    cloudfront_dict[""list_of_strings""] = [str(obj) for obj in cloudfront_dict[""list_of_ipaddress_objects""]]

    # Only write to disk if fresh data is requested.
    if write_to_disk and retrieve_new_data:
        print(f""Writing CloudFront IP networks to disk: {aws_cloudfront_filename}"")
        with open(aws_cloudfront_filename, ""w"") as fh:
            for ip_network in cloudfront_dict[""list_of_strings""]:
                fh.write(f""{ip_network}\n"")

    # print(f""cloudfront_dict: {cloudfront_dict}"")

    return cloudfront_dict",_32396.py,20,"for ip_network in fh.readlines():
    cloudfront_dict['list_of_ipaddress_objects'].add(ipaddress.ip_network(ip_network.strip()))",cloudfront_dict[‘list_of_ipaddress_objects'] = {ipaddress.ip_network(ip_network.strip()) for ip_network in fh.readlines()},内嵌的数据类型,,
https://github.com/Ekultek/WhatBreach/tree/master/hookers/weleakinfo_hook.py,"def _parse_results(self, content):
        """"""
        parse the results from the requested shit
        """"""
        parsed_leaks = set()
        for item in content:
            if ""-"" in item:
                item = item.split("" "")[0]
            elif ""("" in item:
                item = item.split(""("")[0]
            parsed_leaks.add(str(item))
        return parsed_leaks",_40129.py,6,"for item in content:
    if '-' in item:
        item = item.split(' ')[0]
    elif '(' in item:
        item = item.split('(')[0]
    parsed_leaks.add(str(item))",parsed_leaks = {item.split(' ')[0] if '-' in item else item.split(‘(')[0] if '(' in item else item for item in content},nan,,
https://github.com/quay/quay/tree/master/buildtrigger/basehandler.py,"def get_parent_directory_mappings(cls, dockerfile_path, current_paths=None):
        """"""
        Returns a map of dockerfile_paths to it's possible contexts.
        """"""
        if dockerfile_path == """":
            return {}

        if dockerfile_path[0] != os.path.sep:
            dockerfile_path = os.path.sep + dockerfile_path

        dockerfile_path = os.path.normpath(dockerfile_path)
        all_paths = set()
        path, _ = os.path.split(dockerfile_path)
        if path == """":
            path = os.path.sep

        all_paths.add(path)
        for i in range(1, len(path.split(os.path.sep))):
            path, _ = os.path.split(path)
            all_paths.add(path)

        if current_paths:
            return dict({dockerfile_path: list(all_paths)}, **current_paths)

        return {dockerfile_path: list(all_paths)}",_77865.py,18,"for i in range(1, len(path.split(os.path.sep))):
    (path, _) = os.path.split(path)
    all_paths.add(path)","all_paths |= {os.path.split(path)[0]
for i in range(1, len(path.split(os.path.sep)))}",nan,,
https://github.com/JDAI-CV/FaceX-Zoo/tree/master/addition_module/face_lightning/KDF/test_protocol/remove_noises.py,"def remove_megaface_noises(megaface_noises_file, megaface_feature_dir, megaface_feature_outdir):
    """"""Remove the noise in megaface.
    We set the feature of noise faces to zero vector, 
    since we use cos similarity as the distance metric.

    Args:
        megaface_noises_file(str): the path of megaface noise list provided by deepglint.
        megaface_feature_dir(str): thpe directory which contains the features of megaface.
        megaface_feature_outdir(str): the directory to save the clean features of megaface.
    """"""
    noise_image_set = set()
    for line in open(megaface_noises_file, 'r'):
        if line.startswith('#'):
            continue
        line = line.strip()
        _vec = line.split(""\t"")
        if len(_vec)>1:
            line = _vec[1]
        noise_image_set.add(line)
    logger.info('Total noise images in megaface: %d.' % len(noise_image_set))

    count_noises = 0
    for root, dirs, files in os.walk(megaface_feature_dir):
        for feat_name in files:
            feat_name_ext = os.path.splitext(feat_name)[-1]
            if feat_name_ext == '.npy':
                feat_path = os.path.join(root, feat_name)
                assert(os.path.exists(feat_path))
                id1 = feat_path.split('/')[-3]
                id2 = feat_path.split('/')[-2]
                image_name = feat_name[:-4] + '.jpg'
                cur_feature_outdir = os.path.join(megaface_feature_outdir, id1, id2)
                if not os.path.exists(cur_feature_outdir):
                    os.makedirs(cur_feature_outdir)
                cur_feature_outpath = os.path.join(cur_feature_outdir, feat_name)
                short_image_path = os.path.join(id1, id2, image_name)
                if not short_image_path in noise_image_set:
                    shutil.copyfile(feat_path, cur_feature_outpath)
                else:
                    cur_feature =  np.zeros((feature_dim,), dtype=np.float32)
                    np.save(cur_feature_outpath, cur_feature)
                    count_noises += 1
    logger.info('Total noise images in current megaface dir: %d.' % count_noises)",_127069.py,12,"for line in open(megaface_noises_file, 'r'):
    if line.startswith('#'):
        continue
    line = line.strip()
    _vec = line.split('\t')
    if len(_vec) > 1:
        line = _vec[1]
    noise_image_set.add(line)","noise_image_set = {line.strip(.split('\t'[1] if len(ine.strip(.split(‘\t'))>1 else line.strip()  for line in open(megaface_noises_file, ‘r') if not line.startswith(‘#') }",数据依赖,,
https://github.com/PennyLaneAI/pennylane/tree/master/pennylane/ops/qubit/hamiltonian.py,"def _obs_data(self):
        r""""""Extracts the data from a Hamiltonian and serializes it in an order-independent fashion.

        This allows for comparison between Hamiltonians that are equivalent, but are defined with terms and tensors
        expressed in different orders. For example, `qml.PauliX(0) @ qml.PauliZ(1)` and
        `qml.PauliZ(1) @ qml.PauliX(0)` are equivalent observables with different orderings.

        .. Note::

            In order to store the data from each term of the Hamiltonian in an order-independent serialization,
            we make use of sets. Note that all data contained within each term must be immutable, hence the use of
            strings and frozensets.

        **Example**

        >>> H = qml.Hamiltonian([1, 1], [qml.PauliX(0) @ qml.PauliX(1), qml.PauliZ(0)])
        >>> print(H._obs_data())
        {(1, frozenset({('PauliX', <Wires = [1]>, ()), ('PauliX', <Wires = [0]>, ())})),
         (1, frozenset({('PauliZ', <Wires = [0]>, ())}))}
        """"""
        data = set()

        coeffs_arr = qml.math.toarray(self.coeffs)
        for co, op in zip(coeffs_arr, self.ops):
            obs = op.non_identity_obs if isinstance(op, Tensor) else [op]
            tensor = []
            for ob in obs:
                parameters = tuple(
                    str(param) for param in ob.parameters
                )  # Converts params into immutable type
                tensor.append((ob.name, ob.wires, parameters))
            data.add((co, frozenset(tensor)))

        return data",_143293.py,24,"for (co, op) in zip(coeffs_arr, self.ops):
    obs = op.non_identity_obs if isinstance(op, Tensor) else [op]
    tensor = []
    for ob in obs:
        parameters = tuple((str(param) for param in ob.parameters))
        tensor.append((ob.name, ob.wires, parameters))
    data.add((co, frozenset(tensor)))","data = { (co, frozenset()[(ob.name, ob.wires, tuple((str(param) for param in ob.parameters))) 
for ob in op.non_identity_obs if isinstance(op, Tensor) else [op]])
for (co, op) in zip(coeffs_arr, self.ops)}",数据依赖,,
https://github.com/LangziFun/LangSrcCurise/tree/master/core/Subdomain_Api.py,"def Sitedossier_Api(domain):
    result = set()
    try:
        url = ""http://www.sitedossier.com/parentdomain/{}"".format(domain)
        response = requests.get(url)
        rest = response.text
        res = re.findall('<a href=""/site/(.*?)"">',rest)
        for r in res:
            if domain in r:
                result.add(r.strip('.'))
    except Exception as e:
        pass
    print('[+ Sitedossier API] Sitedossier接口 : {} 捕获子域名总数 : {}'.format(domain, len(result)))
    return list(result)",_3179.py,8,"for r in res:
    if domain in r:
        result.add(r.strip('.'))",result = {r.strip('.') for r in res if domain in r},1,nan,nan
https://github.com/sympy/sympy/tree/master/sympy/assumptions/facts.py,"def get_known_facts_keys():
    """"""
    Return every unary predicates registered to ``Q``.

    This function is used to generate the keys for
    ``generate_known_facts_dict``.

    """"""
    exclude = set()
    for pred in [Q.eq, Q.ne, Q.gt, Q.lt, Q.ge, Q.le]:
        # exclude polyadic predicates
        exclude.add(pred)

    result = []
    for attr in Q.__class__.__dict__:
        if attr.startswith('__'):
            continue
        pred = getattr(Q, attr)
        if pred in exclude:
            continue
        result.append(pred)
    return result",_3986.py,10,"for pred in [Q.eq, Q.ne, Q.gt, Q.lt, Q.ge, Q.le]:
    exclude.add(pred)","exclude = {pred for pred in [Q.eq, Q.ne, Q.gt, Q.lt, Q.ge, Q.le]}",1,nan,nan
https://github.com/PaddlePaddle/PaddleSlim/tree/master/paddleslim/prune/pruner.py,"def _transform(self, items):
        ret = []
        for name, axis, pruned_idx, transforms in items:
            src = pruned_idx
            for trans in transforms:
                target = []
                if 'src_start' in trans:
                    src_start = trans['src_start']
                    src_end = trans['src_end']
                    src_len = src_end - src_start
                    target_start = trans['target_start']
                    target_end = trans['target_end']
                    starts = np.array(range(target_start, target_end, src_len))
                    for idx in src:
                        if idx >= src_start and idx < src_end:
                            idx -= src_start
                            target.extend(list(idx + starts))
                elif ""repeat"" in trans:
                    repeat = trans['repeat']
                    for idx in src:
                        idx = idx * repeat
                        target.extend(range(idx, idx + repeat))
                elif ""squeeze"" in trans:
                    repeat = trans['repeat']
                    targets_set = set()
                    for idx in src:
                        targets_set.add(idx / repeat)
                    target = list(targets_set)

                src = target

            ret.append((name, axis, src))
        return ret",_4299.py,26,"for idx in src:
    targets_set.add(idx / repeat)",targets_set = {idx / repeat for idx in src},1,nan,nan
https://github.com/google/osv/tree/master/docker/importer/importer.py,"def _process_updates_git(self, source_repo: osv.SourceRepository):
    """"""Process updates for a git source_repo.""""""
    logging.info(""Begin processing git: %s"", source_repo.name)

    repo = self.checkout(source_repo)

    # Get list of changed files since last sync.
    changed_entries = set()

    if source_repo.last_synced_hash:
      # Syncing from a previous commit.
      changed_entries, _ = self._sync_from_previous_commit(source_repo, repo)

    else:
      # First sync from scratch.
      logging.info('Syncing repo from scratch')
      for root, _, filenames in os.walk(osv.repo_path(repo)):
        for filename in filenames:
          path = os.path.join(root, filename)
          rel_path = os.path.relpath(path, osv.repo_path(repo))
          if _is_vulnerability_file(source_repo, rel_path):
            changed_entries.add(rel_path)

    import_failure_logs = []
    # Create tasks for changed files.
    for changed_entry in changed_entries:
      path = os.path.join(osv.repo_path(repo), changed_entry)
      if not os.path.exists(path):
        # Path no longer exists. It must have been deleted in another commit.
        continue

      try:
        _ = osv.parse_vulnerability(
            path, key_path=source_repo.key_path, strict=self._strict_validation)
      except osv.sources.KeyPathError:
        # Key path doesn't exist in the vulnerability.
        # No need to log a full error, as this is expected result.
        logging.info('Entry does not have an OSV entry: %s', changed_entry)
        continue
      except Exception as e:
        logging.error('Failed to parse %s: %s', changed_entry, str(e))
        # Don't include error stack trace as that might leak sensitive info
        import_failure_logs.append('Failed to parse vulnerability ""' + path +
                                   '""')
        continue

      logging.info('Re-analysis triggered for %s', changed_entry)
      original_sha256 = osv.sha256(path)
      self._request_analysis_external(source_repo, original_sha256,
                                      changed_entry)

    replace_importer_log(storage.Client(), source_repo.name,
                         import_failure_logs)
    source_repo.last_synced_hash = str(repo.head.target)
    source_repo.put()

    logging.info(""Finish processing git: %s"", source_repo.name)",_4828.py,17,"for (root, _, filenames) in os.walk(osv.repo_path(repo)):
    for filename in filenames:
        path = os.path.join(root, filename)
        rel_path = os.path.relpath(path, osv.repo_path(repo))
        if _is_vulnerability_file(source_repo, rel_path):
            changed_entries.add(rel_path)","changed_entries |= {os.path.relpath(os.path.join(root, filename), osv.repo_path(repo)) for (root, _, filenames) in os.walk(osv.repo_path(repo)) for filename in filenames if _is_vulnerability_file(source_repo, os.path.relpath(os.path.join(root, filename), osv.repo_path(repo)))}",1,nan,nan
https://github.com/deoplete-plugins/deoplete-go/tree/master/rplugin/python3/deoplete/sources/deoplete_go/cgo.py,"def complete(index, cache, cgo_options, line_count, source):
        cgo_pattern = r""#cgo (\S+): (.+)""
        flags = set()
        for key, value in re.findall(cgo_pattern, source):
            if key == ""pkg-config"":
                for flag in cgo.get_pkgconfig(value.split()):
                    flags.add(flag)
            else:
                if ""${SRCDIR}"" in key:
                    key = key.replace(""${SRCDIR}"", ""./"")
                flags.add(""%s=%s"" % (key, value))

        cgo_flags = [""-std"", cgo_options[""std""]] + list(flags)

        fname = ""cgo_inline.c""
        main = """"""
    int main(void) {
    }
    """"""
        template = source + main
        files = [(fname, template)]

        # clang.TranslationUnit
        # PARSE_NONE = 0
        # PARSE_DETAILED_PROCESSING_RECORD = 1
        # PARSE_INCOMPLETE = 2
        # PARSE_PRECOMPILED_PREAMBLE = 4
        # PARSE_CACHE_COMPLETION_RESULTS = 8
        # PARSE_SKIP_FUNCTION_BODIES = 64
        # PARSE_INCLUDE_BRIEF_COMMENTS_IN_CODE_COMPLETION = 128
        options = 15

        # Index.parse(path, args=None, unsaved_files=None, options = 0)
        tu = index.parse(fname, cgo_flags, unsaved_files=files, options=options)

        # TranslationUnit.codeComplete(path, line, column, ...)
        cr = tu.codeComplete(
            fname,
            (line_count + 2),
            1,
            unsaved_files=files,
            include_macros=True,
            include_code_patterns=True,
            include_brief_comments=False,
        )

        if cgo_options[""sort_algo""] == ""priority"":
            results = sorted(cr.results, key=cgo.get_priority)
        elif cgo_options[""sort_algo""] == ""alphabetical"":
            results = sorted(cr.results, key=cgo.get_abbrevation)
        else:
            results = cr.results

        # Go string to C string
        #  The C string is allocated in the C heap using malloc.
        #  It is the caller's responsibility to arrange for it to be
        #  freed, such as by calling C.free (be sure to include stdlib.h
        #  if C.free is needed).
        #  func C.CString(string) *C.char
        #
        # Go []byte slice to C array
        #  The C array is allocated in the C heap using malloc.
        #  It is the caller's responsibility to arrange for it to be
        #  freed, such as by calling C.free (be sure to include stdlib.h
        #  if C.free is needed).
        #  func C.CBytes([]byte) unsafe.Pointer
        #
        # C string to Go string
        #  func C.GoString(*C.char) string
        #
        # C data with explicit length to Go string
        #  func C.GoStringN(*C.char, C.int) string
        #
        # C data with explicit length to Go []byte
        #  func C.GoBytes(unsafe.Pointer, C.int) []byte
        cache[source] = [
            {
                ""word"": ""CString"",
                ""abbr"": ""CString(string) *C.char"",
                ""info"": ""CString(string) *C.char"",
                ""kind"": ""function"",
                ""dup"": 1,
            },
            {
                ""word"": ""CBytes"",
                ""abbr"": ""CBytes([]byte) unsafe.Pointer"",
                ""info"": ""CBytes([]byte) unsafe.Pointer"",
                ""kind"": ""function"",
                ""dup"": 1,
            },
            {
                ""word"": ""GoString"",
                ""abbr"": ""GoString(*C.char) string"",
                ""info"": ""GoString(*C.char) string"",
                ""kind"": ""function"",
                ""dup"": 1,
            },
            {
                ""word"": ""GoStringN"",
                ""abbr"": ""GoStringN(*C.char, C.int) string"",
                ""info"": ""GoStringN(*C.char, C.int) string"",
                ""kind"": ""function"",
                ""dup"": 1,
            },
            {
                ""word"": ""GoBytes"",
                ""abbr"": ""GoBytes(unsafe.Pointer, C.int) []byte"",
                ""info"": ""GoBytes(unsafe.Pointer, C.int) []byte"",
                ""kind"": ""function"",
                ""dup"": 1,
            },
        ]
        cache[source] += list(map(cgo.parse_candidates, results))
        return cache[source]",_9172.py,6,"for flag in cgo.get_pkgconfig(value.split()):
    flags.add(flag)",flags |= {flag for flag in cgo.get_pkgconfig(value.split())},1,nan,nan
https://github.com/forseti-security/forseti-security/tree/master/google/cloud/forseti/services/dao.py,"def define_model(model_name, dbengine, model_seed):
    """"""Defines table classes which point to the corresponding model.

        This means, for each model being accessed this function needs to
        be called in order to generate a full set of table definitions.

        Models are name spaced via a random model seed such that multiple
        models can exist within the same database. In order to implement
        the name spacing in an abstract way.

    Args:
        model_name (str): model handle
        dbengine (object): db engine
        model_seed (str): seed to get etag

    Returns:
        tuple: (sessionmaker, ModelAccess)
    """"""

    base = declarative_base()

    denormed_group_in_group = '{}_group_in_group'.format(model_name)
    bindings_tablename = '{}_bindings'.format(model_name)
    roles_tablename = '{}_roles'.format(model_name)
    permissions_tablename = '{}_permissions'.format(model_name)
    members_tablename = '{}_members'.format(model_name)
    resources_tablename = '{}_resources'.format(model_name)

    role_permissions = Table('{}_role_permissions'.format(model_name),
                             base.metadata,
                             Column(
                                 'roles_name', ForeignKey(
                                     '{}.name'.format(roles_tablename)),
                                 primary_key=True),
                             Column(
                                 'permissions_name', ForeignKey(
                                     '{}.name'.format(permissions_tablename)),
                                 primary_key=True), )

    binding_members = Table('{}_binding_members'.format(model_name),
                            base.metadata,
                            Column(
                                'bindings_id', ForeignKey(
                                    '{}.id'.format(bindings_tablename)),
                                primary_key=True),
                            Column(
                                'members_name', ForeignKey(
                                    '{}.name'.format(members_tablename)),
                                primary_key=True), )

    group_members = Table(
        '{}_group_members'.format(model_name),
        base.metadata,
        Column('group_name',
               ForeignKey('{}.name'.format(members_tablename)),
               primary_key=True),
        Column('members_name',
               ForeignKey('{}.name'.format(members_tablename)),
               primary_key=True),
    )

    groups_settings = Table(
        '{}_groups_settings'.format(model_name),
        base.metadata,
        Column('group_name',
               ForeignKey('{}.name'.format(members_tablename)),
               primary_key=True),
        Column('settings',
               Text(16777215)),
    )

    def get_string_by_dialect(db_dialect, column_size):
        """"""Get Sqlalchemy String by dialect.
        Sqlite doesn't support collation type, need to define different
        column types for different database engine.

        This is used to make MySQL column case sensitive by adding
        an encoding type.
        Args:
            db_dialect (str): The db dialect.
            column_size (int): The size of the column.

        Returns:
            String: Sqlalchemy String.
        """"""
        if db_dialect.lower() == 'sqlite':
            return String(column_size)
        return String(column_size, collation='utf8mb4_bin')

    class Resource(base):
        """"""Row entry for a GCP resource.""""""
        __tablename__ = resources_tablename

        cai_resource_name = Column(String(4096))
        cai_resource_type = Column(String(512))
        full_name = Column(String(2048), nullable=False)
        type_name = Column(get_string_by_dialect(dbengine.dialect.name, 700),
                           primary_key=True)
        parent_type_name = Column(
            get_string_by_dialect(dbengine.dialect.name, 700),
            ForeignKey('{}.type_name'.format(resources_tablename)))
        name = Column(String(512), nullable=False)
        type = Column(String(128), nullable=False)
        policy_update_counter = Column(Integer, default=0)
        display_name = Column(String(256), default='')
        email = Column(String(256), default='')
        data = Column(Text(16777215))

        parent = relationship('Resource', remote_side=[type_name])
        bindings = relationship('Binding', back_populates='resource')

        def increment_update_counter(self):
            """"""Increments counter for this object's db updates.
            """"""
            self.policy_update_counter += 1

        def get_etag(self):
            """"""Return the etag for this resource.

            Returns:
                str: etag to avoid race condition when set policy
            """"""
            serialized_ctr = struct.pack('>I', self.policy_update_counter)
            msg = binascii.hexlify(serialized_ctr)
            msg += self.full_name.encode()
            seed = (model_seed if isinstance(model_seed, bytes)
                    else model_seed.encode())
            return hmac.new(seed, msg).hexdigest()

        def __repr__(self):
            """"""String representation.

            Returns:
                str: Resource represented as
                    (full_name='{}', name='{}' type='{}')
            """"""
            return '<Resource(full_name={}, name={} type={})>'.format(
                self.full_name, self.name, self.type)

    Resource.children = relationship(
        'Resource', order_by=Resource.full_name, back_populates='parent')

    class Binding(base):
        """"""Row for a binding between resource, roles and members.""""""

        __tablename__ = bindings_tablename
        id = Column(Integer, Sequence('{}_id_seq'.format(bindings_tablename)),
                    primary_key=True)
        resource_type_name = Column(
            get_string_by_dialect(dbengine.dialect.name, 700),
            ForeignKey('{}.type_name'.format(resources_tablename)))

        role_name = Column(get_string_by_dialect(dbengine.dialect.name, 256),
                           ForeignKey('{}.name'.format(roles_tablename)))

        resource = relationship('Resource', remote_side=[resource_type_name])
        role = relationship('Role', remote_side=[role_name])

        members = relationship('Member',
                               secondary=binding_members,
                               back_populates='bindings')

        def __repr__(self):
            """"""String Representation

            Returns:
                str: Binding represented as
                    (id='{}', role='{}', resource='{}' members='{}')
            """"""
            fmt_s = '<Binding(id={}, role={}, resource={} members={})>'
            return fmt_s.format(
                self.id,
                self.role_name,
                self.resource_type_name,
                self.members)

    class Member(base):
        """"""Row entry for a policy member.""""""

        __tablename__ = members_tablename
        name = Column(String(256), primary_key=True)
        type = Column(String(64))
        member_name = Column(String(256))

        parents = relationship(
            'Member',
            secondary=group_members,
            primaryjoin=name == group_members.c.members_name,
            secondaryjoin=name == group_members.c.group_name)

        children = relationship(
            'Member',
            secondary=group_members,
            primaryjoin=name == group_members.c.group_name,
            secondaryjoin=name == group_members.c.members_name)

        bindings = relationship('Binding',
                                secondary=binding_members,
                                back_populates='members')

        def __repr__(self):
            """"""String representation.

            Returns:
                str: Member represented as (name='{}', type='{}')
            """"""
            return '<Member(name={}, type={})>'.format(
                self.name, self.type)

    class GroupInGroup(base):
        """"""Row for a group-in-group membership.""""""

        __tablename__ = denormed_group_in_group
        parent = Column(String(256), primary_key=True)
        member = Column(String(256), primary_key=True)

        def __repr__(self):
            """"""String representation.

            Returns:
                str: GroupInGroup represented as (parent='{}', member='{}')
            """"""
            return '<GroupInGroup(parent={}, member={})>'.format(
                self.parent,
                self.member)

    class Role(base):
        """"""Row entry for an IAM role.""""""

        __tablename__ = roles_tablename
        name = Column(get_string_by_dialect(dbengine.dialect.name, 256),
                      primary_key=True)
        title = Column(String(128), default='')
        stage = Column(String(128), default='')
        description = Column(String(1024), default='')
        custom = Column(Boolean, default=False)
        permissions = relationship('Permission',
                                   secondary=role_permissions,
                                   back_populates='roles')

        def __repr__(self):
            """"""String Representation

            Returns:
                str: Role represented by name
            """"""
            return '<Role(name=%s)>' % self.name

    class Permission(base):
        """"""Row entry for an IAM permission.""""""

        __tablename__ = permissions_tablename
        name = Column(get_string_by_dialect(dbengine.dialect.name, 256),
                      primary_key=True)
        roles = relationship('Role',
                             secondary=role_permissions,
                             back_populates='permissions')

        def __repr__(self):
            """"""String Representation

            Returns:
                str: Permission represented by name
            """"""
            return '<Permission(name=%s)>' % self.name

    # pylint: disable=too-many-public-methods
    class ModelAccess(object):
        """"""Data model facade, implement main API against database.""""""
        TBL_GROUP_IN_GROUP = GroupInGroup
        TBL_GROUPS_SETTINGS = groups_settings
        TBL_BINDING = Binding
        TBL_MEMBER = Member
        TBL_PERMISSION = Permission
        TBL_ROLE = Role
        TBL_RESOURCE = Resource
        TBL_MEMBERSHIP = group_members

        # Set of member binding types that expand like groups.
        GROUP_TYPES = {'group',
                       'projecteditor',
                       'projectowner',
                       'projectviewer'}

        # Members that represent all users
        ALL_USER_MEMBERS = ['allusers', 'allauthenticatedusers']

        @classmethod
        def delete_all(cls, engine):
            """"""Delete all data from the model.

            Args:
                engine (object): database engine
            """"""

            LOGGER.info('Deleting all data from the model.')
            role_permissions.drop(engine)
            binding_members.drop(engine)
            group_members.drop(engine)
            groups_settings.drop(engine)

            Binding.__table__.drop(engine)
            Permission.__table__.drop(engine)
            GroupInGroup.__table__.drop(engine)

            Role.__table__.drop(engine)
            Member.__table__.drop(engine)
            Resource.__table__.drop(engine)

        @classmethod
        def denorm_group_in_group(cls, session):
            """"""Denormalize group-in-group relation.

            This method will fill the GroupInGroup table with
            (parent, member) if parent is an ancestor of member,
            whenever adding or removing a new group or group-group
            relationship, this method should be called to re-denormalize

            Args:
                session (object): Database session to use.

            Returns:
                int: Number of iterations.

            Raises:
                Exception: dernomalize fail
            """"""

            tbl1 = aliased(GroupInGroup.__table__, name='alias1')
            tbl2 = aliased(GroupInGroup.__table__, name='alias2')
            tbl3 = aliased(GroupInGroup.__table__, name='alias3')

            if get_sql_dialect(session) != 'sqlite':
                # Lock tables for denormalization
                # including aliases 1-3
                locked_tables = [
                    '`{}`'.format(GroupInGroup.__tablename__),
                    '`{}` as {}'.format(
                        GroupInGroup.__tablename__,
                        tbl1.name),
                    '`{}` as {}'.format(
                        GroupInGroup.__tablename__,
                        tbl2.name),
                    '`{}` as {}'.format(
                        GroupInGroup.__tablename__,
                        tbl3.name),
                    '`{}`'.format(group_members.name)]
                lock_stmts = ['{} WRITE'.format(tbl) for tbl in locked_tables]
                query = 'LOCK TABLES {}'.format(', '.join(lock_stmts))
                session.execute(query)
            try:
                # Remove all existing rows in the denormalization
                session.execute(GroupInGroup.__table__.delete())

                # Select member relation into GroupInGroup
                qry = (GroupInGroup.__table__.insert().from_select(
                    ['parent', 'member'], group_members.select().where(
                        group_members.c.group_name.startswith('group/')
                    ).where(
                        group_members.c.members_name.startswith('group/')
                    )
                ))

                session.execute(qry)

                iterations = 0
                rows_affected = True
                while rows_affected:
                    # Join membership on its own to find transitive
                    expansion = tbl1.join(tbl2, tbl1.c.member == tbl2.c.parent)

                    # Left outjoin to find the entries that
                    # are already in the table to prevent
                    # inserting already existing entries
                    expansion = expansion.outerjoin(
                        tbl3,
                        and_(tbl1.c.parent == tbl3.c.parent,
                             tbl2.c.member == tbl3.c.member))

                    # Select only such elements that are not
                    # already in the table, indicated as NULL
                    # values through the outer-left-join
                    stmt = (
                        select([tbl1.c.parent,
                                tbl2.c.member])
                        .select_from(expansion)
                        # pylint: disable=singleton-comparison
                        .where(tbl3.c.parent == None)
                        .distinct()
                    )

                    # Execute the query and insert into the table
                    qry = (GroupInGroup.__table__
                           .insert()
                           .from_select(['parent', 'member'], stmt))

                    rows_affected = bool(session.execute(qry).rowcount)
                    iterations += 1
            except Exception as e:
                LOGGER.exception(e)
                session.rollback()
                raise
            finally:
                if get_sql_dialect(session) != 'sqlite':
                    session.execute('UNLOCK TABLES')
                session.commit()
            return iterations

        @classmethod
        def expand_special_members(cls, session):
            """"""Create dynamic groups for project(Editor|Owner|Viewer).

            Should be called after IAM bindings are added to the model.

            Args:
                session (object): Database session to use.
            """"""
            member_type_map = {
                'projecteditor': 'roles/editor',
                'projectowner': 'roles/owner',
                'projectviewer': 'roles/viewer'}
            for parent_member in cls.list_group_members(
                    session, '', member_types=list(member_type_map.keys())):
                member_type, project_id = parent_member.split('/')
                role = member_type_map[member_type]
                try:
                    iam_policy = cls.get_iam_policy(
                        session,
                        'project/{}'.format(project_id),
                        roles=[role])
                    LOGGER.info('iam_policy: %s', iam_policy)
                except NoResultFound:
                    LOGGER.warning('Found a non-existent project, or project '
                                   'outside of the organization, in an IAM '
                                   'binding: %s', parent_member)
                    continue
                members = iam_policy.get('bindings', {}).get(role, [])
                expanded_members = cls.expand_members(session, members)
                for member in expanded_members:
                    stmt = cls.TBL_MEMBERSHIP.insert(
                        {'group_name': parent_member,
                         'members_name': member.name})
                    session.execute(stmt)
                    if member.type == 'group' and member.name in members:
                        session.add(cls.TBL_GROUP_IN_GROUP(
                            parent=parent_member,
                            member=member.name))
            session.commit()

        @classmethod
        def explain_granted(cls, session, member_name, resource_type_name,
                            role, permission):
            """"""Provide info about how the member has access to the resource.

            For example, member m1 can access resource r1 with permission p
            it might be granted by binding (r2, rol, g1),
            r1 is a child resource in a project or folder r2,
            role rol contains permission p,
            m1 is a member in group g1.
            This method list bindings that grant the access, member relation
            and resource hierarchy

            Args:
                session (object): Database session.
                member_name (str): name of the member
                resource_type_name (str): type_name of the resource
                role (str): role to query
                permission (str): permission to query

            Returns:
                tuples: (bindings, member_graph, resource_type_names) bindings,
                    the bindings to grant the access member_graph, the graph to
                    have member included in the binding esource_type_names, the
                    resource tree

            Raises:
                Exception: not granted
            """"""
            members, member_graph = cls.reverse_expand_members(
                session, [member_name], request_graph=True)
            member_names = [m.name for m in members]
            resource_type_names = [r.type_name for r in
                                   cls.find_resource_path(session,
                                                          resource_type_name)]

            if role:
                roles = set([role])
                qry = session.query(Binding, Member).join(
                    binding_members).join(Member)
            else:
                roles = [r.name for r in
                         cls.get_roles_by_permission_names(
                             session,
                             [permission])]
                qry = session.query(Binding, Member)
                qry = qry.join(binding_members).join(Member)
                qry = qry.join(Role).join(role_permissions).join(Permission)

            qry = qry.filter(Binding.role_name.in_(roles))
            qry = qry.filter(Member.name.in_(member_names))
            qry = qry.filter(
                Binding.resource_type_name.in_(resource_type_names))
            result = qry.all()
            if not result:
                error_message = 'Grant not found: ({},{},{})'.format(
                    member_name,
                    resource_type_name,
                    role if role is not None else permission)
                LOGGER.error(error_message)
                raise Exception(error_message)
            else:
                bindings = [(b.resource_type_name, b.role_name, m.name)
                            for b, m in result]
                return bindings, member_graph, resource_type_names

        @classmethod
        def scanner_iter(cls, session, resource_type,
                         parent_type_name=None, stream_results=True):
            """"""Iterate over all resources with the specified type.

            Args:
                session (object): Database session.
                resource_type (str): type of the resource to scan
                parent_type_name (str): type_name of the parent resource
                stream_results (bool): Enable streaming in the query.

            Yields:
                Resource: resource that match the query.
            """"""
            query = (
                session.query(Resource)
                .filter(Resource.type == resource_type)
                .options(joinedload(Resource.parent))
                .enable_eagerloads(True))

            if parent_type_name:
                query = query.filter(
                    Resource.parent_type_name == parent_type_name)

            if stream_results:
                results = query.yield_per(PER_YIELD)
            else:
                results = page_query(query)

            for row in results:
                yield row

        @classmethod
        def scanner_fetch_groups_settings(cls, session, only_iam_groups):
            """"""Fetch Groups Settings.

            Args:
                session (object): Database session.
                only_iam_groups (bool): boolean indicating whether we want to
                only fetch groups settings for which there is at least 1 iam
                policy.

            Yields:
                Resource: resource that match the query
            """"""
            if only_iam_groups:
                query = (session.query(groups_settings)
                         .join(Member).join(binding_members)
                         .distinct().enable_eagerloads(True))
            else:
                query = (session.query(groups_settings).enable_eagerloads(True))
            for resource in query.yield_per(PER_YIELD):
                yield resource

        @classmethod
        def explain_denied(cls, session, member_name, resource_type_names,
                           permission_names, role_names):
            """"""Explain why an access is denied

            Provide information how to grant access to a member if such
            access is denied with current IAM policies.
            For example, member m1 cannot access resource r1 with permission
            p, this method shows the bindings with rol that covered the
            desired permission on the resource r1 and its ancestors.
            If adding this member to any of these bindings, such access
            can be granted. An overgranting level is also provided

            Args:
                session (object): Database session.
                member_name (str): name of the member
                resource_type_names (list): list of type_names of resources
                permission_names (list): list of permissions
                role_names (list): list of roles

            Returns:
                list: list of tuples,
                    (overgranting,[(role_name,member_name,resource_name)])

            Raises:
                Exception: No roles covering requested permission set,
                    Not possible
            """"""

            if not role_names:
                role_names = [r.name for r in
                              cls.get_roles_by_permission_names(
                                  session,
                                  permission_names)]
                if not role_names:
                    error_message = 'No roles covering requested permission set'
                    LOGGER.error(error_message)
                    raise Exception(error_message)

            resource_hierarchy = (
                cls.resource_ancestors(session,
                                       resource_type_names))

            def find_binding_candidates(resource_hierarchy):
                """"""Find the root node in the ancestors.

                    From there, walk down the resource tree and add
                    every node until a node has more than one child.
                    This is the set of nodes which grants access to
                    at least all of the resources requested.
                    There is always a chain with a single node root.

                Args:
                    resource_hierarchy (dict): graph of the resource hierarchy

                Returns:
                    list: candidates to add to bindings that potentially grant
                        access
                """"""

                root = None
                for parent in resource_hierarchy.keys():
                    is_root = True
                    for children in resource_hierarchy.values():
                        if parent in children:
                            is_root = False
                            break
                    if is_root:
                        root = parent
                chain = [root]
                cur = root
                while len(resource_hierarchy[cur]) == 1:
                    cur = next(iter(resource_hierarchy[cur]))
                    chain.append(cur)
                return chain

            bind_res_candidates = find_binding_candidates(
                resource_hierarchy)

            bindings = (
                session.query(Binding, Member)
                .join(binding_members)
                .join(Member)
                .join(Role)
                .filter(Binding.resource_type_name.in_(
                    bind_res_candidates))
                .filter(Role.name.in_(role_names))
                .filter(or_(Member.type == 'group',
                            Member.name == member_name))
                .filter(and_((binding_members.c.bindings_id ==
                              Binding.id),
                             (binding_members.c.members_name ==
                              Member.name)))
                .filter(Role.name == Binding.role_name)
                .all())

            strategies = []
            for resource in bind_res_candidates:
                for role_name in role_names:
                    overgranting = (len(bind_res_candidates) -
                                    bind_res_candidates.index(resource) -
                                    1)
                    strategies.append(
                        (overgranting, [
                            (role, member_name, resource)
                            for role in [role_name]]))
            if bindings:
                for binding, member in bindings:
                    overgranting = (len(bind_res_candidates) - 1 -
                                    bind_res_candidates.index(
                                        binding.resource_type_name))
                    strategies.append(
                        (overgranting, [
                            (binding.role_name,
                             member.name,
                             binding.resource_type_name)]))

            return strategies

        @classmethod
        def query_access_by_member(cls, session, member_name, permission_names,
                                   expand_resources=False,
                                   reverse_expand_members=True):
            """"""Return the set of resources the member has access to.

            By default, this method expand group_member relation,
            so the result includes all resources can be accessed by the
            groups that the member is in.
            By default, this method does not expand resource hierarchy,
            so the result does not include a resource if such resource does
            not have a direct binding to allow access.

            Args:
                session (object): Database session.
                member_name (str): name of the member
                permission_names (list): list of names of permissions to query
                expand_resources (bool): whether to expand resources
                reverse_expand_members (bool): whether to expand members

            Returns:
                list: list of access tuples, (""role_name"", ""resource_type_name"")
            """"""

            if reverse_expand_members:
                member_names = [m.name for m in
                                cls.reverse_expand_members(session,
                                                           [member_name],
                                                           False)]
            else:
                member_names = [member_name]

            roles = cls.get_roles_by_permission_names(
                session, permission_names)

            qry = (
                session.query(Binding)
                .join(binding_members)
                .join(Member)
                .filter(Binding.role_name.in_([r.name for r in roles]))
                .filter(Member.name.in_(member_names))
            )

            bindings = qry.yield_per(1024)
            if not expand_resources:
                return [(binding.role_name,
                         [binding.resource_type_name]) for binding in bindings]

            r_type_names = [binding.resource_type_name for binding in bindings]
            expansion = cls.expand_resources_by_type_names(
                session,
                r_type_names)

            res_exp = {k.type_name: [v.type_name for v in values]
                       for k, values in expansion.items()}

            return [(binding.role_name,
                     res_exp[binding.resource_type_name])
                    for binding in bindings]

        @classmethod
        def query_access_by_permission(cls,
                                       session,
                                       role_name=None,
                                       permission_name=None,
                                       expand_groups=False,
                                       expand_resources=False):
            """"""Query access via the specified permission

            Return all the (Principal, Resource) combinations allowing
            satisfying access via the specified permission.
            By default, the group relation and resource hierarchy will not be
            expanded, so the results will only contains direct bindings
            filtered by permission. But the relations can be expanded

            Args:
                session (object): Database session.
                role_name (str): Role name to query for
                permission_name (str): Permission name to query for.
                expand_groups (bool): Whether or not to expand groups.
                expand_resources (bool): Whether or not to expand resources.

            Yields:
                obejct: A generator of access tuples.

            Raises:
                ValueError: If neither role nor permission is set.
            """"""

            if role_name:
                role_names = [role_name]
            elif permission_name:
                role_names = [p.name for p in
                              cls.get_roles_by_permission_names(
                                  session,
                                  [permission_name])]
            else:
                error_message = 'Either role or permission must be set'
                LOGGER.error(error_message)
                raise ValueError(error_message)

            if expand_resources:
                expanded_resources = aliased(Resource)
                qry = (
                    session.query(expanded_resources, Binding, Member)
                    .filter(binding_members.c.bindings_id == Binding.id)
                    .filter(binding_members.c.members_name == Member.name)
                    .filter(expanded_resources.full_name.startswith(
                        Resource.full_name))
                    .filter((Resource.type_name ==
                             Binding.resource_type_name))
                    .filter(Binding.role_name.in_(role_names))
                    .order_by(expanded_resources.name.asc(),
                              Binding.role_name.asc())
                )
            else:
                qry = (
                    session.query(Resource, Binding, Member)
                    .filter(binding_members.c.bindings_id == Binding.id)
                    .filter(binding_members.c.members_name == Member.name)
                    .filter((Resource.type_name ==
                             Binding.resource_type_name))
                    .filter(Binding.role_name.in_(role_names))
                    .order_by(Resource.name.asc(), Binding.role_name.asc())
                )

            if expand_groups:
                to_expand = set([m.name for _, _, m in
                                 qry.yield_per(PER_YIELD)])
                expansion = cls.expand_members_map(session, to_expand,
                                                   show_group_members=False,
                                                   member_contain_self=True)

            qry = qry.distinct()

            cur_resource = None
            cur_role = None
            cur_members = set()
            for resource, binding, member in qry.yield_per(PER_YIELD):
                if cur_resource != resource.type_name:
                    if cur_resource is not None:
                        yield cur_role, cur_resource, cur_members
                    cur_resource = resource.type_name
                    cur_role = binding.role_name
                    cur_members = set()
                if expand_groups:
                    for member_name in expansion[member.name]:
                        cur_members.add(member_name)
                else:
                    cur_members.add(member.name)
            if cur_resource is not None:
                yield cur_role, cur_resource, cur_members

        @classmethod
        def query_access_by_resource(cls, session, resource_type_name,
                                     permission_names, expand_groups=False):
            """"""Query access by resource

            Return members who have access to the given resource.
            The resource hierarchy will always be expanded, so even if the
            current resource does not have that binding, if its ancestors
            have the binding, the access will be shown
            By default, the group relationship will not be expanded

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource to query
                permission_names (list): list of strs, names of the permissions
                    to query
                expand_groups (bool): whether to expand groups

            Returns:
                dict: role_member_mapping, <""role_name"", ""member_names"">
            """"""

            roles = cls.get_roles_by_permission_names(
                session, permission_names)
            resources = cls.find_resource_path(session, resource_type_name)

            res = (session.query(Binding, Member)
                   .filter(
                       Binding.role_name.in_([r.name for r in roles]),
                       Binding.resource_type_name.in_(
                           [r.type_name for r in resources]))
                   .join(binding_members).join(Member))

            role_member_mapping = collections.defaultdict(set)
            for binding, member in res:
                role_member_mapping[binding.role_name].add(member.name)

            if expand_groups:
                for role in role_member_mapping:
                    role_member_mapping[role] = (
                        [m.name for m in cls.expand_members(
                            session,
                            role_member_mapping[role])])

            return role_member_mapping

        @classmethod
        def query_permissions_by_roles(cls, session, role_names, role_prefixes,
                                       _=1024):
            """"""Resolve permissions for the role.

            Args:
                session (object): db session
                role_names (list): list of strs, names of the roles
                role_prefixes (list): list of strs, prefixes of the roles
                _ (int): place occupation

            Returns:
                list: list of (Role, Permission)

            Raises:
                Exception: No roles or role prefixes specified
            """"""

            if not role_names and not role_prefixes:
                error_message = 'No roles or role prefixes specified'
                LOGGER.error(error_message)
                raise Exception(error_message)
            qry = session.query(Role, Permission).join(
                role_permissions).join(Permission)
            if role_names:
                qry = qry.filter(Role.name.in_(role_names))
            if role_prefixes:
                qry = qry.filter(
                    or_(*[Role.name.startswith(prefix)
                          for prefix in role_prefixes]))
            return qry.all()

        @classmethod
        def set_iam_policy(cls,
                           session,
                           resource_type_name,
                           policy,
                           update_members=False):
            """"""Set IAM policy

            Sets an IAM policy for the resource, check the etag when setting
            new policy and reassign new etag.
            Check etag to avoid race condition

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource
                policy (dict): the policy to set on the resource
                update_members (bool): If true, then add new members to Member
                    table. This must be set when the call to set_iam_policy
                    happens outside of the model InventoryImporter class. Tests
                    or users that manually add an IAM policy need to mark this
                    as true to ensure the model remains consistent.

            Raises:
                Exception: Etag doesn't match
            """"""

            LOGGER.info('Setting IAM policy, resource_type_name = %s, policy'
                        ' = %s, session = %s',
                        resource_type_name, policy, session)
            old_policy = cls.get_iam_policy(session, resource_type_name)
            if policy['etag'] != old_policy['etag']:
                error_message = 'Etags distinct, stored={}, provided={}'.format(
                    old_policy['etag'], policy['etag'])
                LOGGER.error(error_message)
                raise Exception(error_message)

            old_policy = old_policy['bindings']
            policy = policy['bindings']

            def filter_etag(policy):
                """"""Filter etag key/value out of policy map.

                Args:
                    policy (dict): the policy to filter

                Returns:
                    dict: policy without etag, <""bindings"":[<role, members>]>

                Raises:
                """"""

                return {k: v for k, v in policy.items() if k != 'etag'}

            def calculate_diff(policy, old_policy):
                """"""Calculate the grant/revoke difference between policies.
                   The diff = policy['bindings'] - old_policy['bindings']

                Args:
                    policy (dict): the new policy in dict format
                    old_policy (dict): the old policy in dict format

                Returns:
                    dict: <role, members> diff of bindings
                """"""

                diff = collections.defaultdict(list)
                for role, members in filter_etag(policy).items():
                    if role in old_policy:
                        for member in members:
                            if member not in old_policy[role]:
                                diff[role].append(member)
                    else:
                        diff[role] = members
                return diff

            grants = calculate_diff(policy, old_policy)
            revocations = calculate_diff(old_policy, policy)

            for role, members in revocations.items():
                bindings = (
                    session.query(Binding)
                    .filter((Binding.resource_type_name ==
                             resource_type_name))
                    .filter(Binding.role_name == role)
                    .join(binding_members).join(Member)
                    .filter(Member.name.in_(members)).all())

                for binding in bindings:
                    session.delete(binding)

            for role, members in grants.items():
                inserted = False
                existing_bindings = (
                    session.query(Binding)
                    .filter((Binding.resource_type_name ==
                             resource_type_name))
                    .filter(Binding.role_name == role)
                    .all())

                if update_members:
                    for member in members:
                        if not cls.get_member(session, member):
                            try:
                                # This is the default case, e.g. 'group/foobar'
                                m_type, name = member.split('/', 1)
                            except ValueError:
                                # Special groups like 'allUsers'
                                m_type, name = member, member
                            session.add(cls.TBL_MEMBER(
                                name=member,
                                type=m_type,
                                member_name=name))

                for binding in existing_bindings:
                    if binding.role_name == role:
                        inserted = True
                        for member in members:
                            binding.members.append(
                                session.query(Member).filter(
                                    Member.name == member).one())
                if not inserted:
                    binding = Binding(
                        resource_type_name=resource_type_name,
                        role=session.query(Role).filter(
                            Role.name == role).one())
                    binding.members = session.query(Member).filter(
                        Member.name.in_(members)).all()
                    session.add(binding)
            resource = session.query(Resource).filter(
                Resource.type_name == resource_type_name).one()
            resource.increment_update_counter()
            session.commit()

        @classmethod
        def get_iam_policy(cls, session, resource_type_name, roles=None):
            """"""Return the IAM policy for a resource.

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource to query
                roles (list): An optional list of roles to limit the results to

            Returns:
                dict: the IAM policy
            """"""

            resource = session.query(Resource).filter(
                Resource.type_name == resource_type_name).one()
            policy = {'etag': resource.get_etag(),
                      'bindings': {},
                      'resource': resource.type_name}
            bindings = session.query(Binding).filter(
                Binding.resource_type_name == resource_type_name)
            if roles:
                bindings = bindings.filter(Binding.role_name.in_(roles))
            for binding in bindings.all():
                role = binding.role_name
                members = [m.name for m in binding.members]
                policy['bindings'][role] = members
            return policy

        @classmethod
        def check_iam_policy(cls, session, resource_type_name, permission_name,
                             member_name):
            """"""Check access according to the resource IAM policy.

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource to check
                permission_name (str): name of the permission to check
                member_name (str): name of the member to check

            Returns:
                bool: whether such access is allowed

            Raises:
                Exception: member or resource not found
            """"""

            member_names = [m.name for m in
                            cls.reverse_expand_members(
                                session,
                                [member_name])]
            resource_type_names = [r.type_name for r in cls.find_resource_path(
                session,
                resource_type_name)]

            if not member_names:
                error_message = 'Member not found: {}'.format(member_name)
                LOGGER.error(error_message)
                raise Exception(error_message)
            if not resource_type_names:
                error_message = 'Resource not found: {}'.format(
                    resource_type_name)
                LOGGER.error(error_message)
                raise Exception(error_message)

            return (session.query(Permission)
                    .filter(Permission.name == permission_name)
                    .join(role_permissions).join(Role).join(Binding)
                    .filter(Binding.resource_type_name.in_(resource_type_names))
                    .join(binding_members).join(Member)
                    .filter(Member.name.in_(member_names)).first() is not None)

        @classmethod
        def list_roles_by_prefix(cls, session, role_prefix):
            """"""Provides a list of roles matched via name prefix.

            Args:
                session (object): db session
                role_prefix (str): prefix of the role_name

            Returns:
                list: list of role_names that match the query
            """"""

            return [r.name for r in session.query(Role).filter(
                Role.name.startswith(role_prefix)).all()]

        @classmethod
        def add_role_by_name(cls, session, role_name, permission_names):
            """"""Creates a new role.

            Args:
                session (object): db session
                role_name (str): name of the role to add
                permission_names (list): list of permissions in the role
            """"""

            LOGGER.info('Creating a new role, role_name = %s, permission_names'
                        ' = %s, session = %s',
                        role_name, permission_names, session)
            permission_names = set(permission_names)
            existing_permissions = session.query(Permission).filter(
                Permission.name.in_(permission_names)).all()
            for existing_permission in existing_permissions:
                try:
                    permission_names.remove(existing_permission.name)
                except KeyError:
                    LOGGER.warning('existing_permissions.name = %s, KeyError',
                                   existing_permission.name)

            new_permissions = [Permission(name=n) for n in permission_names]
            for perm in new_permissions:
                session.add(perm)
            cls.add_role(session, role_name,
                         existing_permissions + new_permissions)
            session.commit()

        @classmethod
        def add_group_member(cls,
                             session,
                             member_type_name,
                             parent_type_names,
                             denorm=False):
            """"""Add member, optionally with parent relationship.

            Args:
                session (object): db session
                member_type_name (str): type_name of the member to add
                parent_type_names (list): type_names of the parents
                denorm (bool): whether to denorm the groupingroup table after
                    addition
            """"""

            LOGGER.info('Adding a member, member_type_name = %s,'
                        ' parent_type_names = %s, denorm = %s, session = %s',
                        member_type_name, parent_type_names, denorm, session)

            cls.add_member(session,
                           member_type_name,
                           parent_type_names,
                           denorm)
            session.commit()

        @classmethod
        def list_group_members(cls,
                               session,
                               member_name_prefix,
                               member_types=None):
            """"""Returns members filtered by prefix.

            Args:
                session (object): db session
                member_name_prefix (str): the prefix of the member_name
                member_types (list): an optional list of member types to filter
                    the results by.

            Returns:
                list: list of Members that match the query
            """"""

            qry = session.query(Member).filter(
                Member.member_name.startswith(member_name_prefix))
            if member_types:
                qry = qry.filter(Member.type.in_(member_types))
            return [m.name for m in qry.all()]

        @classmethod
        def iter_groups(cls, session):
            """"""Returns iterator of all groups in model.

            Args:
                session (object): db session

            Yields:
                Member: group in the model
            """"""

            qry = session.query(Member).filter(Member.type == 'group')
            for group in qry.yield_per(1024):
                yield group

        @classmethod
        def iter_resources_by_prefix(cls,
                                     session,
                                     full_resource_name_prefix=None,
                                     type_name_prefix=None,
                                     type_prefix=None,
                                     name_prefix=None):
            """"""Returns iterator to resources filtered by prefix.

            Args:
                session (object): db session
                full_resource_name_prefix (str): the prefix of the
                    full_resource_name
                type_name_prefix (str): the prefix of the type_name
                type_prefix (str): the prefix of the type
                name_prefix (ste): the prefix of the name

            Yields:
                Resource: that match the query

            Raises:
                Exception: No prefix given
            """"""

            if not any([arg is not None for arg in [full_resource_name_prefix,
                                                    type_name_prefix,
                                                    type_prefix,
                                                    name_prefix]]):
                error_message = 'At least one prefix must be set'
                LOGGER.error(error_message)
                raise Exception(error_message)

            qry = session.query(Resource)
            if full_resource_name_prefix:
                qry = qry.filter(Resource.full_name.startswith(
                    full_resource_name_prefix))
            if type_name_prefix:
                qry = qry.filter(Resource.type_name.startswith(
                    type_name_prefix))
            if type_prefix:
                qry = qry.filter(Resource.type.startswith(
                    type_prefix))
            if name_prefix:
                qry = qry.filter(Resource.name.startswith(
                    name_prefix))

            for resource in qry.yield_per(1024):
                yield resource

        @classmethod
        def list_resources_by_prefix(cls,
                                     session,
                                     full_resource_name_prefix=None,
                                     type_name_prefix=None,
                                     type_prefix=None,
                                     name_prefix=None):
            """"""Returns resources filtered by prefix.

            Args:
                session (object): db session
                full_resource_name_prefix (str): the prefix of the
                    full_resource_name
                type_name_prefix (str): the prefix of the type_name
                type_prefix (str): the prefix of the type
                name_prefix (ste): the prefix of the name

            Returns:
                list: list of Resources match the query

            Raises:
            """"""

            return list(
                cls.iter_resources_by_prefix(session,
                                             full_resource_name_prefix,
                                             type_name_prefix,
                                             type_prefix,
                                             name_prefix))

        @classmethod
        def add_resource_by_name(cls,
                                 session,
                                 resource_type_name,
                                 parent_type_name,
                                 no_require_parent):
            """"""Adds resource specified via full name.

            Args:
                session (object): db session
                resource_type_name (str): name of the resource
                parent_type_name (str): name of the parent resource
                no_require_parent (bool): if this resource has a parent

            Returns:
                Resource: Created new resource
            """"""

            LOGGER.info('Adding resource via full name, resource_type_name'
                        ' = %s, parent_type_name = %s, no_require_parent = %s,'
                        ' session = %s', resource_type_name,
                        parent_type_name, no_require_parent, session)
            if not no_require_parent:
                parent = session.query(Resource).filter(
                    Resource.type_name == parent_type_name).one()
            else:
                parent = None
            return cls.add_resource(session, resource_type_name, parent)

        @classmethod
        def add_resource(cls, session, resource_type_name, parent=None):
            """"""Adds resource by name.

            Args:
                session (object): db session
                resource_type_name (str): name of the resource
                parent (Resource): parent of the resource

            Returns:
                Resource: Created new resource
            """"""

            LOGGER.info('Adding resource by name, resource_type_name = %s,'
                        ' session = %s', resource_type_name, session)
            res_type, res_name = resource_type_name.split('/')
            parent_full_resource_name = (
                '' if parent is None else parent.full_name)

            full_resource_name = to_full_resource_name(
                parent_full_resource_name,
                resource_type_name)

            resource = Resource(full_name=full_resource_name,
                                type_name=resource_type_name,
                                name=res_name,
                                type=res_type,
                                parent=parent)
            session.add(resource)
            return resource

        @classmethod
        def add_role(cls, session, name, permissions=None):
            """"""Add role by name.

            Args:
                session (object): db session
                name (str): name of the role to add
                permissions (list): permissions to add in the role

            Returns:
                Role: The created role
            """"""

            LOGGER.info('Adding role, name = %s, permissions = %s,'
                        ' session = %s', name, permissions, session)
            permissions = [] if permissions is None else permissions
            role = Role(name=name, permissions=permissions)
            session.add(role)
            return role

        @classmethod
        def add_permission(cls, session, name, roles=None):
            """"""Add permission by name.

            Args:
                session (object): db session
                name (str): name of the permission
                roles (list): list od roles to add the permission

            Returns:
                Permission: The created permission
            """"""

            LOGGER.info('Adding permission, name = %s, roles = %s'
                        ' session = %s', name, roles, session)
            roles = [] if roles is None else roles
            permission = Permission(name=name, roles=roles)
            session.add(permission)
            return permission

        @classmethod
        def add_binding(cls, session, resource, role, members):
            """"""Add a binding to the model.

            Args:
                session (object): db session
                resource (str): Resource to be added in the binding
                role (str): Role to be added in the binding
                members (list): members to be added in the binding

            Returns:
                Binding: the created binding
            """"""

            LOGGER.info('Adding a binding to the model, resource = %s,'
                        ' role = %s, members = %s, session = %s',
                        resource, role, members, session)
            binding = Binding(resource=resource, role=role, members=members)
            session.add(binding)
            return binding

        @classmethod
        def add_member(cls,
                       session,
                       type_name,
                       parent_type_names=None,
                       denorm=False):
            """"""Add a member to the model.

            Args:
                session (object): db session
                type_name (str): type_name of the resource to add
                parent_type_names (list): list of parent names to add
                denorm (bool): whether to denormalize the GroupInGroup relation

            Returns:
                Member: the created member

            Raises:
                Exception: parent not found
            """"""

            LOGGER.info('Adding a member to the model, type_name = %s,'
                        ' parent_type_names = %s, denorm = %s, session = %s',
                        type_name, parent_type_names, denorm, session)
            if not parent_type_names:
                parent_type_names = []
            res_type, name = type_name.split('/', 1)
            parents = session.query(Member).filter(
                Member.name.in_(parent_type_names)).all()
            if len(parents) != len(parent_type_names):
                msg = 'Parents: {}, expected: {}'.format(
                    parents, parent_type_names)
                error_message = 'Parent not found, {}'.format(msg)
                LOGGER.error(error_message)
                raise Exception(error_message)

            member = Member(name=type_name,
                            member_name=name,
                            type=res_type,
                            parents=parents)
            session.add(member)
            session.commit()
            if denorm and res_type == 'group' and parents:
                cls.denorm_group_in_group(session)
            return member

        @classmethod
        def expand_resources_by_type_names(cls, session, res_type_names):
            """"""Expand resources by type/name format.

            Args:
                session (object): db session
                res_type_names (list): list of resources in type_names

            Returns:
                dict: mapping in the form:
                      {res_type_name: Expansion(res_type_name), ... }
            """"""

            res_key = aliased(Resource, name='res_key')
            res_values = aliased(Resource, name='res_values')

            expressions = []
            for res_type_name in res_type_names:
                expressions.append(and_(
                    res_key.type_name == res_type_name))

            res = (
                session.query(res_key, res_values)
                .filter(res_key.type_name.in_(res_type_names))
                .filter(res_values.full_name.startswith(
                    res_key.full_name))
                .yield_per(1024)
            )

            mapping = collections.defaultdict(set)
            for k, value in res:
                mapping[k].add(value)
            return mapping

        @classmethod
        def reverse_expand_members(cls, session, member_names,
                                   request_graph=False):
            """"""Expand members to their groups.

            List all groups that contains these members. Also return
            the graph if requested.

            Args:
                session (object): db session
                member_names (list): list of members to expand
                request_graph (bool): wether the parent-child graph is provided

            Returns:
                object: set if graph not requested, set and graph if requested
            """"""
            member_names.extend(cls.ALL_USER_MEMBERS)
            members = session.query(Member).filter(
                Member.name.in_(member_names)).all()
            membership_graph = collections.defaultdict(set)
            member_set = set()
            new_member_set = set()

            def add_to_sets(members, child):
                """"""Adds the members & children to the sets.

                Args:
                    members (list): list of Members to be added
                    child (Member): child to be added
                """"""

                for member in members:
                    if request_graph and child:
                        membership_graph[child.name].add(member.name)
                    if request_graph and not child:
                        if member.name not in membership_graph:
                            membership_graph[member.name] = set()
                    if member not in member_set:
                        new_member_set.add(member)
                        member_set.add(member)

            add_to_sets(members, None)
            while new_member_set:
                members_to_walk = new_member_set
                new_member_set = set()
                for member in members_to_walk:
                    add_to_sets(member.parents, member)

            if request_graph:
                return member_set, membership_graph
            return member_set

        @classmethod
        def expand_members_map(cls,
                               session,
                               member_names,
                               show_group_members=True,
                               member_contain_self=True):
            """"""Expand group membership keyed by member.

            Args:
                session (object): db session
                member_names (set): Member names to expand
                show_group_members (bool): Whether to include subgroups
                member_contain_self (bool): Whether to include a parent
                    as its own member
            Returns:
                dict: <Member, set(Children)>
            """"""

            def separate_groups(member_names):
                """"""Separate groups and other members in two lists.

                This is a helper function. groups are needed to query on
                group_in_group table

                Args:
                    member_names (list): list of members to be separated

                Returns:
                    tuples: two lists of strs containing groups and others
                """"""
                groups = []
                others = []
                for name in member_names:
                    member_type = name.split('/')[0]
                    if member_type in cls.GROUP_TYPES:
                        groups.append(name)
                    else:
                        others.append(name)
                return groups, others

            selectables = []
            group_names, other_names = separate_groups(member_names)

            t_ging = GroupInGroup.__table__
            t_members = group_members

            # This resolves groups to its transitive non-group members.
            transitive_membership = (
                select([t_ging.c.parent, t_members.c.members_name])
                .select_from(t_ging.join(t_members,
                                         (t_ging.c.member ==
                                          t_members.c.group_name)))
            ).where(t_ging.c.parent.in_(group_names))

            if not show_group_members:
                transitive_membership = transitive_membership.where(
                    not_(t_members.c.members_name.startswith('group/')))

            selectables.append(
                transitive_membership.alias('transitive_membership'))

            direct_membership = (
                select([t_members.c.group_name,
                        t_members.c.members_name])
                .where(t_members.c.group_name.in_(group_names))
            )

            if not show_group_members:
                direct_membership = direct_membership.where(
                    not_(t_members.c.members_name.startswith('group/')))

            selectables.append(
                direct_membership.alias('direct_membership'))

            if show_group_members:
                # Show groups as members of other groups
                group_in_groups = (
                    select([t_ging.c.parent,
                            t_ging.c.member]).where(
                                t_ging.c.parent.in_(group_names))
                )
                selectables.append(
                    group_in_groups.alias('group_in_groups'))

            # Union all the queries
            qry = union(*selectables)

            # Build the result dict
            result = collections.defaultdict(set)
            for parent, child in session.execute(qry):
                result[parent].add(child)
            for parent in other_names:
                result[parent] = set()

            # Add each parent as its own member
            if member_contain_self:
                for name in member_names:
                    result[name].add(name)
            return result

        @classmethod
        def expand_members(cls, session, member_names):
            """"""Expand group membership towards the members.

            Args:
                session (object): db session
                member_names (list): list of strs of member names

            Returns:
                set: expanded group members
            """"""

            members = session.query(Member).filter(
                Member.name.in_(member_names)).all()

            def is_group(member):
                """"""Returns true iff the member is a group.

                Args:
                    member (Member): member to check

                Returns:
                    bool: whether the member is a group
                """"""
                return member.type in cls.GROUP_TYPES

            group_set = set()
            non_group_set = set()
            new_group_set = set()

            def add_to_sets(members):
                """"""Adds new members to the sets.

                Args:
                    members (list): members to be added
                """"""
                for member in members:
                    if is_group(member):
                        if member not in group_set:
                            new_group_set.add(member)
                        group_set.add(member)
                    else:
                        non_group_set.add(member)

            add_to_sets(members)

            while new_group_set:
                groups_to_walk = new_group_set
                new_group_set = set()
                for group in groups_to_walk:
                    add_to_sets(group.children)

            return group_set.union(non_group_set)

        @classmethod
        def resource_ancestors(cls, session, resource_type_names):
            """"""Resolve the transitive ancestors by type/name format.

            Given a group of resource and find out all their parents.
            Then this method group the pairs with parent. Used to determine
            resource candidates to grant access in explain denied.

            Args:
                session (object): db session
                resource_type_names (list): list of strs, resources to query

            Returns:
                dict: <parent, childs> graph of the resource hierarchy
            """"""

            resource_names = resource_type_names
            resource_graph = collections.defaultdict(set)

            res_childs = aliased(Resource, name='res_childs')
            res_anc = aliased(Resource, name='resource_parent')

            resources_set = set(resource_names)
            resources_new = set(resource_names)

            for resource in resources_new:
                resource_graph[resource] = set()

            while resources_new:
                resources_new = set()
                for parent, child in (
                        session.query(res_anc, res_childs)
                        .filter(res_childs.type_name.in_(resources_set))
                        .filter(res_childs.parent_type_name ==
                                res_anc.type_name)
                        .all()):

                    if parent.type_name not in resources_set:
                        resources_new.add(parent.type_name)

                    resources_set.add(parent.type_name)
                    resources_set.add(child.type_name)

                    resource_graph[parent.type_name].add(child.type_name)

            return resource_graph

        @classmethod
        def find_resource_path(cls, session, resource_type_name):
            """"""Find resource ancestors by type/name format.

            Find all ancestors of a resource and return them in order

            Args:
                session (object): db session
                resource_type_name (str): resource to query

            Returns:
                list: list of Resources, transitive ancestors for the given
                    resource
            """"""

            qry = (
                session.query(Resource).filter(
                    Resource.type_name == resource_type_name)
            )

            resources = qry.all()
            return cls._find_resource_path(session, resources)

        @classmethod
        def _find_resource_path(cls, _, resources):
            """"""Find the list of transitive ancestors for the given resource.

            Args:
                _ (object): position holder
                resources (list): list of the resources to query

            Returns:
                list: list of Resources, transitive ancestors for the given
                    resource
            """"""

            if not resources:
                return []

            path = []
            resource = resources[0]

            path.append(resource)
            while resource.parent:
                resource = resource.parent
                path.append(resource)

            return path

        @classmethod
        def get_roles_by_permission_names(cls, session, permission_names):
            """"""Return the list of roles covering the specified permissions.

            Args:
                session (object): db session
                permission_names (list): permissions to be covered by.

            Returns:
                set: roles set that cover the permissions
            """"""

            permission_set = set(permission_names)
            qry = session.query(Permission)
            if permission_set:
                qry = qry.filter(Permission.name.in_(permission_set))
            permissions = qry.all()

            roles = set()
            for permission in permissions:
                for role in permission.roles:
                    roles.add(role)

            result_set = set()
            for role in roles:
                role_permissions = set(
                    [p.name for p in role.permissions])
                if permission_set.issubset(role_permissions):
                    result_set.add(role)

            return result_set

        @classmethod
        def get_member(cls, session, name):
            """"""Get member by name.

            Args:
                session (object): db session
                name (str): the name the member to query

            Returns:
                list: Members from the query
            """"""

            return session.query(Member).filter(Member.name == name).all()

    base.metadata.create_all(dbengine)
    return sessionmaker(bind=dbengine), ModelAccess",_10354.py,1834,"for permission in permissions:
    for role in permission.roles:
        roles.add(role)",roles = {role for permission in permissions for role in permission.roles},1,nan,nan
https://github.com/forseti-security/forseti-security/tree/master/google/cloud/forseti/services/dao.py,"def define_model(model_name, dbengine, model_seed):
    """"""Defines table classes which point to the corresponding model.

        This means, for each model being accessed this function needs to
        be called in order to generate a full set of table definitions.

        Models are name spaced via a random model seed such that multiple
        models can exist within the same database. In order to implement
        the name spacing in an abstract way.

    Args:
        model_name (str): model handle
        dbengine (object): db engine
        model_seed (str): seed to get etag

    Returns:
        tuple: (sessionmaker, ModelAccess)
    """"""

    base = declarative_base()

    denormed_group_in_group = '{}_group_in_group'.format(model_name)
    bindings_tablename = '{}_bindings'.format(model_name)
    roles_tablename = '{}_roles'.format(model_name)
    permissions_tablename = '{}_permissions'.format(model_name)
    members_tablename = '{}_members'.format(model_name)
    resources_tablename = '{}_resources'.format(model_name)

    role_permissions = Table('{}_role_permissions'.format(model_name),
                             base.metadata,
                             Column(
                                 'roles_name', ForeignKey(
                                     '{}.name'.format(roles_tablename)),
                                 primary_key=True),
                             Column(
                                 'permissions_name', ForeignKey(
                                     '{}.name'.format(permissions_tablename)),
                                 primary_key=True), )

    binding_members = Table('{}_binding_members'.format(model_name),
                            base.metadata,
                            Column(
                                'bindings_id', ForeignKey(
                                    '{}.id'.format(bindings_tablename)),
                                primary_key=True),
                            Column(
                                'members_name', ForeignKey(
                                    '{}.name'.format(members_tablename)),
                                primary_key=True), )

    group_members = Table(
        '{}_group_members'.format(model_name),
        base.metadata,
        Column('group_name',
               ForeignKey('{}.name'.format(members_tablename)),
               primary_key=True),
        Column('members_name',
               ForeignKey('{}.name'.format(members_tablename)),
               primary_key=True),
    )

    groups_settings = Table(
        '{}_groups_settings'.format(model_name),
        base.metadata,
        Column('group_name',
               ForeignKey('{}.name'.format(members_tablename)),
               primary_key=True),
        Column('settings',
               Text(16777215)),
    )

    def get_string_by_dialect(db_dialect, column_size):
        """"""Get Sqlalchemy String by dialect.
        Sqlite doesn't support collation type, need to define different
        column types for different database engine.

        This is used to make MySQL column case sensitive by adding
        an encoding type.
        Args:
            db_dialect (str): The db dialect.
            column_size (int): The size of the column.

        Returns:
            String: Sqlalchemy String.
        """"""
        if db_dialect.lower() == 'sqlite':
            return String(column_size)
        return String(column_size, collation='utf8mb4_bin')

    class Resource(base):
        """"""Row entry for a GCP resource.""""""
        __tablename__ = resources_tablename

        cai_resource_name = Column(String(4096))
        cai_resource_type = Column(String(512))
        full_name = Column(String(2048), nullable=False)
        type_name = Column(get_string_by_dialect(dbengine.dialect.name, 700),
                           primary_key=True)
        parent_type_name = Column(
            get_string_by_dialect(dbengine.dialect.name, 700),
            ForeignKey('{}.type_name'.format(resources_tablename)))
        name = Column(String(512), nullable=False)
        type = Column(String(128), nullable=False)
        policy_update_counter = Column(Integer, default=0)
        display_name = Column(String(256), default='')
        email = Column(String(256), default='')
        data = Column(Text(16777215))

        parent = relationship('Resource', remote_side=[type_name])
        bindings = relationship('Binding', back_populates='resource')

        def increment_update_counter(self):
            """"""Increments counter for this object's db updates.
            """"""
            self.policy_update_counter += 1

        def get_etag(self):
            """"""Return the etag for this resource.

            Returns:
                str: etag to avoid race condition when set policy
            """"""
            serialized_ctr = struct.pack('>I', self.policy_update_counter)
            msg = binascii.hexlify(serialized_ctr)
            msg += self.full_name.encode()
            seed = (model_seed if isinstance(model_seed, bytes)
                    else model_seed.encode())
            return hmac.new(seed, msg).hexdigest()

        def __repr__(self):
            """"""String representation.

            Returns:
                str: Resource represented as
                    (full_name='{}', name='{}' type='{}')
            """"""
            return '<Resource(full_name={}, name={} type={})>'.format(
                self.full_name, self.name, self.type)

    Resource.children = relationship(
        'Resource', order_by=Resource.full_name, back_populates='parent')

    class Binding(base):
        """"""Row for a binding between resource, roles and members.""""""

        __tablename__ = bindings_tablename
        id = Column(Integer, Sequence('{}_id_seq'.format(bindings_tablename)),
                    primary_key=True)
        resource_type_name = Column(
            get_string_by_dialect(dbengine.dialect.name, 700),
            ForeignKey('{}.type_name'.format(resources_tablename)))

        role_name = Column(get_string_by_dialect(dbengine.dialect.name, 256),
                           ForeignKey('{}.name'.format(roles_tablename)))

        resource = relationship('Resource', remote_side=[resource_type_name])
        role = relationship('Role', remote_side=[role_name])

        members = relationship('Member',
                               secondary=binding_members,
                               back_populates='bindings')

        def __repr__(self):
            """"""String Representation

            Returns:
                str: Binding represented as
                    (id='{}', role='{}', resource='{}' members='{}')
            """"""
            fmt_s = '<Binding(id={}, role={}, resource={} members={})>'
            return fmt_s.format(
                self.id,
                self.role_name,
                self.resource_type_name,
                self.members)

    class Member(base):
        """"""Row entry for a policy member.""""""

        __tablename__ = members_tablename
        name = Column(String(256), primary_key=True)
        type = Column(String(64))
        member_name = Column(String(256))

        parents = relationship(
            'Member',
            secondary=group_members,
            primaryjoin=name == group_members.c.members_name,
            secondaryjoin=name == group_members.c.group_name)

        children = relationship(
            'Member',
            secondary=group_members,
            primaryjoin=name == group_members.c.group_name,
            secondaryjoin=name == group_members.c.members_name)

        bindings = relationship('Binding',
                                secondary=binding_members,
                                back_populates='members')

        def __repr__(self):
            """"""String representation.

            Returns:
                str: Member represented as (name='{}', type='{}')
            """"""
            return '<Member(name={}, type={})>'.format(
                self.name, self.type)

    class GroupInGroup(base):
        """"""Row for a group-in-group membership.""""""

        __tablename__ = denormed_group_in_group
        parent = Column(String(256), primary_key=True)
        member = Column(String(256), primary_key=True)

        def __repr__(self):
            """"""String representation.

            Returns:
                str: GroupInGroup represented as (parent='{}', member='{}')
            """"""
            return '<GroupInGroup(parent={}, member={})>'.format(
                self.parent,
                self.member)

    class Role(base):
        """"""Row entry for an IAM role.""""""

        __tablename__ = roles_tablename
        name = Column(get_string_by_dialect(dbengine.dialect.name, 256),
                      primary_key=True)
        title = Column(String(128), default='')
        stage = Column(String(128), default='')
        description = Column(String(1024), default='')
        custom = Column(Boolean, default=False)
        permissions = relationship('Permission',
                                   secondary=role_permissions,
                                   back_populates='roles')

        def __repr__(self):
            """"""String Representation

            Returns:
                str: Role represented by name
            """"""
            return '<Role(name=%s)>' % self.name

    class Permission(base):
        """"""Row entry for an IAM permission.""""""

        __tablename__ = permissions_tablename
        name = Column(get_string_by_dialect(dbengine.dialect.name, 256),
                      primary_key=True)
        roles = relationship('Role',
                             secondary=role_permissions,
                             back_populates='permissions')

        def __repr__(self):
            """"""String Representation

            Returns:
                str: Permission represented by name
            """"""
            return '<Permission(name=%s)>' % self.name

    # pylint: disable=too-many-public-methods
    class ModelAccess(object):
        """"""Data model facade, implement main API against database.""""""
        TBL_GROUP_IN_GROUP = GroupInGroup
        TBL_GROUPS_SETTINGS = groups_settings
        TBL_BINDING = Binding
        TBL_MEMBER = Member
        TBL_PERMISSION = Permission
        TBL_ROLE = Role
        TBL_RESOURCE = Resource
        TBL_MEMBERSHIP = group_members

        # Set of member binding types that expand like groups.
        GROUP_TYPES = {'group',
                       'projecteditor',
                       'projectowner',
                       'projectviewer'}

        # Members that represent all users
        ALL_USER_MEMBERS = ['allusers', 'allauthenticatedusers']

        @classmethod
        def delete_all(cls, engine):
            """"""Delete all data from the model.

            Args:
                engine (object): database engine
            """"""

            LOGGER.info('Deleting all data from the model.')
            role_permissions.drop(engine)
            binding_members.drop(engine)
            group_members.drop(engine)
            groups_settings.drop(engine)

            Binding.__table__.drop(engine)
            Permission.__table__.drop(engine)
            GroupInGroup.__table__.drop(engine)

            Role.__table__.drop(engine)
            Member.__table__.drop(engine)
            Resource.__table__.drop(engine)

        @classmethod
        def denorm_group_in_group(cls, session):
            """"""Denormalize group-in-group relation.

            This method will fill the GroupInGroup table with
            (parent, member) if parent is an ancestor of member,
            whenever adding or removing a new group or group-group
            relationship, this method should be called to re-denormalize

            Args:
                session (object): Database session to use.

            Returns:
                int: Number of iterations.

            Raises:
                Exception: dernomalize fail
            """"""

            tbl1 = aliased(GroupInGroup.__table__, name='alias1')
            tbl2 = aliased(GroupInGroup.__table__, name='alias2')
            tbl3 = aliased(GroupInGroup.__table__, name='alias3')

            if get_sql_dialect(session) != 'sqlite':
                # Lock tables for denormalization
                # including aliases 1-3
                locked_tables = [
                    '`{}`'.format(GroupInGroup.__tablename__),
                    '`{}` as {}'.format(
                        GroupInGroup.__tablename__,
                        tbl1.name),
                    '`{}` as {}'.format(
                        GroupInGroup.__tablename__,
                        tbl2.name),
                    '`{}` as {}'.format(
                        GroupInGroup.__tablename__,
                        tbl3.name),
                    '`{}`'.format(group_members.name)]
                lock_stmts = ['{} WRITE'.format(tbl) for tbl in locked_tables]
                query = 'LOCK TABLES {}'.format(', '.join(lock_stmts))
                session.execute(query)
            try:
                # Remove all existing rows in the denormalization
                session.execute(GroupInGroup.__table__.delete())

                # Select member relation into GroupInGroup
                qry = (GroupInGroup.__table__.insert().from_select(
                    ['parent', 'member'], group_members.select().where(
                        group_members.c.group_name.startswith('group/')
                    ).where(
                        group_members.c.members_name.startswith('group/')
                    )
                ))

                session.execute(qry)

                iterations = 0
                rows_affected = True
                while rows_affected:
                    # Join membership on its own to find transitive
                    expansion = tbl1.join(tbl2, tbl1.c.member == tbl2.c.parent)

                    # Left outjoin to find the entries that
                    # are already in the table to prevent
                    # inserting already existing entries
                    expansion = expansion.outerjoin(
                        tbl3,
                        and_(tbl1.c.parent == tbl3.c.parent,
                             tbl2.c.member == tbl3.c.member))

                    # Select only such elements that are not
                    # already in the table, indicated as NULL
                    # values through the outer-left-join
                    stmt = (
                        select([tbl1.c.parent,
                                tbl2.c.member])
                        .select_from(expansion)
                        # pylint: disable=singleton-comparison
                        .where(tbl3.c.parent == None)
                        .distinct()
                    )

                    # Execute the query and insert into the table
                    qry = (GroupInGroup.__table__
                           .insert()
                           .from_select(['parent', 'member'], stmt))

                    rows_affected = bool(session.execute(qry).rowcount)
                    iterations += 1
            except Exception as e:
                LOGGER.exception(e)
                session.rollback()
                raise
            finally:
                if get_sql_dialect(session) != 'sqlite':
                    session.execute('UNLOCK TABLES')
                session.commit()
            return iterations

        @classmethod
        def expand_special_members(cls, session):
            """"""Create dynamic groups for project(Editor|Owner|Viewer).

            Should be called after IAM bindings are added to the model.

            Args:
                session (object): Database session to use.
            """"""
            member_type_map = {
                'projecteditor': 'roles/editor',
                'projectowner': 'roles/owner',
                'projectviewer': 'roles/viewer'}
            for parent_member in cls.list_group_members(
                    session, '', member_types=list(member_type_map.keys())):
                member_type, project_id = parent_member.split('/')
                role = member_type_map[member_type]
                try:
                    iam_policy = cls.get_iam_policy(
                        session,
                        'project/{}'.format(project_id),
                        roles=[role])
                    LOGGER.info('iam_policy: %s', iam_policy)
                except NoResultFound:
                    LOGGER.warning('Found a non-existent project, or project '
                                   'outside of the organization, in an IAM '
                                   'binding: %s', parent_member)
                    continue
                members = iam_policy.get('bindings', {}).get(role, [])
                expanded_members = cls.expand_members(session, members)
                for member in expanded_members:
                    stmt = cls.TBL_MEMBERSHIP.insert(
                        {'group_name': parent_member,
                         'members_name': member.name})
                    session.execute(stmt)
                    if member.type == 'group' and member.name in members:
                        session.add(cls.TBL_GROUP_IN_GROUP(
                            parent=parent_member,
                            member=member.name))
            session.commit()

        @classmethod
        def explain_granted(cls, session, member_name, resource_type_name,
                            role, permission):
            """"""Provide info about how the member has access to the resource.

            For example, member m1 can access resource r1 with permission p
            it might be granted by binding (r2, rol, g1),
            r1 is a child resource in a project or folder r2,
            role rol contains permission p,
            m1 is a member in group g1.
            This method list bindings that grant the access, member relation
            and resource hierarchy

            Args:
                session (object): Database session.
                member_name (str): name of the member
                resource_type_name (str): type_name of the resource
                role (str): role to query
                permission (str): permission to query

            Returns:
                tuples: (bindings, member_graph, resource_type_names) bindings,
                    the bindings to grant the access member_graph, the graph to
                    have member included in the binding esource_type_names, the
                    resource tree

            Raises:
                Exception: not granted
            """"""
            members, member_graph = cls.reverse_expand_members(
                session, [member_name], request_graph=True)
            member_names = [m.name for m in members]
            resource_type_names = [r.type_name for r in
                                   cls.find_resource_path(session,
                                                          resource_type_name)]

            if role:
                roles = set([role])
                qry = session.query(Binding, Member).join(
                    binding_members).join(Member)
            else:
                roles = [r.name for r in
                         cls.get_roles_by_permission_names(
                             session,
                             [permission])]
                qry = session.query(Binding, Member)
                qry = qry.join(binding_members).join(Member)
                qry = qry.join(Role).join(role_permissions).join(Permission)

            qry = qry.filter(Binding.role_name.in_(roles))
            qry = qry.filter(Member.name.in_(member_names))
            qry = qry.filter(
                Binding.resource_type_name.in_(resource_type_names))
            result = qry.all()
            if not result:
                error_message = 'Grant not found: ({},{},{})'.format(
                    member_name,
                    resource_type_name,
                    role if role is not None else permission)
                LOGGER.error(error_message)
                raise Exception(error_message)
            else:
                bindings = [(b.resource_type_name, b.role_name, m.name)
                            for b, m in result]
                return bindings, member_graph, resource_type_names

        @classmethod
        def scanner_iter(cls, session, resource_type,
                         parent_type_name=None, stream_results=True):
            """"""Iterate over all resources with the specified type.

            Args:
                session (object): Database session.
                resource_type (str): type of the resource to scan
                parent_type_name (str): type_name of the parent resource
                stream_results (bool): Enable streaming in the query.

            Yields:
                Resource: resource that match the query.
            """"""
            query = (
                session.query(Resource)
                .filter(Resource.type == resource_type)
                .options(joinedload(Resource.parent))
                .enable_eagerloads(True))

            if parent_type_name:
                query = query.filter(
                    Resource.parent_type_name == parent_type_name)

            if stream_results:
                results = query.yield_per(PER_YIELD)
            else:
                results = page_query(query)

            for row in results:
                yield row

        @classmethod
        def scanner_fetch_groups_settings(cls, session, only_iam_groups):
            """"""Fetch Groups Settings.

            Args:
                session (object): Database session.
                only_iam_groups (bool): boolean indicating whether we want to
                only fetch groups settings for which there is at least 1 iam
                policy.

            Yields:
                Resource: resource that match the query
            """"""
            if only_iam_groups:
                query = (session.query(groups_settings)
                         .join(Member).join(binding_members)
                         .distinct().enable_eagerloads(True))
            else:
                query = (session.query(groups_settings).enable_eagerloads(True))
            for resource in query.yield_per(PER_YIELD):
                yield resource

        @classmethod
        def explain_denied(cls, session, member_name, resource_type_names,
                           permission_names, role_names):
            """"""Explain why an access is denied

            Provide information how to grant access to a member if such
            access is denied with current IAM policies.
            For example, member m1 cannot access resource r1 with permission
            p, this method shows the bindings with rol that covered the
            desired permission on the resource r1 and its ancestors.
            If adding this member to any of these bindings, such access
            can be granted. An overgranting level is also provided

            Args:
                session (object): Database session.
                member_name (str): name of the member
                resource_type_names (list): list of type_names of resources
                permission_names (list): list of permissions
                role_names (list): list of roles

            Returns:
                list: list of tuples,
                    (overgranting,[(role_name,member_name,resource_name)])

            Raises:
                Exception: No roles covering requested permission set,
                    Not possible
            """"""

            if not role_names:
                role_names = [r.name for r in
                              cls.get_roles_by_permission_names(
                                  session,
                                  permission_names)]
                if not role_names:
                    error_message = 'No roles covering requested permission set'
                    LOGGER.error(error_message)
                    raise Exception(error_message)

            resource_hierarchy = (
                cls.resource_ancestors(session,
                                       resource_type_names))

            def find_binding_candidates(resource_hierarchy):
                """"""Find the root node in the ancestors.

                    From there, walk down the resource tree and add
                    every node until a node has more than one child.
                    This is the set of nodes which grants access to
                    at least all of the resources requested.
                    There is always a chain with a single node root.

                Args:
                    resource_hierarchy (dict): graph of the resource hierarchy

                Returns:
                    list: candidates to add to bindings that potentially grant
                        access
                """"""

                root = None
                for parent in resource_hierarchy.keys():
                    is_root = True
                    for children in resource_hierarchy.values():
                        if parent in children:
                            is_root = False
                            break
                    if is_root:
                        root = parent
                chain = [root]
                cur = root
                while len(resource_hierarchy[cur]) == 1:
                    cur = next(iter(resource_hierarchy[cur]))
                    chain.append(cur)
                return chain

            bind_res_candidates = find_binding_candidates(
                resource_hierarchy)

            bindings = (
                session.query(Binding, Member)
                .join(binding_members)
                .join(Member)
                .join(Role)
                .filter(Binding.resource_type_name.in_(
                    bind_res_candidates))
                .filter(Role.name.in_(role_names))
                .filter(or_(Member.type == 'group',
                            Member.name == member_name))
                .filter(and_((binding_members.c.bindings_id ==
                              Binding.id),
                             (binding_members.c.members_name ==
                              Member.name)))
                .filter(Role.name == Binding.role_name)
                .all())

            strategies = []
            for resource in bind_res_candidates:
                for role_name in role_names:
                    overgranting = (len(bind_res_candidates) -
                                    bind_res_candidates.index(resource) -
                                    1)
                    strategies.append(
                        (overgranting, [
                            (role, member_name, resource)
                            for role in [role_name]]))
            if bindings:
                for binding, member in bindings:
                    overgranting = (len(bind_res_candidates) - 1 -
                                    bind_res_candidates.index(
                                        binding.resource_type_name))
                    strategies.append(
                        (overgranting, [
                            (binding.role_name,
                             member.name,
                             binding.resource_type_name)]))

            return strategies

        @classmethod
        def query_access_by_member(cls, session, member_name, permission_names,
                                   expand_resources=False,
                                   reverse_expand_members=True):
            """"""Return the set of resources the member has access to.

            By default, this method expand group_member relation,
            so the result includes all resources can be accessed by the
            groups that the member is in.
            By default, this method does not expand resource hierarchy,
            so the result does not include a resource if such resource does
            not have a direct binding to allow access.

            Args:
                session (object): Database session.
                member_name (str): name of the member
                permission_names (list): list of names of permissions to query
                expand_resources (bool): whether to expand resources
                reverse_expand_members (bool): whether to expand members

            Returns:
                list: list of access tuples, (""role_name"", ""resource_type_name"")
            """"""

            if reverse_expand_members:
                member_names = [m.name for m in
                                cls.reverse_expand_members(session,
                                                           [member_name],
                                                           False)]
            else:
                member_names = [member_name]

            roles = cls.get_roles_by_permission_names(
                session, permission_names)

            qry = (
                session.query(Binding)
                .join(binding_members)
                .join(Member)
                .filter(Binding.role_name.in_([r.name for r in roles]))
                .filter(Member.name.in_(member_names))
            )

            bindings = qry.yield_per(1024)
            if not expand_resources:
                return [(binding.role_name,
                         [binding.resource_type_name]) for binding in bindings]

            r_type_names = [binding.resource_type_name for binding in bindings]
            expansion = cls.expand_resources_by_type_names(
                session,
                r_type_names)

            res_exp = {k.type_name: [v.type_name for v in values]
                       for k, values in expansion.items()}

            return [(binding.role_name,
                     res_exp[binding.resource_type_name])
                    for binding in bindings]

        @classmethod
        def query_access_by_permission(cls,
                                       session,
                                       role_name=None,
                                       permission_name=None,
                                       expand_groups=False,
                                       expand_resources=False):
            """"""Query access via the specified permission

            Return all the (Principal, Resource) combinations allowing
            satisfying access via the specified permission.
            By default, the group relation and resource hierarchy will not be
            expanded, so the results will only contains direct bindings
            filtered by permission. But the relations can be expanded

            Args:
                session (object): Database session.
                role_name (str): Role name to query for
                permission_name (str): Permission name to query for.
                expand_groups (bool): Whether or not to expand groups.
                expand_resources (bool): Whether or not to expand resources.

            Yields:
                obejct: A generator of access tuples.

            Raises:
                ValueError: If neither role nor permission is set.
            """"""

            if role_name:
                role_names = [role_name]
            elif permission_name:
                role_names = [p.name for p in
                              cls.get_roles_by_permission_names(
                                  session,
                                  [permission_name])]
            else:
                error_message = 'Either role or permission must be set'
                LOGGER.error(error_message)
                raise ValueError(error_message)

            if expand_resources:
                expanded_resources = aliased(Resource)
                qry = (
                    session.query(expanded_resources, Binding, Member)
                    .filter(binding_members.c.bindings_id == Binding.id)
                    .filter(binding_members.c.members_name == Member.name)
                    .filter(expanded_resources.full_name.startswith(
                        Resource.full_name))
                    .filter((Resource.type_name ==
                             Binding.resource_type_name))
                    .filter(Binding.role_name.in_(role_names))
                    .order_by(expanded_resources.name.asc(),
                              Binding.role_name.asc())
                )
            else:
                qry = (
                    session.query(Resource, Binding, Member)
                    .filter(binding_members.c.bindings_id == Binding.id)
                    .filter(binding_members.c.members_name == Member.name)
                    .filter((Resource.type_name ==
                             Binding.resource_type_name))
                    .filter(Binding.role_name.in_(role_names))
                    .order_by(Resource.name.asc(), Binding.role_name.asc())
                )

            if expand_groups:
                to_expand = set([m.name for _, _, m in
                                 qry.yield_per(PER_YIELD)])
                expansion = cls.expand_members_map(session, to_expand,
                                                   show_group_members=False,
                                                   member_contain_self=True)

            qry = qry.distinct()

            cur_resource = None
            cur_role = None
            cur_members = set()
            for resource, binding, member in qry.yield_per(PER_YIELD):
                if cur_resource != resource.type_name:
                    if cur_resource is not None:
                        yield cur_role, cur_resource, cur_members
                    cur_resource = resource.type_name
                    cur_role = binding.role_name
                    cur_members = set()
                if expand_groups:
                    for member_name in expansion[member.name]:
                        cur_members.add(member_name)
                else:
                    cur_members.add(member.name)
            if cur_resource is not None:
                yield cur_role, cur_resource, cur_members

        @classmethod
        def query_access_by_resource(cls, session, resource_type_name,
                                     permission_names, expand_groups=False):
            """"""Query access by resource

            Return members who have access to the given resource.
            The resource hierarchy will always be expanded, so even if the
            current resource does not have that binding, if its ancestors
            have the binding, the access will be shown
            By default, the group relationship will not be expanded

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource to query
                permission_names (list): list of strs, names of the permissions
                    to query
                expand_groups (bool): whether to expand groups

            Returns:
                dict: role_member_mapping, <""role_name"", ""member_names"">
            """"""

            roles = cls.get_roles_by_permission_names(
                session, permission_names)
            resources = cls.find_resource_path(session, resource_type_name)

            res = (session.query(Binding, Member)
                   .filter(
                       Binding.role_name.in_([r.name for r in roles]),
                       Binding.resource_type_name.in_(
                           [r.type_name for r in resources]))
                   .join(binding_members).join(Member))

            role_member_mapping = collections.defaultdict(set)
            for binding, member in res:
                role_member_mapping[binding.role_name].add(member.name)

            if expand_groups:
                for role in role_member_mapping:
                    role_member_mapping[role] = (
                        [m.name for m in cls.expand_members(
                            session,
                            role_member_mapping[role])])

            return role_member_mapping

        @classmethod
        def query_permissions_by_roles(cls, session, role_names, role_prefixes,
                                       _=1024):
            """"""Resolve permissions for the role.

            Args:
                session (object): db session
                role_names (list): list of strs, names of the roles
                role_prefixes (list): list of strs, prefixes of the roles
                _ (int): place occupation

            Returns:
                list: list of (Role, Permission)

            Raises:
                Exception: No roles or role prefixes specified
            """"""

            if not role_names and not role_prefixes:
                error_message = 'No roles or role prefixes specified'
                LOGGER.error(error_message)
                raise Exception(error_message)
            qry = session.query(Role, Permission).join(
                role_permissions).join(Permission)
            if role_names:
                qry = qry.filter(Role.name.in_(role_names))
            if role_prefixes:
                qry = qry.filter(
                    or_(*[Role.name.startswith(prefix)
                          for prefix in role_prefixes]))
            return qry.all()

        @classmethod
        def set_iam_policy(cls,
                           session,
                           resource_type_name,
                           policy,
                           update_members=False):
            """"""Set IAM policy

            Sets an IAM policy for the resource, check the etag when setting
            new policy and reassign new etag.
            Check etag to avoid race condition

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource
                policy (dict): the policy to set on the resource
                update_members (bool): If true, then add new members to Member
                    table. This must be set when the call to set_iam_policy
                    happens outside of the model InventoryImporter class. Tests
                    or users that manually add an IAM policy need to mark this
                    as true to ensure the model remains consistent.

            Raises:
                Exception: Etag doesn't match
            """"""

            LOGGER.info('Setting IAM policy, resource_type_name = %s, policy'
                        ' = %s, session = %s',
                        resource_type_name, policy, session)
            old_policy = cls.get_iam_policy(session, resource_type_name)
            if policy['etag'] != old_policy['etag']:
                error_message = 'Etags distinct, stored={}, provided={}'.format(
                    old_policy['etag'], policy['etag'])
                LOGGER.error(error_message)
                raise Exception(error_message)

            old_policy = old_policy['bindings']
            policy = policy['bindings']

            def filter_etag(policy):
                """"""Filter etag key/value out of policy map.

                Args:
                    policy (dict): the policy to filter

                Returns:
                    dict: policy without etag, <""bindings"":[<role, members>]>

                Raises:
                """"""

                return {k: v for k, v in policy.items() if k != 'etag'}

            def calculate_diff(policy, old_policy):
                """"""Calculate the grant/revoke difference between policies.
                   The diff = policy['bindings'] - old_policy['bindings']

                Args:
                    policy (dict): the new policy in dict format
                    old_policy (dict): the old policy in dict format

                Returns:
                    dict: <role, members> diff of bindings
                """"""

                diff = collections.defaultdict(list)
                for role, members in filter_etag(policy).items():
                    if role in old_policy:
                        for member in members:
                            if member not in old_policy[role]:
                                diff[role].append(member)
                    else:
                        diff[role] = members
                return diff

            grants = calculate_diff(policy, old_policy)
            revocations = calculate_diff(old_policy, policy)

            for role, members in revocations.items():
                bindings = (
                    session.query(Binding)
                    .filter((Binding.resource_type_name ==
                             resource_type_name))
                    .filter(Binding.role_name == role)
                    .join(binding_members).join(Member)
                    .filter(Member.name.in_(members)).all())

                for binding in bindings:
                    session.delete(binding)

            for role, members in grants.items():
                inserted = False
                existing_bindings = (
                    session.query(Binding)
                    .filter((Binding.resource_type_name ==
                             resource_type_name))
                    .filter(Binding.role_name == role)
                    .all())

                if update_members:
                    for member in members:
                        if not cls.get_member(session, member):
                            try:
                                # This is the default case, e.g. 'group/foobar'
                                m_type, name = member.split('/', 1)
                            except ValueError:
                                # Special groups like 'allUsers'
                                m_type, name = member, member
                            session.add(cls.TBL_MEMBER(
                                name=member,
                                type=m_type,
                                member_name=name))

                for binding in existing_bindings:
                    if binding.role_name == role:
                        inserted = True
                        for member in members:
                            binding.members.append(
                                session.query(Member).filter(
                                    Member.name == member).one())
                if not inserted:
                    binding = Binding(
                        resource_type_name=resource_type_name,
                        role=session.query(Role).filter(
                            Role.name == role).one())
                    binding.members = session.query(Member).filter(
                        Member.name.in_(members)).all()
                    session.add(binding)
            resource = session.query(Resource).filter(
                Resource.type_name == resource_type_name).one()
            resource.increment_update_counter()
            session.commit()

        @classmethod
        def get_iam_policy(cls, session, resource_type_name, roles=None):
            """"""Return the IAM policy for a resource.

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource to query
                roles (list): An optional list of roles to limit the results to

            Returns:
                dict: the IAM policy
            """"""

            resource = session.query(Resource).filter(
                Resource.type_name == resource_type_name).one()
            policy = {'etag': resource.get_etag(),
                      'bindings': {},
                      'resource': resource.type_name}
            bindings = session.query(Binding).filter(
                Binding.resource_type_name == resource_type_name)
            if roles:
                bindings = bindings.filter(Binding.role_name.in_(roles))
            for binding in bindings.all():
                role = binding.role_name
                members = [m.name for m in binding.members]
                policy['bindings'][role] = members
            return policy

        @classmethod
        def check_iam_policy(cls, session, resource_type_name, permission_name,
                             member_name):
            """"""Check access according to the resource IAM policy.

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource to check
                permission_name (str): name of the permission to check
                member_name (str): name of the member to check

            Returns:
                bool: whether such access is allowed

            Raises:
                Exception: member or resource not found
            """"""

            member_names = [m.name for m in
                            cls.reverse_expand_members(
                                session,
                                [member_name])]
            resource_type_names = [r.type_name for r in cls.find_resource_path(
                session,
                resource_type_name)]

            if not member_names:
                error_message = 'Member not found: {}'.format(member_name)
                LOGGER.error(error_message)
                raise Exception(error_message)
            if not resource_type_names:
                error_message = 'Resource not found: {}'.format(
                    resource_type_name)
                LOGGER.error(error_message)
                raise Exception(error_message)

            return (session.query(Permission)
                    .filter(Permission.name == permission_name)
                    .join(role_permissions).join(Role).join(Binding)
                    .filter(Binding.resource_type_name.in_(resource_type_names))
                    .join(binding_members).join(Member)
                    .filter(Member.name.in_(member_names)).first() is not None)

        @classmethod
        def list_roles_by_prefix(cls, session, role_prefix):
            """"""Provides a list of roles matched via name prefix.

            Args:
                session (object): db session
                role_prefix (str): prefix of the role_name

            Returns:
                list: list of role_names that match the query
            """"""

            return [r.name for r in session.query(Role).filter(
                Role.name.startswith(role_prefix)).all()]

        @classmethod
        def add_role_by_name(cls, session, role_name, permission_names):
            """"""Creates a new role.

            Args:
                session (object): db session
                role_name (str): name of the role to add
                permission_names (list): list of permissions in the role
            """"""

            LOGGER.info('Creating a new role, role_name = %s, permission_names'
                        ' = %s, session = %s',
                        role_name, permission_names, session)
            permission_names = set(permission_names)
            existing_permissions = session.query(Permission).filter(
                Permission.name.in_(permission_names)).all()
            for existing_permission in existing_permissions:
                try:
                    permission_names.remove(existing_permission.name)
                except KeyError:
                    LOGGER.warning('existing_permissions.name = %s, KeyError',
                                   existing_permission.name)

            new_permissions = [Permission(name=n) for n in permission_names]
            for perm in new_permissions:
                session.add(perm)
            cls.add_role(session, role_name,
                         existing_permissions + new_permissions)
            session.commit()

        @classmethod
        def add_group_member(cls,
                             session,
                             member_type_name,
                             parent_type_names,
                             denorm=False):
            """"""Add member, optionally with parent relationship.

            Args:
                session (object): db session
                member_type_name (str): type_name of the member to add
                parent_type_names (list): type_names of the parents
                denorm (bool): whether to denorm the groupingroup table after
                    addition
            """"""

            LOGGER.info('Adding a member, member_type_name = %s,'
                        ' parent_type_names = %s, denorm = %s, session = %s',
                        member_type_name, parent_type_names, denorm, session)

            cls.add_member(session,
                           member_type_name,
                           parent_type_names,
                           denorm)
            session.commit()

        @classmethod
        def list_group_members(cls,
                               session,
                               member_name_prefix,
                               member_types=None):
            """"""Returns members filtered by prefix.

            Args:
                session (object): db session
                member_name_prefix (str): the prefix of the member_name
                member_types (list): an optional list of member types to filter
                    the results by.

            Returns:
                list: list of Members that match the query
            """"""

            qry = session.query(Member).filter(
                Member.member_name.startswith(member_name_prefix))
            if member_types:
                qry = qry.filter(Member.type.in_(member_types))
            return [m.name for m in qry.all()]

        @classmethod
        def iter_groups(cls, session):
            """"""Returns iterator of all groups in model.

            Args:
                session (object): db session

            Yields:
                Member: group in the model
            """"""

            qry = session.query(Member).filter(Member.type == 'group')
            for group in qry.yield_per(1024):
                yield group

        @classmethod
        def iter_resources_by_prefix(cls,
                                     session,
                                     full_resource_name_prefix=None,
                                     type_name_prefix=None,
                                     type_prefix=None,
                                     name_prefix=None):
            """"""Returns iterator to resources filtered by prefix.

            Args:
                session (object): db session
                full_resource_name_prefix (str): the prefix of the
                    full_resource_name
                type_name_prefix (str): the prefix of the type_name
                type_prefix (str): the prefix of the type
                name_prefix (ste): the prefix of the name

            Yields:
                Resource: that match the query

            Raises:
                Exception: No prefix given
            """"""

            if not any([arg is not None for arg in [full_resource_name_prefix,
                                                    type_name_prefix,
                                                    type_prefix,
                                                    name_prefix]]):
                error_message = 'At least one prefix must be set'
                LOGGER.error(error_message)
                raise Exception(error_message)

            qry = session.query(Resource)
            if full_resource_name_prefix:
                qry = qry.filter(Resource.full_name.startswith(
                    full_resource_name_prefix))
            if type_name_prefix:
                qry = qry.filter(Resource.type_name.startswith(
                    type_name_prefix))
            if type_prefix:
                qry = qry.filter(Resource.type.startswith(
                    type_prefix))
            if name_prefix:
                qry = qry.filter(Resource.name.startswith(
                    name_prefix))

            for resource in qry.yield_per(1024):
                yield resource

        @classmethod
        def list_resources_by_prefix(cls,
                                     session,
                                     full_resource_name_prefix=None,
                                     type_name_prefix=None,
                                     type_prefix=None,
                                     name_prefix=None):
            """"""Returns resources filtered by prefix.

            Args:
                session (object): db session
                full_resource_name_prefix (str): the prefix of the
                    full_resource_name
                type_name_prefix (str): the prefix of the type_name
                type_prefix (str): the prefix of the type
                name_prefix (ste): the prefix of the name

            Returns:
                list: list of Resources match the query

            Raises:
            """"""

            return list(
                cls.iter_resources_by_prefix(session,
                                             full_resource_name_prefix,
                                             type_name_prefix,
                                             type_prefix,
                                             name_prefix))

        @classmethod
        def add_resource_by_name(cls,
                                 session,
                                 resource_type_name,
                                 parent_type_name,
                                 no_require_parent):
            """"""Adds resource specified via full name.

            Args:
                session (object): db session
                resource_type_name (str): name of the resource
                parent_type_name (str): name of the parent resource
                no_require_parent (bool): if this resource has a parent

            Returns:
                Resource: Created new resource
            """"""

            LOGGER.info('Adding resource via full name, resource_type_name'
                        ' = %s, parent_type_name = %s, no_require_parent = %s,'
                        ' session = %s', resource_type_name,
                        parent_type_name, no_require_parent, session)
            if not no_require_parent:
                parent = session.query(Resource).filter(
                    Resource.type_name == parent_type_name).one()
            else:
                parent = None
            return cls.add_resource(session, resource_type_name, parent)

        @classmethod
        def add_resource(cls, session, resource_type_name, parent=None):
            """"""Adds resource by name.

            Args:
                session (object): db session
                resource_type_name (str): name of the resource
                parent (Resource): parent of the resource

            Returns:
                Resource: Created new resource
            """"""

            LOGGER.info('Adding resource by name, resource_type_name = %s,'
                        ' session = %s', resource_type_name, session)
            res_type, res_name = resource_type_name.split('/')
            parent_full_resource_name = (
                '' if parent is None else parent.full_name)

            full_resource_name = to_full_resource_name(
                parent_full_resource_name,
                resource_type_name)

            resource = Resource(full_name=full_resource_name,
                                type_name=resource_type_name,
                                name=res_name,
                                type=res_type,
                                parent=parent)
            session.add(resource)
            return resource

        @classmethod
        def add_role(cls, session, name, permissions=None):
            """"""Add role by name.

            Args:
                session (object): db session
                name (str): name of the role to add
                permissions (list): permissions to add in the role

            Returns:
                Role: The created role
            """"""

            LOGGER.info('Adding role, name = %s, permissions = %s,'
                        ' session = %s', name, permissions, session)
            permissions = [] if permissions is None else permissions
            role = Role(name=name, permissions=permissions)
            session.add(role)
            return role

        @classmethod
        def add_permission(cls, session, name, roles=None):
            """"""Add permission by name.

            Args:
                session (object): db session
                name (str): name of the permission
                roles (list): list od roles to add the permission

            Returns:
                Permission: The created permission
            """"""

            LOGGER.info('Adding permission, name = %s, roles = %s'
                        ' session = %s', name, roles, session)
            roles = [] if roles is None else roles
            permission = Permission(name=name, roles=roles)
            session.add(permission)
            return permission

        @classmethod
        def add_binding(cls, session, resource, role, members):
            """"""Add a binding to the model.

            Args:
                session (object): db session
                resource (str): Resource to be added in the binding
                role (str): Role to be added in the binding
                members (list): members to be added in the binding

            Returns:
                Binding: the created binding
            """"""

            LOGGER.info('Adding a binding to the model, resource = %s,'
                        ' role = %s, members = %s, session = %s',
                        resource, role, members, session)
            binding = Binding(resource=resource, role=role, members=members)
            session.add(binding)
            return binding

        @classmethod
        def add_member(cls,
                       session,
                       type_name,
                       parent_type_names=None,
                       denorm=False):
            """"""Add a member to the model.

            Args:
                session (object): db session
                type_name (str): type_name of the resource to add
                parent_type_names (list): list of parent names to add
                denorm (bool): whether to denormalize the GroupInGroup relation

            Returns:
                Member: the created member

            Raises:
                Exception: parent not found
            """"""

            LOGGER.info('Adding a member to the model, type_name = %s,'
                        ' parent_type_names = %s, denorm = %s, session = %s',
                        type_name, parent_type_names, denorm, session)
            if not parent_type_names:
                parent_type_names = []
            res_type, name = type_name.split('/', 1)
            parents = session.query(Member).filter(
                Member.name.in_(parent_type_names)).all()
            if len(parents) != len(parent_type_names):
                msg = 'Parents: {}, expected: {}'.format(
                    parents, parent_type_names)
                error_message = 'Parent not found, {}'.format(msg)
                LOGGER.error(error_message)
                raise Exception(error_message)

            member = Member(name=type_name,
                            member_name=name,
                            type=res_type,
                            parents=parents)
            session.add(member)
            session.commit()
            if denorm and res_type == 'group' and parents:
                cls.denorm_group_in_group(session)
            return member

        @classmethod
        def expand_resources_by_type_names(cls, session, res_type_names):
            """"""Expand resources by type/name format.

            Args:
                session (object): db session
                res_type_names (list): list of resources in type_names

            Returns:
                dict: mapping in the form:
                      {res_type_name: Expansion(res_type_name), ... }
            """"""

            res_key = aliased(Resource, name='res_key')
            res_values = aliased(Resource, name='res_values')

            expressions = []
            for res_type_name in res_type_names:
                expressions.append(and_(
                    res_key.type_name == res_type_name))

            res = (
                session.query(res_key, res_values)
                .filter(res_key.type_name.in_(res_type_names))
                .filter(res_values.full_name.startswith(
                    res_key.full_name))
                .yield_per(1024)
            )

            mapping = collections.defaultdict(set)
            for k, value in res:
                mapping[k].add(value)
            return mapping

        @classmethod
        def reverse_expand_members(cls, session, member_names,
                                   request_graph=False):
            """"""Expand members to their groups.

            List all groups that contains these members. Also return
            the graph if requested.

            Args:
                session (object): db session
                member_names (list): list of members to expand
                request_graph (bool): wether the parent-child graph is provided

            Returns:
                object: set if graph not requested, set and graph if requested
            """"""
            member_names.extend(cls.ALL_USER_MEMBERS)
            members = session.query(Member).filter(
                Member.name.in_(member_names)).all()
            membership_graph = collections.defaultdict(set)
            member_set = set()
            new_member_set = set()

            def add_to_sets(members, child):
                """"""Adds the members & children to the sets.

                Args:
                    members (list): list of Members to be added
                    child (Member): child to be added
                """"""

                for member in members:
                    if request_graph and child:
                        membership_graph[child.name].add(member.name)
                    if request_graph and not child:
                        if member.name not in membership_graph:
                            membership_graph[member.name] = set()
                    if member not in member_set:
                        new_member_set.add(member)
                        member_set.add(member)

            add_to_sets(members, None)
            while new_member_set:
                members_to_walk = new_member_set
                new_member_set = set()
                for member in members_to_walk:
                    add_to_sets(member.parents, member)

            if request_graph:
                return member_set, membership_graph
            return member_set

        @classmethod
        def expand_members_map(cls,
                               session,
                               member_names,
                               show_group_members=True,
                               member_contain_self=True):
            """"""Expand group membership keyed by member.

            Args:
                session (object): db session
                member_names (set): Member names to expand
                show_group_members (bool): Whether to include subgroups
                member_contain_self (bool): Whether to include a parent
                    as its own member
            Returns:
                dict: <Member, set(Children)>
            """"""

            def separate_groups(member_names):
                """"""Separate groups and other members in two lists.

                This is a helper function. groups are needed to query on
                group_in_group table

                Args:
                    member_names (list): list of members to be separated

                Returns:
                    tuples: two lists of strs containing groups and others
                """"""
                groups = []
                others = []
                for name in member_names:
                    member_type = name.split('/')[0]
                    if member_type in cls.GROUP_TYPES:
                        groups.append(name)
                    else:
                        others.append(name)
                return groups, others

            selectables = []
            group_names, other_names = separate_groups(member_names)

            t_ging = GroupInGroup.__table__
            t_members = group_members

            # This resolves groups to its transitive non-group members.
            transitive_membership = (
                select([t_ging.c.parent, t_members.c.members_name])
                .select_from(t_ging.join(t_members,
                                         (t_ging.c.member ==
                                          t_members.c.group_name)))
            ).where(t_ging.c.parent.in_(group_names))

            if not show_group_members:
                transitive_membership = transitive_membership.where(
                    not_(t_members.c.members_name.startswith('group/')))

            selectables.append(
                transitive_membership.alias('transitive_membership'))

            direct_membership = (
                select([t_members.c.group_name,
                        t_members.c.members_name])
                .where(t_members.c.group_name.in_(group_names))
            )

            if not show_group_members:
                direct_membership = direct_membership.where(
                    not_(t_members.c.members_name.startswith('group/')))

            selectables.append(
                direct_membership.alias('direct_membership'))

            if show_group_members:
                # Show groups as members of other groups
                group_in_groups = (
                    select([t_ging.c.parent,
                            t_ging.c.member]).where(
                                t_ging.c.parent.in_(group_names))
                )
                selectables.append(
                    group_in_groups.alias('group_in_groups'))

            # Union all the queries
            qry = union(*selectables)

            # Build the result dict
            result = collections.defaultdict(set)
            for parent, child in session.execute(qry):
                result[parent].add(child)
            for parent in other_names:
                result[parent] = set()

            # Add each parent as its own member
            if member_contain_self:
                for name in member_names:
                    result[name].add(name)
            return result

        @classmethod
        def expand_members(cls, session, member_names):
            """"""Expand group membership towards the members.

            Args:
                session (object): db session
                member_names (list): list of strs of member names

            Returns:
                set: expanded group members
            """"""

            members = session.query(Member).filter(
                Member.name.in_(member_names)).all()

            def is_group(member):
                """"""Returns true iff the member is a group.

                Args:
                    member (Member): member to check

                Returns:
                    bool: whether the member is a group
                """"""
                return member.type in cls.GROUP_TYPES

            group_set = set()
            non_group_set = set()
            new_group_set = set()

            def add_to_sets(members):
                """"""Adds new members to the sets.

                Args:
                    members (list): members to be added
                """"""
                for member in members:
                    if is_group(member):
                        if member not in group_set:
                            new_group_set.add(member)
                        group_set.add(member)
                    else:
                        non_group_set.add(member)

            add_to_sets(members)

            while new_group_set:
                groups_to_walk = new_group_set
                new_group_set = set()
                for group in groups_to_walk:
                    add_to_sets(group.children)

            return group_set.union(non_group_set)

        @classmethod
        def resource_ancestors(cls, session, resource_type_names):
            """"""Resolve the transitive ancestors by type/name format.

            Given a group of resource and find out all their parents.
            Then this method group the pairs with parent. Used to determine
            resource candidates to grant access in explain denied.

            Args:
                session (object): db session
                resource_type_names (list): list of strs, resources to query

            Returns:
                dict: <parent, childs> graph of the resource hierarchy
            """"""

            resource_names = resource_type_names
            resource_graph = collections.defaultdict(set)

            res_childs = aliased(Resource, name='res_childs')
            res_anc = aliased(Resource, name='resource_parent')

            resources_set = set(resource_names)
            resources_new = set(resource_names)

            for resource in resources_new:
                resource_graph[resource] = set()

            while resources_new:
                resources_new = set()
                for parent, child in (
                        session.query(res_anc, res_childs)
                        .filter(res_childs.type_name.in_(resources_set))
                        .filter(res_childs.parent_type_name ==
                                res_anc.type_name)
                        .all()):

                    if parent.type_name not in resources_set:
                        resources_new.add(parent.type_name)

                    resources_set.add(parent.type_name)
                    resources_set.add(child.type_name)

                    resource_graph[parent.type_name].add(child.type_name)

            return resource_graph

        @classmethod
        def find_resource_path(cls, session, resource_type_name):
            """"""Find resource ancestors by type/name format.

            Find all ancestors of a resource and return them in order

            Args:
                session (object): db session
                resource_type_name (str): resource to query

            Returns:
                list: list of Resources, transitive ancestors for the given
                    resource
            """"""

            qry = (
                session.query(Resource).filter(
                    Resource.type_name == resource_type_name)
            )

            resources = qry.all()
            return cls._find_resource_path(session, resources)

        @classmethod
        def _find_resource_path(cls, _, resources):
            """"""Find the list of transitive ancestors for the given resource.

            Args:
                _ (object): position holder
                resources (list): list of the resources to query

            Returns:
                list: list of Resources, transitive ancestors for the given
                    resource
            """"""

            if not resources:
                return []

            path = []
            resource = resources[0]

            path.append(resource)
            while resource.parent:
                resource = resource.parent
                path.append(resource)

            return path

        @classmethod
        def get_roles_by_permission_names(cls, session, permission_names):
            """"""Return the list of roles covering the specified permissions.

            Args:
                session (object): db session
                permission_names (list): permissions to be covered by.

            Returns:
                set: roles set that cover the permissions
            """"""

            permission_set = set(permission_names)
            qry = session.query(Permission)
            if permission_set:
                qry = qry.filter(Permission.name.in_(permission_set))
            permissions = qry.all()

            roles = set()
            for permission in permissions:
                for role in permission.roles:
                    roles.add(role)

            result_set = set()
            for role in roles:
                role_permissions = set(
                    [p.name for p in role.permissions])
                if permission_set.issubset(role_permissions):
                    result_set.add(role)

            return result_set

        @classmethod
        def get_member(cls, session, name):
            """"""Get member by name.

            Args:
                session (object): db session
                name (str): the name the member to query

            Returns:
                list: Members from the query
            """"""

            return session.query(Member).filter(Member.name == name).all()

    base.metadata.create_all(dbengine)
    return sessionmaker(bind=dbengine), ModelAccess",_10354.py,1839,"for role in roles:
    role_permissions = set([p.name for p in role.permissions])
    if permission_set.issubset(role_permissions):
        result_set.add(role)",result_set = {role for role in roles if permission_set.issubset(set([p.name for p in role.permissions]))},1,nan,nan
https://github.com/forseti-security/forseti-security/tree/master/google/cloud/forseti/services/dao.py,"def define_model(model_name, dbengine, model_seed):
    """"""Defines table classes which point to the corresponding model.

        This means, for each model being accessed this function needs to
        be called in order to generate a full set of table definitions.

        Models are name spaced via a random model seed such that multiple
        models can exist within the same database. In order to implement
        the name spacing in an abstract way.

    Args:
        model_name (str): model handle
        dbengine (object): db engine
        model_seed (str): seed to get etag

    Returns:
        tuple: (sessionmaker, ModelAccess)
    """"""

    base = declarative_base()

    denormed_group_in_group = '{}_group_in_group'.format(model_name)
    bindings_tablename = '{}_bindings'.format(model_name)
    roles_tablename = '{}_roles'.format(model_name)
    permissions_tablename = '{}_permissions'.format(model_name)
    members_tablename = '{}_members'.format(model_name)
    resources_tablename = '{}_resources'.format(model_name)

    role_permissions = Table('{}_role_permissions'.format(model_name),
                             base.metadata,
                             Column(
                                 'roles_name', ForeignKey(
                                     '{}.name'.format(roles_tablename)),
                                 primary_key=True),
                             Column(
                                 'permissions_name', ForeignKey(
                                     '{}.name'.format(permissions_tablename)),
                                 primary_key=True), )

    binding_members = Table('{}_binding_members'.format(model_name),
                            base.metadata,
                            Column(
                                'bindings_id', ForeignKey(
                                    '{}.id'.format(bindings_tablename)),
                                primary_key=True),
                            Column(
                                'members_name', ForeignKey(
                                    '{}.name'.format(members_tablename)),
                                primary_key=True), )

    group_members = Table(
        '{}_group_members'.format(model_name),
        base.metadata,
        Column('group_name',
               ForeignKey('{}.name'.format(members_tablename)),
               primary_key=True),
        Column('members_name',
               ForeignKey('{}.name'.format(members_tablename)),
               primary_key=True),
    )

    groups_settings = Table(
        '{}_groups_settings'.format(model_name),
        base.metadata,
        Column('group_name',
               ForeignKey('{}.name'.format(members_tablename)),
               primary_key=True),
        Column('settings',
               Text(16777215)),
    )

    def get_string_by_dialect(db_dialect, column_size):
        """"""Get Sqlalchemy String by dialect.
        Sqlite doesn't support collation type, need to define different
        column types for different database engine.

        This is used to make MySQL column case sensitive by adding
        an encoding type.
        Args:
            db_dialect (str): The db dialect.
            column_size (int): The size of the column.

        Returns:
            String: Sqlalchemy String.
        """"""
        if db_dialect.lower() == 'sqlite':
            return String(column_size)
        return String(column_size, collation='utf8mb4_bin')

    class Resource(base):
        """"""Row entry for a GCP resource.""""""
        __tablename__ = resources_tablename

        cai_resource_name = Column(String(4096))
        cai_resource_type = Column(String(512))
        full_name = Column(String(2048), nullable=False)
        type_name = Column(get_string_by_dialect(dbengine.dialect.name, 700),
                           primary_key=True)
        parent_type_name = Column(
            get_string_by_dialect(dbengine.dialect.name, 700),
            ForeignKey('{}.type_name'.format(resources_tablename)))
        name = Column(String(512), nullable=False)
        type = Column(String(128), nullable=False)
        policy_update_counter = Column(Integer, default=0)
        display_name = Column(String(256), default='')
        email = Column(String(256), default='')
        data = Column(Text(16777215))

        parent = relationship('Resource', remote_side=[type_name])
        bindings = relationship('Binding', back_populates='resource')

        def increment_update_counter(self):
            """"""Increments counter for this object's db updates.
            """"""
            self.policy_update_counter += 1

        def get_etag(self):
            """"""Return the etag for this resource.

            Returns:
                str: etag to avoid race condition when set policy
            """"""
            serialized_ctr = struct.pack('>I', self.policy_update_counter)
            msg = binascii.hexlify(serialized_ctr)
            msg += self.full_name.encode()
            seed = (model_seed if isinstance(model_seed, bytes)
                    else model_seed.encode())
            return hmac.new(seed, msg).hexdigest()

        def __repr__(self):
            """"""String representation.

            Returns:
                str: Resource represented as
                    (full_name='{}', name='{}' type='{}')
            """"""
            return '<Resource(full_name={}, name={} type={})>'.format(
                self.full_name, self.name, self.type)

    Resource.children = relationship(
        'Resource', order_by=Resource.full_name, back_populates='parent')

    class Binding(base):
        """"""Row for a binding between resource, roles and members.""""""

        __tablename__ = bindings_tablename
        id = Column(Integer, Sequence('{}_id_seq'.format(bindings_tablename)),
                    primary_key=True)
        resource_type_name = Column(
            get_string_by_dialect(dbengine.dialect.name, 700),
            ForeignKey('{}.type_name'.format(resources_tablename)))

        role_name = Column(get_string_by_dialect(dbengine.dialect.name, 256),
                           ForeignKey('{}.name'.format(roles_tablename)))

        resource = relationship('Resource', remote_side=[resource_type_name])
        role = relationship('Role', remote_side=[role_name])

        members = relationship('Member',
                               secondary=binding_members,
                               back_populates='bindings')

        def __repr__(self):
            """"""String Representation

            Returns:
                str: Binding represented as
                    (id='{}', role='{}', resource='{}' members='{}')
            """"""
            fmt_s = '<Binding(id={}, role={}, resource={} members={})>'
            return fmt_s.format(
                self.id,
                self.role_name,
                self.resource_type_name,
                self.members)

    class Member(base):
        """"""Row entry for a policy member.""""""

        __tablename__ = members_tablename
        name = Column(String(256), primary_key=True)
        type = Column(String(64))
        member_name = Column(String(256))

        parents = relationship(
            'Member',
            secondary=group_members,
            primaryjoin=name == group_members.c.members_name,
            secondaryjoin=name == group_members.c.group_name)

        children = relationship(
            'Member',
            secondary=group_members,
            primaryjoin=name == group_members.c.group_name,
            secondaryjoin=name == group_members.c.members_name)

        bindings = relationship('Binding',
                                secondary=binding_members,
                                back_populates='members')

        def __repr__(self):
            """"""String representation.

            Returns:
                str: Member represented as (name='{}', type='{}')
            """"""
            return '<Member(name={}, type={})>'.format(
                self.name, self.type)

    class GroupInGroup(base):
        """"""Row for a group-in-group membership.""""""

        __tablename__ = denormed_group_in_group
        parent = Column(String(256), primary_key=True)
        member = Column(String(256), primary_key=True)

        def __repr__(self):
            """"""String representation.

            Returns:
                str: GroupInGroup represented as (parent='{}', member='{}')
            """"""
            return '<GroupInGroup(parent={}, member={})>'.format(
                self.parent,
                self.member)

    class Role(base):
        """"""Row entry for an IAM role.""""""

        __tablename__ = roles_tablename
        name = Column(get_string_by_dialect(dbengine.dialect.name, 256),
                      primary_key=True)
        title = Column(String(128), default='')
        stage = Column(String(128), default='')
        description = Column(String(1024), default='')
        custom = Column(Boolean, default=False)
        permissions = relationship('Permission',
                                   secondary=role_permissions,
                                   back_populates='roles')

        def __repr__(self):
            """"""String Representation

            Returns:
                str: Role represented by name
            """"""
            return '<Role(name=%s)>' % self.name

    class Permission(base):
        """"""Row entry for an IAM permission.""""""

        __tablename__ = permissions_tablename
        name = Column(get_string_by_dialect(dbengine.dialect.name, 256),
                      primary_key=True)
        roles = relationship('Role',
                             secondary=role_permissions,
                             back_populates='permissions')

        def __repr__(self):
            """"""String Representation

            Returns:
                str: Permission represented by name
            """"""
            return '<Permission(name=%s)>' % self.name

    # pylint: disable=too-many-public-methods
    class ModelAccess(object):
        """"""Data model facade, implement main API against database.""""""
        TBL_GROUP_IN_GROUP = GroupInGroup
        TBL_GROUPS_SETTINGS = groups_settings
        TBL_BINDING = Binding
        TBL_MEMBER = Member
        TBL_PERMISSION = Permission
        TBL_ROLE = Role
        TBL_RESOURCE = Resource
        TBL_MEMBERSHIP = group_members

        # Set of member binding types that expand like groups.
        GROUP_TYPES = {'group',
                       'projecteditor',
                       'projectowner',
                       'projectviewer'}

        # Members that represent all users
        ALL_USER_MEMBERS = ['allusers', 'allauthenticatedusers']

        @classmethod
        def delete_all(cls, engine):
            """"""Delete all data from the model.

            Args:
                engine (object): database engine
            """"""

            LOGGER.info('Deleting all data from the model.')
            role_permissions.drop(engine)
            binding_members.drop(engine)
            group_members.drop(engine)
            groups_settings.drop(engine)

            Binding.__table__.drop(engine)
            Permission.__table__.drop(engine)
            GroupInGroup.__table__.drop(engine)

            Role.__table__.drop(engine)
            Member.__table__.drop(engine)
            Resource.__table__.drop(engine)

        @classmethod
        def denorm_group_in_group(cls, session):
            """"""Denormalize group-in-group relation.

            This method will fill the GroupInGroup table with
            (parent, member) if parent is an ancestor of member,
            whenever adding or removing a new group or group-group
            relationship, this method should be called to re-denormalize

            Args:
                session (object): Database session to use.

            Returns:
                int: Number of iterations.

            Raises:
                Exception: dernomalize fail
            """"""

            tbl1 = aliased(GroupInGroup.__table__, name='alias1')
            tbl2 = aliased(GroupInGroup.__table__, name='alias2')
            tbl3 = aliased(GroupInGroup.__table__, name='alias3')

            if get_sql_dialect(session) != 'sqlite':
                # Lock tables for denormalization
                # including aliases 1-3
                locked_tables = [
                    '`{}`'.format(GroupInGroup.__tablename__),
                    '`{}` as {}'.format(
                        GroupInGroup.__tablename__,
                        tbl1.name),
                    '`{}` as {}'.format(
                        GroupInGroup.__tablename__,
                        tbl2.name),
                    '`{}` as {}'.format(
                        GroupInGroup.__tablename__,
                        tbl3.name),
                    '`{}`'.format(group_members.name)]
                lock_stmts = ['{} WRITE'.format(tbl) for tbl in locked_tables]
                query = 'LOCK TABLES {}'.format(', '.join(lock_stmts))
                session.execute(query)
            try:
                # Remove all existing rows in the denormalization
                session.execute(GroupInGroup.__table__.delete())

                # Select member relation into GroupInGroup
                qry = (GroupInGroup.__table__.insert().from_select(
                    ['parent', 'member'], group_members.select().where(
                        group_members.c.group_name.startswith('group/')
                    ).where(
                        group_members.c.members_name.startswith('group/')
                    )
                ))

                session.execute(qry)

                iterations = 0
                rows_affected = True
                while rows_affected:
                    # Join membership on its own to find transitive
                    expansion = tbl1.join(tbl2, tbl1.c.member == tbl2.c.parent)

                    # Left outjoin to find the entries that
                    # are already in the table to prevent
                    # inserting already existing entries
                    expansion = expansion.outerjoin(
                        tbl3,
                        and_(tbl1.c.parent == tbl3.c.parent,
                             tbl2.c.member == tbl3.c.member))

                    # Select only such elements that are not
                    # already in the table, indicated as NULL
                    # values through the outer-left-join
                    stmt = (
                        select([tbl1.c.parent,
                                tbl2.c.member])
                        .select_from(expansion)
                        # pylint: disable=singleton-comparison
                        .where(tbl3.c.parent == None)
                        .distinct()
                    )

                    # Execute the query and insert into the table
                    qry = (GroupInGroup.__table__
                           .insert()
                           .from_select(['parent', 'member'], stmt))

                    rows_affected = bool(session.execute(qry).rowcount)
                    iterations += 1
            except Exception as e:
                LOGGER.exception(e)
                session.rollback()
                raise
            finally:
                if get_sql_dialect(session) != 'sqlite':
                    session.execute('UNLOCK TABLES')
                session.commit()
            return iterations

        @classmethod
        def expand_special_members(cls, session):
            """"""Create dynamic groups for project(Editor|Owner|Viewer).

            Should be called after IAM bindings are added to the model.

            Args:
                session (object): Database session to use.
            """"""
            member_type_map = {
                'projecteditor': 'roles/editor',
                'projectowner': 'roles/owner',
                'projectviewer': 'roles/viewer'}
            for parent_member in cls.list_group_members(
                    session, '', member_types=list(member_type_map.keys())):
                member_type, project_id = parent_member.split('/')
                role = member_type_map[member_type]
                try:
                    iam_policy = cls.get_iam_policy(
                        session,
                        'project/{}'.format(project_id),
                        roles=[role])
                    LOGGER.info('iam_policy: %s', iam_policy)
                except NoResultFound:
                    LOGGER.warning('Found a non-existent project, or project '
                                   'outside of the organization, in an IAM '
                                   'binding: %s', parent_member)
                    continue
                members = iam_policy.get('bindings', {}).get(role, [])
                expanded_members = cls.expand_members(session, members)
                for member in expanded_members:
                    stmt = cls.TBL_MEMBERSHIP.insert(
                        {'group_name': parent_member,
                         'members_name': member.name})
                    session.execute(stmt)
                    if member.type == 'group' and member.name in members:
                        session.add(cls.TBL_GROUP_IN_GROUP(
                            parent=parent_member,
                            member=member.name))
            session.commit()

        @classmethod
        def explain_granted(cls, session, member_name, resource_type_name,
                            role, permission):
            """"""Provide info about how the member has access to the resource.

            For example, member m1 can access resource r1 with permission p
            it might be granted by binding (r2, rol, g1),
            r1 is a child resource in a project or folder r2,
            role rol contains permission p,
            m1 is a member in group g1.
            This method list bindings that grant the access, member relation
            and resource hierarchy

            Args:
                session (object): Database session.
                member_name (str): name of the member
                resource_type_name (str): type_name of the resource
                role (str): role to query
                permission (str): permission to query

            Returns:
                tuples: (bindings, member_graph, resource_type_names) bindings,
                    the bindings to grant the access member_graph, the graph to
                    have member included in the binding esource_type_names, the
                    resource tree

            Raises:
                Exception: not granted
            """"""
            members, member_graph = cls.reverse_expand_members(
                session, [member_name], request_graph=True)
            member_names = [m.name for m in members]
            resource_type_names = [r.type_name for r in
                                   cls.find_resource_path(session,
                                                          resource_type_name)]

            if role:
                roles = set([role])
                qry = session.query(Binding, Member).join(
                    binding_members).join(Member)
            else:
                roles = [r.name for r in
                         cls.get_roles_by_permission_names(
                             session,
                             [permission])]
                qry = session.query(Binding, Member)
                qry = qry.join(binding_members).join(Member)
                qry = qry.join(Role).join(role_permissions).join(Permission)

            qry = qry.filter(Binding.role_name.in_(roles))
            qry = qry.filter(Member.name.in_(member_names))
            qry = qry.filter(
                Binding.resource_type_name.in_(resource_type_names))
            result = qry.all()
            if not result:
                error_message = 'Grant not found: ({},{},{})'.format(
                    member_name,
                    resource_type_name,
                    role if role is not None else permission)
                LOGGER.error(error_message)
                raise Exception(error_message)
            else:
                bindings = [(b.resource_type_name, b.role_name, m.name)
                            for b, m in result]
                return bindings, member_graph, resource_type_names

        @classmethod
        def scanner_iter(cls, session, resource_type,
                         parent_type_name=None, stream_results=True):
            """"""Iterate over all resources with the specified type.

            Args:
                session (object): Database session.
                resource_type (str): type of the resource to scan
                parent_type_name (str): type_name of the parent resource
                stream_results (bool): Enable streaming in the query.

            Yields:
                Resource: resource that match the query.
            """"""
            query = (
                session.query(Resource)
                .filter(Resource.type == resource_type)
                .options(joinedload(Resource.parent))
                .enable_eagerloads(True))

            if parent_type_name:
                query = query.filter(
                    Resource.parent_type_name == parent_type_name)

            if stream_results:
                results = query.yield_per(PER_YIELD)
            else:
                results = page_query(query)

            for row in results:
                yield row

        @classmethod
        def scanner_fetch_groups_settings(cls, session, only_iam_groups):
            """"""Fetch Groups Settings.

            Args:
                session (object): Database session.
                only_iam_groups (bool): boolean indicating whether we want to
                only fetch groups settings for which there is at least 1 iam
                policy.

            Yields:
                Resource: resource that match the query
            """"""
            if only_iam_groups:
                query = (session.query(groups_settings)
                         .join(Member).join(binding_members)
                         .distinct().enable_eagerloads(True))
            else:
                query = (session.query(groups_settings).enable_eagerloads(True))
            for resource in query.yield_per(PER_YIELD):
                yield resource

        @classmethod
        def explain_denied(cls, session, member_name, resource_type_names,
                           permission_names, role_names):
            """"""Explain why an access is denied

            Provide information how to grant access to a member if such
            access is denied with current IAM policies.
            For example, member m1 cannot access resource r1 with permission
            p, this method shows the bindings with rol that covered the
            desired permission on the resource r1 and its ancestors.
            If adding this member to any of these bindings, such access
            can be granted. An overgranting level is also provided

            Args:
                session (object): Database session.
                member_name (str): name of the member
                resource_type_names (list): list of type_names of resources
                permission_names (list): list of permissions
                role_names (list): list of roles

            Returns:
                list: list of tuples,
                    (overgranting,[(role_name,member_name,resource_name)])

            Raises:
                Exception: No roles covering requested permission set,
                    Not possible
            """"""

            if not role_names:
                role_names = [r.name for r in
                              cls.get_roles_by_permission_names(
                                  session,
                                  permission_names)]
                if not role_names:
                    error_message = 'No roles covering requested permission set'
                    LOGGER.error(error_message)
                    raise Exception(error_message)

            resource_hierarchy = (
                cls.resource_ancestors(session,
                                       resource_type_names))

            def find_binding_candidates(resource_hierarchy):
                """"""Find the root node in the ancestors.

                    From there, walk down the resource tree and add
                    every node until a node has more than one child.
                    This is the set of nodes which grants access to
                    at least all of the resources requested.
                    There is always a chain with a single node root.

                Args:
                    resource_hierarchy (dict): graph of the resource hierarchy

                Returns:
                    list: candidates to add to bindings that potentially grant
                        access
                """"""

                root = None
                for parent in resource_hierarchy.keys():
                    is_root = True
                    for children in resource_hierarchy.values():
                        if parent in children:
                            is_root = False
                            break
                    if is_root:
                        root = parent
                chain = [root]
                cur = root
                while len(resource_hierarchy[cur]) == 1:
                    cur = next(iter(resource_hierarchy[cur]))
                    chain.append(cur)
                return chain

            bind_res_candidates = find_binding_candidates(
                resource_hierarchy)

            bindings = (
                session.query(Binding, Member)
                .join(binding_members)
                .join(Member)
                .join(Role)
                .filter(Binding.resource_type_name.in_(
                    bind_res_candidates))
                .filter(Role.name.in_(role_names))
                .filter(or_(Member.type == 'group',
                            Member.name == member_name))
                .filter(and_((binding_members.c.bindings_id ==
                              Binding.id),
                             (binding_members.c.members_name ==
                              Member.name)))
                .filter(Role.name == Binding.role_name)
                .all())

            strategies = []
            for resource in bind_res_candidates:
                for role_name in role_names:
                    overgranting = (len(bind_res_candidates) -
                                    bind_res_candidates.index(resource) -
                                    1)
                    strategies.append(
                        (overgranting, [
                            (role, member_name, resource)
                            for role in [role_name]]))
            if bindings:
                for binding, member in bindings:
                    overgranting = (len(bind_res_candidates) - 1 -
                                    bind_res_candidates.index(
                                        binding.resource_type_name))
                    strategies.append(
                        (overgranting, [
                            (binding.role_name,
                             member.name,
                             binding.resource_type_name)]))

            return strategies

        @classmethod
        def query_access_by_member(cls, session, member_name, permission_names,
                                   expand_resources=False,
                                   reverse_expand_members=True):
            """"""Return the set of resources the member has access to.

            By default, this method expand group_member relation,
            so the result includes all resources can be accessed by the
            groups that the member is in.
            By default, this method does not expand resource hierarchy,
            so the result does not include a resource if such resource does
            not have a direct binding to allow access.

            Args:
                session (object): Database session.
                member_name (str): name of the member
                permission_names (list): list of names of permissions to query
                expand_resources (bool): whether to expand resources
                reverse_expand_members (bool): whether to expand members

            Returns:
                list: list of access tuples, (""role_name"", ""resource_type_name"")
            """"""

            if reverse_expand_members:
                member_names = [m.name for m in
                                cls.reverse_expand_members(session,
                                                           [member_name],
                                                           False)]
            else:
                member_names = [member_name]

            roles = cls.get_roles_by_permission_names(
                session, permission_names)

            qry = (
                session.query(Binding)
                .join(binding_members)
                .join(Member)
                .filter(Binding.role_name.in_([r.name for r in roles]))
                .filter(Member.name.in_(member_names))
            )

            bindings = qry.yield_per(1024)
            if not expand_resources:
                return [(binding.role_name,
                         [binding.resource_type_name]) for binding in bindings]

            r_type_names = [binding.resource_type_name for binding in bindings]
            expansion = cls.expand_resources_by_type_names(
                session,
                r_type_names)

            res_exp = {k.type_name: [v.type_name for v in values]
                       for k, values in expansion.items()}

            return [(binding.role_name,
                     res_exp[binding.resource_type_name])
                    for binding in bindings]

        @classmethod
        def query_access_by_permission(cls,
                                       session,
                                       role_name=None,
                                       permission_name=None,
                                       expand_groups=False,
                                       expand_resources=False):
            """"""Query access via the specified permission

            Return all the (Principal, Resource) combinations allowing
            satisfying access via the specified permission.
            By default, the group relation and resource hierarchy will not be
            expanded, so the results will only contains direct bindings
            filtered by permission. But the relations can be expanded

            Args:
                session (object): Database session.
                role_name (str): Role name to query for
                permission_name (str): Permission name to query for.
                expand_groups (bool): Whether or not to expand groups.
                expand_resources (bool): Whether or not to expand resources.

            Yields:
                obejct: A generator of access tuples.

            Raises:
                ValueError: If neither role nor permission is set.
            """"""

            if role_name:
                role_names = [role_name]
            elif permission_name:
                role_names = [p.name for p in
                              cls.get_roles_by_permission_names(
                                  session,
                                  [permission_name])]
            else:
                error_message = 'Either role or permission must be set'
                LOGGER.error(error_message)
                raise ValueError(error_message)

            if expand_resources:
                expanded_resources = aliased(Resource)
                qry = (
                    session.query(expanded_resources, Binding, Member)
                    .filter(binding_members.c.bindings_id == Binding.id)
                    .filter(binding_members.c.members_name == Member.name)
                    .filter(expanded_resources.full_name.startswith(
                        Resource.full_name))
                    .filter((Resource.type_name ==
                             Binding.resource_type_name))
                    .filter(Binding.role_name.in_(role_names))
                    .order_by(expanded_resources.name.asc(),
                              Binding.role_name.asc())
                )
            else:
                qry = (
                    session.query(Resource, Binding, Member)
                    .filter(binding_members.c.bindings_id == Binding.id)
                    .filter(binding_members.c.members_name == Member.name)
                    .filter((Resource.type_name ==
                             Binding.resource_type_name))
                    .filter(Binding.role_name.in_(role_names))
                    .order_by(Resource.name.asc(), Binding.role_name.asc())
                )

            if expand_groups:
                to_expand = set([m.name for _, _, m in
                                 qry.yield_per(PER_YIELD)])
                expansion = cls.expand_members_map(session, to_expand,
                                                   show_group_members=False,
                                                   member_contain_self=True)

            qry = qry.distinct()

            cur_resource = None
            cur_role = None
            cur_members = set()
            for resource, binding, member in qry.yield_per(PER_YIELD):
                if cur_resource != resource.type_name:
                    if cur_resource is not None:
                        yield cur_role, cur_resource, cur_members
                    cur_resource = resource.type_name
                    cur_role = binding.role_name
                    cur_members = set()
                if expand_groups:
                    for member_name in expansion[member.name]:
                        cur_members.add(member_name)
                else:
                    cur_members.add(member.name)
            if cur_resource is not None:
                yield cur_role, cur_resource, cur_members

        @classmethod
        def query_access_by_resource(cls, session, resource_type_name,
                                     permission_names, expand_groups=False):
            """"""Query access by resource

            Return members who have access to the given resource.
            The resource hierarchy will always be expanded, so even if the
            current resource does not have that binding, if its ancestors
            have the binding, the access will be shown
            By default, the group relationship will not be expanded

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource to query
                permission_names (list): list of strs, names of the permissions
                    to query
                expand_groups (bool): whether to expand groups

            Returns:
                dict: role_member_mapping, <""role_name"", ""member_names"">
            """"""

            roles = cls.get_roles_by_permission_names(
                session, permission_names)
            resources = cls.find_resource_path(session, resource_type_name)

            res = (session.query(Binding, Member)
                   .filter(
                       Binding.role_name.in_([r.name for r in roles]),
                       Binding.resource_type_name.in_(
                           [r.type_name for r in resources]))
                   .join(binding_members).join(Member))

            role_member_mapping = collections.defaultdict(set)
            for binding, member in res:
                role_member_mapping[binding.role_name].add(member.name)

            if expand_groups:
                for role in role_member_mapping:
                    role_member_mapping[role] = (
                        [m.name for m in cls.expand_members(
                            session,
                            role_member_mapping[role])])

            return role_member_mapping

        @classmethod
        def query_permissions_by_roles(cls, session, role_names, role_prefixes,
                                       _=1024):
            """"""Resolve permissions for the role.

            Args:
                session (object): db session
                role_names (list): list of strs, names of the roles
                role_prefixes (list): list of strs, prefixes of the roles
                _ (int): place occupation

            Returns:
                list: list of (Role, Permission)

            Raises:
                Exception: No roles or role prefixes specified
            """"""

            if not role_names and not role_prefixes:
                error_message = 'No roles or role prefixes specified'
                LOGGER.error(error_message)
                raise Exception(error_message)
            qry = session.query(Role, Permission).join(
                role_permissions).join(Permission)
            if role_names:
                qry = qry.filter(Role.name.in_(role_names))
            if role_prefixes:
                qry = qry.filter(
                    or_(*[Role.name.startswith(prefix)
                          for prefix in role_prefixes]))
            return qry.all()

        @classmethod
        def set_iam_policy(cls,
                           session,
                           resource_type_name,
                           policy,
                           update_members=False):
            """"""Set IAM policy

            Sets an IAM policy for the resource, check the etag when setting
            new policy and reassign new etag.
            Check etag to avoid race condition

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource
                policy (dict): the policy to set on the resource
                update_members (bool): If true, then add new members to Member
                    table. This must be set when the call to set_iam_policy
                    happens outside of the model InventoryImporter class. Tests
                    or users that manually add an IAM policy need to mark this
                    as true to ensure the model remains consistent.

            Raises:
                Exception: Etag doesn't match
            """"""

            LOGGER.info('Setting IAM policy, resource_type_name = %s, policy'
                        ' = %s, session = %s',
                        resource_type_name, policy, session)
            old_policy = cls.get_iam_policy(session, resource_type_name)
            if policy['etag'] != old_policy['etag']:
                error_message = 'Etags distinct, stored={}, provided={}'.format(
                    old_policy['etag'], policy['etag'])
                LOGGER.error(error_message)
                raise Exception(error_message)

            old_policy = old_policy['bindings']
            policy = policy['bindings']

            def filter_etag(policy):
                """"""Filter etag key/value out of policy map.

                Args:
                    policy (dict): the policy to filter

                Returns:
                    dict: policy without etag, <""bindings"":[<role, members>]>

                Raises:
                """"""

                return {k: v for k, v in policy.items() if k != 'etag'}

            def calculate_diff(policy, old_policy):
                """"""Calculate the grant/revoke difference between policies.
                   The diff = policy['bindings'] - old_policy['bindings']

                Args:
                    policy (dict): the new policy in dict format
                    old_policy (dict): the old policy in dict format

                Returns:
                    dict: <role, members> diff of bindings
                """"""

                diff = collections.defaultdict(list)
                for role, members in filter_etag(policy).items():
                    if role in old_policy:
                        for member in members:
                            if member not in old_policy[role]:
                                diff[role].append(member)
                    else:
                        diff[role] = members
                return diff

            grants = calculate_diff(policy, old_policy)
            revocations = calculate_diff(old_policy, policy)

            for role, members in revocations.items():
                bindings = (
                    session.query(Binding)
                    .filter((Binding.resource_type_name ==
                             resource_type_name))
                    .filter(Binding.role_name == role)
                    .join(binding_members).join(Member)
                    .filter(Member.name.in_(members)).all())

                for binding in bindings:
                    session.delete(binding)

            for role, members in grants.items():
                inserted = False
                existing_bindings = (
                    session.query(Binding)
                    .filter((Binding.resource_type_name ==
                             resource_type_name))
                    .filter(Binding.role_name == role)
                    .all())

                if update_members:
                    for member in members:
                        if not cls.get_member(session, member):
                            try:
                                # This is the default case, e.g. 'group/foobar'
                                m_type, name = member.split('/', 1)
                            except ValueError:
                                # Special groups like 'allUsers'
                                m_type, name = member, member
                            session.add(cls.TBL_MEMBER(
                                name=member,
                                type=m_type,
                                member_name=name))

                for binding in existing_bindings:
                    if binding.role_name == role:
                        inserted = True
                        for member in members:
                            binding.members.append(
                                session.query(Member).filter(
                                    Member.name == member).one())
                if not inserted:
                    binding = Binding(
                        resource_type_name=resource_type_name,
                        role=session.query(Role).filter(
                            Role.name == role).one())
                    binding.members = session.query(Member).filter(
                        Member.name.in_(members)).all()
                    session.add(binding)
            resource = session.query(Resource).filter(
                Resource.type_name == resource_type_name).one()
            resource.increment_update_counter()
            session.commit()

        @classmethod
        def get_iam_policy(cls, session, resource_type_name, roles=None):
            """"""Return the IAM policy for a resource.

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource to query
                roles (list): An optional list of roles to limit the results to

            Returns:
                dict: the IAM policy
            """"""

            resource = session.query(Resource).filter(
                Resource.type_name == resource_type_name).one()
            policy = {'etag': resource.get_etag(),
                      'bindings': {},
                      'resource': resource.type_name}
            bindings = session.query(Binding).filter(
                Binding.resource_type_name == resource_type_name)
            if roles:
                bindings = bindings.filter(Binding.role_name.in_(roles))
            for binding in bindings.all():
                role = binding.role_name
                members = [m.name for m in binding.members]
                policy['bindings'][role] = members
            return policy

        @classmethod
        def check_iam_policy(cls, session, resource_type_name, permission_name,
                             member_name):
            """"""Check access according to the resource IAM policy.

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource to check
                permission_name (str): name of the permission to check
                member_name (str): name of the member to check

            Returns:
                bool: whether such access is allowed

            Raises:
                Exception: member or resource not found
            """"""

            member_names = [m.name for m in
                            cls.reverse_expand_members(
                                session,
                                [member_name])]
            resource_type_names = [r.type_name for r in cls.find_resource_path(
                session,
                resource_type_name)]

            if not member_names:
                error_message = 'Member not found: {}'.format(member_name)
                LOGGER.error(error_message)
                raise Exception(error_message)
            if not resource_type_names:
                error_message = 'Resource not found: {}'.format(
                    resource_type_name)
                LOGGER.error(error_message)
                raise Exception(error_message)

            return (session.query(Permission)
                    .filter(Permission.name == permission_name)
                    .join(role_permissions).join(Role).join(Binding)
                    .filter(Binding.resource_type_name.in_(resource_type_names))
                    .join(binding_members).join(Member)
                    .filter(Member.name.in_(member_names)).first() is not None)

        @classmethod
        def list_roles_by_prefix(cls, session, role_prefix):
            """"""Provides a list of roles matched via name prefix.

            Args:
                session (object): db session
                role_prefix (str): prefix of the role_name

            Returns:
                list: list of role_names that match the query
            """"""

            return [r.name for r in session.query(Role).filter(
                Role.name.startswith(role_prefix)).all()]

        @classmethod
        def add_role_by_name(cls, session, role_name, permission_names):
            """"""Creates a new role.

            Args:
                session (object): db session
                role_name (str): name of the role to add
                permission_names (list): list of permissions in the role
            """"""

            LOGGER.info('Creating a new role, role_name = %s, permission_names'
                        ' = %s, session = %s',
                        role_name, permission_names, session)
            permission_names = set(permission_names)
            existing_permissions = session.query(Permission).filter(
                Permission.name.in_(permission_names)).all()
            for existing_permission in existing_permissions:
                try:
                    permission_names.remove(existing_permission.name)
                except KeyError:
                    LOGGER.warning('existing_permissions.name = %s, KeyError',
                                   existing_permission.name)

            new_permissions = [Permission(name=n) for n in permission_names]
            for perm in new_permissions:
                session.add(perm)
            cls.add_role(session, role_name,
                         existing_permissions + new_permissions)
            session.commit()

        @classmethod
        def add_group_member(cls,
                             session,
                             member_type_name,
                             parent_type_names,
                             denorm=False):
            """"""Add member, optionally with parent relationship.

            Args:
                session (object): db session
                member_type_name (str): type_name of the member to add
                parent_type_names (list): type_names of the parents
                denorm (bool): whether to denorm the groupingroup table after
                    addition
            """"""

            LOGGER.info('Adding a member, member_type_name = %s,'
                        ' parent_type_names = %s, denorm = %s, session = %s',
                        member_type_name, parent_type_names, denorm, session)

            cls.add_member(session,
                           member_type_name,
                           parent_type_names,
                           denorm)
            session.commit()

        @classmethod
        def list_group_members(cls,
                               session,
                               member_name_prefix,
                               member_types=None):
            """"""Returns members filtered by prefix.

            Args:
                session (object): db session
                member_name_prefix (str): the prefix of the member_name
                member_types (list): an optional list of member types to filter
                    the results by.

            Returns:
                list: list of Members that match the query
            """"""

            qry = session.query(Member).filter(
                Member.member_name.startswith(member_name_prefix))
            if member_types:
                qry = qry.filter(Member.type.in_(member_types))
            return [m.name for m in qry.all()]

        @classmethod
        def iter_groups(cls, session):
            """"""Returns iterator of all groups in model.

            Args:
                session (object): db session

            Yields:
                Member: group in the model
            """"""

            qry = session.query(Member).filter(Member.type == 'group')
            for group in qry.yield_per(1024):
                yield group

        @classmethod
        def iter_resources_by_prefix(cls,
                                     session,
                                     full_resource_name_prefix=None,
                                     type_name_prefix=None,
                                     type_prefix=None,
                                     name_prefix=None):
            """"""Returns iterator to resources filtered by prefix.

            Args:
                session (object): db session
                full_resource_name_prefix (str): the prefix of the
                    full_resource_name
                type_name_prefix (str): the prefix of the type_name
                type_prefix (str): the prefix of the type
                name_prefix (ste): the prefix of the name

            Yields:
                Resource: that match the query

            Raises:
                Exception: No prefix given
            """"""

            if not any([arg is not None for arg in [full_resource_name_prefix,
                                                    type_name_prefix,
                                                    type_prefix,
                                                    name_prefix]]):
                error_message = 'At least one prefix must be set'
                LOGGER.error(error_message)
                raise Exception(error_message)

            qry = session.query(Resource)
            if full_resource_name_prefix:
                qry = qry.filter(Resource.full_name.startswith(
                    full_resource_name_prefix))
            if type_name_prefix:
                qry = qry.filter(Resource.type_name.startswith(
                    type_name_prefix))
            if type_prefix:
                qry = qry.filter(Resource.type.startswith(
                    type_prefix))
            if name_prefix:
                qry = qry.filter(Resource.name.startswith(
                    name_prefix))

            for resource in qry.yield_per(1024):
                yield resource

        @classmethod
        def list_resources_by_prefix(cls,
                                     session,
                                     full_resource_name_prefix=None,
                                     type_name_prefix=None,
                                     type_prefix=None,
                                     name_prefix=None):
            """"""Returns resources filtered by prefix.

            Args:
                session (object): db session
                full_resource_name_prefix (str): the prefix of the
                    full_resource_name
                type_name_prefix (str): the prefix of the type_name
                type_prefix (str): the prefix of the type
                name_prefix (ste): the prefix of the name

            Returns:
                list: list of Resources match the query

            Raises:
            """"""

            return list(
                cls.iter_resources_by_prefix(session,
                                             full_resource_name_prefix,
                                             type_name_prefix,
                                             type_prefix,
                                             name_prefix))

        @classmethod
        def add_resource_by_name(cls,
                                 session,
                                 resource_type_name,
                                 parent_type_name,
                                 no_require_parent):
            """"""Adds resource specified via full name.

            Args:
                session (object): db session
                resource_type_name (str): name of the resource
                parent_type_name (str): name of the parent resource
                no_require_parent (bool): if this resource has a parent

            Returns:
                Resource: Created new resource
            """"""

            LOGGER.info('Adding resource via full name, resource_type_name'
                        ' = %s, parent_type_name = %s, no_require_parent = %s,'
                        ' session = %s', resource_type_name,
                        parent_type_name, no_require_parent, session)
            if not no_require_parent:
                parent = session.query(Resource).filter(
                    Resource.type_name == parent_type_name).one()
            else:
                parent = None
            return cls.add_resource(session, resource_type_name, parent)

        @classmethod
        def add_resource(cls, session, resource_type_name, parent=None):
            """"""Adds resource by name.

            Args:
                session (object): db session
                resource_type_name (str): name of the resource
                parent (Resource): parent of the resource

            Returns:
                Resource: Created new resource
            """"""

            LOGGER.info('Adding resource by name, resource_type_name = %s,'
                        ' session = %s', resource_type_name, session)
            res_type, res_name = resource_type_name.split('/')
            parent_full_resource_name = (
                '' if parent is None else parent.full_name)

            full_resource_name = to_full_resource_name(
                parent_full_resource_name,
                resource_type_name)

            resource = Resource(full_name=full_resource_name,
                                type_name=resource_type_name,
                                name=res_name,
                                type=res_type,
                                parent=parent)
            session.add(resource)
            return resource

        @classmethod
        def add_role(cls, session, name, permissions=None):
            """"""Add role by name.

            Args:
                session (object): db session
                name (str): name of the role to add
                permissions (list): permissions to add in the role

            Returns:
                Role: The created role
            """"""

            LOGGER.info('Adding role, name = %s, permissions = %s,'
                        ' session = %s', name, permissions, session)
            permissions = [] if permissions is None else permissions
            role = Role(name=name, permissions=permissions)
            session.add(role)
            return role

        @classmethod
        def add_permission(cls, session, name, roles=None):
            """"""Add permission by name.

            Args:
                session (object): db session
                name (str): name of the permission
                roles (list): list od roles to add the permission

            Returns:
                Permission: The created permission
            """"""

            LOGGER.info('Adding permission, name = %s, roles = %s'
                        ' session = %s', name, roles, session)
            roles = [] if roles is None else roles
            permission = Permission(name=name, roles=roles)
            session.add(permission)
            return permission

        @classmethod
        def add_binding(cls, session, resource, role, members):
            """"""Add a binding to the model.

            Args:
                session (object): db session
                resource (str): Resource to be added in the binding
                role (str): Role to be added in the binding
                members (list): members to be added in the binding

            Returns:
                Binding: the created binding
            """"""

            LOGGER.info('Adding a binding to the model, resource = %s,'
                        ' role = %s, members = %s, session = %s',
                        resource, role, members, session)
            binding = Binding(resource=resource, role=role, members=members)
            session.add(binding)
            return binding

        @classmethod
        def add_member(cls,
                       session,
                       type_name,
                       parent_type_names=None,
                       denorm=False):
            """"""Add a member to the model.

            Args:
                session (object): db session
                type_name (str): type_name of the resource to add
                parent_type_names (list): list of parent names to add
                denorm (bool): whether to denormalize the GroupInGroup relation

            Returns:
                Member: the created member

            Raises:
                Exception: parent not found
            """"""

            LOGGER.info('Adding a member to the model, type_name = %s,'
                        ' parent_type_names = %s, denorm = %s, session = %s',
                        type_name, parent_type_names, denorm, session)
            if not parent_type_names:
                parent_type_names = []
            res_type, name = type_name.split('/', 1)
            parents = session.query(Member).filter(
                Member.name.in_(parent_type_names)).all()
            if len(parents) != len(parent_type_names):
                msg = 'Parents: {}, expected: {}'.format(
                    parents, parent_type_names)
                error_message = 'Parent not found, {}'.format(msg)
                LOGGER.error(error_message)
                raise Exception(error_message)

            member = Member(name=type_name,
                            member_name=name,
                            type=res_type,
                            parents=parents)
            session.add(member)
            session.commit()
            if denorm and res_type == 'group' and parents:
                cls.denorm_group_in_group(session)
            return member

        @classmethod
        def expand_resources_by_type_names(cls, session, res_type_names):
            """"""Expand resources by type/name format.

            Args:
                session (object): db session
                res_type_names (list): list of resources in type_names

            Returns:
                dict: mapping in the form:
                      {res_type_name: Expansion(res_type_name), ... }
            """"""

            res_key = aliased(Resource, name='res_key')
            res_values = aliased(Resource, name='res_values')

            expressions = []
            for res_type_name in res_type_names:
                expressions.append(and_(
                    res_key.type_name == res_type_name))

            res = (
                session.query(res_key, res_values)
                .filter(res_key.type_name.in_(res_type_names))
                .filter(res_values.full_name.startswith(
                    res_key.full_name))
                .yield_per(1024)
            )

            mapping = collections.defaultdict(set)
            for k, value in res:
                mapping[k].add(value)
            return mapping

        @classmethod
        def reverse_expand_members(cls, session, member_names,
                                   request_graph=False):
            """"""Expand members to their groups.

            List all groups that contains these members. Also return
            the graph if requested.

            Args:
                session (object): db session
                member_names (list): list of members to expand
                request_graph (bool): wether the parent-child graph is provided

            Returns:
                object: set if graph not requested, set and graph if requested
            """"""
            member_names.extend(cls.ALL_USER_MEMBERS)
            members = session.query(Member).filter(
                Member.name.in_(member_names)).all()
            membership_graph = collections.defaultdict(set)
            member_set = set()
            new_member_set = set()

            def add_to_sets(members, child):
                """"""Adds the members & children to the sets.

                Args:
                    members (list): list of Members to be added
                    child (Member): child to be added
                """"""

                for member in members:
                    if request_graph and child:
                        membership_graph[child.name].add(member.name)
                    if request_graph and not child:
                        if member.name not in membership_graph:
                            membership_graph[member.name] = set()
                    if member not in member_set:
                        new_member_set.add(member)
                        member_set.add(member)

            add_to_sets(members, None)
            while new_member_set:
                members_to_walk = new_member_set
                new_member_set = set()
                for member in members_to_walk:
                    add_to_sets(member.parents, member)

            if request_graph:
                return member_set, membership_graph
            return member_set

        @classmethod
        def expand_members_map(cls,
                               session,
                               member_names,
                               show_group_members=True,
                               member_contain_self=True):
            """"""Expand group membership keyed by member.

            Args:
                session (object): db session
                member_names (set): Member names to expand
                show_group_members (bool): Whether to include subgroups
                member_contain_self (bool): Whether to include a parent
                    as its own member
            Returns:
                dict: <Member, set(Children)>
            """"""

            def separate_groups(member_names):
                """"""Separate groups and other members in two lists.

                This is a helper function. groups are needed to query on
                group_in_group table

                Args:
                    member_names (list): list of members to be separated

                Returns:
                    tuples: two lists of strs containing groups and others
                """"""
                groups = []
                others = []
                for name in member_names:
                    member_type = name.split('/')[0]
                    if member_type in cls.GROUP_TYPES:
                        groups.append(name)
                    else:
                        others.append(name)
                return groups, others

            selectables = []
            group_names, other_names = separate_groups(member_names)

            t_ging = GroupInGroup.__table__
            t_members = group_members

            # This resolves groups to its transitive non-group members.
            transitive_membership = (
                select([t_ging.c.parent, t_members.c.members_name])
                .select_from(t_ging.join(t_members,
                                         (t_ging.c.member ==
                                          t_members.c.group_name)))
            ).where(t_ging.c.parent.in_(group_names))

            if not show_group_members:
                transitive_membership = transitive_membership.where(
                    not_(t_members.c.members_name.startswith('group/')))

            selectables.append(
                transitive_membership.alias('transitive_membership'))

            direct_membership = (
                select([t_members.c.group_name,
                        t_members.c.members_name])
                .where(t_members.c.group_name.in_(group_names))
            )

            if not show_group_members:
                direct_membership = direct_membership.where(
                    not_(t_members.c.members_name.startswith('group/')))

            selectables.append(
                direct_membership.alias('direct_membership'))

            if show_group_members:
                # Show groups as members of other groups
                group_in_groups = (
                    select([t_ging.c.parent,
                            t_ging.c.member]).where(
                                t_ging.c.parent.in_(group_names))
                )
                selectables.append(
                    group_in_groups.alias('group_in_groups'))

            # Union all the queries
            qry = union(*selectables)

            # Build the result dict
            result = collections.defaultdict(set)
            for parent, child in session.execute(qry):
                result[parent].add(child)
            for parent in other_names:
                result[parent] = set()

            # Add each parent as its own member
            if member_contain_self:
                for name in member_names:
                    result[name].add(name)
            return result

        @classmethod
        def expand_members(cls, session, member_names):
            """"""Expand group membership towards the members.

            Args:
                session (object): db session
                member_names (list): list of strs of member names

            Returns:
                set: expanded group members
            """"""

            members = session.query(Member).filter(
                Member.name.in_(member_names)).all()

            def is_group(member):
                """"""Returns true iff the member is a group.

                Args:
                    member (Member): member to check

                Returns:
                    bool: whether the member is a group
                """"""
                return member.type in cls.GROUP_TYPES

            group_set = set()
            non_group_set = set()
            new_group_set = set()

            def add_to_sets(members):
                """"""Adds new members to the sets.

                Args:
                    members (list): members to be added
                """"""
                for member in members:
                    if is_group(member):
                        if member not in group_set:
                            new_group_set.add(member)
                        group_set.add(member)
                    else:
                        non_group_set.add(member)

            add_to_sets(members)

            while new_group_set:
                groups_to_walk = new_group_set
                new_group_set = set()
                for group in groups_to_walk:
                    add_to_sets(group.children)

            return group_set.union(non_group_set)

        @classmethod
        def resource_ancestors(cls, session, resource_type_names):
            """"""Resolve the transitive ancestors by type/name format.

            Given a group of resource and find out all their parents.
            Then this method group the pairs with parent. Used to determine
            resource candidates to grant access in explain denied.

            Args:
                session (object): db session
                resource_type_names (list): list of strs, resources to query

            Returns:
                dict: <parent, childs> graph of the resource hierarchy
            """"""

            resource_names = resource_type_names
            resource_graph = collections.defaultdict(set)

            res_childs = aliased(Resource, name='res_childs')
            res_anc = aliased(Resource, name='resource_parent')

            resources_set = set(resource_names)
            resources_new = set(resource_names)

            for resource in resources_new:
                resource_graph[resource] = set()

            while resources_new:
                resources_new = set()
                for parent, child in (
                        session.query(res_anc, res_childs)
                        .filter(res_childs.type_name.in_(resources_set))
                        .filter(res_childs.parent_type_name ==
                                res_anc.type_name)
                        .all()):

                    if parent.type_name not in resources_set:
                        resources_new.add(parent.type_name)

                    resources_set.add(parent.type_name)
                    resources_set.add(child.type_name)

                    resource_graph[parent.type_name].add(child.type_name)

            return resource_graph

        @classmethod
        def find_resource_path(cls, session, resource_type_name):
            """"""Find resource ancestors by type/name format.

            Find all ancestors of a resource and return them in order

            Args:
                session (object): db session
                resource_type_name (str): resource to query

            Returns:
                list: list of Resources, transitive ancestors for the given
                    resource
            """"""

            qry = (
                session.query(Resource).filter(
                    Resource.type_name == resource_type_name)
            )

            resources = qry.all()
            return cls._find_resource_path(session, resources)

        @classmethod
        def _find_resource_path(cls, _, resources):
            """"""Find the list of transitive ancestors for the given resource.

            Args:
                _ (object): position holder
                resources (list): list of the resources to query

            Returns:
                list: list of Resources, transitive ancestors for the given
                    resource
            """"""

            if not resources:
                return []

            path = []
            resource = resources[0]

            path.append(resource)
            while resource.parent:
                resource = resource.parent
                path.append(resource)

            return path

        @classmethod
        def get_roles_by_permission_names(cls, session, permission_names):
            """"""Return the list of roles covering the specified permissions.

            Args:
                session (object): db session
                permission_names (list): permissions to be covered by.

            Returns:
                set: roles set that cover the permissions
            """"""

            permission_set = set(permission_names)
            qry = session.query(Permission)
            if permission_set:
                qry = qry.filter(Permission.name.in_(permission_set))
            permissions = qry.all()

            roles = set()
            for permission in permissions:
                for role in permission.roles:
                    roles.add(role)

            result_set = set()
            for role in roles:
                role_permissions = set(
                    [p.name for p in role.permissions])
                if permission_set.issubset(role_permissions):
                    result_set.add(role)

            return result_set

        @classmethod
        def get_member(cls, session, name):
            """"""Get member by name.

            Args:
                session (object): db session
                name (str): the name the member to query

            Returns:
                list: Members from the query
            """"""

            return session.query(Member).filter(Member.name == name).all()

    base.metadata.create_all(dbengine)
    return sessionmaker(bind=dbengine), ModelAccess",_10354.py,835,"for member_name in expansion[member.name]:
    cur_members.add(member_name)",cur_members |= {member_name for member_name in expansion[member.name]},1,nan,nan
https://github.com/openstack/neutron/tree/master/neutron/plugins/ml2/drivers/ovn/mech_driver/mech_driver.py,"def get_supported_vif_types(self):
        vif_types = set()
        for ch in self.sb_ovn.chassis_list().execute(check_error=True):
            other_config = ovn_utils.get_ovn_chassis_other_config(ch)
            dp_type = other_config.get('datapath-type', '')
            if dp_type == ovn_const.CHASSIS_DATAPATH_NETDEV:
                vif_types.add(portbindings.VIF_TYPE_VHOST_USER)
            else:
                vif_types.add(portbindings.VIF_TYPE_OVS)
        return list(vif_types)",_10844.py,3,"for ch in self.sb_ovn.chassis_list().execute(check_error=True):
    other_config = ovn_utils.get_ovn_chassis_other_config(ch)
    dp_type = other_config.get('datapath-type', '')
    if dp_type == ovn_const.CHASSIS_DATAPATH_NETDEV:
        vif_types.add(portbindings.VIF_TYPE_VHOST_USER)
    else:
        vif_types.add(portbindings.VIF_TYPE_OVS)","vif_types = {portbindings.VIF_TYPE_VHOST_USER if ovn_utils.get_ovn_chassis_other_config(ch).get('datapath-type', '') == ovn_const.CHASSIS_DATAPATH_NETDEV else portbindings.VIF_TYPE_OVS for ch in self.sb_ovn.chassis_list().execute(check_error=True)}",1,nan,nan
https://github.com/cloud-custodian/cloud-custodian/tree/master/c7n/resources/vpc.py,"def filter_peered_refs(self, resources):
        if not resources:
            return resources
        # Check that groups are not referenced across accounts
        client = local_session(self.manager.session_factory).client('ec2')
        peered_ids = set()
        for resource_set in chunks(resources, 200):
            for sg_ref in client.describe_security_group_references(
                    GroupId=[r['GroupId'] for r in resource_set]
            )['SecurityGroupReferenceSet']:
                peered_ids.add(sg_ref['GroupId'])
        self.log.debug(
            ""%d of %d groups w/ peered refs"", len(peered_ids), len(resources))
        return [r for r in resources if r['GroupId'] not in peered_ids]",_14698.py,7,"for resource_set in chunks(resources, 200):
    for sg_ref in client.describe_security_group_references(GroupId=[r['GroupId'] for r in resource_set])['SecurityGroupReferenceSet']:
        peered_ids.add(sg_ref['GroupId'])","peered_ids = {sg_ref['GroupId'] for resource_set in chunks(resources, 200) for sg_ref in client.describe_security_group_references(GroupId=[r['GroupId'] for r in resource_set])['SecurityGroupReferenceSet']}",1,nan,nan
https://github.com/mysql/mysql-connector-python/tree/master/lib/mysql/connector/django/introspection.py,"def get_indexes(self, cursor, table_name):
        cursor.execute(""SHOW INDEX FROM {0}""
                       """".format(self.connection.ops.quote_name(table_name)))
        # Do a two-pass search for indexes: on first pass check which indexes
        # are multicolumn, on second pass check which single-column indexes
        # are present.
        rows = list(cursor.fetchall())
        multicol_indexes = set()
        for row in rows:
            if row[3] > 1:
                multicol_indexes.add(row[2])
        indexes = {}
        for row in rows:
            if row[2] in multicol_indexes:
                continue
            if row[4] not in indexes:
                indexes[row[4]] = {'primary_key': False, 'unique': False}
            # It's possible to have the unique and PK constraints in
            # separate indexes.
            if row[2] == 'PRIMARY':
                indexes[row[4]]['primary_key'] = True
            if not row[1]:
                indexes[row[4]]['unique'] = True
        return indexes",_14751.py,9,"for row in rows:
    if row[3] > 1:
        multicol_indexes.add(row[2])",multicol_indexes = {row[2] for row in rows if row[3] > 1},1,nan,nan
https://github.com/dagster-io/dagster/tree/master/python_modules/dagster/dagster_tests/core_tests/launcher_tests/test_default_run_launcher.py,"def _get_successful_step_keys(event_records):

    step_keys = set()

    for er in event_records:
        if er.dagster_event and er.dagster_event.is_step_success:
            step_keys.add(er.dagster_event.step_key)

    return step_keys",_17509.py,5,"for er in event_records:
    if er.dagster_event and er.dagster_event.is_step_success:
        step_keys.add(er.dagster_event.step_key)",step_keys = {er.dagster_event.step_key for er in event_records if er.dagster_event and er.dagster_event.is_step_success},1,nan,nan
https://github.com/pyproj4/pyproj/tree/master/test/test_sync.py,"def test_get_transform_grid_list__contains():
    grids = get_transform_grid_list(
        bbox=BBox(170, -90, -170, 90),
        spatial_test=""contains"",
        include_already_downloaded=True,
    )
    assert len(grids) > 5
    source_ids = set()
    for grid in grids:
        source_ids.add(grid[""properties""][""source_id""])
    assert sorted(source_ids) == [""nz_linz""]",_19972.py,9,"for grid in grids:
    source_ids.add(grid['properties']['source_id'])",source_ids = {grid['properties']['source_id'] for grid in grids},1,nan,nan
https://github.com/gaogaotiantian/viztracer/tree/master/tests/test_cmdline.py,"def check_func(data):
            tids = set()
            for entry in data[""traceEvents""]:
                tids.add(entry[""tid""])
            self.assertGreaterEqual(len(tids), 4)",_21194.py,3,"for entry in data['traceEvents']:
    tids.add(entry['tid'])",tids = {entry['tid'] for entry in data['traceEvents']},1,nan,nan
https://github.com/sympy/sympy/tree/master/sympy/diffgeom/diffgeom.py,"def _find_coords(expr):
    # Finds CoordinateSystems existing in expr
    fields = expr.atoms(BaseScalarField, BaseVectorField)
    result = set()
    for f in fields:
        result.add(f._coord_sys)
    return result",_22760.py,5,"for f in fields:
    result.add(f._coord_sys)",result = {f._coord_sys for f in fields},1,nan,nan
https://github.com/open-telemetry/opentelemetry-python/tree/master/propagator/opentelemetry-propagator-b3/tests/test_b3_format.py,"def test_fields(self):
        """"""Make sure the fields attribute returns the fields used in inject""""""

        propagator = self.get_propagator()
        tracer = trace.TracerProvider().get_tracer(""sdk_tracer_provider"")

        mock_setter = Mock()

        with tracer.start_as_current_span(""parent""):
            with tracer.start_as_current_span(""child""):
                propagator.inject({}, setter=mock_setter)

        inject_fields = set()

        for call in mock_setter.mock_calls:
            inject_fields.add(call[1][1])

        self.assertEqual(propagator.fields, inject_fields)",_30755.py,15,"for call in mock_setter.mock_calls:
    inject_fields.add(call[1][1])",inject_fields = {call[1][1] for call in mock_setter.mock_calls},1,nan,nan
https://github.com/pychess/pychess/tree/master/lib/pychess/ic/managers/ICCSeekManager.py,"def on_icc_seek_add(self, data):
        log.debug(""DG_SEEK_ADD %s"" % data)
        # index name titles rating provisional-status wild rating-type time
        # inc rated color minrating maxrating autoaccept formula fancy-time-control
        # 195 Tinker {C} 2402 2 0 Blitz 5 3 1 -1 0 9999 1 1 {}

        parts = data.split("" "", 2)
        index = int(parts[0])
        player = self.connection.players.get(parts[1])

        titles_end = parts[2].find(""}"")
        titles = parts[2][1:titles_end]
        tit = set()
        for title in titles.split():
            tit.add(TITLES[title])
        player.titles |= tit

        parts = parts[2][titles_end + 1:].split()
        rating = int(parts[0])
        deviation = None  # parts[1]
        # wild = parts[2]
        try:
            gametype = GAME_TYPES[parts[3].lower()]
        except KeyError:
            return
        minutes = int(parts[4])
        increment = int(parts[5])
        rated = parts[6] == ""1""
        color = parts[7]
        if color == ""-1"":
            color = None
        else:
            color = ""white"" if color == '1' else ""black""
        rmin = int(parts[8])
        rmax = int(parts[9])
        automatic = parts[10] == ""1""
        # formula = parts[11]
        # fancy_tc = parts[12]

        if gametype.variant_type in UNSUPPORTED:
            log.debug(""!!! unsupported variant in seek: %s"" % data)
            return

        if gametype.rating_type in RATING_TYPES and player.ratings[gametype.rating_type] != rating:
            player.ratings[gametype.rating_type] = rating
            player.deviations[gametype.rating_type] = deviation
            player.emit(""ratings_changed"", gametype.rating_type, player)

        seek = FICSSeek(index,
                        player,
                        minutes,
                        increment,
                        rated,
                        color,
                        gametype,
                        rmin=rmin,
                        rmax=rmax,
                        automatic=automatic)
        self.emit(""addSeek"", seek)",_31062.py,14,"for title in titles.split():
    tit.add(TITLES[title])",tit = {TITLES[title] for title in titles.split()},1,nan,nan
https://github.com/rasbt/mlxtend/tree/master/mlxtend/preprocessing/transactionencoder.py,"def fit(self, X):
        """"""Learn unique column names from transaction DataFrame

        Parameters
        ------------
        X : list of lists
          A python list of lists, where the outer list stores the
          n transactions and the inner list stores the items in each
          transaction.

          For example,
          [['Apple', 'Beer', 'Rice', 'Chicken'],
           ['Apple', 'Beer', 'Rice'],
           ['Apple', 'Beer'],
           ['Apple', 'Bananas'],
           ['Milk', 'Beer', 'Rice', 'Chicken'],
           ['Milk', 'Beer', 'Rice'],
           ['Milk', 'Beer'],
           ['Apple', 'Bananas']]

        """"""
        unique_items = set()
        for transaction in X:
            for item in transaction:
                unique_items.add(item)
        self.columns_ = sorted(unique_items)
        columns_mapping = {}
        for col_idx, item in enumerate(self.columns_):
            columns_mapping[item] = col_idx
        self.columns_mapping_ = columns_mapping
        return self",_32266.py,23,"for transaction in X:
    for item in transaction:
        unique_items.add(item)",unique_items = {item for transaction in X for item in transaction},1,nan,nan
https://github.com/crossbario/crossbar/tree/master/crossbar/router/broker.py,"def _filter_publish_receivers(self, receivers, publish):
        """"""
        Internal helper.

        Does all filtering on a candidate set of Publish receivers,
        based on all the white/blacklist options in 'publish'.
        """"""
        # filter by ""eligible"" receivers
        #
        if publish.eligible:

            # map eligible session IDs to eligible sessions
            eligible = set()
            for session_id in publish.eligible:
                if session_id in self._router._session_id_to_session:
                    eligible.add(self._router._session_id_to_session[session_id])

            # filter receivers for eligible sessions
            receivers = eligible & receivers

        # if ""eligible_authid"" we only accept receivers that have the correct authid
        if publish.eligible_authid:
            eligible = set()
            for aid in publish.eligible_authid:
                eligible.update(self._router._authid_to_sessions.get(aid, set()))
            receivers = receivers & eligible

        # if ""eligible_authrole"" we only accept receivers that have the correct authrole
        if publish.eligible_authrole:
            eligible = set()
            for ar in publish.eligible_authrole:
                eligible.update(self._router._authrole_to_sessions.get(ar, set()))
            receivers = receivers & eligible

        # remove ""excluded"" receivers
        #
        if publish.exclude:

            # map excluded session IDs to excluded sessions
            exclude = set()
            for s in publish.exclude:
                if s in self._router._session_id_to_session:
                    exclude.add(self._router._session_id_to_session[s])

            # filter receivers for excluded sessions
            if exclude:
                receivers = receivers - exclude

        # remove auth-id based receivers
        if publish.exclude_authid:
            for aid in publish.exclude_authid:
                receivers = receivers - self._router._authid_to_sessions.get(aid, set())

        # remove authrole based receivers
        if publish.exclude_authrole:
            for ar in publish.exclude_authrole:
                receivers = receivers - self._router._authrole_to_sessions.get(ar, set())

        return receivers",_34455.py,14,"for session_id in publish.eligible:
    if session_id in self._router._session_id_to_session:
        eligible.add(self._router._session_id_to_session[session_id])",eligible = {self._router._session_id_to_session[session_id] for session_id in publish.eligible if session_id in self._router._session_id_to_session},1,nan,nan
https://github.com/crossbario/crossbar/tree/master/crossbar/router/broker.py,"def _filter_publish_receivers(self, receivers, publish):
        """"""
        Internal helper.

        Does all filtering on a candidate set of Publish receivers,
        based on all the white/blacklist options in 'publish'.
        """"""
        # filter by ""eligible"" receivers
        #
        if publish.eligible:

            # map eligible session IDs to eligible sessions
            eligible = set()
            for session_id in publish.eligible:
                if session_id in self._router._session_id_to_session:
                    eligible.add(self._router._session_id_to_session[session_id])

            # filter receivers for eligible sessions
            receivers = eligible & receivers

        # if ""eligible_authid"" we only accept receivers that have the correct authid
        if publish.eligible_authid:
            eligible = set()
            for aid in publish.eligible_authid:
                eligible.update(self._router._authid_to_sessions.get(aid, set()))
            receivers = receivers & eligible

        # if ""eligible_authrole"" we only accept receivers that have the correct authrole
        if publish.eligible_authrole:
            eligible = set()
            for ar in publish.eligible_authrole:
                eligible.update(self._router._authrole_to_sessions.get(ar, set()))
            receivers = receivers & eligible

        # remove ""excluded"" receivers
        #
        if publish.exclude:

            # map excluded session IDs to excluded sessions
            exclude = set()
            for s in publish.exclude:
                if s in self._router._session_id_to_session:
                    exclude.add(self._router._session_id_to_session[s])

            # filter receivers for excluded sessions
            if exclude:
                receivers = receivers - exclude

        # remove auth-id based receivers
        if publish.exclude_authid:
            for aid in publish.exclude_authid:
                receivers = receivers - self._router._authid_to_sessions.get(aid, set())

        # remove authrole based receivers
        if publish.exclude_authrole:
            for ar in publish.exclude_authrole:
                receivers = receivers - self._router._authrole_to_sessions.get(ar, set())

        return receivers",_34455.py,41,"for s in publish.exclude:
    if s in self._router._session_id_to_session:
        exclude.add(self._router._session_id_to_session[s])",exclude = {self._router._session_id_to_session[s] for s in publish.exclude if s in self._router._session_id_to_session},1,nan,nan
https://github.com/ansible/ansible/tree/master/lib/ansible/module_utils/facts/collector.py,"def build_dep_data(collector_names, all_fact_subsets):
    dep_map = defaultdict(set)
    for collector_name in collector_names:
        collector_deps = set()
        for collector in all_fact_subsets[collector_name]:
            for dep in collector.required_facts:
                collector_deps.add(dep)
        dep_map[collector_name] = collector_deps
    return dep_map",_36250.py,5,"for collector in all_fact_subsets[collector_name]:
    for dep in collector.required_facts:
        collector_deps.add(dep)",collector_deps = {dep for collector in all_fact_subsets[collector_name] for dep in collector.required_facts},1,nan,nan
https://github.com/sanic-org/sanic/tree/master/tests/test_multiprocessing.py,"def test_multiprocessing_legacy_unix(app):
    """"""Tests that the number of children we produce is correct""""""
    # Selects a number at random so we can spot check
    num_workers = random.choice(range(2, multiprocessing.cpu_count() * 2 + 1))
    process_list = set()

    @app.after_server_start
    async def shutdown(app):
        await sleep(2.1)
        app.stop()

    def stop_on_alarm(*args):
        for process in multiprocessing.active_children():
            process_list.add(process.pid)

    signal.signal(signal.SIGALRM, stop_on_alarm)
    signal.alarm(2)
    app.run(workers=num_workers, debug=True, legacy=True, unix=""./test.sock"")

    assert len(process_list) == num_workers",_38409.py,13,"for process in multiprocessing.active_children():
    process_list.add(process.pid)",process_list = {process.pid for process in multiprocessing.active_children()},1,nan,nan
https://github.com/csujedihy/lc-all-solutions/tree/master/269.alien-dictionary/alien-dictionary.py,"def alienOrder(self, words):
    """"""
    :type words: List[str]
    :rtype: str
    """"""

    def dfs(root, graph, visited):
      visited[root] = 1
      for nbr in graph[root].getNbrs():
        if visited[nbr.val] == 0:
          if not dfs(nbr.val, graph, visited):
            return False
        elif visited[nbr.val] == 1:
          return False

      visited[root] = 2
      self.ans += root
      return True

    self.ans = """"
    graph = {}
    visited = collections.defaultdict(int)
    self.topNum = 0
    for i in range(0, len(words) - 1):
      a = words[i]
      b = words[i + 1]
      i = 0
      while i < len(a) and i < len(b):
        if a[i] != b[i]:
          nodeA = nodeB = None
          if a[i] not in graph:
            nodeA = Node(a[i])
            graph[a[i]] = nodeA
          else:
            nodeA = graph[a[i]]
          if b[i] not in graph:
            nodeB = Node(b[i])
            graph[b[i]] = nodeB
          else:
            nodeB = graph[b[i]]
          nodeA.connect(nodeB)
          break
        i += 1
      if i < len(a) and i >= len(b):
        return """"

    for c in graph:
      if visited[c] == 0:
        if not dfs(c, graph, visited):
          return """"

    unUsedSet = set()
    for word in words:
      for c in word:
        unUsedSet.add(c)

    for c in unUsedSet:
      if c not in graph:
        self.ans += c
    return self.ans[::-1]",_38651.py,53,"for word in words:
    for c in word:
        unUsedSet.add(c)",unUsedSet = {c for word in words for c in word},1,nan,nan
https://github.com/edx/configuration/tree/master/util/parsefiles.py,"if __name__ == '__main__':

    args = arg_parse()

    # configure logging
    logging.basicConfig()

    if not args.verbose:
        logging.disable(logging.WARNING)

    # set of modified files in the commit range
    change_set = set()

    # read from standard in
    for line in sys.stdin:
        change_set.add(line.rstrip())

    # configuration file is expected to be in the following format:
    #
    # roles_paths:
    #       - <all paths relative to configuration repository that contain Ansible roles>
    # aws_plays_paths:
    #       - <all paths relative to configuration repository that contain aws Ansible playbooks>
    # docker_plays_paths:
    #       - <all paths relative to configuration repository that contain Docker Ansible playbooks>

    # read config file
    config = _open_yaml_file(CONFIG_FILE_PATH)

    # build graph
    graph = build_graph(TRAVIS_BUILD_DIR, config[""roles_paths""], config[""aws_plays_paths""], config[""docker_plays_paths""])

    # gets any playbooks in the commit range
    plays = get_plays(change_set, TRAVIS_BUILD_DIR, config[""aws_plays_paths""])

    # transforms list of roles and plays into list of original roles and the roles contained in the plays
    roles = change_set_to_roles(change_set, TRAVIS_BUILD_DIR, config[""roles_paths""], config[""aws_plays_paths""], graph)

    # expands roles set to include roles that are dependent on existing roles
    dependent_roles = get_dependencies(roles, graph)

    # determine which docker plays cover at least one role
    docker_plays = get_docker_plays(dependent_roles, graph)

    docker_plays = docker_plays | plays

    # filter out docker plays without a Dockerfile
    docker_plays = filter_docker_plays(docker_plays, TRAVIS_BUILD_DIR)

    # Add playbooks to the list whose docker file has been modified
    modified_docker_files = _get_modified_dockerfiles(change_set, TRAVIS_BUILD_DIR)

    # Add plays to the list which got changed in docker/plays directory
    docker_plays_dir = get_modified_dockerfiles_plays(change_set, TRAVIS_BUILD_DIR)

    all_plays = set(set(docker_plays) | set( modified_docker_files) | set(docker_plays_dir))

    print("" "".join(all_plays))",_45343.py,15,"for line in sys.stdin:
    change_set.add(line.rstrip())",change_set = {line.rstrip() for line in sys.stdin},1,nan,nan
https://github.com/openstack/neutron/tree/master/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_db_sync.py,"def _calculate_fip_pfs_differences(self, ovn_rtr_lb_pfs, db_pfs):
        to_add_or_update = set()
        to_remove = []
        ovn_pfs = utils.parse_ovn_lb_port_forwarding(ovn_rtr_lb_pfs)

        # check that all pfs are accounted for in ovn_pfs by building
        # a set for each protocol and then comparing it with ovn_pfs
        db_mapped_pfs = {}
        for db_pf in db_pfs:
            for pf in self._unroll_port_forwarding(db_pf):
                fip_id = pf.get('floatingip_id')
                protocol = self.l3_plugin.port_forwarding.ovn_lb_protocol(
                    pf.get('protocol'))
                db_vip = ""{}:{} {}:{}"".format(
                    pf.get('floating_ip_address'), pf.get('external_port'),
                    pf.get('internal_ip_address'), pf.get('internal_port'))

                fip_dict = db_mapped_pfs.get(fip_id, {})
                fip_dict_proto = fip_dict.get(protocol, set())
                fip_dict_proto.add(db_vip)
                if protocol not in fip_dict:
                    fip_dict[protocol] = fip_dict_proto
                if fip_id not in db_mapped_pfs:
                    db_mapped_pfs[fip_id] = fip_dict
        for fip_id in db_mapped_pfs:
            ovn_pfs_fip_id = ovn_pfs.get(fip_id, {})
            # check for cases when ovn has lbs for protocols that are not in
            # neutron db
            if len(db_mapped_pfs[fip_id]) != len(ovn_pfs_fip_id):
                to_add_or_update.add(fip_id)
                continue
            # check that vips in each protocol are an exact match
            for protocol in db_mapped_pfs[fip_id]:
                ovn_fip_dict_proto = ovn_pfs_fip_id.get(protocol)
                if db_mapped_pfs[fip_id][protocol] != ovn_fip_dict_proto:
                    to_add_or_update.add(fip_id)

        # remove pf entries that exist in ovn lb but have no fip in
        # neutron db.
        for fip_id in ovn_pfs:
            for db_pf in db_pfs:
                pf_fip_id = db_pf.get('floatingip_id')
                if pf_fip_id == fip_id:
                    break
            else:
                to_remove.append(fip_id)

        return list(to_add_or_update), to_remove",_49158.py,33,"for protocol in db_mapped_pfs[fip_id]:
    ovn_fip_dict_proto = ovn_pfs_fip_id.get(protocol)
    if db_mapped_pfs[fip_id][protocol] != ovn_fip_dict_proto:
        to_add_or_update.add(fip_id)",to_add_or_update |= {fip_id for protocol in db_mapped_pfs[fip_id] if db_mapped_pfs[fip_id][protocol] != ovn_pfs_fip_id.get(protocol)},1,nan,nan
https://github.com/SiCKRAGE/SiCKRAGE/tree/master/sickrage/core/updaters/show_updater.py,"def task(self, force=False):
        if self.running and not force:
            return

        try:
            self.running = True

            # set thread name
            threading.current_thread().name = self.name

            session = sickrage.app.cache_db.session()

            update_timestamp = int(time.mktime(datetime.datetime.now().timetuple()))

            try:
                dbData = session.query(CacheDB.LastUpdate).filter_by(provider='theTVDB').one()
                last_update = int(dbData.time)
            except orm.exc.NoResultFound:
                last_update = update_timestamp
                dbData = CacheDB.LastUpdate(**{
                    'provider': 'theTVDB',
                    'time': 0
                })
                session.add(dbData)
            finally:
                session.commit()

            # get list of updated series from a series provider
            updated_shows = set()
            for series_provider_id in SeriesProviderID:
                resp = sickrage.app.series_providers[series_provider_id].updates(last_update)
                if resp:
                    for series in resp:
                        updated_shows.add(series['id'])

            # start update process
            pi_list = []
            for show_obj in get_show_list():
                # if show_obj.paused:
                #     sickrage.app.log.info('Show update skipped, show: {} is paused.'.format(show_obj.name))
                #     continue

                if show_obj.status == 'Ended':
                    if not sickrage.app.config.general.show_update_stale:
                        sickrage.app.log.info('Show update skipped, show: {} status is ended.'.format(show_obj.name))
                        continue
                    elif not (datetime.datetime.now() - show_obj.last_update).days >= 90:
                        sickrage.app.log.info('Show update skipped, show: {} status is ended and recently updated.'.format(show_obj.name))
                        continue

                try:
                    if show_obj.series_id in updated_shows:
                        pi_list.append(sickrage.app.show_queue.refresh_show(show_obj.series_id, show_obj.series_provider_id, force=False))
                    elif (datetime.datetime.now() - show_obj.last_update).days >= 7:
                        pi_list.append(sickrage.app.show_queue.update_show(show_obj.series_id, show_obj.series_provider_id, force=False))
                except (CantUpdateShowException, CantRefreshShowException) as e:
                    sickrage.app.log.debug(""Automatic update failed: {}"".format(e))

            # ProgressIndicators.setIndicator('dailyShowUpdates', QueueProgressIndicator(""Daily Show Updates"", pi_list))

            dbData.time = update_timestamp
            session.commit()
        finally:
            self.running = False",_49530.py,30,"for series_provider_id in SeriesProviderID:
    resp = sickrage.app.series_providers[series_provider_id].updates(last_update)
    if resp:
        for series in resp:
            updated_shows.add(series['id'])",updated_shows = {series['id'] for series_provider_id in SeriesProviderID if sickrage.app.series_providers[series_provider_id].updates(last_update) for series in sickrage.app.series_providers[series_provider_id].updates(last_update)},1,nan,nan
https://github.com/pgmpy/pgmpy/tree/master/pgmpy/inference/ExactInference.py,"def induced_graph(self, elimination_order):
        """"""
        Returns the induced graph formed by running Variable Elimination on the network.

        Parameters
        ----------
        elimination_order: list, array like
            List of variables in the order in which they are to be eliminated.

        Examples
        --------
        >>> import numpy as np
        >>> import pandas as pd
        >>> from pgmpy.models import BayesianNetwork
        >>> from pgmpy.inference import VariableElimination
        >>> values = pd.DataFrame(np.random.randint(low=0, high=2, size=(1000, 5)),
        ...                       columns=['A', 'B', 'C', 'D', 'E'])
        >>> model = BayesianNetwork([('A', 'B'), ('C', 'B'), ('C', 'D'), ('B', 'E')])
        >>> model.fit(values)
        >>> inference = VariableElimination(model)
        >>> inference.induced_graph(['C', 'D', 'A', 'B', 'E'])
        """"""
        self._initialize_structures()

        # If the elimination order does not contain the same variables as the model
        if set(elimination_order) != set(self.variables):
            raise ValueError(
                ""Set of variables in elimination order""
                "" different from variables in model""
            )

        eliminated_variables = set()
        working_factors = {
            node: [factor.scope() for factor in self.factors[node]]
            for node in self.factors
        }

        # The set of cliques that should be in the induced graph
        cliques = set()
        for factors in working_factors.values():
            for factor in factors:
                cliques.add(tuple(factor))

        # Removing all the factors containing the variables which are
        # eliminated (as all the factors should be considered only once)
        for var in elimination_order:
            factors = [
                factor
                for factor in working_factors[var]
                if not set(factor).intersection(eliminated_variables)
            ]
            phi = set(itertools.chain(*factors)).difference({var})
            cliques.add(tuple(phi))
            del working_factors[var]
            for variable in phi:
                working_factors[variable].append(list(phi))
            eliminated_variables.add(var)

        edges_comb = [
            itertools.combinations(c, 2) for c in filter(lambda x: len(x) > 1, cliques)
        ]
        return nx.Graph(itertools.chain(*edges_comb))",_50874.py,40,"for factors in working_factors.values():
    for factor in factors:
        cliques.add(tuple(factor))",cliques = {tuple(factor) for factors in working_factors.values() for factor in factors},1,nan,nan
https://github.com/rlworkgroup/metaworld/tree/master/tests/integration/test_new_api.py,"def test_all_ml1(env_name):
    ml1 = ML1(env_name)
    train_env_instances = {env_name: env_cls()
                           for (env_name, env_cls) in ml1.train_classes.items()}
    train_env_rand_vecs = check_tasks_unique(ml1.train_tasks,
                                       ml1._train_classes.keys())
    for task in ml1.train_tasks:
        env = train_env_instances[task.env_name]
        env.set_task(task)
        env.reset()
        assert env.random_init == True
        old_obj_init = env.obj_init_pos
        old_target_pos = env._target_pos
        step_env(env, max_path_length=STEPS, render=False)
        assert np.all(np.allclose(old_obj_init, env.obj_init_pos))
        assert np.all(np.allclose(old_target_pos, env._target_pos))
    for env in train_env_instances.values():
        env.close()
    del train_env_instances

    test_env_instances = {env_name: env_cls()
                          for (env_name, env_cls) in ml1.test_classes.items()}
    test_env_rand_vecs = check_tasks_unique(ml1.test_tasks,
                                       ml1._test_classes.keys())
    for task in ml1.test_tasks:
        env = test_env_instances[task.env_name]
        env.set_task(task)
        env.reset()
        assert env.random_init == True
        old_obj_init = env.obj_init_pos
        old_target_pos = env._target_pos
        step_env(env, max_path_length=STEPS, render=False)
        assert np.all(np.allclose(old_obj_init, env.obj_init_pos))
        assert np.all(np.allclose(old_target_pos, env._target_pos))
    for env in test_env_instances.values():
        env.close()
    train_test_rand_vecs = set()
    for rand_vecs in train_env_rand_vecs.values():
        for rand_vec in rand_vecs:
            train_test_rand_vecs.add(tuple(rand_vec))
    for rand_vecs in test_env_rand_vecs.values():
        for rand_vec in rand_vecs:
            train_test_rand_vecs.add(tuple(rand_vec))
    assert len(train_test_rand_vecs) == (len(ml1.test_classes.keys()) + len(ml1.train_classes.keys())) * metaworld._N_GOALS
    del test_env_instances",_51527.py,38,"for rand_vecs in train_env_rand_vecs.values():
    for rand_vec in rand_vecs:
        train_test_rand_vecs.add(tuple(rand_vec))",train_test_rand_vecs = {tuple(rand_vec) for rand_vecs in train_env_rand_vecs.values() for rand_vec in rand_vecs},1,nan,nan
https://github.com/rlworkgroup/metaworld/tree/master/tests/integration/test_new_api.py,"def test_all_ml1(env_name):
    ml1 = ML1(env_name)
    train_env_instances = {env_name: env_cls()
                           for (env_name, env_cls) in ml1.train_classes.items()}
    train_env_rand_vecs = check_tasks_unique(ml1.train_tasks,
                                       ml1._train_classes.keys())
    for task in ml1.train_tasks:
        env = train_env_instances[task.env_name]
        env.set_task(task)
        env.reset()
        assert env.random_init == True
        old_obj_init = env.obj_init_pos
        old_target_pos = env._target_pos
        step_env(env, max_path_length=STEPS, render=False)
        assert np.all(np.allclose(old_obj_init, env.obj_init_pos))
        assert np.all(np.allclose(old_target_pos, env._target_pos))
    for env in train_env_instances.values():
        env.close()
    del train_env_instances

    test_env_instances = {env_name: env_cls()
                          for (env_name, env_cls) in ml1.test_classes.items()}
    test_env_rand_vecs = check_tasks_unique(ml1.test_tasks,
                                       ml1._test_classes.keys())
    for task in ml1.test_tasks:
        env = test_env_instances[task.env_name]
        env.set_task(task)
        env.reset()
        assert env.random_init == True
        old_obj_init = env.obj_init_pos
        old_target_pos = env._target_pos
        step_env(env, max_path_length=STEPS, render=False)
        assert np.all(np.allclose(old_obj_init, env.obj_init_pos))
        assert np.all(np.allclose(old_target_pos, env._target_pos))
    for env in test_env_instances.values():
        env.close()
    train_test_rand_vecs = set()
    for rand_vecs in train_env_rand_vecs.values():
        for rand_vec in rand_vecs:
            train_test_rand_vecs.add(tuple(rand_vec))
    for rand_vecs in test_env_rand_vecs.values():
        for rand_vec in rand_vecs:
            train_test_rand_vecs.add(tuple(rand_vec))
    assert len(train_test_rand_vecs) == (len(ml1.test_classes.keys()) + len(ml1.train_classes.keys())) * metaworld._N_GOALS
    del test_env_instances",_51527.py,41,"for rand_vecs in test_env_rand_vecs.values():
    for rand_vec in rand_vecs:
        train_test_rand_vecs.add(tuple(rand_vec))",train_test_rand_vecs |= {tuple(rand_vec) for rand_vecs in test_env_rand_vecs.values() for rand_vec in rand_vecs},1,nan,nan
https://github.com/dano/aioprocessing/tree/master/aioprocessing/executor.py,"def __new__(cls, clsname, bases, dct, **kwargs):
        coro_list = dct.get(""coroutines"", [])
        existing_coros = set()

        def find_existing_coros(d):
            for attr in d:
                if attr.startswith(""coro_"") or attr.startswith(""thread_""):
                    existing_coros.add(attr)

        # Determine if any bases include the coroutines attribute, or
        # if either this class or a base class provides an actual
        # implementation for a coroutine method.
        find_existing_coros(dct)
        for b in bases:
            b_dct = b.__dict__
            coro_list.extend(b_dct.get(""coroutines"", []))
            find_existing_coros(b_dct)

        # Add _ExecutorMixin to bases.
        if _ExecutorMixin not in bases:
            bases += (_ExecutorMixin,)

        # Add coro funcs to dct, but only if a definition
        # is not already provided by dct or one of our bases.
        for func in coro_list:
            coro_name = ""coro_{}"".format(func)
            if coro_name not in existing_coros:
                dct[coro_name] = cls.coro_maker(func)

        return super().__new__(cls, clsname, bases, dct)",_52406.py,6,"for attr in d:
    if attr.startswith('coro_') or attr.startswith('thread_'):
        existing_coros.add(attr)",existing_coros = {attr for attr in d if attr.startswith('coro_') or attr.startswith('thread_')},1,nan,nan
https://github.com/apache/tvm/tree/master/python/tvm/relay/frontend/caffe2.py,"def from_caffe2(self, init_net, predict_net):
        """"""Construct Relay expression from caffe2 graph.

        Parameters
        ----------
        init_net : protobuf object
        predict_net : protobuf object

        Returns
        -------
        mod : tvm.IRModule
            The module that optimizations will be performed on.

        params : dict
            A dict of name: tvm.nd.array pairs, used as pretrained weights
        """"""
        # pylint: disable=import-outside-toplevel
        from caffe2.python import workspace

        workspace.RunNetOnce(init_net)

        # Input
        input_name = predict_net.op[0].input[0]

        # Params
        self._params = {}
        used_blobs = set()
        for c2_op in predict_net.op:
            for i in c2_op.input:
                used_blobs.add(i)
        for blob in workspace.Blobs():
            if blob in used_blobs and blob != input_name:
                self._params[blob] = _nd.array(workspace.FetchBlob(blob))

        # Variables
        self._nodes = {}
        for blob in predict_net.external_input:
            if blob in self._params:
                self._nodes[blob] = new_var(
                    blob, shape=self._params[blob].shape, dtype=self._params[blob].dtype
                )
            else:
                shape = self._shape[blob] if blob in self._shape else ()
                if isinstance(self._dtype, dict) and blob in self._dtype:
                    dtype = str(self._dtype[blob])
                elif isinstance(self._dtype, str):
                    dtype = self._dtype
                else:
                    dtype = ""float32""
                self._nodes[blob] = new_var(blob, shape=shape, dtype=dtype)

        # Ops
        for c2_op in predict_net.op:
            for blob in c2_op.output:
                self._ops[blob] = c2_op

        for c2_op in predict_net.op:
            self._process_op(c2_op)

        # Outputs
        out = []
        for blob in predict_net.external_output:
            out.append(self._nodes[blob])

        if len(out) > 1:
            outputs = _expr.Tuple(out)
        else:
            outputs = out[0]

        func = _function.Function(analysis.free_vars(outputs), outputs)
        self._mod[""main""] = func

        return self._mod, self._params",_52813.py,28,"for c2_op in predict_net.op:
    for i in c2_op.input:
        used_blobs.add(i)",used_blobs = {i for c2_op in predict_net.op for i in c2_op.input},1,nan,nan
https://github.com/cloud-custodian/cloud-custodian/tree/master/tests/test_policy.py,"def test_resource_arn_override_generator(self):
        overrides = set()
        for k, v in manager.resources.items():
            arn_gen = bool(v.__dict__.get('get_arns') or v.__dict__.get('generate_arn'))

            if arn_gen:
                overrides.add(k)

        overrides = overrides.difference(
            {'account', 's3', 'hostedzone', 'log-group', 'rest-api', 'redshift-snapshot',
             'rest-stage', 'codedeploy-app', 'codedeploy-group'})
        if overrides:
            raise ValueError(""unknown arn overrides in %s"" % ("", "".join(overrides)))",_53122.py,3,"for (k, v) in manager.resources.items():
    arn_gen = bool(v.__dict__.get('get_arns') or v.__dict__.get('generate_arn'))
    if arn_gen:
        overrides.add(k)","overrides = {k for (k, v) in manager.resources.items() if bool(v.__dict__.get('get_arns') or v.__dict__.get('generate_arn'))}",1,nan,nan
https://github.com/cloud-custodian/cloud-custodian/tree/master/tools/c7n_gcp/tests/test_gcp_storage.py,"def test_bucket_iam_policy_filter(self):
        factory = self.replay_flight_data('bucket-iam-policy')
        p = self.load_policy(
            {'name': 'bucket',
             'resource': 'gcp.bucket',
             'filters': [{
                 'type': 'iam-policy',
                 'doc': {'key': 'bindings[*].members[]',
                 'op': 'intersect',
                 'value': ['allUsers', 'allAuthenticatedUsers']}
             }]},
            session_factory=factory)
        resources = p.run()
        self.assertEqual(len(resources), 2)

        for resource in resources:
            self.assertTrue('c7n:iamPolicy' in resource)
            bindings = resource['c7n:iamPolicy']['bindings']
            members = set()
            for binding in bindings:
                for member in binding['members']:
                    members.add(member)
            self.assertTrue('allUsers' in members or 'allAuthenticatedUsers' in members)",_53373.py,20,"for binding in bindings:
    for member in binding['members']:
        members.add(member)",members = {member for binding in bindings for member in binding['members']},1,nan,nan
https://github.com/ParallelSSH/parallel-ssh/tree/master//versioneer.py,"def do_setup():
    """"""Main VCS-independent setup function for installing Versioneer.""""""
    root = get_root()
    try:
        cfg = get_config_from_root(root)
    except (EnvironmentError, configparser.NoSectionError,
            configparser.NoOptionError) as e:
        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):
            print(""Adding sample versioneer config to setup.cfg"",
                  file=sys.stderr)
            with open(os.path.join(root, ""setup.cfg""), ""a"") as f:
                f.write(SAMPLE_CONFIG)
        print(CONFIG_ERROR, file=sys.stderr)
        return 1

    print("" creating %s"" % cfg.versionfile_source)
    with open(cfg.versionfile_source, ""w"") as f:
        LONG = LONG_VERSION_PY[cfg.VCS]
        f.write(LONG % {""DOLLAR"": ""$"",
                        ""STYLE"": cfg.style,
                        ""TAG_PREFIX"": cfg.tag_prefix,
                        ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,
                        ""VERSIONFILE_SOURCE"": cfg.versionfile_source,
                        })

    ipy = os.path.join(os.path.dirname(cfg.versionfile_source),
                       ""__init__.py"")
    if os.path.exists(ipy):
        try:
            with open(ipy, ""r"") as f:
                old = f.read()
        except EnvironmentError:
            old = """"
        if INIT_PY_SNIPPET not in old:
            print("" appending to %s"" % ipy)
            with open(ipy, ""a"") as f:
                f.write(INIT_PY_SNIPPET)
        else:
            print("" %s unmodified"" % ipy)
    else:
        print("" %s doesn't exist, ok"" % ipy)
        ipy = None

    # Make sure both the top-level ""versioneer.py"" and versionfile_source
    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so
    # they'll be copied into source distributions. Pip won't be able to
    # install the package without this.
    manifest_in = os.path.join(root, ""MANIFEST.in"")
    simple_includes = set()
    try:
        with open(manifest_in, ""r"") as f:
            for line in f:
                if line.startswith(""include ""):
                    for include in line.split()[1:]:
                        simple_includes.add(include)
    except EnvironmentError:
        pass
    # That doesn't cover everything MANIFEST.in can do
    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so
    # it might give some false negatives. Appending redundant 'include'
    # lines is safe, though.
    if ""versioneer.py"" not in simple_includes:
        print("" appending 'versioneer.py' to MANIFEST.in"")
        with open(manifest_in, ""a"") as f:
            f.write(""include versioneer.py\n"")
    else:
        print("" 'versioneer.py' already in MANIFEST.in"")
    if cfg.versionfile_source not in simple_includes:
        print("" appending versionfile_source ('%s') to MANIFEST.in"" %
              cfg.versionfile_source)
        with open(manifest_in, ""a"") as f:
            f.write(""include %s\n"" % cfg.versionfile_source)
    else:
        print("" versionfile_source already in MANIFEST.in"")

    # Make VCS-specific changes. For git, this means creating/changing
    # .gitattributes to mark _version.py for export-subst keyword
    # substitution.
    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)
    return 0",_57836.py,52,"for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]},1,nan,nan
https://github.com/ansible/ansible/tree/master/test/support/network-integration/collections/ansible_collections/cisco/ios/plugins/cliconf/ios.py,"def get_defaults_flag(self):
        """"""
        The method identifies the filter that should be used to fetch running-configuration
        with defaults.
        :return: valid default filter
        """"""
        out = self.get(""show running-config ?"")
        out = to_text(out, errors=""surrogate_then_replace"")

        commands = set()
        for line in out.splitlines():
            if line.strip():
                commands.add(line.strip().split()[0])

        if ""all"" in commands:
            return ""all""
        else:
            return ""full""",_62961.py,11,"for line in out.splitlines():
    if line.strip():
        commands.add(line.strip().split()[0])",commands = {line.strip().split()[0] for line in out.splitlines() if line.strip()},1,nan,nan
https://github.com/quantumblacklabs/kedro/tree/master/kedro/runner/parallel_runner.py,"def _run(  # pylint: disable=too-many-locals,useless-suppression
        self,
        pipeline: Pipeline,
        catalog: DataCatalog,
        hook_manager: PluginManager,
        session_id: str = None,
    ) -> None:
        """"""The abstract interface for running pipelines.

        Args:
            pipeline: The ``Pipeline`` to run.
            catalog: The ``DataCatalog`` from which to fetch data.
            hook_manager: The ``PluginManager`` to activate hooks.
            session_id: The id of the session.

        Raises:
            AttributeError: When the provided pipeline is not suitable for
                parallel execution.
            RuntimeError: If the runner is unable to schedule the execution of
                all pipeline nodes.
            Exception: In case of any downstream node failure.

        """"""
        # pylint: disable=import-outside-toplevel,cyclic-import

        nodes = pipeline.nodes
        self._validate_catalog(catalog, pipeline)
        self._validate_nodes(nodes)

        load_counts = Counter(chain.from_iterable(n.inputs for n in nodes))
        node_dependencies = pipeline.node_dependencies
        todo_nodes = set(node_dependencies.keys())
        done_nodes = set()  # type: Set[Node]
        futures = set()
        done = None
        max_workers = self._get_required_workers_count(pipeline)

        from kedro.framework.project import LOGGING, PACKAGE_NAME

        with ProcessPoolExecutor(max_workers=max_workers) as pool:
            while True:
                ready = {n for n in todo_nodes if node_dependencies[n] <= done_nodes}
                todo_nodes -= ready
                for node in ready:
                    futures.add(
                        pool.submit(
                            _run_node_synchronization,
                            node,
                            catalog,
                            self._is_async,
                            session_id,
                            package_name=PACKAGE_NAME,
                            logging_config=LOGGING,  # type: ignore
                        )
                    )
                if not futures:
                    if todo_nodes:
                        debug_data = {
                            ""todo_nodes"": todo_nodes,
                            ""done_nodes"": done_nodes,
                            ""ready_nodes"": ready,
                            ""done_futures"": done,
                        }
                        debug_data_str = ""\n"".join(
                            f""{k} = {v}"" for k, v in debug_data.items()
                        )
                        raise RuntimeError(
                            f""Unable to schedule new tasks although some nodes ""
                            f""have not been run:\n{debug_data_str}""
                        )
                    break  # pragma: no cover
                done, futures = wait(futures, return_when=FIRST_COMPLETED)
                for future in done:
                    node = future.result()
                    done_nodes.add(node)

                    # Decrement load counts, and release any datasets we
                    # have finished with. This is particularly important
                    # for the shared, default datasets we created above.
                    for data_set in node.inputs:
                        load_counts[data_set] -= 1
                        if (
                            load_counts[data_set] < 1
                            and data_set not in pipeline.inputs()
                        ):
                            catalog.release(data_set)
                    for data_set in node.outputs:
                        if (
                            load_counts[data_set] < 1
                            and data_set not in pipeline.outputs()
                        ):
                            catalog.release(data_set)",_63108.py,44,"for node in ready:
    futures.add(pool.submit(_run_node_synchronization, node, catalog, self._is_async, session_id, package_name=PACKAGE_NAME, logging_config=LOGGING))","futures |= {pool.submit(_run_node_synchronization, node, catalog, self._is_async, session_id, package_name=PACKAGE_NAME, logging_config=LOGGING) for node in ready}",1,nan,nan
https://github.com/fsspec/s3fs/tree/master//versioneer.py,"def do_setup():
    """"""Do main VCS-independent setup function for installing Versioneer.""""""
    root = get_root()
    try:
        cfg = get_config_from_root(root)
    except (
        EnvironmentError,
        configparser.NoSectionError,
        configparser.NoOptionError,
    ) as e:
        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):
            print(""Adding sample versioneer config to setup.cfg"", file=sys.stderr)
            with open(os.path.join(root, ""setup.cfg""), ""a"") as f:
                f.write(SAMPLE_CONFIG)
        print(CONFIG_ERROR, file=sys.stderr)
        return 1

    print("" creating %s"" % cfg.versionfile_source)
    with open(cfg.versionfile_source, ""w"") as f:
        LONG = LONG_VERSION_PY[cfg.VCS]
        f.write(
            LONG
            % {
                ""DOLLAR"": ""$"",
                ""STYLE"": cfg.style,
                ""TAG_PREFIX"": cfg.tag_prefix,
                ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,
                ""VERSIONFILE_SOURCE"": cfg.versionfile_source,
            }
        )

    ipy = os.path.join(os.path.dirname(cfg.versionfile_source), ""__init__.py"")
    if os.path.exists(ipy):
        try:
            with open(ipy, ""r"") as f:
                old = f.read()
        except EnvironmentError:
            old = """"
        module = os.path.splitext(os.path.basename(cfg.versionfile_source))[0]
        snippet = INIT_PY_SNIPPET.format(module)
        if OLD_SNIPPET in old:
            print("" replacing boilerplate in %s"" % ipy)
            with open(ipy, ""w"") as f:
                f.write(old.replace(OLD_SNIPPET, snippet))
        elif snippet not in old:
            print("" appending to %s"" % ipy)
            with open(ipy, ""a"") as f:
                f.write(snippet)
        else:
            print("" %s unmodified"" % ipy)
    else:
        print("" %s doesn't exist, ok"" % ipy)
        ipy = None

    # Make sure both the top-level ""versioneer.py"" and versionfile_source
    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so
    # they'll be copied into source distributions. Pip won't be able to
    # install the package without this.
    manifest_in = os.path.join(root, ""MANIFEST.in"")
    simple_includes = set()
    try:
        with open(manifest_in, ""r"") as f:
            for line in f:
                if line.startswith(""include ""):
                    for include in line.split()[1:]:
                        simple_includes.add(include)
    except EnvironmentError:
        pass
    # That doesn't cover everything MANIFEST.in can do
    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so
    # it might give some false negatives. Appending redundant 'include'
    # lines is safe, though.
    if ""versioneer.py"" not in simple_includes:
        print("" appending 'versioneer.py' to MANIFEST.in"")
        with open(manifest_in, ""a"") as f:
            f.write(""include versioneer.py\n"")
    else:
        print("" 'versioneer.py' already in MANIFEST.in"")
    if cfg.versionfile_source not in simple_includes:
        print(
            "" appending versionfile_source ('%s') to MANIFEST.in""
            % cfg.versionfile_source
        )
        with open(manifest_in, ""a"") as f:
            f.write(""include %s\n"" % cfg.versionfile_source)
    else:
        print("" versionfile_source already in MANIFEST.in"")

    # Make VCS-specific changes. For git, this means creating/changing
    # .gitattributes to mark _version.py for export-subst keyword
    # substitution.
    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)
    return 0",_63618.py,63,"for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]},1,nan,nan
https://github.com/duo-labs/cloudtracker/tree/master/cloudtracker/datasources/athena.py,"def __init__(self, config, account, start, end, args):
        # Mute boto except errors
        logging.getLogger(""botocore"").setLevel(logging.WARN)
        logging.info(
            ""Source of CloudTrail logs: s3://{bucket}/{path}"".format(
                bucket=config[""s3_bucket""], path=config[""path""]
            )
        )

        # Check start date is not older than a year, as we only create partitions for that far back
        if (
            datetime.datetime.now() - datetime.datetime.strptime(start, ""%Y-%m-%d"")
        ).days > 365:
            raise Exception(
                ""Start date is over a year old. CloudTracker does not create or use partitions over a year old.""
            )

        #
        # Create date filtering
        #
        month_restrictions = set()
        start = start.split(""-"")
        end = end.split(""-"")

        if start[0] == end[0]:
            for month in range(int(start[1]), int(end[1]) + 1):
                month_restrictions.add(
                    ""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month)
                )
        else:
            # Add restrictions for months in start year
            for month in range(int(start[1]), 12 + 1):
                month_restrictions.add(
                    ""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month)
                )
            # Add restrictions for months in middle years
            for year in range(int(start[0]), int(end[0])):
                for month in (1, 12 + 1):
                    month_restrictions.add(
                        ""(year = '{:0>2}' and month = '{:0>2}')"".format(year, month)
                    )
            # Add restrictions for months in final year
            for month in range(1, int(end[1]) + 1):
                month_restrictions.add(
                    ""(year = '{:0>2}' and month = '{:0>2}')"".format(end[0], month)
                )

        # Combine date filters and add error filter
        self.search_filter = (
            ""(("" + "" or "".join(month_restrictions) + "") and errorcode IS NULL)""
        )

        self.table_name = ""cloudtrail_logs_{}"".format(account[""id""])

        #
        # Display the AWS identity (doubles as a check that boto creds are setup)
        #
        sts = boto3.client(""sts"")
        identity = sts.get_caller_identity()
        logging.info(""Using AWS identity: {}"".format(identity[""Arn""]))
        current_account_id = identity[""Account""]
        region = boto3.session.Session().region_name

        if ""output_s3_bucket"" in config:
            self.output_bucket = config[""output_s3_bucket""]
        else:
            self.output_bucket = ""s3://aws-athena-query-results-{}-{}"".format(
                current_account_id, region
            )
        logging.info(""Using output bucket: {}"".format(self.output_bucket))

        if ""workgroup"" in config:
            self.workgroup = config[""workgroup""]
        logging.info(""Using workgroup: {}"".format(self.workgroup))

        if not config.get('org_id'):
            cloudtrail_log_path = ""s3://{bucket}/{path}/AWSLogs/{account_id}/CloudTrail"".format(
                bucket=config[""s3_bucket""], path=config[""path""], account_id=account[""id""]
            )
        else:
            cloudtrail_log_path = ""s3://{bucket}/{path}/AWSLogs/{org_id}/{account_id}/CloudTrail"".format(
                bucket=config[""s3_bucket""], path=config[""path""], org_id=config[""org_id""], account_id=account[""id""]
            )

        logging.info(""Account cloudtrail log path: {}"".format(cloudtrail_log_path))

        # Open connections to needed AWS services
        self.athena = boto3.client(""athena"")
        self.s3 = boto3.client(""s3"")

        if args.skip_setup:
            logging.info(""Skipping initial table creation"")
            return

        # Check we can access the S3 bucket
        resp = self.s3.list_objects_v2(
            Bucket=config[""s3_bucket""], Prefix=config[""path""], MaxKeys=1
        )
        if ""Contents"" not in resp or len(resp[""Contents""]) == 0:
            exit(
                ""ERROR: S3 bucket has no contents.  Ensure you have logs at s3://{bucket}/{path}"".format(
                    bucket=config[""s3_bucket""], path=config[""path""]
                )
            )

        # Ensure our database exists
        self.query_athena(
            ""CREATE DATABASE IF NOT EXISTS {db} {comment}"".format(
                db=self.database, comment=""COMMENT 'Created by CloudTracker'""
            ),
            context=None,
        )

        #
        # Set up table
        #
        query = """"""CREATE EXTERNAL TABLE IF NOT EXISTS `{table_name}` (
            `eventversion` string COMMENT 'from deserializer', 
            `useridentity` struct<type:string,principalid:string,arn:string,accountid:string,invokedby:string,accesskeyid:string,username:string,sessioncontext:struct<attributes:struct<mfaauthenticated:string,creationdate:string>,sessionissuer:struct<type:string,principalid:string,arn:string,accountid:string,username:string>>> COMMENT 'from deserializer', 
            `eventtime` string COMMENT 'from deserializer', 
            `eventsource` string COMMENT 'from deserializer', 
            `eventname` string COMMENT 'from deserializer', 
            `awsregion` string COMMENT 'from deserializer', 
            `sourceipaddress` string COMMENT 'from deserializer', 
            `useragent` string COMMENT 'from deserializer', 
            `errorcode` string COMMENT 'from deserializer', 
            `errormessage` string COMMENT 'from deserializer', 
            `requestparameters` string COMMENT 'from deserializer', 
            `responseelements` string COMMENT 'from deserializer', 
            `additionaleventdata` string COMMENT 'from deserializer', 
            `requestid` string COMMENT 'from deserializer', 
            `eventid` string COMMENT 'from deserializer', 
            `resources` array<struct<arn:string,accountid:string,type:string>> COMMENT 'from deserializer', 
            `eventtype` string COMMENT 'from deserializer', 
            `apiversion` string COMMENT 'from deserializer', 
            `readonly` string COMMENT 'from deserializer', 
            `recipientaccountid` string COMMENT 'from deserializer', 
            `serviceeventdetails` string COMMENT 'from deserializer', 
            `sharedeventid` string COMMENT 'from deserializer', 
            `vpcendpointid` string COMMENT 'from deserializer')
            PARTITIONED BY (region string, year string, month string)
            ROW FORMAT SERDE 
            'com.amazon.emr.hive.serde.CloudTrailSerde' 
            STORED AS INPUTFORMAT 
            'com.amazon.emr.cloudtrail.CloudTrailInputFormat' 
            OUTPUTFORMAT 
            'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
            LOCATION '{cloudtrail_log_path}'"""""".format(
            table_name=self.table_name, cloudtrail_log_path=cloudtrail_log_path
        )
        self.query_athena(query)

        #
        # Create partitions
        #

        logging.info(
            ""Checking if all partitions for the past {} months exist"".format(
                NUM_MONTHS_FOR_PARTITIONS
            )
        )

        # Get list of current partitions
        query = ""SHOW PARTITIONS {table_name}"".format(table_name=self.table_name)
        partition_list = self.query_athena(query, skip_header=False)

        partition_set = set()
        for partition in partition_list:
            partition_set.add(partition[0])

        # Get region list. Using ec2 here just because it exists in all regions.
        regions = boto3.session.Session().get_available_regions(""ec2"")

        queries_to_make = set()

        # Iterate over every month for the past year and build queries to run to create partitions
        for num_months_ago in range(0, NUM_MONTHS_FOR_PARTITIONS):
            date_of_interest = datetime.datetime.now() - relativedelta(
                months=num_months_ago
            )
            year = date_of_interest.year
            month = ""{:0>2}"".format(date_of_interest.month)

            query = """"

            for region in regions:
                if (
                    ""region={region}/year={year}/month={month}"".format(
                        region=region, year=year, month=month
                    )
                    in partition_set
                ):
                    continue

                query += ""PARTITION (region='{region}',year='{year}',month='{month}') location '{cloudtrail_log_path}/{region}/{year}/{month}/'\n"".format(
                    region=region,
                    year=year,
                    month=month,
                    cloudtrail_log_path=cloudtrail_log_path,
                )
            if query != """":
                queries_to_make.add(
                    ""ALTER TABLE {table_name} ADD "".format(table_name=self.table_name)
                    + query
                )

        # Run the queries
        query_count = len(queries_to_make)
        for query in queries_to_make:
            logging.info(""Partition groups remaining to create: {}"".format(query_count))
            self.query_athena(query)
            query_count -= 1",_63940.py,168,"for partition in partition_list:
    partition_set.add(partition[0])",partition_set = {partition[0] for partition in partition_list},1,nan,nan
https://github.com/duo-labs/cloudtracker/tree/master/cloudtracker/datasources/athena.py,"def __init__(self, config, account, start, end, args):
        # Mute boto except errors
        logging.getLogger(""botocore"").setLevel(logging.WARN)
        logging.info(
            ""Source of CloudTrail logs: s3://{bucket}/{path}"".format(
                bucket=config[""s3_bucket""], path=config[""path""]
            )
        )

        # Check start date is not older than a year, as we only create partitions for that far back
        if (
            datetime.datetime.now() - datetime.datetime.strptime(start, ""%Y-%m-%d"")
        ).days > 365:
            raise Exception(
                ""Start date is over a year old. CloudTracker does not create or use partitions over a year old.""
            )

        #
        # Create date filtering
        #
        month_restrictions = set()
        start = start.split(""-"")
        end = end.split(""-"")

        if start[0] == end[0]:
            for month in range(int(start[1]), int(end[1]) + 1):
                month_restrictions.add(
                    ""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month)
                )
        else:
            # Add restrictions for months in start year
            for month in range(int(start[1]), 12 + 1):
                month_restrictions.add(
                    ""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month)
                )
            # Add restrictions for months in middle years
            for year in range(int(start[0]), int(end[0])):
                for month in (1, 12 + 1):
                    month_restrictions.add(
                        ""(year = '{:0>2}' and month = '{:0>2}')"".format(year, month)
                    )
            # Add restrictions for months in final year
            for month in range(1, int(end[1]) + 1):
                month_restrictions.add(
                    ""(year = '{:0>2}' and month = '{:0>2}')"".format(end[0], month)
                )

        # Combine date filters and add error filter
        self.search_filter = (
            ""(("" + "" or "".join(month_restrictions) + "") and errorcode IS NULL)""
        )

        self.table_name = ""cloudtrail_logs_{}"".format(account[""id""])

        #
        # Display the AWS identity (doubles as a check that boto creds are setup)
        #
        sts = boto3.client(""sts"")
        identity = sts.get_caller_identity()
        logging.info(""Using AWS identity: {}"".format(identity[""Arn""]))
        current_account_id = identity[""Account""]
        region = boto3.session.Session().region_name

        if ""output_s3_bucket"" in config:
            self.output_bucket = config[""output_s3_bucket""]
        else:
            self.output_bucket = ""s3://aws-athena-query-results-{}-{}"".format(
                current_account_id, region
            )
        logging.info(""Using output bucket: {}"".format(self.output_bucket))

        if ""workgroup"" in config:
            self.workgroup = config[""workgroup""]
        logging.info(""Using workgroup: {}"".format(self.workgroup))

        if not config.get('org_id'):
            cloudtrail_log_path = ""s3://{bucket}/{path}/AWSLogs/{account_id}/CloudTrail"".format(
                bucket=config[""s3_bucket""], path=config[""path""], account_id=account[""id""]
            )
        else:
            cloudtrail_log_path = ""s3://{bucket}/{path}/AWSLogs/{org_id}/{account_id}/CloudTrail"".format(
                bucket=config[""s3_bucket""], path=config[""path""], org_id=config[""org_id""], account_id=account[""id""]
            )

        logging.info(""Account cloudtrail log path: {}"".format(cloudtrail_log_path))

        # Open connections to needed AWS services
        self.athena = boto3.client(""athena"")
        self.s3 = boto3.client(""s3"")

        if args.skip_setup:
            logging.info(""Skipping initial table creation"")
            return

        # Check we can access the S3 bucket
        resp = self.s3.list_objects_v2(
            Bucket=config[""s3_bucket""], Prefix=config[""path""], MaxKeys=1
        )
        if ""Contents"" not in resp or len(resp[""Contents""]) == 0:
            exit(
                ""ERROR: S3 bucket has no contents.  Ensure you have logs at s3://{bucket}/{path}"".format(
                    bucket=config[""s3_bucket""], path=config[""path""]
                )
            )

        # Ensure our database exists
        self.query_athena(
            ""CREATE DATABASE IF NOT EXISTS {db} {comment}"".format(
                db=self.database, comment=""COMMENT 'Created by CloudTracker'""
            ),
            context=None,
        )

        #
        # Set up table
        #
        query = """"""CREATE EXTERNAL TABLE IF NOT EXISTS `{table_name}` (
            `eventversion` string COMMENT 'from deserializer', 
            `useridentity` struct<type:string,principalid:string,arn:string,accountid:string,invokedby:string,accesskeyid:string,username:string,sessioncontext:struct<attributes:struct<mfaauthenticated:string,creationdate:string>,sessionissuer:struct<type:string,principalid:string,arn:string,accountid:string,username:string>>> COMMENT 'from deserializer', 
            `eventtime` string COMMENT 'from deserializer', 
            `eventsource` string COMMENT 'from deserializer', 
            `eventname` string COMMENT 'from deserializer', 
            `awsregion` string COMMENT 'from deserializer', 
            `sourceipaddress` string COMMENT 'from deserializer', 
            `useragent` string COMMENT 'from deserializer', 
            `errorcode` string COMMENT 'from deserializer', 
            `errormessage` string COMMENT 'from deserializer', 
            `requestparameters` string COMMENT 'from deserializer', 
            `responseelements` string COMMENT 'from deserializer', 
            `additionaleventdata` string COMMENT 'from deserializer', 
            `requestid` string COMMENT 'from deserializer', 
            `eventid` string COMMENT 'from deserializer', 
            `resources` array<struct<arn:string,accountid:string,type:string>> COMMENT 'from deserializer', 
            `eventtype` string COMMENT 'from deserializer', 
            `apiversion` string COMMENT 'from deserializer', 
            `readonly` string COMMENT 'from deserializer', 
            `recipientaccountid` string COMMENT 'from deserializer', 
            `serviceeventdetails` string COMMENT 'from deserializer', 
            `sharedeventid` string COMMENT 'from deserializer', 
            `vpcendpointid` string COMMENT 'from deserializer')
            PARTITIONED BY (region string, year string, month string)
            ROW FORMAT SERDE 
            'com.amazon.emr.hive.serde.CloudTrailSerde' 
            STORED AS INPUTFORMAT 
            'com.amazon.emr.cloudtrail.CloudTrailInputFormat' 
            OUTPUTFORMAT 
            'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
            LOCATION '{cloudtrail_log_path}'"""""".format(
            table_name=self.table_name, cloudtrail_log_path=cloudtrail_log_path
        )
        self.query_athena(query)

        #
        # Create partitions
        #

        logging.info(
            ""Checking if all partitions for the past {} months exist"".format(
                NUM_MONTHS_FOR_PARTITIONS
            )
        )

        # Get list of current partitions
        query = ""SHOW PARTITIONS {table_name}"".format(table_name=self.table_name)
        partition_list = self.query_athena(query, skip_header=False)

        partition_set = set()
        for partition in partition_list:
            partition_set.add(partition[0])

        # Get region list. Using ec2 here just because it exists in all regions.
        regions = boto3.session.Session().get_available_regions(""ec2"")

        queries_to_make = set()

        # Iterate over every month for the past year and build queries to run to create partitions
        for num_months_ago in range(0, NUM_MONTHS_FOR_PARTITIONS):
            date_of_interest = datetime.datetime.now() - relativedelta(
                months=num_months_ago
            )
            year = date_of_interest.year
            month = ""{:0>2}"".format(date_of_interest.month)

            query = """"

            for region in regions:
                if (
                    ""region={region}/year={year}/month={month}"".format(
                        region=region, year=year, month=month
                    )
                    in partition_set
                ):
                    continue

                query += ""PARTITION (region='{region}',year='{year}',month='{month}') location '{cloudtrail_log_path}/{region}/{year}/{month}/'\n"".format(
                    region=region,
                    year=year,
                    month=month,
                    cloudtrail_log_path=cloudtrail_log_path,
                )
            if query != """":
                queries_to_make.add(
                    ""ALTER TABLE {table_name} ADD "".format(table_name=self.table_name)
                    + query
                )

        # Run the queries
        query_count = len(queries_to_make)
        for query in queries_to_make:
            logging.info(""Partition groups remaining to create: {}"".format(query_count))
            self.query_athena(query)
            query_count -= 1",_63940.py,26,"for month in range(int(start[1]), int(end[1]) + 1):
    month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month))","month_restrictions = {""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month) for month in range(int(start[1]), int(end[1]) + 1)}",1,nan,nan
https://github.com/duo-labs/cloudtracker/tree/master/cloudtracker/datasources/athena.py,"def __init__(self, config, account, start, end, args):
        # Mute boto except errors
        logging.getLogger(""botocore"").setLevel(logging.WARN)
        logging.info(
            ""Source of CloudTrail logs: s3://{bucket}/{path}"".format(
                bucket=config[""s3_bucket""], path=config[""path""]
            )
        )

        # Check start date is not older than a year, as we only create partitions for that far back
        if (
            datetime.datetime.now() - datetime.datetime.strptime(start, ""%Y-%m-%d"")
        ).days > 365:
            raise Exception(
                ""Start date is over a year old. CloudTracker does not create or use partitions over a year old.""
            )

        #
        # Create date filtering
        #
        month_restrictions = set()
        start = start.split(""-"")
        end = end.split(""-"")

        if start[0] == end[0]:
            for month in range(int(start[1]), int(end[1]) + 1):
                month_restrictions.add(
                    ""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month)
                )
        else:
            # Add restrictions for months in start year
            for month in range(int(start[1]), 12 + 1):
                month_restrictions.add(
                    ""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month)
                )
            # Add restrictions for months in middle years
            for year in range(int(start[0]), int(end[0])):
                for month in (1, 12 + 1):
                    month_restrictions.add(
                        ""(year = '{:0>2}' and month = '{:0>2}')"".format(year, month)
                    )
            # Add restrictions for months in final year
            for month in range(1, int(end[1]) + 1):
                month_restrictions.add(
                    ""(year = '{:0>2}' and month = '{:0>2}')"".format(end[0], month)
                )

        # Combine date filters and add error filter
        self.search_filter = (
            ""(("" + "" or "".join(month_restrictions) + "") and errorcode IS NULL)""
        )

        self.table_name = ""cloudtrail_logs_{}"".format(account[""id""])

        #
        # Display the AWS identity (doubles as a check that boto creds are setup)
        #
        sts = boto3.client(""sts"")
        identity = sts.get_caller_identity()
        logging.info(""Using AWS identity: {}"".format(identity[""Arn""]))
        current_account_id = identity[""Account""]
        region = boto3.session.Session().region_name

        if ""output_s3_bucket"" in config:
            self.output_bucket = config[""output_s3_bucket""]
        else:
            self.output_bucket = ""s3://aws-athena-query-results-{}-{}"".format(
                current_account_id, region
            )
        logging.info(""Using output bucket: {}"".format(self.output_bucket))

        if ""workgroup"" in config:
            self.workgroup = config[""workgroup""]
        logging.info(""Using workgroup: {}"".format(self.workgroup))

        if not config.get('org_id'):
            cloudtrail_log_path = ""s3://{bucket}/{path}/AWSLogs/{account_id}/CloudTrail"".format(
                bucket=config[""s3_bucket""], path=config[""path""], account_id=account[""id""]
            )
        else:
            cloudtrail_log_path = ""s3://{bucket}/{path}/AWSLogs/{org_id}/{account_id}/CloudTrail"".format(
                bucket=config[""s3_bucket""], path=config[""path""], org_id=config[""org_id""], account_id=account[""id""]
            )

        logging.info(""Account cloudtrail log path: {}"".format(cloudtrail_log_path))

        # Open connections to needed AWS services
        self.athena = boto3.client(""athena"")
        self.s3 = boto3.client(""s3"")

        if args.skip_setup:
            logging.info(""Skipping initial table creation"")
            return

        # Check we can access the S3 bucket
        resp = self.s3.list_objects_v2(
            Bucket=config[""s3_bucket""], Prefix=config[""path""], MaxKeys=1
        )
        if ""Contents"" not in resp or len(resp[""Contents""]) == 0:
            exit(
                ""ERROR: S3 bucket has no contents.  Ensure you have logs at s3://{bucket}/{path}"".format(
                    bucket=config[""s3_bucket""], path=config[""path""]
                )
            )

        # Ensure our database exists
        self.query_athena(
            ""CREATE DATABASE IF NOT EXISTS {db} {comment}"".format(
                db=self.database, comment=""COMMENT 'Created by CloudTracker'""
            ),
            context=None,
        )

        #
        # Set up table
        #
        query = """"""CREATE EXTERNAL TABLE IF NOT EXISTS `{table_name}` (
            `eventversion` string COMMENT 'from deserializer', 
            `useridentity` struct<type:string,principalid:string,arn:string,accountid:string,invokedby:string,accesskeyid:string,username:string,sessioncontext:struct<attributes:struct<mfaauthenticated:string,creationdate:string>,sessionissuer:struct<type:string,principalid:string,arn:string,accountid:string,username:string>>> COMMENT 'from deserializer', 
            `eventtime` string COMMENT 'from deserializer', 
            `eventsource` string COMMENT 'from deserializer', 
            `eventname` string COMMENT 'from deserializer', 
            `awsregion` string COMMENT 'from deserializer', 
            `sourceipaddress` string COMMENT 'from deserializer', 
            `useragent` string COMMENT 'from deserializer', 
            `errorcode` string COMMENT 'from deserializer', 
            `errormessage` string COMMENT 'from deserializer', 
            `requestparameters` string COMMENT 'from deserializer', 
            `responseelements` string COMMENT 'from deserializer', 
            `additionaleventdata` string COMMENT 'from deserializer', 
            `requestid` string COMMENT 'from deserializer', 
            `eventid` string COMMENT 'from deserializer', 
            `resources` array<struct<arn:string,accountid:string,type:string>> COMMENT 'from deserializer', 
            `eventtype` string COMMENT 'from deserializer', 
            `apiversion` string COMMENT 'from deserializer', 
            `readonly` string COMMENT 'from deserializer', 
            `recipientaccountid` string COMMENT 'from deserializer', 
            `serviceeventdetails` string COMMENT 'from deserializer', 
            `sharedeventid` string COMMENT 'from deserializer', 
            `vpcendpointid` string COMMENT 'from deserializer')
            PARTITIONED BY (region string, year string, month string)
            ROW FORMAT SERDE 
            'com.amazon.emr.hive.serde.CloudTrailSerde' 
            STORED AS INPUTFORMAT 
            'com.amazon.emr.cloudtrail.CloudTrailInputFormat' 
            OUTPUTFORMAT 
            'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
            LOCATION '{cloudtrail_log_path}'"""""".format(
            table_name=self.table_name, cloudtrail_log_path=cloudtrail_log_path
        )
        self.query_athena(query)

        #
        # Create partitions
        #

        logging.info(
            ""Checking if all partitions for the past {} months exist"".format(
                NUM_MONTHS_FOR_PARTITIONS
            )
        )

        # Get list of current partitions
        query = ""SHOW PARTITIONS {table_name}"".format(table_name=self.table_name)
        partition_list = self.query_athena(query, skip_header=False)

        partition_set = set()
        for partition in partition_list:
            partition_set.add(partition[0])

        # Get region list. Using ec2 here just because it exists in all regions.
        regions = boto3.session.Session().get_available_regions(""ec2"")

        queries_to_make = set()

        # Iterate over every month for the past year and build queries to run to create partitions
        for num_months_ago in range(0, NUM_MONTHS_FOR_PARTITIONS):
            date_of_interest = datetime.datetime.now() - relativedelta(
                months=num_months_ago
            )
            year = date_of_interest.year
            month = ""{:0>2}"".format(date_of_interest.month)

            query = """"

            for region in regions:
                if (
                    ""region={region}/year={year}/month={month}"".format(
                        region=region, year=year, month=month
                    )
                    in partition_set
                ):
                    continue

                query += ""PARTITION (region='{region}',year='{year}',month='{month}') location '{cloudtrail_log_path}/{region}/{year}/{month}/'\n"".format(
                    region=region,
                    year=year,
                    month=month,
                    cloudtrail_log_path=cloudtrail_log_path,
                )
            if query != """":
                queries_to_make.add(
                    ""ALTER TABLE {table_name} ADD "".format(table_name=self.table_name)
                    + query
                )

        # Run the queries
        query_count = len(queries_to_make)
        for query in queries_to_make:
            logging.info(""Partition groups remaining to create: {}"".format(query_count))
            self.query_athena(query)
            query_count -= 1",_63940.py,32,"for month in range(int(start[1]), 12 + 1):
    month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month))","month_restrictions |= {""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month) for month in range(int(start[1]), 12 + 1)}",1,nan,nan
https://github.com/duo-labs/cloudtracker/tree/master/cloudtracker/datasources/athena.py,"def __init__(self, config, account, start, end, args):
        # Mute boto except errors
        logging.getLogger(""botocore"").setLevel(logging.WARN)
        logging.info(
            ""Source of CloudTrail logs: s3://{bucket}/{path}"".format(
                bucket=config[""s3_bucket""], path=config[""path""]
            )
        )

        # Check start date is not older than a year, as we only create partitions for that far back
        if (
            datetime.datetime.now() - datetime.datetime.strptime(start, ""%Y-%m-%d"")
        ).days > 365:
            raise Exception(
                ""Start date is over a year old. CloudTracker does not create or use partitions over a year old.""
            )

        #
        # Create date filtering
        #
        month_restrictions = set()
        start = start.split(""-"")
        end = end.split(""-"")

        if start[0] == end[0]:
            for month in range(int(start[1]), int(end[1]) + 1):
                month_restrictions.add(
                    ""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month)
                )
        else:
            # Add restrictions for months in start year
            for month in range(int(start[1]), 12 + 1):
                month_restrictions.add(
                    ""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month)
                )
            # Add restrictions for months in middle years
            for year in range(int(start[0]), int(end[0])):
                for month in (1, 12 + 1):
                    month_restrictions.add(
                        ""(year = '{:0>2}' and month = '{:0>2}')"".format(year, month)
                    )
            # Add restrictions for months in final year
            for month in range(1, int(end[1]) + 1):
                month_restrictions.add(
                    ""(year = '{:0>2}' and month = '{:0>2}')"".format(end[0], month)
                )

        # Combine date filters and add error filter
        self.search_filter = (
            ""(("" + "" or "".join(month_restrictions) + "") and errorcode IS NULL)""
        )

        self.table_name = ""cloudtrail_logs_{}"".format(account[""id""])

        #
        # Display the AWS identity (doubles as a check that boto creds are setup)
        #
        sts = boto3.client(""sts"")
        identity = sts.get_caller_identity()
        logging.info(""Using AWS identity: {}"".format(identity[""Arn""]))
        current_account_id = identity[""Account""]
        region = boto3.session.Session().region_name

        if ""output_s3_bucket"" in config:
            self.output_bucket = config[""output_s3_bucket""]
        else:
            self.output_bucket = ""s3://aws-athena-query-results-{}-{}"".format(
                current_account_id, region
            )
        logging.info(""Using output bucket: {}"".format(self.output_bucket))

        if ""workgroup"" in config:
            self.workgroup = config[""workgroup""]
        logging.info(""Using workgroup: {}"".format(self.workgroup))

        if not config.get('org_id'):
            cloudtrail_log_path = ""s3://{bucket}/{path}/AWSLogs/{account_id}/CloudTrail"".format(
                bucket=config[""s3_bucket""], path=config[""path""], account_id=account[""id""]
            )
        else:
            cloudtrail_log_path = ""s3://{bucket}/{path}/AWSLogs/{org_id}/{account_id}/CloudTrail"".format(
                bucket=config[""s3_bucket""], path=config[""path""], org_id=config[""org_id""], account_id=account[""id""]
            )

        logging.info(""Account cloudtrail log path: {}"".format(cloudtrail_log_path))

        # Open connections to needed AWS services
        self.athena = boto3.client(""athena"")
        self.s3 = boto3.client(""s3"")

        if args.skip_setup:
            logging.info(""Skipping initial table creation"")
            return

        # Check we can access the S3 bucket
        resp = self.s3.list_objects_v2(
            Bucket=config[""s3_bucket""], Prefix=config[""path""], MaxKeys=1
        )
        if ""Contents"" not in resp or len(resp[""Contents""]) == 0:
            exit(
                ""ERROR: S3 bucket has no contents.  Ensure you have logs at s3://{bucket}/{path}"".format(
                    bucket=config[""s3_bucket""], path=config[""path""]
                )
            )

        # Ensure our database exists
        self.query_athena(
            ""CREATE DATABASE IF NOT EXISTS {db} {comment}"".format(
                db=self.database, comment=""COMMENT 'Created by CloudTracker'""
            ),
            context=None,
        )

        #
        # Set up table
        #
        query = """"""CREATE EXTERNAL TABLE IF NOT EXISTS `{table_name}` (
            `eventversion` string COMMENT 'from deserializer', 
            `useridentity` struct<type:string,principalid:string,arn:string,accountid:string,invokedby:string,accesskeyid:string,username:string,sessioncontext:struct<attributes:struct<mfaauthenticated:string,creationdate:string>,sessionissuer:struct<type:string,principalid:string,arn:string,accountid:string,username:string>>> COMMENT 'from deserializer', 
            `eventtime` string COMMENT 'from deserializer', 
            `eventsource` string COMMENT 'from deserializer', 
            `eventname` string COMMENT 'from deserializer', 
            `awsregion` string COMMENT 'from deserializer', 
            `sourceipaddress` string COMMENT 'from deserializer', 
            `useragent` string COMMENT 'from deserializer', 
            `errorcode` string COMMENT 'from deserializer', 
            `errormessage` string COMMENT 'from deserializer', 
            `requestparameters` string COMMENT 'from deserializer', 
            `responseelements` string COMMENT 'from deserializer', 
            `additionaleventdata` string COMMENT 'from deserializer', 
            `requestid` string COMMENT 'from deserializer', 
            `eventid` string COMMENT 'from deserializer', 
            `resources` array<struct<arn:string,accountid:string,type:string>> COMMENT 'from deserializer', 
            `eventtype` string COMMENT 'from deserializer', 
            `apiversion` string COMMENT 'from deserializer', 
            `readonly` string COMMENT 'from deserializer', 
            `recipientaccountid` string COMMENT 'from deserializer', 
            `serviceeventdetails` string COMMENT 'from deserializer', 
            `sharedeventid` string COMMENT 'from deserializer', 
            `vpcendpointid` string COMMENT 'from deserializer')
            PARTITIONED BY (region string, year string, month string)
            ROW FORMAT SERDE 
            'com.amazon.emr.hive.serde.CloudTrailSerde' 
            STORED AS INPUTFORMAT 
            'com.amazon.emr.cloudtrail.CloudTrailInputFormat' 
            OUTPUTFORMAT 
            'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
            LOCATION '{cloudtrail_log_path}'"""""".format(
            table_name=self.table_name, cloudtrail_log_path=cloudtrail_log_path
        )
        self.query_athena(query)

        #
        # Create partitions
        #

        logging.info(
            ""Checking if all partitions for the past {} months exist"".format(
                NUM_MONTHS_FOR_PARTITIONS
            )
        )

        # Get list of current partitions
        query = ""SHOW PARTITIONS {table_name}"".format(table_name=self.table_name)
        partition_list = self.query_athena(query, skip_header=False)

        partition_set = set()
        for partition in partition_list:
            partition_set.add(partition[0])

        # Get region list. Using ec2 here just because it exists in all regions.
        regions = boto3.session.Session().get_available_regions(""ec2"")

        queries_to_make = set()

        # Iterate over every month for the past year and build queries to run to create partitions
        for num_months_ago in range(0, NUM_MONTHS_FOR_PARTITIONS):
            date_of_interest = datetime.datetime.now() - relativedelta(
                months=num_months_ago
            )
            year = date_of_interest.year
            month = ""{:0>2}"".format(date_of_interest.month)

            query = """"

            for region in regions:
                if (
                    ""region={region}/year={year}/month={month}"".format(
                        region=region, year=year, month=month
                    )
                    in partition_set
                ):
                    continue

                query += ""PARTITION (region='{region}',year='{year}',month='{month}') location '{cloudtrail_log_path}/{region}/{year}/{month}/'\n"".format(
                    region=region,
                    year=year,
                    month=month,
                    cloudtrail_log_path=cloudtrail_log_path,
                )
            if query != """":
                queries_to_make.add(
                    ""ALTER TABLE {table_name} ADD "".format(table_name=self.table_name)
                    + query
                )

        # Run the queries
        query_count = len(queries_to_make)
        for query in queries_to_make:
            logging.info(""Partition groups remaining to create: {}"".format(query_count))
            self.query_athena(query)
            query_count -= 1",_63940.py,37,"for year in range(int(start[0]), int(end[0])):
    for month in (1, 12 + 1):
        month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(year, month))","month_restrictions |= {""(year = '{:0>2}' and month = '{:0>2}')"".format(year, month) for year in range(int(start[0]), int(end[0])) for month in (1, 12 + 1)}",1,nan,nan
https://github.com/duo-labs/cloudtracker/tree/master/cloudtracker/datasources/athena.py,"def __init__(self, config, account, start, end, args):
        # Mute boto except errors
        logging.getLogger(""botocore"").setLevel(logging.WARN)
        logging.info(
            ""Source of CloudTrail logs: s3://{bucket}/{path}"".format(
                bucket=config[""s3_bucket""], path=config[""path""]
            )
        )

        # Check start date is not older than a year, as we only create partitions for that far back
        if (
            datetime.datetime.now() - datetime.datetime.strptime(start, ""%Y-%m-%d"")
        ).days > 365:
            raise Exception(
                ""Start date is over a year old. CloudTracker does not create or use partitions over a year old.""
            )

        #
        # Create date filtering
        #
        month_restrictions = set()
        start = start.split(""-"")
        end = end.split(""-"")

        if start[0] == end[0]:
            for month in range(int(start[1]), int(end[1]) + 1):
                month_restrictions.add(
                    ""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month)
                )
        else:
            # Add restrictions for months in start year
            for month in range(int(start[1]), 12 + 1):
                month_restrictions.add(
                    ""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month)
                )
            # Add restrictions for months in middle years
            for year in range(int(start[0]), int(end[0])):
                for month in (1, 12 + 1):
                    month_restrictions.add(
                        ""(year = '{:0>2}' and month = '{:0>2}')"".format(year, month)
                    )
            # Add restrictions for months in final year
            for month in range(1, int(end[1]) + 1):
                month_restrictions.add(
                    ""(year = '{:0>2}' and month = '{:0>2}')"".format(end[0], month)
                )

        # Combine date filters and add error filter
        self.search_filter = (
            ""(("" + "" or "".join(month_restrictions) + "") and errorcode IS NULL)""
        )

        self.table_name = ""cloudtrail_logs_{}"".format(account[""id""])

        #
        # Display the AWS identity (doubles as a check that boto creds are setup)
        #
        sts = boto3.client(""sts"")
        identity = sts.get_caller_identity()
        logging.info(""Using AWS identity: {}"".format(identity[""Arn""]))
        current_account_id = identity[""Account""]
        region = boto3.session.Session().region_name

        if ""output_s3_bucket"" in config:
            self.output_bucket = config[""output_s3_bucket""]
        else:
            self.output_bucket = ""s3://aws-athena-query-results-{}-{}"".format(
                current_account_id, region
            )
        logging.info(""Using output bucket: {}"".format(self.output_bucket))

        if ""workgroup"" in config:
            self.workgroup = config[""workgroup""]
        logging.info(""Using workgroup: {}"".format(self.workgroup))

        if not config.get('org_id'):
            cloudtrail_log_path = ""s3://{bucket}/{path}/AWSLogs/{account_id}/CloudTrail"".format(
                bucket=config[""s3_bucket""], path=config[""path""], account_id=account[""id""]
            )
        else:
            cloudtrail_log_path = ""s3://{bucket}/{path}/AWSLogs/{org_id}/{account_id}/CloudTrail"".format(
                bucket=config[""s3_bucket""], path=config[""path""], org_id=config[""org_id""], account_id=account[""id""]
            )

        logging.info(""Account cloudtrail log path: {}"".format(cloudtrail_log_path))

        # Open connections to needed AWS services
        self.athena = boto3.client(""athena"")
        self.s3 = boto3.client(""s3"")

        if args.skip_setup:
            logging.info(""Skipping initial table creation"")
            return

        # Check we can access the S3 bucket
        resp = self.s3.list_objects_v2(
            Bucket=config[""s3_bucket""], Prefix=config[""path""], MaxKeys=1
        )
        if ""Contents"" not in resp or len(resp[""Contents""]) == 0:
            exit(
                ""ERROR: S3 bucket has no contents.  Ensure you have logs at s3://{bucket}/{path}"".format(
                    bucket=config[""s3_bucket""], path=config[""path""]
                )
            )

        # Ensure our database exists
        self.query_athena(
            ""CREATE DATABASE IF NOT EXISTS {db} {comment}"".format(
                db=self.database, comment=""COMMENT 'Created by CloudTracker'""
            ),
            context=None,
        )

        #
        # Set up table
        #
        query = """"""CREATE EXTERNAL TABLE IF NOT EXISTS `{table_name}` (
            `eventversion` string COMMENT 'from deserializer', 
            `useridentity` struct<type:string,principalid:string,arn:string,accountid:string,invokedby:string,accesskeyid:string,username:string,sessioncontext:struct<attributes:struct<mfaauthenticated:string,creationdate:string>,sessionissuer:struct<type:string,principalid:string,arn:string,accountid:string,username:string>>> COMMENT 'from deserializer', 
            `eventtime` string COMMENT 'from deserializer', 
            `eventsource` string COMMENT 'from deserializer', 
            `eventname` string COMMENT 'from deserializer', 
            `awsregion` string COMMENT 'from deserializer', 
            `sourceipaddress` string COMMENT 'from deserializer', 
            `useragent` string COMMENT 'from deserializer', 
            `errorcode` string COMMENT 'from deserializer', 
            `errormessage` string COMMENT 'from deserializer', 
            `requestparameters` string COMMENT 'from deserializer', 
            `responseelements` string COMMENT 'from deserializer', 
            `additionaleventdata` string COMMENT 'from deserializer', 
            `requestid` string COMMENT 'from deserializer', 
            `eventid` string COMMENT 'from deserializer', 
            `resources` array<struct<arn:string,accountid:string,type:string>> COMMENT 'from deserializer', 
            `eventtype` string COMMENT 'from deserializer', 
            `apiversion` string COMMENT 'from deserializer', 
            `readonly` string COMMENT 'from deserializer', 
            `recipientaccountid` string COMMENT 'from deserializer', 
            `serviceeventdetails` string COMMENT 'from deserializer', 
            `sharedeventid` string COMMENT 'from deserializer', 
            `vpcendpointid` string COMMENT 'from deserializer')
            PARTITIONED BY (region string, year string, month string)
            ROW FORMAT SERDE 
            'com.amazon.emr.hive.serde.CloudTrailSerde' 
            STORED AS INPUTFORMAT 
            'com.amazon.emr.cloudtrail.CloudTrailInputFormat' 
            OUTPUTFORMAT 
            'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
            LOCATION '{cloudtrail_log_path}'"""""".format(
            table_name=self.table_name, cloudtrail_log_path=cloudtrail_log_path
        )
        self.query_athena(query)

        #
        # Create partitions
        #

        logging.info(
            ""Checking if all partitions for the past {} months exist"".format(
                NUM_MONTHS_FOR_PARTITIONS
            )
        )

        # Get list of current partitions
        query = ""SHOW PARTITIONS {table_name}"".format(table_name=self.table_name)
        partition_list = self.query_athena(query, skip_header=False)

        partition_set = set()
        for partition in partition_list:
            partition_set.add(partition[0])

        # Get region list. Using ec2 here just because it exists in all regions.
        regions = boto3.session.Session().get_available_regions(""ec2"")

        queries_to_make = set()

        # Iterate over every month for the past year and build queries to run to create partitions
        for num_months_ago in range(0, NUM_MONTHS_FOR_PARTITIONS):
            date_of_interest = datetime.datetime.now() - relativedelta(
                months=num_months_ago
            )
            year = date_of_interest.year
            month = ""{:0>2}"".format(date_of_interest.month)

            query = """"

            for region in regions:
                if (
                    ""region={region}/year={year}/month={month}"".format(
                        region=region, year=year, month=month
                    )
                    in partition_set
                ):
                    continue

                query += ""PARTITION (region='{region}',year='{year}',month='{month}') location '{cloudtrail_log_path}/{region}/{year}/{month}/'\n"".format(
                    region=region,
                    year=year,
                    month=month,
                    cloudtrail_log_path=cloudtrail_log_path,
                )
            if query != """":
                queries_to_make.add(
                    ""ALTER TABLE {table_name} ADD "".format(table_name=self.table_name)
                    + query
                )

        # Run the queries
        query_count = len(queries_to_make)
        for query in queries_to_make:
            logging.info(""Partition groups remaining to create: {}"".format(query_count))
            self.query_athena(query)
            query_count -= 1",_63940.py,43,"for month in range(1, int(end[1]) + 1):
    month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(end[0], month))","month_restrictions |= {""(year = '{:0>2}' and month = '{:0>2}')"".format(end[0], month) for month in range(1, int(end[1]) + 1)}",1,nan,nan
https://github.com/boto/botocore/tree/master/botocore/loaders.py,"def list_api_versions(self, service_name, type_name):
        """"""List all API versions available for a particular service type

        :type service_name: str
        :param service_name: The name of the service

        :type type_name: str
        :param type_name: The type name for the service (i.e service-2,
            paginators-1, etc.)

        :rtype: list
        :return: A list of API version strings in sorted order.

        """"""
        known_api_versions = set()
        for possible_path in self._potential_locations(service_name,
                                                       must_exist=True,
                                                       is_dir=True):
            for dirname in os.listdir(possible_path):
                full_path = os.path.join(possible_path, dirname, type_name)
                # Only add to the known_api_versions if the directory
                # contains a service-2, paginators-1, etc. file corresponding
                # to the type_name passed in.
                if self.file_loader.exists(full_path):
                    known_api_versions.add(dirname)
        if not known_api_versions:
            raise DataNotFoundError(data_path=service_name)
        return sorted(known_api_versions)",_64105.py,16,"for possible_path in self._potential_locations(service_name, must_exist=True, is_dir=True):
    for dirname in os.listdir(possible_path):
        full_path = os.path.join(possible_path, dirname, type_name)
        if self.file_loader.exists(full_path):
            known_api_versions.add(dirname)","known_api_versions = {dirname for possible_path in self._potential_locations(service_name, must_exist=True, is_dir=True) for dirname in os.listdir(possible_path) if self.file_loader.exists(os.path.join(possible_path, dirname, type_name))}",1,nan,nan
https://github.com/dgasmith/opt_einsum/tree/master//versioneer.py,"def do_setup():
    """"""Main VCS-independent setup function for installing Versioneer.""""""
    root = get_root()
    try:
        cfg = get_config_from_root(root)
    except (EnvironmentError, configparser.NoSectionError,
            configparser.NoOptionError) as e:
        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):
            print(""Adding sample versioneer config to setup.cfg"",
                  file=sys.stderr)
            with open(os.path.join(root, ""setup.cfg""), ""a"") as f:
                f.write(SAMPLE_CONFIG)
        print(CONFIG_ERROR, file=sys.stderr)
        return 1

    print("" creating %s"" % cfg.versionfile_source)
    with open(cfg.versionfile_source, ""w"") as f:
        LONG = LONG_VERSION_PY[cfg.VCS]
        f.write(LONG % {""DOLLAR"": ""$"",
                        ""STYLE"": cfg.style,
                        ""TAG_PREFIX"": cfg.tag_prefix,
                        ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,
                        ""VERSIONFILE_SOURCE"": cfg.versionfile_source,
                        })

    ipy = os.path.join(os.path.dirname(cfg.versionfile_source),
                       ""__init__.py"")
    if os.path.exists(ipy):
        try:
            with open(ipy, ""r"") as f:
                old = f.read()
        except EnvironmentError:
            old = """"
        if INIT_PY_SNIPPET not in old:
            print("" appending to %s"" % ipy)
            with open(ipy, ""a"") as f:
                f.write(INIT_PY_SNIPPET)
        else:
            print("" %s unmodified"" % ipy)
    else:
        print("" %s doesn't exist, ok"" % ipy)
        ipy = None

    # Make sure both the top-level ""versioneer.py"" and versionfile_source
    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so
    # they'll be copied into source distributions. Pip won't be able to
    # install the package without this.
    manifest_in = os.path.join(root, ""MANIFEST.in"")
    simple_includes = set()
    try:
        with open(manifest_in, ""r"") as f:
            for line in f:
                if line.startswith(""include ""):
                    for include in line.split()[1:]:
                        simple_includes.add(include)
    except EnvironmentError:
        pass
    # That doesn't cover everything MANIFEST.in can do
    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so
    # it might give some false negatives. Appending redundant 'include'
    # lines is safe, though.
    if ""versioneer.py"" not in simple_includes:
        print("" appending 'versioneer.py' to MANIFEST.in"")
        with open(manifest_in, ""a"") as f:
            f.write(""include versioneer.py\n"")
    else:
        print("" 'versioneer.py' already in MANIFEST.in"")
    if cfg.versionfile_source not in simple_includes:
        print("" appending versionfile_source ('%s') to MANIFEST.in"" %
              cfg.versionfile_source)
        with open(manifest_in, ""a"") as f:
            f.write(""include %s\n"" % cfg.versionfile_source)
    else:
        print("" versionfile_source already in MANIFEST.in"")

    # Make VCS-specific changes. For git, this means creating/changing
    # .gitattributes to mark _version.py for export-subst keyword
    # substitution.
    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)
    return 0",_64758.py,52,"for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]},1,nan,nan
https://github.com/yihong0618/GitHubPoster/tree/master/github_poster/utils.py,"def reduce_year_list(year_list, tracks_dict):
    """"""
    format year list
    [2012, 2013, 2014, 2015, 2016]
    if 2012, 2013 values are 0
    year list to [2013, 2015, 2016]
    """"""
    year_list_keys = list(tracks_dict.keys())
    year_list_keys.sort()
    s = set()
    for key in year_list_keys:
        if tracks_dict.get(key) > 0:
            s.add(key[:4])
    year_list.sort()
    i = 0
    for year in year_list:
        if str(year) not in s:
            i += 1
        else:
            break
    return year_list[i:]",_64904.py,11,"for key in year_list_keys:
    if tracks_dict.get(key) > 0:
        s.add(key[:4])",s = {key[:4] for key in year_list_keys if tracks_dict.get(key) > 0},1,nan,nan
https://github.com/ansible/galaxy/tree/master/lib/galaxy/tools/parameters/dataset_matcher.py,"def __init__(self, dataset_matcher_factory, trans, param, other_values):
        self.dataset_matcher_factory = dataset_matcher_factory
        self.trans = trans
        self.param = param
        self.tool = param.tool
        filter_values = set()
        if param.options and other_values:
            try:
                for v in param.options.get_options(trans, other_values):
                    filter_values.add(v[0])
            except IndexError:
                pass  # no valid options
        self.filter_values = filter_values",_65301.py,9,"for v in param.options.get_options(trans, other_values):
    filter_values.add(v[0])","filter_values = {v[0] for v in param.options.get_options(trans, other_values)}",1,nan,nan
https://github.com/cltk/cltk/tree/master/src/cltk/wordnet/wordnet.py,"def _iter_hypernym_lists(self):
        """"""Get hypernyms.
        :return: An iterator over ``Synset`` objects that are either proper
        hypernyms or instance of hypernyms of the synset.
        """"""
        todo = [self]
        seen = set()
        while todo:
            for synset in todo:
                seen.add(synset)
            yield todo
            todo = [
                hypernym
                for synset in todo
                for hypernym in synset.hypernyms()
                if hypernym not in seen
            ]",_66027.py,9,"for synset in todo:
    seen.add(synset)",seen |= {synset for synset in todo},1,nan,nan
https://github.com/tensorflow/tfx/tree/master/tfx/experimental/distributed_inference/graphdef_experiments/subgraph_partitioning/graph_partition.py,"def __next__(self) -> Set[str]:
    """"""Gets the remote op names for the next remote op layer.

    Returns:
      A set of remote op names.
    """"""
    if not self._not_processed:
      raise StopIteration

    layer_node_names = set()
    for remote_op_name in self._not_processed:
      remote_op_immediate_dep = set(
          self.remote_op_to_immediate_dep[remote_op_name])
      if not remote_op_immediate_dep & self._not_processed:
        layer_node_names.add(remote_op_name)

    self._not_processed -= layer_node_names

    return layer_node_names",_66303.py,11,"for remote_op_name in self._not_processed:
    remote_op_immediate_dep = set(self.remote_op_to_immediate_dep[remote_op_name])
    if not remote_op_immediate_dep & self._not_processed:
        layer_node_names.add(remote_op_name)",layer_node_names = {remote_op_name for remote_op_name in self._not_processed if not set(self.remote_op_to_immediate_dep[remote_op_name]) & self._not_processed},1,nan,nan
https://github.com/PyTorchLightning/lightning-flash/tree/master/flash/core/integrations/labelstudio/input.py,"def _load_json_data(data, data_folder, multi_label=False):
    """"""Utility method to extract data from Label Studio json files.""""""
    results = []
    test_results = []
    data_types = set()
    tag_types = set()
    classes = set()
    for task in data:
        for annotation in task[""annotations""]:
            # extracting data types from tasks
            for key in task.get(""data""):
                data_types.add(key)
            # Adding ground_truth annotation to separate dataset
            result = annotation[""result""]
            for res in result:
                t = res[""type""]
                tag_types.add(t)
                for label in res[""value""][t]:
                    # check if labeling result is a list of labels
                    if isinstance(label, list) and not multi_label:
                        for sublabel in label:
                            classes.add(sublabel)
                            temp = {}
                            temp[""file_upload""] = task.get(""file_upload"")
                            temp[""data""] = task.get(""data"")
                            if temp[""file_upload""]:
                                temp[""file_upload""] = os.path.join(data_folder, temp[""file_upload""])
                            else:
                                for key in temp[""data""]:
                                    p = temp[""data""].get(key)
                                path = Path(p)
                                if path and data_folder:
                                    temp[""file_upload""] = os.path.join(data_folder, path.name)
                            temp[""label""] = sublabel
                            temp[""result""] = res.get(""value"")
                            if annotation[""ground_truth""]:
                                test_results.append(temp)
                            elif not annotation[""ground_truth""]:
                                results.append(temp)
                    else:
                        if isinstance(label, list):
                            for item in label:
                                classes.add(item)
                        else:
                            classes.add(label)
                        temp = {}
                        temp[""file_upload""] = task.get(""file_upload"")
                        temp[""data""] = task.get(""data"")
                        if temp[""file_upload""] and data_folder:
                            temp[""file_upload""] = os.path.join(data_folder, temp[""file_upload""])
                        else:
                            for key in temp[""data""]:
                                p = temp[""data""].get(key)
                            path = Path(p)
                            if path and data_folder:
                                temp[""file_upload""] = os.path.join(data_folder, path.name)
                        temp[""label""] = label
                        temp[""result""] = res.get(""value"")
                        if annotation[""ground_truth""]:
                            test_results.append(temp)
                        elif not annotation[""ground_truth""]:
                            results.append(temp)
    return results, test_results, classes, data_types, tag_types",_67911.py,11,"for key in task.get('data'):
    data_types.add(key)",data_types |= {key for key in task.get('data')},1,nan,nan
https://github.com/PyTorchLightning/lightning-flash/tree/master/flash/core/integrations/labelstudio/input.py,"def _load_json_data(data, data_folder, multi_label=False):
    """"""Utility method to extract data from Label Studio json files.""""""
    results = []
    test_results = []
    data_types = set()
    tag_types = set()
    classes = set()
    for task in data:
        for annotation in task[""annotations""]:
            # extracting data types from tasks
            for key in task.get(""data""):
                data_types.add(key)
            # Adding ground_truth annotation to separate dataset
            result = annotation[""result""]
            for res in result:
                t = res[""type""]
                tag_types.add(t)
                for label in res[""value""][t]:
                    # check if labeling result is a list of labels
                    if isinstance(label, list) and not multi_label:
                        for sublabel in label:
                            classes.add(sublabel)
                            temp = {}
                            temp[""file_upload""] = task.get(""file_upload"")
                            temp[""data""] = task.get(""data"")
                            if temp[""file_upload""]:
                                temp[""file_upload""] = os.path.join(data_folder, temp[""file_upload""])
                            else:
                                for key in temp[""data""]:
                                    p = temp[""data""].get(key)
                                path = Path(p)
                                if path and data_folder:
                                    temp[""file_upload""] = os.path.join(data_folder, path.name)
                            temp[""label""] = sublabel
                            temp[""result""] = res.get(""value"")
                            if annotation[""ground_truth""]:
                                test_results.append(temp)
                            elif not annotation[""ground_truth""]:
                                results.append(temp)
                    else:
                        if isinstance(label, list):
                            for item in label:
                                classes.add(item)
                        else:
                            classes.add(label)
                        temp = {}
                        temp[""file_upload""] = task.get(""file_upload"")
                        temp[""data""] = task.get(""data"")
                        if temp[""file_upload""] and data_folder:
                            temp[""file_upload""] = os.path.join(data_folder, temp[""file_upload""])
                        else:
                            for key in temp[""data""]:
                                p = temp[""data""].get(key)
                            path = Path(p)
                            if path and data_folder:
                                temp[""file_upload""] = os.path.join(data_folder, path.name)
                        temp[""label""] = label
                        temp[""result""] = res.get(""value"")
                        if annotation[""ground_truth""]:
                            test_results.append(temp)
                        elif not annotation[""ground_truth""]:
                            results.append(temp)
    return results, test_results, classes, data_types, tag_types",_67911.py,42,"for item in label:
    classes.add(item)",classes |= {item for item in label},1,nan,nan
https://github.com/petl-developers/petl/tree/master/petl/transform/hashjoins.py,"def iterhashantijoin(left, right, lkey, rkey):
    lit = iter(left)
    rit = iter(right)

    lhdr = next(lit)
    rhdr = next(rit)
    yield tuple(lhdr)

    # determine indices of the key fields in left and right tables
    lkind = asindices(lhdr, lkey)
    rkind = asindices(rhdr, rkey)
    
    # construct functions to extract key values from both tables
    lgetk = operator.itemgetter(*lkind)
    rgetk = operator.itemgetter(*rkind)
    
    rkeys = set()
    for rrow in rit:
        rk = rgetk(rrow)
        rkeys.add(rk)
        
    for lrow in lit:
        lk = lgetk(lrow)
        if lk not in rkeys:
            yield tuple(lrow)",_72164.py,18,"for rrow in rit:
    rk = rgetk(rrow)
    rkeys.add(rk)",rkeys = {rgetk(rrow) for rrow in rit},1,nan,nan
https://github.com/lovit/soynlp/tree/master/soynlp/noun/_noun_postprocessing.py,"def ignore_features(nouns, features, logpath=None, logheader=None):

    if not logheader:
        logheader = '## Ignored noun candidates these are same with features'

    removals = set()

    for word in nouns:
        if word in features:
            removals.add(word)

    if logpath:
        write_log(logpath, logheader, removals)

    nouns_ = _select_true_nouns(nouns, removals)
    return nouns_, removals",_73427.py,8,"for word in nouns:
    if word in features:
        removals.add(word)",removals = {word for word in nouns if word in features},1,nan,nan
https://github.com/localstack/localstack/tree/master/tests/integration/conftest.py,"def pytest_runtestloop(session):
    # second pytest lifecycle hook (before test runner starts)

    # collect test classes
    test_classes = set()
    for item in session.items:
        if item.parent and item.parent.cls:
            test_classes.add(item.parent.cls)

    # add init functions for certain tests that download/install things
    for test_class in test_classes:
        # set flag that terraform will be used
        if TestTerraform is test_class:
            logger.info(""will initialize TestTerraform"")
            test_init_functions.add(TestTerraform.init_async)
            continue
        if ElasticsearchTest is test_class:
            # FIXME: there are other elasticsearch test classes
            logger.info(""will initialize ElasticsearchTest"")
            test_init_functions.add(ElasticsearchTest.init_async)
            continue

    if not session.items:
        return

    if session.config.option.collectonly:
        return

    # trigger localstack startup in startup_monitor and wait until it becomes ready
    startup_monitor_event.set()
    localstack_started.wait()",_74346.py,6,"for item in session.items:
    if item.parent and item.parent.cls:
        test_classes.add(item.parent.cls)",test_classes = {item.parent.cls for item in session.items if item.parent and item.parent.cls},1,nan,nan
https://github.com/guillaumegenthial/sequence_tagging/tree/master/model/data_utils.py,"def get_glove_vocab(filename):
    """"""Load vocab from file

    Args:
        filename: path to the glove vectors

    Returns:
        vocab: set() of strings
    """"""
    print(""Building vocab..."")
    vocab = set()
    with open(filename) as f:
        for line in f:
            word = line.strip().split(' ')[0]
            vocab.add(word)
    print(""- done. {} tokens"".format(len(vocab)))
    return vocab",_75793.py,13,"for line in f:
    word = line.strip().split(' ')[0]
    vocab.add(word)",vocab = {line.strip().split(' ')[0] for line in f},1,nan,nan
https://github.com/SFTtech/openage/tree/master/buildsystem/check_py_file_list.py,"def main():
    """""" CLI entry point """"""
    cli = argparse.ArgumentParser()
    cli.add_argument('py_file_list', help=(
        ""semicolon-separated list of listed .py files""
    ))
    cli.add_argument('py_module_dir', help=(
        ""directory containing the python module to check""
    ))
    cli.add_argument('-v', '--verbose', action='store_true', help=(
        ""produce verbose output""
    ))
    args = cli.parse_args()

    openage_dir = os.path.realpath(args.py_module_dir)

    listed = set()
    with open(args.py_file_list, encoding='utf8') as fileobj:
        for filename in fileobj.read().strip().split(';'):
            filepath = os.path.realpath(os.path.normpath(filename))
            if filepath.startswith(openage_dir):
                listed.add(filepath)
            elif args.verbose:
                print(""Ignoring "" + filepath + "" outside "" + openage_dir)

    if args.verbose:
        print(""Files listed in "" + args.py_file_list + "":"",
              *sorted(listed), sep='\n\t')

    actual = set()
    for dirname, _, files in os.walk(openage_dir):
        dirname = os.path.realpath(os.path.abspath(dirname))
        for filename in files:
            if filename.endswith('.py'):
                actual.add(os.path.join(dirname, filename))

    if args.verbose:
        print(""Files available:"", *sorted(actual), sep='\n\t')

    success = True
    for filename in sorted(actual - listed):
        success = False
        print(""file was not listed via add_py_module: "" +
              os.path.relpath(filename, openage_dir))

    for filename in sorted(listed - actual):
        success = False
        print(""file was listed via add_py_module but does not exist: "" +
              os.path.relpath(filename, openage_dir))

    if success:
        return 0

    return 1",_76555.py,31,"for (dirname, _, files) in os.walk(openage_dir):
    dirname = os.path.realpath(os.path.abspath(dirname))
    for filename in files:
        if filename.endswith('.py'):
            actual.add(os.path.join(dirname, filename))","actual = {os.path.join(os.path.realpath(os.path.abspath(dirname)), filename) for (dirname, _, files) in os.walk(openage_dir) for filename in files if filename.endswith('.py')}",1,nan,nan
https://github.com/facebookresearch/KILT/tree/master/kilt/eval_downstream.py,"def get_gold_answers(gold):
    ground_truths = set()
    for item in gold[""output""]:
        if ""answer"" in item and item[""answer""] and len(item[""answer""].strip()) > 0:
            ground_truths.add(item[""answer""].strip())
    return ground_truths",_81354.py,3,"for item in gold['output']:
    if 'answer' in item and item['answer'] and (len(item['answer'].strip()) > 0):
        ground_truths.add(item['answer'].strip())",ground_truths = {item['answer'].strip() for item in gold['output'] if 'answer' in item and item['answer'] and (len(item['answer'].strip()) > 0)},1,nan,nan
https://github.com/SymbiFlow/prjxray/tree/master/experiments/pipsroute/maketodo.py,"def maketodo(pipfile, dbfile):
    todos = set()
    with open(pipfile, ""r"") as f:
        for line in f:
            todos.add(line.split()[0])
    with open(dbfile, ""r"") as f:
        for line in f:
            todos.remove(line.split()[0])
    for line in todos:
        if line.endswith("".VCC_WIRE""):
            continue
        if line.endswith("".GND_WIRE""):
            continue
        if re.match(r"".*\.(L[HV]B?|G?CLK)(_L)?(_B)?[0-9]"", line):
            continue
        if re.match(r""^INT_[LR]\.(CTRL|GFAN)(_L)?[0-9]"", line):
            continue
        print(line)",_82072.py,4,"for line in f:
    todos.add(line.split()[0])",todos = {line.split()[0] for line in f},1,nan,nan
https://github.com/hydrusnetwork/hydrus/tree/master/hydrus/client/networking/ClientNetworkingDomain.py,"def STATICLinkURLClassesAndParsers( url_classes, parsers, existing_url_class_keys_to_parser_keys ):
        
        url_classes = list( url_classes )
        
        NetworkDomainManager.STATICSortURLClassesDescendingComplexity( url_classes )
        
        parsers = list( parsers )
        
        parsers.sort( key = lambda p: p.GetName() )
        
        new_url_class_keys_to_parser_keys = {}
        
        api_pairs = ConvertURLClassesIntoAPIPairs( url_classes )
        
        # anything that goes to an api url will be parsed by that api's parser--it can't have its own
        api_pair_unparsable_url_classes = set()
        
        for ( a, b ) in api_pairs:
            
            api_pair_unparsable_url_classes.add( a )
            
        
        #
        
        # I have to do this backwards, going through parsers and then url_classes, so I can do a proper url match lookup like the real domain manager does it
        # otherwise, if we iterate through url matches looking for parsers to match them, we have gallery url matches thinking they match parser post urls
        # e.g.
        # The page parser might say it supports https://danbooru.donmai.us/posts/3198277
        # But the gallery url class might think it recognises that as https://danbooru.donmai.us/posts
        # 
        # So we have to do the normal lookup in the proper descending complexity order, not searching any further than the first, correct match
        
        for parser in parsers:
            
            example_urls = parser.GetExampleURLs()
            
            for example_url in example_urls:
                
                for url_class in url_classes:
                    
                    if url_class in api_pair_unparsable_url_classes:
                        
                        continue
                        
                    
                    if url_class.Matches( example_url ):
                        
                        # we have a match. this is the 'correct' match for this example url, and we should not search any more, so we break below
                        
                        url_class_key = url_class.GetClassKey()
                        
                        parsable = url_class.IsParsable()
                        linkable = url_class_key not in existing_url_class_keys_to_parser_keys and url_class_key not in new_url_class_keys_to_parser_keys
                        
                        if parsable and linkable:
                            
                            new_url_class_keys_to_parser_keys[ url_class_key ] = parser.GetParserKey()
                            
                        
                        break
                        
                    
                
            
        '''
        #
        
        for url_class in url_classes:
            
            if not url_class.IsParsable() or url_class in api_pair_unparsable_url_classes:
                
                continue
                
            
            url_class_key = url_class.GetClassKey()
            
            if url_class_key in existing_url_class_keys_to_parser_keys:
                
                continue
                
            
            for parser in parsers:
                
                example_urls = parser.GetExampleURLs()
                
                if True in ( url_class.Matches( example_url ) for example_url in example_urls ):
                    
                    new_url_class_keys_to_parser_keys[ url_class_key ] = parser.GetParserKey()
                    
                    break
                    
                
            
        '''
        return new_url_class_keys_to_parser_keys",_87551.py,18,"for (a, b) in api_pairs:
    api_pair_unparsable_url_classes.add(a)","api_pair_unparsable_url_classes = {a for (a, b) in api_pairs}",1,nan,nan
https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/parse.py,"def get_docx_variables(the_path):
    import docassemble.base.legal
    names = set()
    if not os.path.isfile(the_path):
        raise DAError(""Missing docx template file "" + os.path.basename(the_path))
    try:
        docx_template = docassemble.base.file_docx.DocxTemplate(the_path)
        the_env = custom_jinja_env()
        the_xml = docx_template.get_xml()
        the_xml = re.sub(r'<w:p>', '\n<w:p>', the_xml)
        the_xml = re.sub(r'({[\%\{].*?[\%\}]})', fix_quotes, the_xml)
        the_xml = docx_template.patch_xml(the_xml)
        parsed_content = the_env.parse(the_xml)
    except Exception as the_err:
        raise DAError(""There was an error parsing the docx file: "" + the_err.__class__.__name__ + "" "" + str(the_err))
    for key in jinja2meta.find_undeclared_variables(parsed_content):
        if not key.startswith('_'):
            names.add(key)
    for name in docassemble.base.legal.__all__:
        if name in names:
            names.remove(name)
    return sorted(list(names))",_90397.py,16,"for key in jinja2meta.find_undeclared_variables(parsed_content):
    if not key.startswith('_'):
        names.add(key)",names = {key for key in jinja2meta.find_undeclared_variables(parsed_content) if not key.startswith('_')},1,nan,nan
https://github.com/rsennrich/subword-nmt/tree/master//learn_bpe.py,"def learn_bpe(infile, outfile, num_symbols, min_frequency=2, verbose=False, is_dict=False, total_symbols=False, num_workers=1):
    """"""Learn num_symbols BPE operations from vocabulary, and write to outfile.
    """"""

    # version 0.2 changes the handling of the end-of-word token ('</w>');
    # version numbering allows bckward compatibility
    outfile.write('#version: 0.2\n')

    vocab = get_vocabulary(infile, is_dict, num_workers)
    vocab = dict([(tuple(x[:-1])+(x[-1]+'</w>',) ,y) for (x,y) in vocab.items()])
    sorted_vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True)

    stats, indices = get_pair_statistics(sorted_vocab)
    big_stats = copy.deepcopy(stats)

    if total_symbols:
        uniq_char_internal = set()
        uniq_char_final = set()
        for word in vocab:
            for char in word[:-1]:
                uniq_char_internal.add(char)
            uniq_char_final.add(word[-1])
        sys.stderr.write('Number of word-internal characters: {0}\n'.format(len(uniq_char_internal)))
        sys.stderr.write('Number of word-final characters: {0}\n'.format(len(uniq_char_final)))
        sys.stderr.write('Reducing number of merge operations by {0}\n'.format(len(uniq_char_internal) + len(uniq_char_final)))
        num_symbols -= len(uniq_char_internal) + len(uniq_char_final)

    # threshold is inspired by Zipfian assumption, but should only affect speed
    threshold = max(stats.values()) / 10
    for i in range(num_symbols):
        if stats:
            most_frequent = max(stats, key=lambda x: (stats[x], x))

        # we probably missed the best pair because of pruning; go back to full statistics
        if not stats or (i and stats[most_frequent] < threshold):
            prune_stats(stats, big_stats, threshold)
            stats = copy.deepcopy(big_stats)
            most_frequent = max(stats, key=lambda x: (stats[x], x))
            # threshold is inspired by Zipfian assumption, but should only affect speed
            threshold = stats[most_frequent] * i/(i+10000.0)
            prune_stats(stats, big_stats, threshold)

        if stats[most_frequent] < min_frequency:
            sys.stderr.write('no pair has frequency >= {0}. Stopping\n'.format(min_frequency))
            break

        if verbose:
            sys.stderr.write('pair {0}: {1} {2} -> {1}{2} (frequency {3})\n'.format(i, most_frequent[0], most_frequent[1], stats[most_frequent]))
        outfile.write('{0} {1}\n'.format(*most_frequent))
        changes = replace_pair(most_frequent, sorted_vocab, indices)
        update_pair_statistics(most_frequent, changes, stats, indices)
        stats[most_frequent] = 0
        if not i % 100:
            prune_stats(stats, big_stats, threshold)",_91710.py,20,"for char in word[:-1]:
    uniq_char_internal.add(char)",uniq_char_internal |= {char for char in word[:-1]},1,nan,nan
https://github.com/ansible/ansible-modules-core/tree/master/cloud/docker/_docker.py,"def get_differing_containers(self):
        """"""
        Inspect all matching, running containers, and return those that were
        started with parameters that differ from the ones that are provided
        during this module run. A list containing the differing
        containers will be returned, and a short string describing the specific
        difference encountered in each container will be appended to
        reload_reasons.

        This generates the set of containers that need to be stopped and
        started with new parameters with state=reloaded.
        """"""

        running = self.get_running_containers()
        current = self.get_inspect_containers(running)
        defaults = self.client.info()

        #Get API version
        api_version = self.client.version()['ApiVersion']

        image = self.get_inspect_image()
        if image is None:
            # The image isn't present. Assume that we're about to pull a new
            # tag and *everything* will be restarted.
            #
            # This will give false positives if you untag an image on the host
            # and there's nothing more to pull.
            return current

        differing = []

        for container in current:

            # IMAGE
            # Compare the image by ID rather than name, so that containers
            # will be restarted when new versions of an existing image are
            # pulled.
            if container['Image'] != image['Id']:
                self.reload_reasons.append('image ({0} => {1})'.format(container['Image'], image['Id']))
                differing.append(container)
                continue

            # ENTRYPOINT

            expected_entrypoint = self.module.params.get('entrypoint')
            if expected_entrypoint:
                expected_entrypoint = shlex.split(expected_entrypoint)
                actual_entrypoint = container[""Config""][""Entrypoint""]

                if actual_entrypoint != expected_entrypoint:
                    self.reload_reasons.append(
                        'entrypoint ({0} => {1})'
                        .format(actual_entrypoint, expected_entrypoint)
                    )
                    differing.append(container)
                    continue

            # COMMAND

            expected_command = self.module.params.get('command')
            if expected_command:
                expected_command = shlex.split(expected_command)
                actual_command = container[""Config""][""Cmd""]

                if actual_command != expected_command:
                    self.reload_reasons.append('command ({0} => {1})'.format(actual_command, expected_command))
                    differing.append(container)
                    continue

            # EXPOSED PORTS
            expected_exposed_ports = set((image['ContainerConfig'].get('ExposedPorts') or {}).keys())
            for p in (self.exposed_ports or []):
                expected_exposed_ports.add(""/"".join(p))

            actually_exposed_ports = set((container[""Config""].get(""ExposedPorts"") or {}).keys())

            if actually_exposed_ports != expected_exposed_ports:
                self.reload_reasons.append('exposed_ports ({0} => {1})'.format(actually_exposed_ports, expected_exposed_ports))
                differing.append(container)
                continue

            # VOLUMES

            expected_volume_keys = set((image['ContainerConfig']['Volumes'] or {}).keys())
            if self.volumes:
                expected_volume_keys.update(self.volumes)

            actual_volume_keys = set((container['Config']['Volumes'] or {}).keys())

            if actual_volume_keys != expected_volume_keys:
                self.reload_reasons.append('volumes ({0} => {1})'.format(actual_volume_keys, expected_volume_keys))
                differing.append(container)
                continue

            # ULIMITS

            expected_ulimit_keys = set(map(lambda x: '%s:%s:%s' % (x['name'],x['soft'],x['hard']), self.ulimits or []))
            actual_ulimit_keys = set(map(lambda x: '%s:%s:%s' % (x['Name'],x['Soft'],x['Hard']), (container['HostConfig']['Ulimits'] or [])))

            if actual_ulimit_keys != expected_ulimit_keys:
                self.reload_reasons.append('ulimits ({0} => {1})'.format(actual_ulimit_keys, expected_ulimit_keys))
                differing.append(container)
                continue

            # CPU_SHARES

            expected_cpu_shares = self.module.params.get('cpu_shares')
            actual_cpu_shares = container['HostConfig']['CpuShares']

            if expected_cpu_shares and actual_cpu_shares != expected_cpu_shares:
                self.reload_reasons.append('cpu_shares ({0} => {1})'.format(actual_cpu_shares, expected_cpu_shares))
                differing.append(container)
                continue

            # MEM_LIMIT

            try:
                expected_mem = _human_to_bytes(self.module.params.get('memory_limit'))
            except ValueError as e:
                self.module.fail_json(msg=str(e))

            #For v1.19 API and above use HostConfig, otherwise use Config
            if docker.utils.compare_version('1.19', api_version) >= 0:
                actual_mem = container['HostConfig']['Memory']
            else:
                actual_mem = container['Config']['Memory']

            if expected_mem and actual_mem != expected_mem:
                self.reload_reasons.append('memory ({0} => {1})'.format(actual_mem, expected_mem))
                differing.append(container)
                continue

            # ENVIRONMENT
            # actual_env is likely to include environment variables injected by
            # the Dockerfile.

            expected_env = {}

            for image_env in image['ContainerConfig']['Env'] or []:
                name, value = image_env.split('=', 1)
                expected_env[name] = value

            if self.environment:
                for name, value in self.environment.iteritems():
                    expected_env[name] = str(value)

            actual_env = {}
            for container_env in container['Config']['Env'] or []:
                name, value = container_env.split('=', 1)
                actual_env[name] = value

            if actual_env != expected_env:
                # Don't include the environment difference in the output.
                self.reload_reasons.append('environment {0} => {1}'.format(actual_env, expected_env))
                differing.append(container)
                continue

            # LABELS

            expected_labels = {}
            for name, value in self.module.params.get('labels').iteritems():
                expected_labels[name] = str(value)

            if isinstance(container['Config']['Labels'], dict):
                actual_labels = container['Config']['Labels']
            else:
                for container_label in container['Config']['Labels'] or []:
                    name, value = container_label.split('=', 1)
                    actual_labels[name] = value

            if actual_labels != expected_labels:
                self.reload_reasons.append('labels {0} => {1}'.format(actual_labels, expected_labels))
                differing.append(container)
                continue

            # HOSTNAME

            expected_hostname = self.module.params.get('hostname')
            actual_hostname = container['Config']['Hostname']
            if expected_hostname and actual_hostname != expected_hostname:
                self.reload_reasons.append('hostname ({0} => {1})'.format(actual_hostname, expected_hostname))
                differing.append(container)
                continue

            # DOMAINNAME

            expected_domainname = self.module.params.get('domainname')
            actual_domainname = container['Config']['Domainname']
            if expected_domainname and actual_domainname != expected_domainname:
                self.reload_reasons.append('domainname ({0} => {1})'.format(actual_domainname, expected_domainname))
                differing.append(container)
                continue

            # DETACH

            # We don't have to check for undetached containers. If it wasn't
            # detached, it would have stopped before the playbook continued!

            # NAME

            # We also don't have to check name, because this is one of the
            # criteria that's used to determine which container(s) match in
            # the first place.

            # STDIN_OPEN

            expected_stdin_open = self.module.params.get('stdin_open')
            actual_stdin_open = container['Config']['OpenStdin']
            if actual_stdin_open != expected_stdin_open:
                self.reload_reasons.append('stdin_open ({0} => {1})'.format(actual_stdin_open, expected_stdin_open))
                differing.append(container)
                continue

            # TTY

            expected_tty = self.module.params.get('tty')
            actual_tty = container['Config']['Tty']
            if actual_tty != expected_tty:
                self.reload_reasons.append('tty ({0} => {1})'.format(actual_tty, expected_tty))
                differing.append(container)
                continue

            # -- ""start"" call differences --

            # LXC_CONF

            if self.lxc_conf:
                expected_lxc = set(self.lxc_conf)
                actual_lxc = set(container['HostConfig']['LxcConf'] or [])
                if actual_lxc != expected_lxc:
                    self.reload_reasons.append('lxc_conf ({0} => {1})'.format(actual_lxc, expected_lxc))
                    differing.append(container)
                    continue

            # BINDS

            expected_binds = set()
            if self.binds:
                for bind in self.binds:
                    expected_binds.add(bind)

            actual_binds = set()
            for bind in (container['HostConfig']['Binds'] or []):
                if len(bind.split(':')) == 2:
                    actual_binds.add(bind + "":rw"")
                else:
                    actual_binds.add(bind)

            if actual_binds != expected_binds:
                self.reload_reasons.append('binds ({0} => {1})'.format(actual_binds, expected_binds))
                differing.append(container)
                continue

            # PORT BINDINGS

            expected_bound_ports = {}
            if self.port_bindings:
                for container_port, config in self.port_bindings.iteritems():
                    if isinstance(container_port, int):
                        container_port = ""{0}/tcp"".format(container_port)
                    if len(config) == 1:
                        expected_bound_ports[container_port] = [{'HostIp': ""0.0.0.0"", 'HostPort': """"}]
                    elif isinstance(config[0], tuple):
                        expected_bound_ports[container_port] = []
                        for hostip, hostport in config:
                            expected_bound_ports[container_port].append({ 'HostIp': hostip, 'HostPort': str(hostport)})
                    else:
                        expected_bound_ports[container_port] = [{'HostIp': config[0], 'HostPort': str(config[1])}]

            actual_bound_ports = container['HostConfig']['PortBindings'] or {}

            if actual_bound_ports != expected_bound_ports:
                self.reload_reasons.append('port bindings ({0} => {1})'.format(actual_bound_ports, expected_bound_ports))
                differing.append(container)
                continue

            # PUBLISHING ALL PORTS

            # What we really care about is the set of ports that is actually
            # published. That should be caught above.

            # PRIVILEGED

            expected_privileged = self.module.params.get('privileged')
            actual_privileged = container['HostConfig']['Privileged']
            if actual_privileged != expected_privileged:
                self.reload_reasons.append('privileged ({0} => {1})'.format(actual_privileged, expected_privileged))
                differing.append(container)
                continue

            # LINKS

            expected_links = set()
            for link, alias in (self.links or {}).iteritems():
                expected_links.add(""/{0}:{1}/{2}"".format(link, container[""Name""], alias))

            actual_links = set(container['HostConfig']['Links'] or [])
            if actual_links != expected_links:
                self.reload_reasons.append('links ({0} => {1})'.format(actual_links, expected_links))
                differing.append(container)
                continue

            # NETWORK MODE

            expected_netmode = self.module.params.get('net') or 'bridge'
            actual_netmode = container['HostConfig']['NetworkMode'] or 'bridge'
            if actual_netmode != expected_netmode:
                self.reload_reasons.append('net ({0} => {1})'.format(actual_netmode, expected_netmode))
                differing.append(container)
                continue

            # DEVICES

            expected_devices = set()
            for device in (self.module.params.get('devices') or []):
                if len(device.split(':')) == 2:
                    expected_devices.add(device + "":rwm"")
                else:
                    expected_devices.add(device)

            actual_devices = set()
            for device in (container['HostConfig']['Devices'] or []):
                actual_devices.add(""{PathOnHost}:{PathInContainer}:{CgroupPermissions}"".format(**device))

            if actual_devices != expected_devices:
                self.reload_reasons.append('devices ({0} => {1})'.format(actual_devices, expected_devices))
                differing.append(container)
                continue

            # DNS

            expected_dns = set(self.module.params.get('dns') or [])
            actual_dns = set(container['HostConfig']['Dns'] or [])
            if actual_dns != expected_dns:
                self.reload_reasons.append('dns ({0} => {1})'.format(actual_dns, expected_dns))
                differing.append(container)
                continue

            # VOLUMES_FROM

            expected_volumes_from = set(self.module.params.get('volumes_from') or [])
            actual_volumes_from = set(container['HostConfig']['VolumesFrom'] or [])
            if actual_volumes_from != expected_volumes_from:
                self.reload_reasons.append('volumes_from ({0} => {1})'.format(actual_volumes_from, expected_volumes_from))
                differing.append(container)

            # LOG_DRIVER

            if self.ensure_capability('log_driver', False):
                expected_log_driver = self.module.params.get('log_driver') or defaults['LoggingDriver']
                actual_log_driver = container['HostConfig']['LogConfig']['Type']
                if actual_log_driver != expected_log_driver:
                    self.reload_reasons.append('log_driver ({0} => {1})'.format(actual_log_driver, expected_log_driver))
                    differing.append(container)
                    continue

            if self.ensure_capability('log_opt', False):
                expected_logging_opts = self.module.params.get('log_opt') or {}
                actual_log_opts = container['HostConfig']['LogConfig']['Config']
                if len(set(expected_logging_opts.items()) - set(actual_log_opts.items())) != 0:
                    log_opt_reasons = {
                        'added': dict(set(expected_logging_opts.items()) - set(actual_log_opts.items())),
                        'removed': dict(set(actual_log_opts.items()) - set(expected_logging_opts.items()))
                    }
                    self.reload_reasons.append('log_opt ({0})'.format(log_opt_reasons))
                    differing.append(container)

        return differing",_93270.py,243,"for bind in container['HostConfig']['Binds'] or []:
    if len(bind.split(':')) == 2:
        actual_binds.add(bind + ':rw')
    else:
        actual_binds.add(bind)",actual_binds = {bind + ':rw' if len(bind.split(':')) == 2 else bind for bind in container['HostConfig']['Binds'] or []},1,nan,nan
https://github.com/ansible/ansible-modules-core/tree/master/cloud/docker/_docker.py,"def get_differing_containers(self):
        """"""
        Inspect all matching, running containers, and return those that were
        started with parameters that differ from the ones that are provided
        during this module run. A list containing the differing
        containers will be returned, and a short string describing the specific
        difference encountered in each container will be appended to
        reload_reasons.

        This generates the set of containers that need to be stopped and
        started with new parameters with state=reloaded.
        """"""

        running = self.get_running_containers()
        current = self.get_inspect_containers(running)
        defaults = self.client.info()

        #Get API version
        api_version = self.client.version()['ApiVersion']

        image = self.get_inspect_image()
        if image is None:
            # The image isn't present. Assume that we're about to pull a new
            # tag and *everything* will be restarted.
            #
            # This will give false positives if you untag an image on the host
            # and there's nothing more to pull.
            return current

        differing = []

        for container in current:

            # IMAGE
            # Compare the image by ID rather than name, so that containers
            # will be restarted when new versions of an existing image are
            # pulled.
            if container['Image'] != image['Id']:
                self.reload_reasons.append('image ({0} => {1})'.format(container['Image'], image['Id']))
                differing.append(container)
                continue

            # ENTRYPOINT

            expected_entrypoint = self.module.params.get('entrypoint')
            if expected_entrypoint:
                expected_entrypoint = shlex.split(expected_entrypoint)
                actual_entrypoint = container[""Config""][""Entrypoint""]

                if actual_entrypoint != expected_entrypoint:
                    self.reload_reasons.append(
                        'entrypoint ({0} => {1})'
                        .format(actual_entrypoint, expected_entrypoint)
                    )
                    differing.append(container)
                    continue

            # COMMAND

            expected_command = self.module.params.get('command')
            if expected_command:
                expected_command = shlex.split(expected_command)
                actual_command = container[""Config""][""Cmd""]

                if actual_command != expected_command:
                    self.reload_reasons.append('command ({0} => {1})'.format(actual_command, expected_command))
                    differing.append(container)
                    continue

            # EXPOSED PORTS
            expected_exposed_ports = set((image['ContainerConfig'].get('ExposedPorts') or {}).keys())
            for p in (self.exposed_ports or []):
                expected_exposed_ports.add(""/"".join(p))

            actually_exposed_ports = set((container[""Config""].get(""ExposedPorts"") or {}).keys())

            if actually_exposed_ports != expected_exposed_ports:
                self.reload_reasons.append('exposed_ports ({0} => {1})'.format(actually_exposed_ports, expected_exposed_ports))
                differing.append(container)
                continue

            # VOLUMES

            expected_volume_keys = set((image['ContainerConfig']['Volumes'] or {}).keys())
            if self.volumes:
                expected_volume_keys.update(self.volumes)

            actual_volume_keys = set((container['Config']['Volumes'] or {}).keys())

            if actual_volume_keys != expected_volume_keys:
                self.reload_reasons.append('volumes ({0} => {1})'.format(actual_volume_keys, expected_volume_keys))
                differing.append(container)
                continue

            # ULIMITS

            expected_ulimit_keys = set(map(lambda x: '%s:%s:%s' % (x['name'],x['soft'],x['hard']), self.ulimits or []))
            actual_ulimit_keys = set(map(lambda x: '%s:%s:%s' % (x['Name'],x['Soft'],x['Hard']), (container['HostConfig']['Ulimits'] or [])))

            if actual_ulimit_keys != expected_ulimit_keys:
                self.reload_reasons.append('ulimits ({0} => {1})'.format(actual_ulimit_keys, expected_ulimit_keys))
                differing.append(container)
                continue

            # CPU_SHARES

            expected_cpu_shares = self.module.params.get('cpu_shares')
            actual_cpu_shares = container['HostConfig']['CpuShares']

            if expected_cpu_shares and actual_cpu_shares != expected_cpu_shares:
                self.reload_reasons.append('cpu_shares ({0} => {1})'.format(actual_cpu_shares, expected_cpu_shares))
                differing.append(container)
                continue

            # MEM_LIMIT

            try:
                expected_mem = _human_to_bytes(self.module.params.get('memory_limit'))
            except ValueError as e:
                self.module.fail_json(msg=str(e))

            #For v1.19 API and above use HostConfig, otherwise use Config
            if docker.utils.compare_version('1.19', api_version) >= 0:
                actual_mem = container['HostConfig']['Memory']
            else:
                actual_mem = container['Config']['Memory']

            if expected_mem and actual_mem != expected_mem:
                self.reload_reasons.append('memory ({0} => {1})'.format(actual_mem, expected_mem))
                differing.append(container)
                continue

            # ENVIRONMENT
            # actual_env is likely to include environment variables injected by
            # the Dockerfile.

            expected_env = {}

            for image_env in image['ContainerConfig']['Env'] or []:
                name, value = image_env.split('=', 1)
                expected_env[name] = value

            if self.environment:
                for name, value in self.environment.iteritems():
                    expected_env[name] = str(value)

            actual_env = {}
            for container_env in container['Config']['Env'] or []:
                name, value = container_env.split('=', 1)
                actual_env[name] = value

            if actual_env != expected_env:
                # Don't include the environment difference in the output.
                self.reload_reasons.append('environment {0} => {1}'.format(actual_env, expected_env))
                differing.append(container)
                continue

            # LABELS

            expected_labels = {}
            for name, value in self.module.params.get('labels').iteritems():
                expected_labels[name] = str(value)

            if isinstance(container['Config']['Labels'], dict):
                actual_labels = container['Config']['Labels']
            else:
                for container_label in container['Config']['Labels'] or []:
                    name, value = container_label.split('=', 1)
                    actual_labels[name] = value

            if actual_labels != expected_labels:
                self.reload_reasons.append('labels {0} => {1}'.format(actual_labels, expected_labels))
                differing.append(container)
                continue

            # HOSTNAME

            expected_hostname = self.module.params.get('hostname')
            actual_hostname = container['Config']['Hostname']
            if expected_hostname and actual_hostname != expected_hostname:
                self.reload_reasons.append('hostname ({0} => {1})'.format(actual_hostname, expected_hostname))
                differing.append(container)
                continue

            # DOMAINNAME

            expected_domainname = self.module.params.get('domainname')
            actual_domainname = container['Config']['Domainname']
            if expected_domainname and actual_domainname != expected_domainname:
                self.reload_reasons.append('domainname ({0} => {1})'.format(actual_domainname, expected_domainname))
                differing.append(container)
                continue

            # DETACH

            # We don't have to check for undetached containers. If it wasn't
            # detached, it would have stopped before the playbook continued!

            # NAME

            # We also don't have to check name, because this is one of the
            # criteria that's used to determine which container(s) match in
            # the first place.

            # STDIN_OPEN

            expected_stdin_open = self.module.params.get('stdin_open')
            actual_stdin_open = container['Config']['OpenStdin']
            if actual_stdin_open != expected_stdin_open:
                self.reload_reasons.append('stdin_open ({0} => {1})'.format(actual_stdin_open, expected_stdin_open))
                differing.append(container)
                continue

            # TTY

            expected_tty = self.module.params.get('tty')
            actual_tty = container['Config']['Tty']
            if actual_tty != expected_tty:
                self.reload_reasons.append('tty ({0} => {1})'.format(actual_tty, expected_tty))
                differing.append(container)
                continue

            # -- ""start"" call differences --

            # LXC_CONF

            if self.lxc_conf:
                expected_lxc = set(self.lxc_conf)
                actual_lxc = set(container['HostConfig']['LxcConf'] or [])
                if actual_lxc != expected_lxc:
                    self.reload_reasons.append('lxc_conf ({0} => {1})'.format(actual_lxc, expected_lxc))
                    differing.append(container)
                    continue

            # BINDS

            expected_binds = set()
            if self.binds:
                for bind in self.binds:
                    expected_binds.add(bind)

            actual_binds = set()
            for bind in (container['HostConfig']['Binds'] or []):
                if len(bind.split(':')) == 2:
                    actual_binds.add(bind + "":rw"")
                else:
                    actual_binds.add(bind)

            if actual_binds != expected_binds:
                self.reload_reasons.append('binds ({0} => {1})'.format(actual_binds, expected_binds))
                differing.append(container)
                continue

            # PORT BINDINGS

            expected_bound_ports = {}
            if self.port_bindings:
                for container_port, config in self.port_bindings.iteritems():
                    if isinstance(container_port, int):
                        container_port = ""{0}/tcp"".format(container_port)
                    if len(config) == 1:
                        expected_bound_ports[container_port] = [{'HostIp': ""0.0.0.0"", 'HostPort': """"}]
                    elif isinstance(config[0], tuple):
                        expected_bound_ports[container_port] = []
                        for hostip, hostport in config:
                            expected_bound_ports[container_port].append({ 'HostIp': hostip, 'HostPort': str(hostport)})
                    else:
                        expected_bound_ports[container_port] = [{'HostIp': config[0], 'HostPort': str(config[1])}]

            actual_bound_ports = container['HostConfig']['PortBindings'] or {}

            if actual_bound_ports != expected_bound_ports:
                self.reload_reasons.append('port bindings ({0} => {1})'.format(actual_bound_ports, expected_bound_ports))
                differing.append(container)
                continue

            # PUBLISHING ALL PORTS

            # What we really care about is the set of ports that is actually
            # published. That should be caught above.

            # PRIVILEGED

            expected_privileged = self.module.params.get('privileged')
            actual_privileged = container['HostConfig']['Privileged']
            if actual_privileged != expected_privileged:
                self.reload_reasons.append('privileged ({0} => {1})'.format(actual_privileged, expected_privileged))
                differing.append(container)
                continue

            # LINKS

            expected_links = set()
            for link, alias in (self.links or {}).iteritems():
                expected_links.add(""/{0}:{1}/{2}"".format(link, container[""Name""], alias))

            actual_links = set(container['HostConfig']['Links'] or [])
            if actual_links != expected_links:
                self.reload_reasons.append('links ({0} => {1})'.format(actual_links, expected_links))
                differing.append(container)
                continue

            # NETWORK MODE

            expected_netmode = self.module.params.get('net') or 'bridge'
            actual_netmode = container['HostConfig']['NetworkMode'] or 'bridge'
            if actual_netmode != expected_netmode:
                self.reload_reasons.append('net ({0} => {1})'.format(actual_netmode, expected_netmode))
                differing.append(container)
                continue

            # DEVICES

            expected_devices = set()
            for device in (self.module.params.get('devices') or []):
                if len(device.split(':')) == 2:
                    expected_devices.add(device + "":rwm"")
                else:
                    expected_devices.add(device)

            actual_devices = set()
            for device in (container['HostConfig']['Devices'] or []):
                actual_devices.add(""{PathOnHost}:{PathInContainer}:{CgroupPermissions}"".format(**device))

            if actual_devices != expected_devices:
                self.reload_reasons.append('devices ({0} => {1})'.format(actual_devices, expected_devices))
                differing.append(container)
                continue

            # DNS

            expected_dns = set(self.module.params.get('dns') or [])
            actual_dns = set(container['HostConfig']['Dns'] or [])
            if actual_dns != expected_dns:
                self.reload_reasons.append('dns ({0} => {1})'.format(actual_dns, expected_dns))
                differing.append(container)
                continue

            # VOLUMES_FROM

            expected_volumes_from = set(self.module.params.get('volumes_from') or [])
            actual_volumes_from = set(container['HostConfig']['VolumesFrom'] or [])
            if actual_volumes_from != expected_volumes_from:
                self.reload_reasons.append('volumes_from ({0} => {1})'.format(actual_volumes_from, expected_volumes_from))
                differing.append(container)

            # LOG_DRIVER

            if self.ensure_capability('log_driver', False):
                expected_log_driver = self.module.params.get('log_driver') or defaults['LoggingDriver']
                actual_log_driver = container['HostConfig']['LogConfig']['Type']
                if actual_log_driver != expected_log_driver:
                    self.reload_reasons.append('log_driver ({0} => {1})'.format(actual_log_driver, expected_log_driver))
                    differing.append(container)
                    continue

            if self.ensure_capability('log_opt', False):
                expected_logging_opts = self.module.params.get('log_opt') or {}
                actual_log_opts = container['HostConfig']['LogConfig']['Config']
                if len(set(expected_logging_opts.items()) - set(actual_log_opts.items())) != 0:
                    log_opt_reasons = {
                        'added': dict(set(expected_logging_opts.items()) - set(actual_log_opts.items())),
                        'removed': dict(set(actual_log_opts.items()) - set(expected_logging_opts.items()))
                    }
                    self.reload_reasons.append('log_opt ({0})'.format(log_opt_reasons))
                    differing.append(container)

        return differing",_93270.py,294,"for (link, alias) in (self.links or {}).iteritems():
    expected_links.add('/{0}:{1}/{2}'.format(link, container['Name'], alias))","expected_links = {'/{0}:{1}/{2}'.format(link, container['Name'], alias) for (link, alias) in (self.links or {}).iteritems()}",1,nan,nan
https://github.com/ansible/ansible-modules-core/tree/master/cloud/docker/_docker.py,"def get_differing_containers(self):
        """"""
        Inspect all matching, running containers, and return those that were
        started with parameters that differ from the ones that are provided
        during this module run. A list containing the differing
        containers will be returned, and a short string describing the specific
        difference encountered in each container will be appended to
        reload_reasons.

        This generates the set of containers that need to be stopped and
        started with new parameters with state=reloaded.
        """"""

        running = self.get_running_containers()
        current = self.get_inspect_containers(running)
        defaults = self.client.info()

        #Get API version
        api_version = self.client.version()['ApiVersion']

        image = self.get_inspect_image()
        if image is None:
            # The image isn't present. Assume that we're about to pull a new
            # tag and *everything* will be restarted.
            #
            # This will give false positives if you untag an image on the host
            # and there's nothing more to pull.
            return current

        differing = []

        for container in current:

            # IMAGE
            # Compare the image by ID rather than name, so that containers
            # will be restarted when new versions of an existing image are
            # pulled.
            if container['Image'] != image['Id']:
                self.reload_reasons.append('image ({0} => {1})'.format(container['Image'], image['Id']))
                differing.append(container)
                continue

            # ENTRYPOINT

            expected_entrypoint = self.module.params.get('entrypoint')
            if expected_entrypoint:
                expected_entrypoint = shlex.split(expected_entrypoint)
                actual_entrypoint = container[""Config""][""Entrypoint""]

                if actual_entrypoint != expected_entrypoint:
                    self.reload_reasons.append(
                        'entrypoint ({0} => {1})'
                        .format(actual_entrypoint, expected_entrypoint)
                    )
                    differing.append(container)
                    continue

            # COMMAND

            expected_command = self.module.params.get('command')
            if expected_command:
                expected_command = shlex.split(expected_command)
                actual_command = container[""Config""][""Cmd""]

                if actual_command != expected_command:
                    self.reload_reasons.append('command ({0} => {1})'.format(actual_command, expected_command))
                    differing.append(container)
                    continue

            # EXPOSED PORTS
            expected_exposed_ports = set((image['ContainerConfig'].get('ExposedPorts') or {}).keys())
            for p in (self.exposed_ports or []):
                expected_exposed_ports.add(""/"".join(p))

            actually_exposed_ports = set((container[""Config""].get(""ExposedPorts"") or {}).keys())

            if actually_exposed_ports != expected_exposed_ports:
                self.reload_reasons.append('exposed_ports ({0} => {1})'.format(actually_exposed_ports, expected_exposed_ports))
                differing.append(container)
                continue

            # VOLUMES

            expected_volume_keys = set((image['ContainerConfig']['Volumes'] or {}).keys())
            if self.volumes:
                expected_volume_keys.update(self.volumes)

            actual_volume_keys = set((container['Config']['Volumes'] or {}).keys())

            if actual_volume_keys != expected_volume_keys:
                self.reload_reasons.append('volumes ({0} => {1})'.format(actual_volume_keys, expected_volume_keys))
                differing.append(container)
                continue

            # ULIMITS

            expected_ulimit_keys = set(map(lambda x: '%s:%s:%s' % (x['name'],x['soft'],x['hard']), self.ulimits or []))
            actual_ulimit_keys = set(map(lambda x: '%s:%s:%s' % (x['Name'],x['Soft'],x['Hard']), (container['HostConfig']['Ulimits'] or [])))

            if actual_ulimit_keys != expected_ulimit_keys:
                self.reload_reasons.append('ulimits ({0} => {1})'.format(actual_ulimit_keys, expected_ulimit_keys))
                differing.append(container)
                continue

            # CPU_SHARES

            expected_cpu_shares = self.module.params.get('cpu_shares')
            actual_cpu_shares = container['HostConfig']['CpuShares']

            if expected_cpu_shares and actual_cpu_shares != expected_cpu_shares:
                self.reload_reasons.append('cpu_shares ({0} => {1})'.format(actual_cpu_shares, expected_cpu_shares))
                differing.append(container)
                continue

            # MEM_LIMIT

            try:
                expected_mem = _human_to_bytes(self.module.params.get('memory_limit'))
            except ValueError as e:
                self.module.fail_json(msg=str(e))

            #For v1.19 API and above use HostConfig, otherwise use Config
            if docker.utils.compare_version('1.19', api_version) >= 0:
                actual_mem = container['HostConfig']['Memory']
            else:
                actual_mem = container['Config']['Memory']

            if expected_mem and actual_mem != expected_mem:
                self.reload_reasons.append('memory ({0} => {1})'.format(actual_mem, expected_mem))
                differing.append(container)
                continue

            # ENVIRONMENT
            # actual_env is likely to include environment variables injected by
            # the Dockerfile.

            expected_env = {}

            for image_env in image['ContainerConfig']['Env'] or []:
                name, value = image_env.split('=', 1)
                expected_env[name] = value

            if self.environment:
                for name, value in self.environment.iteritems():
                    expected_env[name] = str(value)

            actual_env = {}
            for container_env in container['Config']['Env'] or []:
                name, value = container_env.split('=', 1)
                actual_env[name] = value

            if actual_env != expected_env:
                # Don't include the environment difference in the output.
                self.reload_reasons.append('environment {0} => {1}'.format(actual_env, expected_env))
                differing.append(container)
                continue

            # LABELS

            expected_labels = {}
            for name, value in self.module.params.get('labels').iteritems():
                expected_labels[name] = str(value)

            if isinstance(container['Config']['Labels'], dict):
                actual_labels = container['Config']['Labels']
            else:
                for container_label in container['Config']['Labels'] or []:
                    name, value = container_label.split('=', 1)
                    actual_labels[name] = value

            if actual_labels != expected_labels:
                self.reload_reasons.append('labels {0} => {1}'.format(actual_labels, expected_labels))
                differing.append(container)
                continue

            # HOSTNAME

            expected_hostname = self.module.params.get('hostname')
            actual_hostname = container['Config']['Hostname']
            if expected_hostname and actual_hostname != expected_hostname:
                self.reload_reasons.append('hostname ({0} => {1})'.format(actual_hostname, expected_hostname))
                differing.append(container)
                continue

            # DOMAINNAME

            expected_domainname = self.module.params.get('domainname')
            actual_domainname = container['Config']['Domainname']
            if expected_domainname and actual_domainname != expected_domainname:
                self.reload_reasons.append('domainname ({0} => {1})'.format(actual_domainname, expected_domainname))
                differing.append(container)
                continue

            # DETACH

            # We don't have to check for undetached containers. If it wasn't
            # detached, it would have stopped before the playbook continued!

            # NAME

            # We also don't have to check name, because this is one of the
            # criteria that's used to determine which container(s) match in
            # the first place.

            # STDIN_OPEN

            expected_stdin_open = self.module.params.get('stdin_open')
            actual_stdin_open = container['Config']['OpenStdin']
            if actual_stdin_open != expected_stdin_open:
                self.reload_reasons.append('stdin_open ({0} => {1})'.format(actual_stdin_open, expected_stdin_open))
                differing.append(container)
                continue

            # TTY

            expected_tty = self.module.params.get('tty')
            actual_tty = container['Config']['Tty']
            if actual_tty != expected_tty:
                self.reload_reasons.append('tty ({0} => {1})'.format(actual_tty, expected_tty))
                differing.append(container)
                continue

            # -- ""start"" call differences --

            # LXC_CONF

            if self.lxc_conf:
                expected_lxc = set(self.lxc_conf)
                actual_lxc = set(container['HostConfig']['LxcConf'] or [])
                if actual_lxc != expected_lxc:
                    self.reload_reasons.append('lxc_conf ({0} => {1})'.format(actual_lxc, expected_lxc))
                    differing.append(container)
                    continue

            # BINDS

            expected_binds = set()
            if self.binds:
                for bind in self.binds:
                    expected_binds.add(bind)

            actual_binds = set()
            for bind in (container['HostConfig']['Binds'] or []):
                if len(bind.split(':')) == 2:
                    actual_binds.add(bind + "":rw"")
                else:
                    actual_binds.add(bind)

            if actual_binds != expected_binds:
                self.reload_reasons.append('binds ({0} => {1})'.format(actual_binds, expected_binds))
                differing.append(container)
                continue

            # PORT BINDINGS

            expected_bound_ports = {}
            if self.port_bindings:
                for container_port, config in self.port_bindings.iteritems():
                    if isinstance(container_port, int):
                        container_port = ""{0}/tcp"".format(container_port)
                    if len(config) == 1:
                        expected_bound_ports[container_port] = [{'HostIp': ""0.0.0.0"", 'HostPort': """"}]
                    elif isinstance(config[0], tuple):
                        expected_bound_ports[container_port] = []
                        for hostip, hostport in config:
                            expected_bound_ports[container_port].append({ 'HostIp': hostip, 'HostPort': str(hostport)})
                    else:
                        expected_bound_ports[container_port] = [{'HostIp': config[0], 'HostPort': str(config[1])}]

            actual_bound_ports = container['HostConfig']['PortBindings'] or {}

            if actual_bound_ports != expected_bound_ports:
                self.reload_reasons.append('port bindings ({0} => {1})'.format(actual_bound_ports, expected_bound_ports))
                differing.append(container)
                continue

            # PUBLISHING ALL PORTS

            # What we really care about is the set of ports that is actually
            # published. That should be caught above.

            # PRIVILEGED

            expected_privileged = self.module.params.get('privileged')
            actual_privileged = container['HostConfig']['Privileged']
            if actual_privileged != expected_privileged:
                self.reload_reasons.append('privileged ({0} => {1})'.format(actual_privileged, expected_privileged))
                differing.append(container)
                continue

            # LINKS

            expected_links = set()
            for link, alias in (self.links or {}).iteritems():
                expected_links.add(""/{0}:{1}/{2}"".format(link, container[""Name""], alias))

            actual_links = set(container['HostConfig']['Links'] or [])
            if actual_links != expected_links:
                self.reload_reasons.append('links ({0} => {1})'.format(actual_links, expected_links))
                differing.append(container)
                continue

            # NETWORK MODE

            expected_netmode = self.module.params.get('net') or 'bridge'
            actual_netmode = container['HostConfig']['NetworkMode'] or 'bridge'
            if actual_netmode != expected_netmode:
                self.reload_reasons.append('net ({0} => {1})'.format(actual_netmode, expected_netmode))
                differing.append(container)
                continue

            # DEVICES

            expected_devices = set()
            for device in (self.module.params.get('devices') or []):
                if len(device.split(':')) == 2:
                    expected_devices.add(device + "":rwm"")
                else:
                    expected_devices.add(device)

            actual_devices = set()
            for device in (container['HostConfig']['Devices'] or []):
                actual_devices.add(""{PathOnHost}:{PathInContainer}:{CgroupPermissions}"".format(**device))

            if actual_devices != expected_devices:
                self.reload_reasons.append('devices ({0} => {1})'.format(actual_devices, expected_devices))
                differing.append(container)
                continue

            # DNS

            expected_dns = set(self.module.params.get('dns') or [])
            actual_dns = set(container['HostConfig']['Dns'] or [])
            if actual_dns != expected_dns:
                self.reload_reasons.append('dns ({0} => {1})'.format(actual_dns, expected_dns))
                differing.append(container)
                continue

            # VOLUMES_FROM

            expected_volumes_from = set(self.module.params.get('volumes_from') or [])
            actual_volumes_from = set(container['HostConfig']['VolumesFrom'] or [])
            if actual_volumes_from != expected_volumes_from:
                self.reload_reasons.append('volumes_from ({0} => {1})'.format(actual_volumes_from, expected_volumes_from))
                differing.append(container)

            # LOG_DRIVER

            if self.ensure_capability('log_driver', False):
                expected_log_driver = self.module.params.get('log_driver') or defaults['LoggingDriver']
                actual_log_driver = container['HostConfig']['LogConfig']['Type']
                if actual_log_driver != expected_log_driver:
                    self.reload_reasons.append('log_driver ({0} => {1})'.format(actual_log_driver, expected_log_driver))
                    differing.append(container)
                    continue

            if self.ensure_capability('log_opt', False):
                expected_logging_opts = self.module.params.get('log_opt') or {}
                actual_log_opts = container['HostConfig']['LogConfig']['Config']
                if len(set(expected_logging_opts.items()) - set(actual_log_opts.items())) != 0:
                    log_opt_reasons = {
                        'added': dict(set(expected_logging_opts.items()) - set(actual_log_opts.items())),
                        'removed': dict(set(actual_log_opts.items()) - set(expected_logging_opts.items()))
                    }
                    self.reload_reasons.append('log_opt ({0})'.format(log_opt_reasons))
                    differing.append(container)

        return differing",_93270.py,315,"for device in self.module.params.get('devices') or []:
    if len(device.split(':')) == 2:
        expected_devices.add(device + ':rwm')
    else:
        expected_devices.add(device)",expected_devices = {device + ':rwm' if len(device.split(':')) == 2 else device for device in self.module.params.get('devices') or []},1,nan,nan
https://github.com/ansible/ansible-modules-core/tree/master/cloud/docker/_docker.py,"def get_differing_containers(self):
        """"""
        Inspect all matching, running containers, and return those that were
        started with parameters that differ from the ones that are provided
        during this module run. A list containing the differing
        containers will be returned, and a short string describing the specific
        difference encountered in each container will be appended to
        reload_reasons.

        This generates the set of containers that need to be stopped and
        started with new parameters with state=reloaded.
        """"""

        running = self.get_running_containers()
        current = self.get_inspect_containers(running)
        defaults = self.client.info()

        #Get API version
        api_version = self.client.version()['ApiVersion']

        image = self.get_inspect_image()
        if image is None:
            # The image isn't present. Assume that we're about to pull a new
            # tag and *everything* will be restarted.
            #
            # This will give false positives if you untag an image on the host
            # and there's nothing more to pull.
            return current

        differing = []

        for container in current:

            # IMAGE
            # Compare the image by ID rather than name, so that containers
            # will be restarted when new versions of an existing image are
            # pulled.
            if container['Image'] != image['Id']:
                self.reload_reasons.append('image ({0} => {1})'.format(container['Image'], image['Id']))
                differing.append(container)
                continue

            # ENTRYPOINT

            expected_entrypoint = self.module.params.get('entrypoint')
            if expected_entrypoint:
                expected_entrypoint = shlex.split(expected_entrypoint)
                actual_entrypoint = container[""Config""][""Entrypoint""]

                if actual_entrypoint != expected_entrypoint:
                    self.reload_reasons.append(
                        'entrypoint ({0} => {1})'
                        .format(actual_entrypoint, expected_entrypoint)
                    )
                    differing.append(container)
                    continue

            # COMMAND

            expected_command = self.module.params.get('command')
            if expected_command:
                expected_command = shlex.split(expected_command)
                actual_command = container[""Config""][""Cmd""]

                if actual_command != expected_command:
                    self.reload_reasons.append('command ({0} => {1})'.format(actual_command, expected_command))
                    differing.append(container)
                    continue

            # EXPOSED PORTS
            expected_exposed_ports = set((image['ContainerConfig'].get('ExposedPorts') or {}).keys())
            for p in (self.exposed_ports or []):
                expected_exposed_ports.add(""/"".join(p))

            actually_exposed_ports = set((container[""Config""].get(""ExposedPorts"") or {}).keys())

            if actually_exposed_ports != expected_exposed_ports:
                self.reload_reasons.append('exposed_ports ({0} => {1})'.format(actually_exposed_ports, expected_exposed_ports))
                differing.append(container)
                continue

            # VOLUMES

            expected_volume_keys = set((image['ContainerConfig']['Volumes'] or {}).keys())
            if self.volumes:
                expected_volume_keys.update(self.volumes)

            actual_volume_keys = set((container['Config']['Volumes'] or {}).keys())

            if actual_volume_keys != expected_volume_keys:
                self.reload_reasons.append('volumes ({0} => {1})'.format(actual_volume_keys, expected_volume_keys))
                differing.append(container)
                continue

            # ULIMITS

            expected_ulimit_keys = set(map(lambda x: '%s:%s:%s' % (x['name'],x['soft'],x['hard']), self.ulimits or []))
            actual_ulimit_keys = set(map(lambda x: '%s:%s:%s' % (x['Name'],x['Soft'],x['Hard']), (container['HostConfig']['Ulimits'] or [])))

            if actual_ulimit_keys != expected_ulimit_keys:
                self.reload_reasons.append('ulimits ({0} => {1})'.format(actual_ulimit_keys, expected_ulimit_keys))
                differing.append(container)
                continue

            # CPU_SHARES

            expected_cpu_shares = self.module.params.get('cpu_shares')
            actual_cpu_shares = container['HostConfig']['CpuShares']

            if expected_cpu_shares and actual_cpu_shares != expected_cpu_shares:
                self.reload_reasons.append('cpu_shares ({0} => {1})'.format(actual_cpu_shares, expected_cpu_shares))
                differing.append(container)
                continue

            # MEM_LIMIT

            try:
                expected_mem = _human_to_bytes(self.module.params.get('memory_limit'))
            except ValueError as e:
                self.module.fail_json(msg=str(e))

            #For v1.19 API and above use HostConfig, otherwise use Config
            if docker.utils.compare_version('1.19', api_version) >= 0:
                actual_mem = container['HostConfig']['Memory']
            else:
                actual_mem = container['Config']['Memory']

            if expected_mem and actual_mem != expected_mem:
                self.reload_reasons.append('memory ({0} => {1})'.format(actual_mem, expected_mem))
                differing.append(container)
                continue

            # ENVIRONMENT
            # actual_env is likely to include environment variables injected by
            # the Dockerfile.

            expected_env = {}

            for image_env in image['ContainerConfig']['Env'] or []:
                name, value = image_env.split('=', 1)
                expected_env[name] = value

            if self.environment:
                for name, value in self.environment.iteritems():
                    expected_env[name] = str(value)

            actual_env = {}
            for container_env in container['Config']['Env'] or []:
                name, value = container_env.split('=', 1)
                actual_env[name] = value

            if actual_env != expected_env:
                # Don't include the environment difference in the output.
                self.reload_reasons.append('environment {0} => {1}'.format(actual_env, expected_env))
                differing.append(container)
                continue

            # LABELS

            expected_labels = {}
            for name, value in self.module.params.get('labels').iteritems():
                expected_labels[name] = str(value)

            if isinstance(container['Config']['Labels'], dict):
                actual_labels = container['Config']['Labels']
            else:
                for container_label in container['Config']['Labels'] or []:
                    name, value = container_label.split('=', 1)
                    actual_labels[name] = value

            if actual_labels != expected_labels:
                self.reload_reasons.append('labels {0} => {1}'.format(actual_labels, expected_labels))
                differing.append(container)
                continue

            # HOSTNAME

            expected_hostname = self.module.params.get('hostname')
            actual_hostname = container['Config']['Hostname']
            if expected_hostname and actual_hostname != expected_hostname:
                self.reload_reasons.append('hostname ({0} => {1})'.format(actual_hostname, expected_hostname))
                differing.append(container)
                continue

            # DOMAINNAME

            expected_domainname = self.module.params.get('domainname')
            actual_domainname = container['Config']['Domainname']
            if expected_domainname and actual_domainname != expected_domainname:
                self.reload_reasons.append('domainname ({0} => {1})'.format(actual_domainname, expected_domainname))
                differing.append(container)
                continue

            # DETACH

            # We don't have to check for undetached containers. If it wasn't
            # detached, it would have stopped before the playbook continued!

            # NAME

            # We also don't have to check name, because this is one of the
            # criteria that's used to determine which container(s) match in
            # the first place.

            # STDIN_OPEN

            expected_stdin_open = self.module.params.get('stdin_open')
            actual_stdin_open = container['Config']['OpenStdin']
            if actual_stdin_open != expected_stdin_open:
                self.reload_reasons.append('stdin_open ({0} => {1})'.format(actual_stdin_open, expected_stdin_open))
                differing.append(container)
                continue

            # TTY

            expected_tty = self.module.params.get('tty')
            actual_tty = container['Config']['Tty']
            if actual_tty != expected_tty:
                self.reload_reasons.append('tty ({0} => {1})'.format(actual_tty, expected_tty))
                differing.append(container)
                continue

            # -- ""start"" call differences --

            # LXC_CONF

            if self.lxc_conf:
                expected_lxc = set(self.lxc_conf)
                actual_lxc = set(container['HostConfig']['LxcConf'] or [])
                if actual_lxc != expected_lxc:
                    self.reload_reasons.append('lxc_conf ({0} => {1})'.format(actual_lxc, expected_lxc))
                    differing.append(container)
                    continue

            # BINDS

            expected_binds = set()
            if self.binds:
                for bind in self.binds:
                    expected_binds.add(bind)

            actual_binds = set()
            for bind in (container['HostConfig']['Binds'] or []):
                if len(bind.split(':')) == 2:
                    actual_binds.add(bind + "":rw"")
                else:
                    actual_binds.add(bind)

            if actual_binds != expected_binds:
                self.reload_reasons.append('binds ({0} => {1})'.format(actual_binds, expected_binds))
                differing.append(container)
                continue

            # PORT BINDINGS

            expected_bound_ports = {}
            if self.port_bindings:
                for container_port, config in self.port_bindings.iteritems():
                    if isinstance(container_port, int):
                        container_port = ""{0}/tcp"".format(container_port)
                    if len(config) == 1:
                        expected_bound_ports[container_port] = [{'HostIp': ""0.0.0.0"", 'HostPort': """"}]
                    elif isinstance(config[0], tuple):
                        expected_bound_ports[container_port] = []
                        for hostip, hostport in config:
                            expected_bound_ports[container_port].append({ 'HostIp': hostip, 'HostPort': str(hostport)})
                    else:
                        expected_bound_ports[container_port] = [{'HostIp': config[0], 'HostPort': str(config[1])}]

            actual_bound_ports = container['HostConfig']['PortBindings'] or {}

            if actual_bound_ports != expected_bound_ports:
                self.reload_reasons.append('port bindings ({0} => {1})'.format(actual_bound_ports, expected_bound_ports))
                differing.append(container)
                continue

            # PUBLISHING ALL PORTS

            # What we really care about is the set of ports that is actually
            # published. That should be caught above.

            # PRIVILEGED

            expected_privileged = self.module.params.get('privileged')
            actual_privileged = container['HostConfig']['Privileged']
            if actual_privileged != expected_privileged:
                self.reload_reasons.append('privileged ({0} => {1})'.format(actual_privileged, expected_privileged))
                differing.append(container)
                continue

            # LINKS

            expected_links = set()
            for link, alias in (self.links or {}).iteritems():
                expected_links.add(""/{0}:{1}/{2}"".format(link, container[""Name""], alias))

            actual_links = set(container['HostConfig']['Links'] or [])
            if actual_links != expected_links:
                self.reload_reasons.append('links ({0} => {1})'.format(actual_links, expected_links))
                differing.append(container)
                continue

            # NETWORK MODE

            expected_netmode = self.module.params.get('net') or 'bridge'
            actual_netmode = container['HostConfig']['NetworkMode'] or 'bridge'
            if actual_netmode != expected_netmode:
                self.reload_reasons.append('net ({0} => {1})'.format(actual_netmode, expected_netmode))
                differing.append(container)
                continue

            # DEVICES

            expected_devices = set()
            for device in (self.module.params.get('devices') or []):
                if len(device.split(':')) == 2:
                    expected_devices.add(device + "":rwm"")
                else:
                    expected_devices.add(device)

            actual_devices = set()
            for device in (container['HostConfig']['Devices'] or []):
                actual_devices.add(""{PathOnHost}:{PathInContainer}:{CgroupPermissions}"".format(**device))

            if actual_devices != expected_devices:
                self.reload_reasons.append('devices ({0} => {1})'.format(actual_devices, expected_devices))
                differing.append(container)
                continue

            # DNS

            expected_dns = set(self.module.params.get('dns') or [])
            actual_dns = set(container['HostConfig']['Dns'] or [])
            if actual_dns != expected_dns:
                self.reload_reasons.append('dns ({0} => {1})'.format(actual_dns, expected_dns))
                differing.append(container)
                continue

            # VOLUMES_FROM

            expected_volumes_from = set(self.module.params.get('volumes_from') or [])
            actual_volumes_from = set(container['HostConfig']['VolumesFrom'] or [])
            if actual_volumes_from != expected_volumes_from:
                self.reload_reasons.append('volumes_from ({0} => {1})'.format(actual_volumes_from, expected_volumes_from))
                differing.append(container)

            # LOG_DRIVER

            if self.ensure_capability('log_driver', False):
                expected_log_driver = self.module.params.get('log_driver') or defaults['LoggingDriver']
                actual_log_driver = container['HostConfig']['LogConfig']['Type']
                if actual_log_driver != expected_log_driver:
                    self.reload_reasons.append('log_driver ({0} => {1})'.format(actual_log_driver, expected_log_driver))
                    differing.append(container)
                    continue

            if self.ensure_capability('log_opt', False):
                expected_logging_opts = self.module.params.get('log_opt') or {}
                actual_log_opts = container['HostConfig']['LogConfig']['Config']
                if len(set(expected_logging_opts.items()) - set(actual_log_opts.items())) != 0:
                    log_opt_reasons = {
                        'added': dict(set(expected_logging_opts.items()) - set(actual_log_opts.items())),
                        'removed': dict(set(actual_log_opts.items()) - set(expected_logging_opts.items()))
                    }
                    self.reload_reasons.append('log_opt ({0})'.format(log_opt_reasons))
                    differing.append(container)

        return differing",_93270.py,322,"for device in container['HostConfig']['Devices'] or []:
    actual_devices.add('{PathOnHost}:{PathInContainer}:{CgroupPermissions}'.format(**device))",actual_devices = {'{PathOnHost}:{PathInContainer}:{CgroupPermissions}'.format(**device) for device in container['HostConfig']['Devices'] or []},1,nan,nan
https://github.com/ansible/ansible-modules-core/tree/master/cloud/docker/_docker.py,"def get_differing_containers(self):
        """"""
        Inspect all matching, running containers, and return those that were
        started with parameters that differ from the ones that are provided
        during this module run. A list containing the differing
        containers will be returned, and a short string describing the specific
        difference encountered in each container will be appended to
        reload_reasons.

        This generates the set of containers that need to be stopped and
        started with new parameters with state=reloaded.
        """"""

        running = self.get_running_containers()
        current = self.get_inspect_containers(running)
        defaults = self.client.info()

        #Get API version
        api_version = self.client.version()['ApiVersion']

        image = self.get_inspect_image()
        if image is None:
            # The image isn't present. Assume that we're about to pull a new
            # tag and *everything* will be restarted.
            #
            # This will give false positives if you untag an image on the host
            # and there's nothing more to pull.
            return current

        differing = []

        for container in current:

            # IMAGE
            # Compare the image by ID rather than name, so that containers
            # will be restarted when new versions of an existing image are
            # pulled.
            if container['Image'] != image['Id']:
                self.reload_reasons.append('image ({0} => {1})'.format(container['Image'], image['Id']))
                differing.append(container)
                continue

            # ENTRYPOINT

            expected_entrypoint = self.module.params.get('entrypoint')
            if expected_entrypoint:
                expected_entrypoint = shlex.split(expected_entrypoint)
                actual_entrypoint = container[""Config""][""Entrypoint""]

                if actual_entrypoint != expected_entrypoint:
                    self.reload_reasons.append(
                        'entrypoint ({0} => {1})'
                        .format(actual_entrypoint, expected_entrypoint)
                    )
                    differing.append(container)
                    continue

            # COMMAND

            expected_command = self.module.params.get('command')
            if expected_command:
                expected_command = shlex.split(expected_command)
                actual_command = container[""Config""][""Cmd""]

                if actual_command != expected_command:
                    self.reload_reasons.append('command ({0} => {1})'.format(actual_command, expected_command))
                    differing.append(container)
                    continue

            # EXPOSED PORTS
            expected_exposed_ports = set((image['ContainerConfig'].get('ExposedPorts') or {}).keys())
            for p in (self.exposed_ports or []):
                expected_exposed_ports.add(""/"".join(p))

            actually_exposed_ports = set((container[""Config""].get(""ExposedPorts"") or {}).keys())

            if actually_exposed_ports != expected_exposed_ports:
                self.reload_reasons.append('exposed_ports ({0} => {1})'.format(actually_exposed_ports, expected_exposed_ports))
                differing.append(container)
                continue

            # VOLUMES

            expected_volume_keys = set((image['ContainerConfig']['Volumes'] or {}).keys())
            if self.volumes:
                expected_volume_keys.update(self.volumes)

            actual_volume_keys = set((container['Config']['Volumes'] or {}).keys())

            if actual_volume_keys != expected_volume_keys:
                self.reload_reasons.append('volumes ({0} => {1})'.format(actual_volume_keys, expected_volume_keys))
                differing.append(container)
                continue

            # ULIMITS

            expected_ulimit_keys = set(map(lambda x: '%s:%s:%s' % (x['name'],x['soft'],x['hard']), self.ulimits or []))
            actual_ulimit_keys = set(map(lambda x: '%s:%s:%s' % (x['Name'],x['Soft'],x['Hard']), (container['HostConfig']['Ulimits'] or [])))

            if actual_ulimit_keys != expected_ulimit_keys:
                self.reload_reasons.append('ulimits ({0} => {1})'.format(actual_ulimit_keys, expected_ulimit_keys))
                differing.append(container)
                continue

            # CPU_SHARES

            expected_cpu_shares = self.module.params.get('cpu_shares')
            actual_cpu_shares = container['HostConfig']['CpuShares']

            if expected_cpu_shares and actual_cpu_shares != expected_cpu_shares:
                self.reload_reasons.append('cpu_shares ({0} => {1})'.format(actual_cpu_shares, expected_cpu_shares))
                differing.append(container)
                continue

            # MEM_LIMIT

            try:
                expected_mem = _human_to_bytes(self.module.params.get('memory_limit'))
            except ValueError as e:
                self.module.fail_json(msg=str(e))

            #For v1.19 API and above use HostConfig, otherwise use Config
            if docker.utils.compare_version('1.19', api_version) >= 0:
                actual_mem = container['HostConfig']['Memory']
            else:
                actual_mem = container['Config']['Memory']

            if expected_mem and actual_mem != expected_mem:
                self.reload_reasons.append('memory ({0} => {1})'.format(actual_mem, expected_mem))
                differing.append(container)
                continue

            # ENVIRONMENT
            # actual_env is likely to include environment variables injected by
            # the Dockerfile.

            expected_env = {}

            for image_env in image['ContainerConfig']['Env'] or []:
                name, value = image_env.split('=', 1)
                expected_env[name] = value

            if self.environment:
                for name, value in self.environment.iteritems():
                    expected_env[name] = str(value)

            actual_env = {}
            for container_env in container['Config']['Env'] or []:
                name, value = container_env.split('=', 1)
                actual_env[name] = value

            if actual_env != expected_env:
                # Don't include the environment difference in the output.
                self.reload_reasons.append('environment {0} => {1}'.format(actual_env, expected_env))
                differing.append(container)
                continue

            # LABELS

            expected_labels = {}
            for name, value in self.module.params.get('labels').iteritems():
                expected_labels[name] = str(value)

            if isinstance(container['Config']['Labels'], dict):
                actual_labels = container['Config']['Labels']
            else:
                for container_label in container['Config']['Labels'] or []:
                    name, value = container_label.split('=', 1)
                    actual_labels[name] = value

            if actual_labels != expected_labels:
                self.reload_reasons.append('labels {0} => {1}'.format(actual_labels, expected_labels))
                differing.append(container)
                continue

            # HOSTNAME

            expected_hostname = self.module.params.get('hostname')
            actual_hostname = container['Config']['Hostname']
            if expected_hostname and actual_hostname != expected_hostname:
                self.reload_reasons.append('hostname ({0} => {1})'.format(actual_hostname, expected_hostname))
                differing.append(container)
                continue

            # DOMAINNAME

            expected_domainname = self.module.params.get('domainname')
            actual_domainname = container['Config']['Domainname']
            if expected_domainname and actual_domainname != expected_domainname:
                self.reload_reasons.append('domainname ({0} => {1})'.format(actual_domainname, expected_domainname))
                differing.append(container)
                continue

            # DETACH

            # We don't have to check for undetached containers. If it wasn't
            # detached, it would have stopped before the playbook continued!

            # NAME

            # We also don't have to check name, because this is one of the
            # criteria that's used to determine which container(s) match in
            # the first place.

            # STDIN_OPEN

            expected_stdin_open = self.module.params.get('stdin_open')
            actual_stdin_open = container['Config']['OpenStdin']
            if actual_stdin_open != expected_stdin_open:
                self.reload_reasons.append('stdin_open ({0} => {1})'.format(actual_stdin_open, expected_stdin_open))
                differing.append(container)
                continue

            # TTY

            expected_tty = self.module.params.get('tty')
            actual_tty = container['Config']['Tty']
            if actual_tty != expected_tty:
                self.reload_reasons.append('tty ({0} => {1})'.format(actual_tty, expected_tty))
                differing.append(container)
                continue

            # -- ""start"" call differences --

            # LXC_CONF

            if self.lxc_conf:
                expected_lxc = set(self.lxc_conf)
                actual_lxc = set(container['HostConfig']['LxcConf'] or [])
                if actual_lxc != expected_lxc:
                    self.reload_reasons.append('lxc_conf ({0} => {1})'.format(actual_lxc, expected_lxc))
                    differing.append(container)
                    continue

            # BINDS

            expected_binds = set()
            if self.binds:
                for bind in self.binds:
                    expected_binds.add(bind)

            actual_binds = set()
            for bind in (container['HostConfig']['Binds'] or []):
                if len(bind.split(':')) == 2:
                    actual_binds.add(bind + "":rw"")
                else:
                    actual_binds.add(bind)

            if actual_binds != expected_binds:
                self.reload_reasons.append('binds ({0} => {1})'.format(actual_binds, expected_binds))
                differing.append(container)
                continue

            # PORT BINDINGS

            expected_bound_ports = {}
            if self.port_bindings:
                for container_port, config in self.port_bindings.iteritems():
                    if isinstance(container_port, int):
                        container_port = ""{0}/tcp"".format(container_port)
                    if len(config) == 1:
                        expected_bound_ports[container_port] = [{'HostIp': ""0.0.0.0"", 'HostPort': """"}]
                    elif isinstance(config[0], tuple):
                        expected_bound_ports[container_port] = []
                        for hostip, hostport in config:
                            expected_bound_ports[container_port].append({ 'HostIp': hostip, 'HostPort': str(hostport)})
                    else:
                        expected_bound_ports[container_port] = [{'HostIp': config[0], 'HostPort': str(config[1])}]

            actual_bound_ports = container['HostConfig']['PortBindings'] or {}

            if actual_bound_ports != expected_bound_ports:
                self.reload_reasons.append('port bindings ({0} => {1})'.format(actual_bound_ports, expected_bound_ports))
                differing.append(container)
                continue

            # PUBLISHING ALL PORTS

            # What we really care about is the set of ports that is actually
            # published. That should be caught above.

            # PRIVILEGED

            expected_privileged = self.module.params.get('privileged')
            actual_privileged = container['HostConfig']['Privileged']
            if actual_privileged != expected_privileged:
                self.reload_reasons.append('privileged ({0} => {1})'.format(actual_privileged, expected_privileged))
                differing.append(container)
                continue

            # LINKS

            expected_links = set()
            for link, alias in (self.links or {}).iteritems():
                expected_links.add(""/{0}:{1}/{2}"".format(link, container[""Name""], alias))

            actual_links = set(container['HostConfig']['Links'] or [])
            if actual_links != expected_links:
                self.reload_reasons.append('links ({0} => {1})'.format(actual_links, expected_links))
                differing.append(container)
                continue

            # NETWORK MODE

            expected_netmode = self.module.params.get('net') or 'bridge'
            actual_netmode = container['HostConfig']['NetworkMode'] or 'bridge'
            if actual_netmode != expected_netmode:
                self.reload_reasons.append('net ({0} => {1})'.format(actual_netmode, expected_netmode))
                differing.append(container)
                continue

            # DEVICES

            expected_devices = set()
            for device in (self.module.params.get('devices') or []):
                if len(device.split(':')) == 2:
                    expected_devices.add(device + "":rwm"")
                else:
                    expected_devices.add(device)

            actual_devices = set()
            for device in (container['HostConfig']['Devices'] or []):
                actual_devices.add(""{PathOnHost}:{PathInContainer}:{CgroupPermissions}"".format(**device))

            if actual_devices != expected_devices:
                self.reload_reasons.append('devices ({0} => {1})'.format(actual_devices, expected_devices))
                differing.append(container)
                continue

            # DNS

            expected_dns = set(self.module.params.get('dns') or [])
            actual_dns = set(container['HostConfig']['Dns'] or [])
            if actual_dns != expected_dns:
                self.reload_reasons.append('dns ({0} => {1})'.format(actual_dns, expected_dns))
                differing.append(container)
                continue

            # VOLUMES_FROM

            expected_volumes_from = set(self.module.params.get('volumes_from') or [])
            actual_volumes_from = set(container['HostConfig']['VolumesFrom'] or [])
            if actual_volumes_from != expected_volumes_from:
                self.reload_reasons.append('volumes_from ({0} => {1})'.format(actual_volumes_from, expected_volumes_from))
                differing.append(container)

            # LOG_DRIVER

            if self.ensure_capability('log_driver', False):
                expected_log_driver = self.module.params.get('log_driver') or defaults['LoggingDriver']
                actual_log_driver = container['HostConfig']['LogConfig']['Type']
                if actual_log_driver != expected_log_driver:
                    self.reload_reasons.append('log_driver ({0} => {1})'.format(actual_log_driver, expected_log_driver))
                    differing.append(container)
                    continue

            if self.ensure_capability('log_opt', False):
                expected_logging_opts = self.module.params.get('log_opt') or {}
                actual_log_opts = container['HostConfig']['LogConfig']['Config']
                if len(set(expected_logging_opts.items()) - set(actual_log_opts.items())) != 0:
                    log_opt_reasons = {
                        'added': dict(set(expected_logging_opts.items()) - set(actual_log_opts.items())),
                        'removed': dict(set(actual_log_opts.items()) - set(expected_logging_opts.items()))
                    }
                    self.reload_reasons.append('log_opt ({0})'.format(log_opt_reasons))
                    differing.append(container)

        return differing",_93270.py,239,"for bind in self.binds:
    expected_binds.add(bind)",expected_binds = {bind for bind in self.binds},1,nan,nan
https://github.com/ProjectAnte/dnsgen/tree/master/dnsgen/dnsgen.py,"def extract_custom_words(domains, wordlen):
	'''
	Extend the dictionary based on target's domain naming conventions
	'''

	valid_tokens = set()

	for domain in domains:
		partition = partiate_domain(domain)[:-1]
		tokens = set(itertools.chain(*[word.lower().split('-') for word in partition]))
		tokens = tokens.union({word.lower() for word in partition})
		for t in tokens:
			if len(t) >= wordlen:
				valid_tokens.add(t)

	return valid_tokens",_94775.py,8,"for domain in domains:
    partition = partiate_domain(domain)[:-1]
    tokens = set(itertools.chain(*[word.lower().split('-') for word in partition]))
    tokens = tokens.union({word.lower() for word in partition})
    for t in tokens:
        if len(t) >= wordlen:
            valid_tokens.add(t)",valid_tokens = {t for domain in domains for t in set(itertools.chain(*[word.lower().split('-') for word in partiate_domain(domain)[:-1]])).union({word.lower() for word in partiate_domain(domain)[:-1]}) if len(t) >= wordlen},1,nan,nan
https://github.com/conda/conda/tree/master/conda/resolve.py,"def get_reduced_index(self, explicit_specs, sort_by_exactness=True, exit_on_conflict=False):
        # TODO: fix this import; this is bad
        from .core.subdir_data import make_feature_record

        strict_channel_priority = context.channel_priority == ChannelPriority.STRICT

        cache_key = strict_channel_priority, tuple(explicit_specs)
        if cache_key in self._reduced_index_cache:
            return self._reduced_index_cache[cache_key]

        if log.isEnabledFor(DEBUG):
            log.debug('Retrieving packages for: %s', dashlist(
                sorted(text_type(s) for s in explicit_specs)))

        explicit_specs, features = self.verify_specs(explicit_specs)
        filter_out = {prec: False if val else ""feature not enabled""
                      for prec, val in iteritems(self.default_filter(features))}
        snames = set()
        top_level_spec = None
        cp_filter_applied = set()  # values are package names
        if sort_by_exactness:
            # prioritize specs that are more exact.  Exact specs will evaluate to 3,
            #    constrained specs will evaluate to 2, and name only will be 1
            explicit_specs = sorted(list(explicit_specs), key=lambda x: (
                exactness_and_number_of_deps(self, x), x.dist_str()), reverse=True)
        # tuple because it needs to be hashable
        explicit_specs = tuple(explicit_specs)

        explicit_spec_package_pool = {}
        for s in explicit_specs:
            explicit_spec_package_pool[s.name] = explicit_spec_package_pool.get(
                s.name, set()) | set(self.find_matches(s))

        def filter_group(_specs):
            # all _specs should be for the same package name
            name = next(iter(_specs)).name
            group = self.groups.get(name, ())

            # implement strict channel priority
            if group and strict_channel_priority and name not in cp_filter_applied:
                sole_source_channel_name = self._get_strict_channel(name)
                for prec in group:
                    if prec.channel.name != sole_source_channel_name:
                        filter_out[prec] = ""removed due to strict channel priority""
                cp_filter_applied.add(name)

            # Prune packages that don't match any of the patterns,
            # have unsatisfiable dependencies, or conflict with the explicit specs
            nold = nnew = 0
            for prec in group:
                if not filter_out.setdefault(prec, False):
                    nold += 1
                    if (not self.match_any(_specs, prec)) or (
                            explicit_spec_package_pool.get(name) and
                            prec not in explicit_spec_package_pool[name]):
                        filter_out[prec] = ""incompatible with required spec %s"" % top_level_spec
                        continue
                    unsatisfiable_dep_specs = set()
                    for ms in self.ms_depends(prec):
                        if not ms.optional and not any(
                                 rec for rec in self.find_matches(ms)
                                if not filter_out.get(rec, False)):
                            unsatisfiable_dep_specs.add(ms)
                    if unsatisfiable_dep_specs:
                        filter_out[prec] = ""unsatisfiable dependencies %s"" % "" "".join(
                            str(s) for s in unsatisfiable_dep_specs
                        )
                        continue
                    filter_out[prec] = False
                    nnew += 1

            reduced = nnew < nold
            if reduced:
                log.debug('%s: pruned from %d -> %d' % (name, nold, nnew))
            if any(ms.optional for ms in _specs):
                return reduced
            elif nnew == 0:
                # Indicates that a conflict was found; we can exit early
                return None

            # Perform the same filtering steps on any dependencies shared across
            # *all* packages in the group. Even if just one of the packages does
            # not have a particular dependency, it must be ignored in this pass.
            # Otherwise, we might do more filtering than we should---and it is
            # better to have extra packages here than missing ones.
            if reduced or name not in snames:
                snames.add(name)

                _dep_specs = groupby(lambda s: s.name, (
                    dep_spec
                    for prec in group if not filter_out.get(prec, False)
                    for dep_spec in self.ms_depends(prec) if not dep_spec.optional
                ))
                _dep_specs.pop(""*"", None)  # discard track_features specs

                for deps_name, deps in sorted(_dep_specs.items(),
                                              key=lambda x: any(_.optional for _ in x[1])):
                    if len(deps) >= nnew:
                        res = filter_group(set(deps))
                        if res:
                            reduced = True
                        elif res is None:
                            # Indicates that a conflict was found; we can exit early
                            return None

            return reduced

        # Iterate on pruning until no progress is made. We've implemented
        # what amounts to ""double-elimination"" here; packages get one additional
        # chance after their first ""False"" reduction. This catches more instances
        # where one package's filter affects another. But we don't have to be
        # perfect about this, so performance matters.
        pruned_to_zero = set()
        for _ in range(2):
            snames.clear()
            slist = deque(explicit_specs)
            while slist:
                s = slist.popleft()
                if filter_group([s]):
                    slist.append(s)
                else:
                    pruned_to_zero.add(s)

        if pruned_to_zero and exit_on_conflict:
            return {}

        # Determine all valid packages in the dependency graph
        reduced_index2 = {prec: prec for prec in (make_feature_record(fstr) for fstr in features)}
        specs_by_name_seed = OrderedDict()
        for s in explicit_specs:
            specs_by_name_seed[s.name] = specs_by_name_seed.get(s.name, list()) + [s]
        for explicit_spec in explicit_specs:
            add_these_precs2 = tuple(
                prec for prec in self.find_matches(explicit_spec)
                if prec not in reduced_index2 and self.valid2(prec, filter_out))

            if strict_channel_priority and add_these_precs2:
                strict_channel_name = self._get_strict_channel(add_these_precs2[0].name)

                add_these_precs2 = tuple(
                    prec for prec in add_these_precs2 if prec.channel.name == strict_channel_name
                )
            reduced_index2.update((prec, prec) for prec in add_these_precs2)

            for pkg in add_these_precs2:
                # what we have seen is only relevant within the context of a single package
                #    that is picked up because of an explicit spec.  We don't want the
                #    broadening check to apply across packages at the explicit level; only
                #    at the level of deps below that explicit package.
                seen_specs = set()
                specs_by_name = copy.deepcopy(specs_by_name_seed)

                dep_specs = set(self.ms_depends(pkg))
                for dep in dep_specs:
                    specs = specs_by_name.get(dep.name, list())
                    if dep not in specs and (not specs or dep.strictness >= specs[0].strictness):
                        specs.insert(0, dep)
                    specs_by_name[dep.name] = specs

                while(dep_specs):
                    # used for debugging
                    # size_index = len(reduced_index2)
                    # specs_added = []
                    ms = dep_specs.pop()
                    seen_specs.add(ms)
                    for dep_pkg in (_ for _ in self.find_matches(ms) if _ not in reduced_index2):
                        if not self.valid2(dep_pkg, filter_out):
                            continue

                        # expand the reduced index if not using strict channel priority,
                        #    or if using it and this package is in the appropriate channel
                        if (not strict_channel_priority or
                                (self._get_strict_channel(dep_pkg.name) ==
                                 dep_pkg.channel.name)):
                            reduced_index2[dep_pkg] = dep_pkg

                            # recurse to deps of this dep
                            new_specs = set(self.ms_depends(dep_pkg)) - seen_specs
                            for new_ms in new_specs:
                                # We do not pull packages into the reduced index due
                                # to a track_features dependency. Remember, a feature
                                # specifies a ""soft"" dependency: it must be in the
                                # environment, but it is not _pulled_ in. The SAT
                                # logic doesn't do a perfect job of capturing this
                                # behavior, but keeping these packags out of the
                                # reduced index helps. Of course, if _another_
                                # package pulls it in by dependency, that's fine.
                                if ('track_features' not in new_ms and not self._broader(
                                        new_ms, tuple(specs_by_name.get(new_ms.name, tuple())))):
                                    dep_specs.add(new_ms)
                                    # if new_ms not in dep_specs:
                                    #     specs_added.append(new_ms)
                                else:
                                    seen_specs.add(new_ms)
                    # debugging info - see what specs are bringing in the largest blobs
                    # if size_index != len(reduced_index2):
                    #     print(""MS {} added {} pkgs to index"".format(ms,
                    #           len(reduced_index2) - size_index))
                    # if specs_added:
                    #     print(""MS {} added {} specs to further examination"".format(ms,
                    #                                                                specs_added))

        reduced_index2 = frozendict(reduced_index2)
        self._reduced_index_cache[cache_key] = reduced_index2
        return reduced_index2",_97638.py,59,"for ms in self.ms_depends(prec):
    if not ms.optional and (not any((rec for rec in self.find_matches(ms) if not filter_out.get(rec, False)))):
        unsatisfiable_dep_specs.add(ms)","unsatisfiable_dep_specs = {ms for ms in self.ms_depends(prec) if not ms.optional and (not any((rec for rec in self.find_matches(ms) if not filter_out.get(rec, False))))}",1,nan,nan
https://github.com/mozilla/pontoon/tree/master/pontoon/insights/tasks.py,"def get_active_users_data(
    start_of_today,
    privileged_users,
    active_users_actions,
    all_reviewers,
    all_contributors,
    months=12,
):
    """"""Get active user counts for the Active users panel.""""""
    active_managers = set()
    active_reviewers = set()
    active_contributors = set()

    # Get active managers
    for user in privileged_users:
        manager = user[""managers_group__user""]
        last_login = user[""managers_group__user__last_login""]

        if last_login:
            if last_login + relativedelta(months=months) > start_of_today:
                active_managers.add(manager)

    # Get active reviewers and contributors. Make sure they are included among all
    # users, otherwise we might include PMs and privileged users of other locales.
    for action in active_users_actions:
        user = action[""performed_by""]
        if user in all_reviewers and action[""action_type""] in (
            ""translation:approved"",
            ""translation:unapproved"",
            ""translation:rejected"",
            ""translation:unrejected"",
        ):
            if action[""created_at""] + relativedelta(months=months) > start_of_today:
                active_reviewers.add(user)

        if user in all_contributors and action[""action_type""] == ""translation:created"":
            if action[""created_at""] + relativedelta(months=months) > start_of_today:
                active_contributors.add(user)

    return {
        ""managers"": len(active_managers),
        ""reviewers"": len(active_reviewers),
        ""contributors"": len(active_contributors),
    }",_97809.py,15,"for user in privileged_users:
    manager = user['managers_group__user']
    last_login = user['managers_group__user__last_login']
    if last_login:
        if last_login + relativedelta(months=months) > start_of_today:
            active_managers.add(manager)",active_managers = {user['managers_group__user'] for user in privileged_users if user['managers_group__user__last_login'] if user['managers_group__user__last_login'] + relativedelta(months=months) > start_of_today},1,nan,nan
https://github.com/shadowmoose/RedditDownloader/tree/master/redditdownloader/__main__.py,"def run():
	logging.basicConfig(level=logging.WARN, format='%(levelname)-5.5s [%(name)s] %(message)s', datefmt='%H:%M:%S')
	su.print_color('green', ""\r\n"" +
		'====================================\r\n' +
		('   Reddit Media Downloader %s\r\n' % meta.current_version) +
		'====================================\r\n' +
		'    (By ShadowMoose @ Github)\r\n')
	if args.version:
		sys.exit(0)

	if args.run_tests:
		error_count = tests.runner.run_tests(test_subdir=args.run_tests)
		sys.exit(error_count)

	if args.list_settings:
		print('All valid overridable settings:')
		for _s in settings.get_all():
			if _s.public:
				print(""%s.%s"" % (_s.category, _s.name))
				print('\tDescription: %s' % _s.description)
				if not _s.opts:
					print('\tValid value: \n\t\tAny %s' % _s.type)
				else:
					print('\tValid values:')
					for o in _s.opts:
						print('\t\t""%s"": %s' % o)
				print()
		sys.exit()

	settings_file = args.settings or fs.find_file('settings.json')
	_loaded = settings.load(settings_file)
	for ua in unknown_args:
		if '=' not in ua or '/comments/' in ua:
			if '/comments/' in ua:
				direct_sources.append(DirectURLSource(url=ua))
				continue
			elif 'r/' or 'u/' in ua:
				direct_sources.append(DirectInputSource(txt=ua, args={'limit': args.limit}))
				continue
			else:
				su.error(""ERROR: Unkown argument: %s"" % ua)
				sys.exit(1)
		k = ua.split('=')[0].strip('- ')
		v = ua.split('=', 2)[1].strip()
		try:
			settings.put(k, v, save_after=False)
		except KeyError:
			print('Unknown setting: %s' % k)
			sys.exit(50)

	if args.source:
		matched_sources = set()
		for s in args.source:
			for stt in settings.get_sources():
				if re.match(s, stt.get_alias()):
					matched_sources.add(stt)
		direct_sources.extend(matched_sources)

	if args.import_csv:
		direct_sources.append(DirectFileSource(file=args.import_csv, slow_fallback=args.full_csv))

	first_time_auth = False

	if not _loaded and not direct_sources and not args.docker:
		# First-time configuration.
		su.error('Could not find an existing settings file. A new one will be generated!')
		if not console.confirm('Would you like to start the WebUI to help set things up?', True):
			su.print_color('red', ""If you don't open the webUI now, you'll need to edit the settings file yourself."")
			if console.confirm(""Are you sure you'd like to edit settings without the UI (if 'yes', these prompts will not show again)?""):
				settings.put('interface.start_server', False, save_after=True)  # Creates a save.
				print('A settings file has been created for you, at ""%s"". Please customize it.' % settings_file)
				first_time_auth = True
			else:
				print('Please re-run RMD to configure again.')
				sys.exit(1)
		else:
			mode = console.prompt_list('How would you like to open the UI?',
									   settings.get('interface.browser', full_obj=True).opts)
			settings.put('interface.browser', mode, save_after=False)
			settings.put('interface.start_server', True)

	if args.docker:
		print('Running in ""Docker"" mode. Assuming some default settings.')
		settings.put('interface.host', '0.0.0.0', save_after=False)
		settings.put('interface.browser', 'off', save_after=False)
		settings.put('interface.keep_open', True, save_after=False)
		settings.put('interface.start_server', True)

	if args.authorize or first_time_auth:  # In-console oAuth authentication flow
		from static import praw_wrapper
		from urllib.parse import urlparse, parse_qs
		url = praw_wrapper.get_reddit_token_url()
		su.print_color('green', '\nTo manually authorize your account, visit the below URL.')
		su.print_color('yellow', 'Once there, authorize RMD, then copy the URL it redirects you to.')
		su.print_color('yellow', 'NOTE: The redirect page will likely not load, and that is ok.')
		su.print_color('cyan', '\n%s\n' % url)
		token_url = console.col_input('Paste the URL you are redirected to here: ')
		if token_url.strip():
			qs = parse_qs(urlparse(token_url).query)
			if 'state' not in qs or 'code' not in qs:
				su.error('The url provided was not a valid reddit redirect. Please make sure you copied it right!')
			elif qs['state'][0].strip() != settings.get('auth.oauth_key').strip():
				su.error('Invalid reddit redirect state. Please restart and try again.')
			else:
				code = qs['code'][0]
				su.print_color('green', 'Got code. Authorizing account...')
				refresh = praw_wrapper.get_refresh_token(code)
				if refresh:
					settings.put('auth.refresh_token', refresh)
					usr = praw_wrapper.get_current_username()
					su.print_color('cyan', 'Authorized to view account: %s' % usr)
					su.print_color('green', 'Saved authorization token! Please restart RMD to begin downloading!')
				else:
					su.error('Failed to gain an account access token from Reddit with that code. Please try again.')
		sys.exit(0)

	if not ffmpeg_download.install_local():
		print(""RMD was unable to locate (or download) a working FFmpeg binary."")
		print(""For downloading and post-processing, this is a required tool."")
		print(""Please Install FFmpeg manually, or download it from here: https://rmd.page.link/ffmpeg"")
		sys.exit(15)

	# Initialize Database
	sql.init_from_settings()
	print('Using manifest file [%s].' % sql.get_file_location())

	if direct_sources:
		settings.disable_saving()
		settings.put('processing.retry_failed', False)
		for s in settings.get_sources():
			settings.remove_source(s, save_after=False)
		for d in direct_sources:
			settings.add_source(d, prevent_duplicate=False, save_after=False)

	if settings.get('interface.start_server') and not direct_sources:
		print(""Starting WebUI..."")
		ui = WebUI()
	else:
		ui = TerminalUI()
	ui.display()",_98153.py,53,"for s in args.source:
    for stt in settings.get_sources():
        if re.match(s, stt.get_alias()):
            matched_sources.add(stt)","matched_sources = {stt for s in args.source for stt in settings.get_sources() if re.match(s, stt.get_alias())}",1,nan,nan
https://github.com/AirtestProject/Poco/tree/master/poco/drivers/osx/sdk/OSXUI.py,"def ConnectWindowsByWindowTitle(self, selector, wlist):
        hn = set()
        for n in wlist:
            if selector['windowtitle'] == n[0]:
                hn.add(n[1])  # 添加窗口索引到集合里
        if len(hn) == 0:
            return -1
        return hn",_98894.py,3,"for n in wlist:
    if selector['windowtitle'] == n[0]:
        hn.add(n[1])",hn = {n[1] for n in wlist if selector['windowtitle'] == n[0]},1,nan,nan
https://github.com/tanghaibao/jcvi/tree/master/jcvi/formats/gff.py,"def populate_children(outfile, ids, gffile, iter=""2"", types=None):
    ids = set(ids)
    fw = must_open(outfile, ""w"")
    logging.debug(""A total of {0} features selected."".format(len(ids)))
    logging.debug(""Populate children. Iteration 1.."")
    gff = Gff(gffile)
    children = set()
    for g in gff:
        if types and g.type in types:
            ids.add(g.accn)
        if ""Parent"" not in g.attributes:
            continue
        for parent in g.attributes[""Parent""]:
            if parent in ids:
                children.add(g.accn)

    if iter == ""2"":
        logging.debug(""Populate grand children. Iteration 2.."")
        gff = Gff(gffile)
        for g in gff:
            if ""Parent"" not in g.attributes:
                continue
            for parent in g.attributes[""Parent""]:
                if parent in children:
                    children.add(g.accn)

    logging.debug(""Populate parents.."")
    gff = Gff(gffile)
    parents = set()
    for g in gff:
        if g.accn not in ids:
            continue
        if ""Parent"" not in g.attributes:
            continue
        for parent in g.attributes[""Parent""]:
            parents.add(parent)

    combined = ids | children | parents
    logging.debug(""Original: {0}"".format(len(ids)))
    logging.debug(""Children: {0}"".format(len(children)))
    logging.debug(""Parents: {0}"".format(len(parents)))
    logging.debug(""Combined: {0}"".format(len(combined)))

    logging.debug(""Filter gff file.."")
    gff = Gff(gffile)
    seen = set()
    for g in gff:
        accn = g.accn
        if accn in seen:
            continue
        if accn in combined:
            seen.add(accn)
            print(g, file=fw)
    fw.close()",_99402.py,13,"for parent in g.attributes['Parent']:
    if parent in ids:
        children.add(g.accn)",children |= {g.accn for parent in g.attributes['Parent'] if parent in ids},1,nan,nan
https://github.com/tanghaibao/jcvi/tree/master/jcvi/formats/gff.py,"def populate_children(outfile, ids, gffile, iter=""2"", types=None):
    ids = set(ids)
    fw = must_open(outfile, ""w"")
    logging.debug(""A total of {0} features selected."".format(len(ids)))
    logging.debug(""Populate children. Iteration 1.."")
    gff = Gff(gffile)
    children = set()
    for g in gff:
        if types and g.type in types:
            ids.add(g.accn)
        if ""Parent"" not in g.attributes:
            continue
        for parent in g.attributes[""Parent""]:
            if parent in ids:
                children.add(g.accn)

    if iter == ""2"":
        logging.debug(""Populate grand children. Iteration 2.."")
        gff = Gff(gffile)
        for g in gff:
            if ""Parent"" not in g.attributes:
                continue
            for parent in g.attributes[""Parent""]:
                if parent in children:
                    children.add(g.accn)

    logging.debug(""Populate parents.."")
    gff = Gff(gffile)
    parents = set()
    for g in gff:
        if g.accn not in ids:
            continue
        if ""Parent"" not in g.attributes:
            continue
        for parent in g.attributes[""Parent""]:
            parents.add(parent)

    combined = ids | children | parents
    logging.debug(""Original: {0}"".format(len(ids)))
    logging.debug(""Children: {0}"".format(len(children)))
    logging.debug(""Parents: {0}"".format(len(parents)))
    logging.debug(""Combined: {0}"".format(len(combined)))

    logging.debug(""Filter gff file.."")
    gff = Gff(gffile)
    seen = set()
    for g in gff:
        accn = g.accn
        if accn in seen:
            continue
        if accn in combined:
            seen.add(accn)
            print(g, file=fw)
    fw.close()",_99402.py,35,"for parent in g.attributes['Parent']:
    parents.add(parent)",parents |= {parent for parent in g.attributes['Parent']},1,nan,nan
https://github.com/tanghaibao/jcvi/tree/master/jcvi/formats/gff.py,"def populate_children(outfile, ids, gffile, iter=""2"", types=None):
    ids = set(ids)
    fw = must_open(outfile, ""w"")
    logging.debug(""A total of {0} features selected."".format(len(ids)))
    logging.debug(""Populate children. Iteration 1.."")
    gff = Gff(gffile)
    children = set()
    for g in gff:
        if types and g.type in types:
            ids.add(g.accn)
        if ""Parent"" not in g.attributes:
            continue
        for parent in g.attributes[""Parent""]:
            if parent in ids:
                children.add(g.accn)

    if iter == ""2"":
        logging.debug(""Populate grand children. Iteration 2.."")
        gff = Gff(gffile)
        for g in gff:
            if ""Parent"" not in g.attributes:
                continue
            for parent in g.attributes[""Parent""]:
                if parent in children:
                    children.add(g.accn)

    logging.debug(""Populate parents.."")
    gff = Gff(gffile)
    parents = set()
    for g in gff:
        if g.accn not in ids:
            continue
        if ""Parent"" not in g.attributes:
            continue
        for parent in g.attributes[""Parent""]:
            parents.add(parent)

    combined = ids | children | parents
    logging.debug(""Original: {0}"".format(len(ids)))
    logging.debug(""Children: {0}"".format(len(children)))
    logging.debug(""Parents: {0}"".format(len(parents)))
    logging.debug(""Combined: {0}"".format(len(combined)))

    logging.debug(""Filter gff file.."")
    gff = Gff(gffile)
    seen = set()
    for g in gff:
        accn = g.accn
        if accn in seen:
            continue
        if accn in combined:
            seen.add(accn)
            print(g, file=fw)
    fw.close()",_99402.py,23,"for parent in g.attributes['Parent']:
    if parent in children:
        children.add(g.accn)",children |= {g.accn for parent in g.attributes['Parent'] if parent in children},1,nan,nan
https://github.com/dask/dask/tree/master/dask/blockwise.py,"def _get_coord_mapping(
    dims,
    output,
    out_indices,
    numblocks,
    argpairs,
    concatenate,
):
    """"""Calculate coordinate mapping for graph construction.

    This function handles the high-level logic behind Blockwise graph
    construction. The output is a tuple containing: The mapping between
    input and output block coordinates (`coord_maps`), the axes along
    which to concatenate for each input (`concat_axes`), and the dummy
    indices needed for broadcasting (`dummies`).

    Used by `make_blockwise_graph` and `Blockwise._cull_dependencies`.

    Parameters
    ----------
    dims : dict
        Mapping between each index specified in `argpairs` and
        the number of output blocks for that index. Corresponds
        to the Blockwise `dims` attribute.
    output : str
        Corresponds to the Blockwise `output` attribute.
    out_indices : tuple
        Corresponds to the Blockwise `output_indices` attribute.
    numblocks : dict
        Corresponds to the Blockwise `numblocks` attribute.
    argpairs : tuple
        Corresponds to the Blockwise `indices` attribute.
    concatenate : bool
        Corresponds to the Blockwise `concatenate` attribute.
    """"""

    block_names = set()
    all_indices = set()
    for name, ind in argpairs:
        if ind is not None:
            block_names.add(name)
            for x in ind:
                all_indices.add(x)
    assert set(numblocks) == block_names

    dummy_indices = all_indices - set(out_indices)

    # For each position in the output space, we'll construct a
    # ""coordinate set"" that consists of
    # - the output indices
    # - the dummy indices
    # - the dummy indices, with indices replaced by zeros (for broadcasting), we
    #   are careful to only emit a single dummy zero when concatenate=True to not
    #   concatenate the same array with itself several times.
    # - a 0 to assist with broadcasting.

    index_pos, zero_pos = {}, {}
    for i, ind in enumerate(out_indices):
        index_pos[ind] = i
        zero_pos[ind] = -1

    _dummies_list = []
    for i, ind in enumerate(dummy_indices):
        index_pos[ind] = 2 * i + len(out_indices)
        zero_pos[ind] = 2 * i + 1 + len(out_indices)
        reps = 1 if concatenate else dims[ind]
        _dummies_list.append([list(range(dims[ind])), [0] * reps])

    # ([0, 1, 2], [0, 0, 0], ...)  For a dummy index of dimension 3
    dummies = tuple(itertools.chain.from_iterable(_dummies_list))
    dummies += (0,)

    # For each coordinate position in each input, gives the position in
    # the coordinate set.
    coord_maps = []

    # Axes along which to concatenate, for each input
    concat_axes = []
    for arg, ind in argpairs:
        if ind is not None:
            coord_maps.append(
                [
                    zero_pos[i] if nb == 1 else index_pos[i]
                    for i, nb in zip(ind, numblocks[arg])
                ]
            )
            concat_axes.append([n for n, i in enumerate(ind) if i in dummy_indices])
        else:
            coord_maps.append(None)
            concat_axes.append(None)

    return coord_maps, concat_axes, dummies",_99457.py,42,"for x in ind:
    all_indices.add(x)",all_indices |= {x for x in ind},1,nan,nan
https://github.com/great-expectations/great_expectations/tree/master//versioneer.py,"def do_setup():
    """"""Main VCS-independent setup function for installing Versioneer.""""""
    root = get_root()
    try:
        cfg = get_config_from_root(root)
    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:
        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):
            print(""Adding sample versioneer config to setup.cfg"", file=sys.stderr)
            with open(os.path.join(root, ""setup.cfg""), ""a"") as f:
                f.write(SAMPLE_CONFIG)
        print(CONFIG_ERROR, file=sys.stderr)
        return 1

    print("" creating %s"" % cfg.versionfile_source)
    with open(cfg.versionfile_source, ""w"") as f:
        LONG = LONG_VERSION_PY[cfg.VCS]
        f.write(
            LONG
            % {
                ""DOLLAR"": ""$"",
                ""STYLE"": cfg.style,
                ""TAG_PREFIX"": cfg.tag_prefix,
                ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,
                ""VERSIONFILE_SOURCE"": cfg.versionfile_source,
            }
        )

    ipy = os.path.join(os.path.dirname(cfg.versionfile_source), ""__init__.py"")
    if os.path.exists(ipy):
        try:
            with open(ipy) as f:
                old = f.read()
        except OSError:
            old = """"
        if INIT_PY_SNIPPET not in old:
            print("" appending to %s"" % ipy)
            with open(ipy, ""a"") as f:
                f.write(INIT_PY_SNIPPET)
        else:
            print("" %s unmodified"" % ipy)
    else:
        print("" %s doesn't exist, ok"" % ipy)
        ipy = None

    # Make sure both the top-level ""versioneer.py"" and versionfile_source
    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so
    # they'll be copied into source distributions. Pip won't be able to
    # install the package without this.
    manifest_in = os.path.join(root, ""MANIFEST.in"")
    simple_includes = set()
    try:
        with open(manifest_in) as f:
            for line in f:
                if line.startswith(""include ""):
                    for include in line.split()[1:]:
                        simple_includes.add(include)
    except OSError:
        pass
    # That doesn't cover everything MANIFEST.in can do
    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so
    # it might give some false negatives. Appending redundant 'include'
    # lines is safe, though.
    if ""versioneer.py"" not in simple_includes:
        print("" appending 'versioneer.py' to MANIFEST.in"")
        with open(manifest_in, ""a"") as f:
            f.write(""include versioneer.py\n"")
    else:
        print("" 'versioneer.py' already in MANIFEST.in"")
    if cfg.versionfile_source not in simple_includes:
        print(
            "" appending versionfile_source ('%s') to MANIFEST.in""
            % cfg.versionfile_source
        )
        with open(manifest_in, ""a"") as f:
            f.write(""include %s\n"" % cfg.versionfile_source)
    else:
        print("" versionfile_source already in MANIFEST.in"")

    # Make VCS-specific changes. For git, this means creating/changing
    # .gitattributes to mark _version.py for export-subst keyword
    # substitution.
    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)
    return 0",_100574.py,53,"for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]},1,nan,nan
https://github.com/dedupeio/dedupe/tree/master/dedupe/convenience.py,"def training_data_dedupe(data: Data,
                         common_key: str,
                         training_size: int = 50000) -> TrainingData:  # pragma: nocover
    '''
    Construct training data for consumption by the func:`mark_pairs`
    method from an already deduplicated dataset.

    Args:

        data: Dictionary of records where the keys are record_ids and
              the values are dictionaries with the keys being field names
        common_key: The name of the record field that uniquely identifies
                    a match
        training_size: the rough limit of the number of training examples,
                       defaults to 50000

    .. note::

         Every match must be identified by the sharing of a common key.
         This function assumes that if two records do not share a common key
         then they are distinct records.
    '''

    identified_records: Dict[str, List[RecordID]]
    identified_records = collections.defaultdict(list)
    matched_pairs: Set[Tuple[RecordID, RecordID]] = set()
    distinct_pairs: Set[Tuple[RecordID, RecordID]] = set()
    unique_record_ids: Set[RecordID] = set()

    # a list of record_ids associated with each common_key
    for record_id, record in data.items():
        unique_record_ids.add(record_id)
        identified_records[record[common_key]].append(record_id)

    # all combinations of matched_pairs from each common_key group
    for record_ids in identified_records.values():
        if len(record_ids) > 1:
            matched_pairs.update(itertools.combinations(sorted(record_ids), 2))  # type: ignore

    # calculate indices using dedupe.core.randomPairs to avoid
    # the memory cost of enumerating all possible pairs
    unique_record_ids_l = list(unique_record_ids)
    pair_indices = randomPairs(len(unique_record_ids), training_size)
    distinct_pairs = set()
    for i, j in pair_indices:
        distinct_pairs.add((unique_record_ids_l[i],
                            unique_record_ids_l[j]))

    distinct_pairs -= matched_pairs

    matched_records = [(data[key_1], data[key_2])
                       for key_1, key_2 in matched_pairs]

    distinct_records = [(data[key_1], data[key_2])
                        for key_1, key_2 in distinct_pairs]

    training_pairs: TrainingData
    training_pairs = {'match': matched_records,
                      'distinct': distinct_records}

    return training_pairs",_103254.py,45,"for (i, j) in pair_indices:
    distinct_pairs.add((unique_record_ids_l[i], unique_record_ids_l[j]))","distinct_pairs = {(unique_record_ids_l[i], unique_record_ids_l[j]) for (i, j) in pair_indices}",1,nan,nan
https://github.com/momosecurity/aswan/tree/master/www/rule/views.py,"def _get_menu_strategy_args(self, uuids, client):
        ret = set()
        dimensions = self._get_one_kind_fields_from_uuids(
            uuids, 'strategy_menu', 'dimension', client
        )
        for name in dimensions:
            ret.add(name)
        return ret",_107118.py,6,"for name in dimensions:
    ret.add(name)",ret = {name for name in dimensions},1,nan,nan
https://github.com/soxoj/maigret/tree/master/tests/test_data.py,"def test_tags_validity(default_db):
    unknown_tags = set()

    tags = default_db._tags

    for site in default_db.sites:
        for tag in filter(lambda x: not is_country_tag(x), site.tags):
            if tag not in tags:
                unknown_tags.add(tag)

    assert unknown_tags == set()",_110320.py,6,"for site in default_db.sites:
    for tag in filter(lambda x: not is_country_tag(x), site.tags):
        if tag not in tags:
            unknown_tags.add(tag)","unknown_tags = {tag for site in default_db.sites for tag in filter(lambda x: not is_country_tag(x), site.tags) if tag not in tags}",1,nan,nan
https://github.com/saltstack/salt/tree/master/salt/states/supervisord.py,"def running(
    name, restart=False, update=False, user=None, conf_file=None, bin_env=None, **kwargs
):
    """"""
    Ensure the named service is running.

    name
        Service name as defined in the supervisor configuration file

    restart
        Whether to force a restart

    update
        Whether to update the supervisor configuration.

    user
        Name of the user to run the supervisorctl command

        .. versionadded:: 0.17.0

    conf_file
        path to supervisorctl config file

    bin_env
        path to supervisorctl bin or path to virtualenv with supervisor
        installed

    """"""
    if name.endswith("":*""):
        name = name[:-1]

    ret = {""name"": name, ""result"": True, ""comment"": """", ""changes"": {}}

    if ""supervisord.status"" not in __salt__:
        ret[""result""] = False
        ret[
            ""comment""
        ] = ""Supervisord module not activated. Do you need to install supervisord?""
        return ret

    all_processes = __salt__[""supervisord.status""](
        user=user, conf_file=conf_file, bin_env=bin_env
    )

    # parse process groups
    process_groups = set()
    for proc in all_processes:
        if "":"" in proc:
            process_groups.add(proc[: proc.index("":"") + 1])
    process_groups = sorted(process_groups)

    matches = {}
    if name in all_processes:
        matches[name] = all_processes[name][""state""].lower() == ""running""
    elif name in process_groups:
        for process in (x for x in all_processes if x.startswith(name)):
            matches[process] = all_processes[process][""state""].lower() == ""running""
    to_add = not bool(matches)

    if __opts__[""test""]:
        if not to_add:
            # Process/group already present, check if any need to be started
            to_start = [x for x, y in matches.items() if y is False]
            if to_start:
                ret[""result""] = None
                if name.endswith("":""):
                    # Process group
                    if len(to_start) == len(matches):
                        ret[
                            ""comment""
                        ] = ""All services in group '{}' will be started"".format(name)
                    else:
                        ret[
                            ""comment""
                        ] = ""The following services will be started: {}"".format(
                            "" "".join(to_start)
                        )
                else:
                    # Single program
                    ret[""comment""] = ""Service {} will be started"".format(name)
            else:
                if name.endswith("":""):
                    # Process group
                    ret[
                        ""comment""
                    ] = ""All services in group '{}' are already running"".format(name)
                else:
                    ret[""comment""] = ""Service {} is already running"".format(name)
        else:
            ret[""result""] = None
            # Process/group needs to be added
            if name.endswith("":""):
                _type = ""Group '{}'"".format(name)
            else:
                _type = ""Service {}"".format(name)
            ret[""comment""] = ""{} will be added and started"".format(_type)
        return ret

    changes = []
    just_updated = False

    if update:
        # If the state explicitly asks to update, we don't care if the process
        # is being added or not, since it'll take care of this for us,
        # so give this condition priority in order
        #
        # That is, unless `to_add` somehow manages to contain processes
        # we don't want running, in which case adding them may be a mistake
        comment = ""Updating supervisor""
        result = __salt__[""supervisord.update""](
            user=user, conf_file=conf_file, bin_env=bin_env
        )
        ret.update(_check_error(result, comment))
        log.debug(comment)

        if ""{}: updated"".format(name) in result:
            just_updated = True
    elif to_add:
        # Not sure if this condition is precise enough.
        comment = ""Adding service: {}"".format(name)
        __salt__[""supervisord.reread""](user=user, conf_file=conf_file, bin_env=bin_env)
        # Causes supervisorctl to throw `ERROR: process group already active`
        # if process group exists. At this moment, I'm not sure how to handle
        # this outside of grepping out the expected string in `_check_error`.
        result = __salt__[""supervisord.add""](
            name, user=user, conf_file=conf_file, bin_env=bin_env
        )

        ret.update(_check_error(result, comment))
        changes.append(comment)
        log.debug(comment)

    is_stopped = None

    process_type = None
    if name in process_groups:
        process_type = ""group""

        # check if any processes in this group are stopped
        is_stopped = False
        for proc in all_processes:
            if proc.startswith(name) and _is_stopped_state(
                all_processes[proc][""state""]
            ):
                is_stopped = True
                break

    elif name in all_processes:
        process_type = ""service""

        if _is_stopped_state(all_processes[name][""state""]):
            is_stopped = True
        else:
            is_stopped = False

    if is_stopped is False:
        if restart and not just_updated:
            comment = ""Restarting{}: {}"".format(
                process_type is not None and "" {}"".format(process_type) or """", name
            )
            log.debug(comment)
            result = __salt__[""supervisord.restart""](
                name, user=user, conf_file=conf_file, bin_env=bin_env
            )
            ret.update(_check_error(result, comment))
            changes.append(comment)
        elif just_updated:
            comment = ""Not starting updated{}: {}"".format(
                process_type is not None and "" {}"".format(process_type) or """", name
            )
            result = comment
            ret.update({""comment"": comment})
        else:
            comment = ""Not starting already running{}: {}"".format(
                process_type is not None and "" {}"".format(process_type) or """", name
            )
            result = comment
            ret.update({""comment"": comment})

    elif not just_updated:
        comment = ""Starting{}: {}"".format(
            process_type is not None and "" {}"".format(process_type) or """", name
        )
        changes.append(comment)
        log.debug(comment)
        result = __salt__[""supervisord.start""](
            name, user=user, conf_file=conf_file, bin_env=bin_env
        )

        ret.update(_check_error(result, comment))
        log.debug(str(result))

    if ret[""result""] and changes:
        ret[""changes""][name] = "" "".join(changes)
    return ret",_112317.py,47,"for proc in all_processes:
    if ':' in proc:
        process_groups.add(proc[:proc.index(':') + 1])",process_groups = {proc[:proc.index(':') + 1] for proc in all_processes if ':' in proc},1,nan,nan
https://github.com/thunlp/THULAC-Python/tree/master/thulac/manage/TimeWord.py,"def __init__(self):
        self.__arabicNumSet = set()
        self.__timeWordSet = set()
        self.__otherSet = set()
        timeWord = {24180, 26376, 26085, 21495, 26102, 28857, 20998, 31186}
        for i in range(48, 58):
            self.__arabicNumSet.add(i)
        for i in range(65296, 65306):
            self.__arabicNumSet.add(i)
        timeWord = {24180, 26376, 26085, 21495, 26102, 28857, 20998, 31186}
        self.__timeWordSet = self.__timeWordSet | timeWord
        for i in range(65, 91):
            self.__otherSet.add(i)
        for i in range(97, 123):
            self.__otherSet.add(i)
        for i in range(48, 58):
            self.__otherSet.add(i)

        other = {65292, 12290, 65311, 65281, 65306, 65307, 8216, 8217, 8220, 8221, 12304, 12305,
                        12289, 12298, 12299, 126, 183, 64, 124, 35, 65509, 37, 8230, 38, 42, 65288,
                        65289, 8212, 45, 43, 61, 44, 46, 60, 62, 63, 47, 33, 59, 58, 39, 34, 123, 125,
                        91, 93, 92, 124, 35, 36, 37, 94, 38, 42, 40, 41, 95, 45, 43, 61, 9700, 9734, 9733}
        self.__otherSet = self.__otherSet | other",_112441.py,6,"for i in range(48, 58):
    self.__arabicNumSet.add(i)","self.__arabicNumSet |= {i for i in range(48, 58)}",1,nan,nan
https://github.com/thunlp/THULAC-Python/tree/master/thulac/manage/TimeWord.py,"def __init__(self):
        self.__arabicNumSet = set()
        self.__timeWordSet = set()
        self.__otherSet = set()
        timeWord = {24180, 26376, 26085, 21495, 26102, 28857, 20998, 31186}
        for i in range(48, 58):
            self.__arabicNumSet.add(i)
        for i in range(65296, 65306):
            self.__arabicNumSet.add(i)
        timeWord = {24180, 26376, 26085, 21495, 26102, 28857, 20998, 31186}
        self.__timeWordSet = self.__timeWordSet | timeWord
        for i in range(65, 91):
            self.__otherSet.add(i)
        for i in range(97, 123):
            self.__otherSet.add(i)
        for i in range(48, 58):
            self.__otherSet.add(i)

        other = {65292, 12290, 65311, 65281, 65306, 65307, 8216, 8217, 8220, 8221, 12304, 12305,
                        12289, 12298, 12299, 126, 183, 64, 124, 35, 65509, 37, 8230, 38, 42, 65288,
                        65289, 8212, 45, 43, 61, 44, 46, 60, 62, 63, 47, 33, 59, 58, 39, 34, 123, 125,
                        91, 93, 92, 124, 35, 36, 37, 94, 38, 42, 40, 41, 95, 45, 43, 61, 9700, 9734, 9733}
        self.__otherSet = self.__otherSet | other",_112441.py,8,"for i in range(65296, 65306):
    self.__arabicNumSet.add(i)","self.__arabicNumSet |= {i for i in range(65296, 65306)}",1,nan,nan
https://github.com/thunlp/THULAC-Python/tree/master/thulac/manage/TimeWord.py,"def __init__(self):
        self.__arabicNumSet = set()
        self.__timeWordSet = set()
        self.__otherSet = set()
        timeWord = {24180, 26376, 26085, 21495, 26102, 28857, 20998, 31186}
        for i in range(48, 58):
            self.__arabicNumSet.add(i)
        for i in range(65296, 65306):
            self.__arabicNumSet.add(i)
        timeWord = {24180, 26376, 26085, 21495, 26102, 28857, 20998, 31186}
        self.__timeWordSet = self.__timeWordSet | timeWord
        for i in range(65, 91):
            self.__otherSet.add(i)
        for i in range(97, 123):
            self.__otherSet.add(i)
        for i in range(48, 58):
            self.__otherSet.add(i)

        other = {65292, 12290, 65311, 65281, 65306, 65307, 8216, 8217, 8220, 8221, 12304, 12305,
                        12289, 12298, 12299, 126, 183, 64, 124, 35, 65509, 37, 8230, 38, 42, 65288,
                        65289, 8212, 45, 43, 61, 44, 46, 60, 62, 63, 47, 33, 59, 58, 39, 34, 123, 125,
                        91, 93, 92, 124, 35, 36, 37, 94, 38, 42, 40, 41, 95, 45, 43, 61, 9700, 9734, 9733}
        self.__otherSet = self.__otherSet | other",_112441.py,12,"for i in range(65, 91):
    self.__otherSet.add(i)","self.__otherSet |= {i for i in range(65, 91)}",1,nan,nan
https://github.com/thunlp/THULAC-Python/tree/master/thulac/manage/TimeWord.py,"def __init__(self):
        self.__arabicNumSet = set()
        self.__timeWordSet = set()
        self.__otherSet = set()
        timeWord = {24180, 26376, 26085, 21495, 26102, 28857, 20998, 31186}
        for i in range(48, 58):
            self.__arabicNumSet.add(i)
        for i in range(65296, 65306):
            self.__arabicNumSet.add(i)
        timeWord = {24180, 26376, 26085, 21495, 26102, 28857, 20998, 31186}
        self.__timeWordSet = self.__timeWordSet | timeWord
        for i in range(65, 91):
            self.__otherSet.add(i)
        for i in range(97, 123):
            self.__otherSet.add(i)
        for i in range(48, 58):
            self.__otherSet.add(i)

        other = {65292, 12290, 65311, 65281, 65306, 65307, 8216, 8217, 8220, 8221, 12304, 12305,
                        12289, 12298, 12299, 126, 183, 64, 124, 35, 65509, 37, 8230, 38, 42, 65288,
                        65289, 8212, 45, 43, 61, 44, 46, 60, 62, 63, 47, 33, 59, 58, 39, 34, 123, 125,
                        91, 93, 92, 124, 35, 36, 37, 94, 38, 42, 40, 41, 95, 45, 43, 61, 9700, 9734, 9733}
        self.__otherSet = self.__otherSet | other",_112441.py,14,"for i in range(97, 123):
    self.__otherSet.add(i)","self.__otherSet |= {i for i in range(97, 123)}",1,nan,nan
https://github.com/thunlp/THULAC-Python/tree/master/thulac/manage/TimeWord.py,"def __init__(self):
        self.__arabicNumSet = set()
        self.__timeWordSet = set()
        self.__otherSet = set()
        timeWord = {24180, 26376, 26085, 21495, 26102, 28857, 20998, 31186}
        for i in range(48, 58):
            self.__arabicNumSet.add(i)
        for i in range(65296, 65306):
            self.__arabicNumSet.add(i)
        timeWord = {24180, 26376, 26085, 21495, 26102, 28857, 20998, 31186}
        self.__timeWordSet = self.__timeWordSet | timeWord
        for i in range(65, 91):
            self.__otherSet.add(i)
        for i in range(97, 123):
            self.__otherSet.add(i)
        for i in range(48, 58):
            self.__otherSet.add(i)

        other = {65292, 12290, 65311, 65281, 65306, 65307, 8216, 8217, 8220, 8221, 12304, 12305,
                        12289, 12298, 12299, 126, 183, 64, 124, 35, 65509, 37, 8230, 38, 42, 65288,
                        65289, 8212, 45, 43, 61, 44, 46, 60, 62, 63, 47, 33, 59, 58, 39, 34, 123, 125,
                        91, 93, 92, 124, 35, 36, 37, 94, 38, 42, 40, 41, 95, 45, 43, 61, 9700, 9734, 9733}
        self.__otherSet = self.__otherSet | other",_112441.py,16,"for i in range(48, 58):
    self.__otherSet.add(i)","self.__otherSet |= {i for i in range(48, 58)}",1,nan,nan
https://github.com/nameko/nameko/tree/master/nameko/cli/actions.py,"def __init__(self, option_strings, dest, default=None,
                 required=False, help=None, metavar=None,
                 positive_prefixes=['--'], negative_prefixes=['--no-']):
        self.positive_strings = set()
        self.negative_strings = set()
        for string in option_strings:
            assert re.match(r'--[A-z]+', string)
            suffix = string[2:]
            for positive_prefix in positive_prefixes:
                self.positive_strings.add(positive_prefix + suffix)
            for negative_prefix in negative_prefixes:
                self.negative_strings.add(negative_prefix + suffix)
        strings = list(self.positive_strings | self.negative_strings)
        super(FlagAction, self).__init__(
            option_strings=strings, dest=dest,
            nargs=0, const=None, default=default, type=bool, choices=None,
            required=required, help=help, metavar=metavar)",_113951.py,9,"for positive_prefix in positive_prefixes:
    self.positive_strings.add(positive_prefix + suffix)",self.positive_strings |= {positive_prefix + suffix for positive_prefix in positive_prefixes},1,nan,nan
https://github.com/nameko/nameko/tree/master/nameko/cli/actions.py,"def __init__(self, option_strings, dest, default=None,
                 required=False, help=None, metavar=None,
                 positive_prefixes=['--'], negative_prefixes=['--no-']):
        self.positive_strings = set()
        self.negative_strings = set()
        for string in option_strings:
            assert re.match(r'--[A-z]+', string)
            suffix = string[2:]
            for positive_prefix in positive_prefixes:
                self.positive_strings.add(positive_prefix + suffix)
            for negative_prefix in negative_prefixes:
                self.negative_strings.add(negative_prefix + suffix)
        strings = list(self.positive_strings | self.negative_strings)
        super(FlagAction, self).__init__(
            option_strings=strings, dest=dest,
            nargs=0, const=None, default=default, type=bool, choices=None,
            required=required, help=help, metavar=metavar)",_113951.py,11,"for negative_prefix in negative_prefixes:
    self.negative_strings.add(negative_prefix + suffix)",self.negative_strings |= {negative_prefix + suffix for negative_prefix in negative_prefixes},1,nan,nan
https://github.com/saltstack/salt/tree/master/salt/modules/win_service.py,"def get_enabled():
    """"""
    Return a list of enabled services. Enabled is defined as a service that is
    marked to Auto Start.

    Returns:
        list: A list of enabled services

    CLI Example:

    .. code-block:: bash

        salt '*' service.get_enabled
    """"""
    raw_services = _get_services()
    services = set()
    for service in raw_services:
        if info(service[""ServiceName""])[""StartType""] in [""Auto""]:
            services.add(service[""ServiceName""])

    return sorted(services)",_115901.py,17,"for service in raw_services:
    if info(service['ServiceName'])['StartType'] in ['Auto']:
        services.add(service['ServiceName'])",services = {service['ServiceName'] for service in raw_services if info(service['ServiceName'])['StartType'] in ['Auto']},1,nan,nan
https://github.com/chainer/chainer/tree/master/chainer/graph_optimizations/static_graph.py,"def append_function(self, func, args, kwargs, func_name=None):
        """"""Append a function to the static schedule.

        Append a function `func` to the static schedule. `func` can
        be any function that is decorated with `@static_code` and that
        was called while executing the static chain's `__call___()`
        method, which contains the define-by-run code. The code
        in the `@static_code` decorator will call this method to
        add the function to the schedule just after it executes in
        the define-by-run code as follows:

        `return_arrays = func(*args, **kwargs)`

        During the next iteration when the static chain switches from define-
        by-run to the static schedule, a corresponding `ScheduleInfo`
        object will call `func` as above, except that the scheduler might
        make modifications
        to some of the arrays in `kwargs` before and after the function is
        called to implement various memory optimizations.

        Args:
            func (function or method): The function to append to the schedule.
                This is a function that was decorated with `@static_code`.
            args: The arguments that were originally supplied to `func` in
                the define-by-run code of the static chain.
            kwargs: The keyword arguments that were originally supplied to
                `func` in the define-by-run code of the static chain.
            func_name (str): Optional name for `func`, for debugging
                purposes.
            return_arrays (tuple of ndarray) or None: The value that is
                returned by `func`, if any.

        """"""

        # Check previous function in the schedule, if available.
        # Check the arrays in the retained inputs/outputs and force them
        # to remain statically allocated in the schedule.
        # ids of any retained arrays.
        retained_ids = set()

        last_sched_info_ind = len(self.schedule_info_list) - 1
        if last_sched_info_ind >= 0:
            prev_sched_info = self.schedule_info_list[last_sched_info_ind]
            if prev_sched_info.function_node is not None:
                # get retained inputs/outputs.
                retained_in_vars = \
                    prev_sched_info.function_node.get_retained_inputs()
                retained_out_vars = \
                    prev_sched_info.function_node.get_retained_outputs()
                if (retained_in_vars is not None and
                        retained_out_vars is not None):
                    retained_vars = retained_in_vars + retained_out_vars
                elif retained_in_vars is not None:
                    retained_vars = retained_in_vars
                elif retained_out_vars is not None:
                    retained_vars = retained_out_vars
                else:
                    retained_vars = None
                if retained_vars is not None:
                    for var in retained_vars:
                        retained_ids.add(id(var.data))

        for keep_id in retained_ids:
            unique_ind = self.array_id_to_unique_index[keep_id]
            array_info = self.unique_array_infos[unique_ind]
            array_info.retain = True
            # Note: the following line is not actually needed.
            # array_info.array = array_info.weak_ref()

        delete_hooks = []
        for unique_ind, ar_info in enumerate(self.unique_array_infos):
            # todo: this is O(N^2) and maybe too slow for large graphs.
            # Optimize it later.
            if ar_info.was_deleted():
                if ar_info.dynamic_deletion_index is None:
                    if self.verbosity_level >= 2:
                        print('Adding delete hook:')
                    delete_hooks.append(unique_ind)
                    ar_info.dynamic_deletion_index = last_sched_info_ind + 1
                    ar_info.dynamic_deletion_pass_depth = self.pass_depth

        # Call the `@static_code`-decorated function.
        ret = func(*args, **kwargs)

        inputs_hooks = []
        if 'inputs' in kwargs:
            in_list = kwargs['inputs']
            assert isinstance(in_list, list)
            for ind, x in enumerate(in_list):
                if _is_xp(x):
                    unique_ind = self.get_unique_index_from_array(x)
                    if unique_ind is None:
                        # Note: we append None here because we cannot store any
                        # additional reference to the array.
                        # Otherwise, it would
                        # prevent garbage collection. Note that a
                        # weak reference
                        # will be stored in the ArrayInfo below.
                        self.unique_arrays.append(None)
                        self.unique_array_infos.append(ArrayInfo(x))
                        unique_ind = len(self.unique_arrays) - 1
                        self.array_id_to_unique_index[id(x)] = unique_ind
                    inputs_hooks.append((ind, unique_ind))
                    # Now that the hook has been added, we can delete
                    # array reference from 'args'.
                    in_list[ind] = None

        outputs_hooks = []
        if 'outputs' in kwargs:
            out_list = kwargs['outputs']
            assert isinstance(out_list, list)
            for ind, x in enumerate(out_list):
                if _is_xp(x):
                    unique_ind = self.get_unique_index_from_array(x)
                    if unique_ind is None:
                        self.unique_arrays.append(x)
                        # todo: enable the following line instead once the
                        # auto-intializing hooks are added. This will further
                        # reduce memory usage.
                        # self.unique_arrays.append(None)
                        self.unique_array_infos.append(ArrayInfo(x))
                        unique_ind = len(self.unique_arrays) - 1
                        self.array_id_to_unique_index[id(x)] = unique_ind
                    outputs_hooks.append((ind, unique_ind))
                    # Now that the hook has been added, we can delete
                    # array reference from 'args'.
                    out_list[ind] = None

        # A list of hooks (each is a tuple) that will be used to set
        # correct array references in 'unique_arrays' after executing
        # the static schedule function 'func'. These hooks update
        # the references in 'unique_arrays' to refer to the arrays
        # that were dynamically allocated in the return value of
        # 'func'.
        return_hooks = []
        if ret is not None:
            assert (isinstance(ret, list) or
                    isinstance(ret, tuple))
            for ret_index, item in enumerate(ret):
                if _is_xp(item):
                    # note: id might not be unique if objects have been
                    # garbage collected.
                    item_id = id(item)
                    unique_index = self.get_unique_index_from_array(item)
                    if unique_index is None:
                        # Note: Append None instead of 'item' to prevent an
                        # extra reference from being stored. Otherwise it
                        # would prevent garbage collection.
                        self.unique_arrays.append(None)
                        ar_info = ArrayInfo(item)
                        ar_info.dynamically_allocated = True
                        sched_info_ind = len(self.schedule_info_list)
                        ar_info.dynamic_allocation_index = sched_info_ind
                        ar_info.dynamic_allocation_pass_depth = self.pass_depth
                        self.unique_array_infos.append(ar_info)
                        unique_index = len(self.unique_arrays) - 1
                        self.array_id_to_unique_index[item_id] = \
                            unique_index
                    else:
                        # Since all of the return arrays are supposed to
                        # have been dynamically allocated inside 'func',
                        # they had better not already be in unique_arrays.
                        # If so, it is an error.
                        unique_index = self.array_id_to_unique_index[item_id]
                        print('the current id: ', item_id)
                        print('the unique_index: ', unique_index)
                        print('array info: ',
                              self.unique_array_infos[unique_ind])
                        raise RuntimeError('Found result array from schedule '
                                           'function already in '
                                           'unique_arrays!')
                    return_hooks.append((ret_index, unique_index))
                    self.dynamically_allocated_unique_index.add(unique_index)

        if self.verbosity_level >= 2:
            print('Adding function to static schedule: ', func)

        self.schedule_info_list.append(ScheduleInfo(func, args, kwargs,
                                                    inputs_hooks,
                                                    outputs_hooks,
                                                    return_hooks,
                                                    delete_hooks,
                                                    self.unique_arrays,
                                                    self.unique_array_infos,
                                                    func_name=func_name))

        return ret",_119442.py,60,"for var in retained_vars:
    retained_ids.add(id(var.data))",retained_ids = {id(var.data) for var in retained_vars},1,nan,nan
https://github.com/tanghaibao/goatools/tree/master/goatools/anno/init/reader_gpad.py,"def _get_qualifier(valstr):
        """"""Get qualifiers. Correct for inconsistent capitalization in GAF files""""""
        quals = set()
        if valstr == '':
            return quals
        #### # https://github.com/geneontology/go-annotation/issues/2885
        #### if valstr[-1] == '|':
        ####     valstr = valstr[:-1]
        for val in valstr.split('|'):
            val = val.lower()
            quals.add(val if val != 'not' else 'NOT')
        return quals",_121329.py,9,"for val in valstr.split('|'):
    val = val.lower()
    quals.add(val if val != 'not' else 'NOT')",quals |= {val.lower() if val.lower() != 'not' else 'NOT' for val in valstr.split('|')},1,nan,nan
https://github.com/DataDog/integrations-core/tree/master/amazon_msk/tests/test_e2e.py,"def assert_jmx_metrics(aggregator, tags):
    expected_metrics = set()

    for raw_metric_name, metric_name in JMX_METRICS_MAP.items():
        if raw_metric_name.endswith('_total') and raw_metric_name not in JMX_METRICS_OVERRIDES:
            expected_metrics.add('{}.count'.format(metric_name[:-6]))
        else:
            expected_metrics.add(metric_name)

    expected_metrics.update(METRICS_FROM_LABELS)
    expected_metrics.update(data['legacy_name'] for data in METRICS_WITH_NAME_AS_LABEL.values())

    for metric in sorted(expected_metrics):
        metric = 'aws.msk.{}'.format(metric)
        for tag in tags:
            aggregator.assert_metric_has_tag(metric, tag)",_122321.py,4,"for (raw_metric_name, metric_name) in JMX_METRICS_MAP.items():
    if raw_metric_name.endswith('_total') and raw_metric_name not in JMX_METRICS_OVERRIDES:
        expected_metrics.add('{}.count'.format(metric_name[:-6]))
    else:
        expected_metrics.add(metric_name)","expected_metrics = {'{}.count'.format(metric_name[:-6]) if raw_metric_name.endswith('_total') and raw_metric_name not in JMX_METRICS_OVERRIDES else metric_name for (raw_metric_name, metric_name) in JMX_METRICS_MAP.items()}",1,nan,nan
https://github.com/tonquer/picacg-qt/tree/master/src/view/download/download_view.py,"def DelRecordingAndFile(self):
        selected = self.tableWidget.selectedIndexes()
        selectRows = set()
        for index in selected:
            selectRows.add(index.row())
        if not selectRows:
            return
        try:
            for row in sorted(selectRows, reverse=True):
                col = 0
                bookId = self.tableWidget.item(row, col).text()
                bookInfo = self.downloadDict.get(bookId)
                if not bookInfo:
                    continue
                self.RemoveRecord(bookId)
                path = os.path.dirname(bookInfo.savePath)
                if os.path.isdir(path):
                    shutil.rmtree(path, True)

        except Exception as es:
            Log.Error(es)
        self.UpdateTableRow()",_123454.py,4,"for index in selected:
    selectRows.add(index.row())",selectRows = {index.row() for index in selected},1,nan,nan
https://github.com/magenta/magenta/tree/master/magenta/models/polyphony_rnn/polyphony_lib.py,"def extract_polyphonic_sequences(
    quantized_sequence, start_step=0, min_steps_discard=None,
    max_steps_discard=None):
  """"""Extracts a polyphonic track from the given quantized NoteSequence.

  Currently, this extracts only one polyphonic sequence from a given track.

  Args:
    quantized_sequence: A quantized NoteSequence.
    start_step: Start extracting a sequence at this time step. Assumed
        to be the beginning of a bar.
    min_steps_discard: Minimum length of tracks in steps. Shorter tracks are
        discarded.
    max_steps_discard: Maximum length of tracks in steps. Longer tracks are
        discarded.

  Returns:
    poly_seqs: A python list of PolyphonicSequence instances.
    stats: A dictionary mapping string names to `statistics.Statistic` objects.
  """"""
  sequences_lib.assert_is_relative_quantized_sequence(quantized_sequence)

  stats = dict((stat_name, statistics.Counter(stat_name)) for stat_name in
               ['polyphonic_tracks_discarded_too_short',
                'polyphonic_tracks_discarded_too_long',
                'polyphonic_tracks_discarded_more_than_1_program'])

  steps_per_bar = sequences_lib.steps_per_bar_in_quantized_sequence(
      quantized_sequence)

  # Create a histogram measuring lengths (in bars not steps).
  stats['polyphonic_track_lengths_in_bars'] = statistics.Histogram(
      'polyphonic_track_lengths_in_bars',
      [0, 1, 10, 20, 30, 40, 50, 100, 200, 500, 1000])

  # Allow only 1 program.
  programs = set()
  for note in quantized_sequence.notes:
    programs.add(note.program)
  if len(programs) > 1:
    stats['polyphonic_tracks_discarded_more_than_1_program'].increment()
    return [], stats.values()

  # Translate the quantized sequence into a PolyphonicSequence.
  poly_seq = PolyphonicSequence(quantized_sequence,
                                start_step=start_step)

  poly_seqs = []
  num_steps = poly_seq.num_steps

  if min_steps_discard is not None and num_steps < min_steps_discard:
    stats['polyphonic_tracks_discarded_too_short'].increment()
  elif max_steps_discard is not None and num_steps > max_steps_discard:
    stats['polyphonic_tracks_discarded_too_long'].increment()
  else:
    poly_seqs.append(poly_seq)
    stats['polyphonic_track_lengths_in_bars'].increment(
        num_steps // steps_per_bar)

  return poly_seqs, stats.values()",_124476.py,38,"for note in quantized_sequence.notes:
    programs.add(note.program)",programs = {note.program for note in quantized_sequence.notes},1,nan,nan
https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/productionbuilder.py,"def _handle_lumberjack_removal(self, building):
		""""""Release the unused trees around the lumberjack building being removed.""""""
		trees_used_by_others = set()
		for lumberjack_building in self.settlement.buildings_by_id.get(BUILDINGS.LUMBERJACK, []):
			if lumberjack_building.worldid == building.worldid:
				continue
			for coords in lumberjack_building.position.get_radius_coordinates(lumberjack_building.radius):
				if coords in self.plan and self.plan[coords][0] == BUILDING_PURPOSE.TREE:
					trees_used_by_others.add(coords)

		coords_list = []
		for coords in building.position.get_radius_coordinates(building.radius):
			if coords not in trees_used_by_others and coords in self.plan and self.plan[coords][0] == BUILDING_PURPOSE.TREE:
				coords_list.append(coords)
		self.register_change_list(coords_list, BUILDING_PURPOSE.NONE, None)",_126438.py,7,"for coords in lumberjack_building.position.get_radius_coordinates(lumberjack_building.radius):
    if coords in self.plan and self.plan[coords][0] == BUILDING_PURPOSE.TREE:
        trees_used_by_others.add(coords)",trees_used_by_others |= {coords for coords in lumberjack_building.position.get_radius_coordinates(lumberjack_building.radius) if coords in self.plan and self.plan[coords][0] == BUILDING_PURPOSE.TREE},1,nan,nan
https://github.com/angr/angr/tree/master/angr/analyses/variable_recovery/variable_recovery_base.py,"def _make_phi_variable(self, values: Set[claripy.ast.Base]) -> Optional[claripy.ast.Base]:
        # we only create a new phi variable if the there is at least one variable involved
        variables = set()
        bits: Optional[int] = None
        for v in values:
            bits = v.size()
            for _, var in self.extract_variables(v):
                variables.add(var)

        if len(variables) <= 1:
            return None

        assert self.successor_block_addr is not None

        # find existing phi variables
        phi_var = self.variable_manager[self.function.addr].make_phi_node(self.successor_block_addr, *variables)
        for var in variables:
            if var is not phi_var:
                self.phi_variables[var] = phi_var

        r = self.top(bits)
        r = self.annotate_with_variables(r, [(0, phi_var)])
        return r",_127847.py,7,"for (_, var) in self.extract_variables(v):
    variables.add(var)","variables |= {var for (_, var) in self.extract_variables(v)}",1,nan,nan
https://github.com/datastax/python-driver/tree/master/tests/integration/standard/test_cluster.py,"def _assert_replica_queried(self, trace, only_replicas=True):
        queried_hosts = set()
        for row in trace.events:
            queried_hosts.add(row.source)
        if only_replicas:
            self.assertEqual(len(queried_hosts), 1, ""The hosts queried where {}"".format(queried_hosts))
        else:
            self.assertGreater(len(queried_hosts), 1, ""The host queried was {}"".format(queried_hosts))
        return queried_hosts",_131347.py,3,"for row in trace.events:
    queried_hosts.add(row.source)",queried_hosts = {row.source for row in trace.events},1,nan,nan
https://github.com/PyCQA/pydocstyle/tree/master/src/pydocstyle/checker.py,"def _check_parameters_section(docstring, definition, context):
        """"""D417: `Parameters` section check for numpy style.

        Check for a valid `Parameters` section. Checks that:
            * The section documents all function arguments (D417)
                except `self` or `cls` if it is a method.

        """"""
        docstring_args = set()
        section_level_indent = leading_space(context.line)
        # Join line continuations, then resplit by line.
        content = (
            '\n'.join(context.following_lines).replace('\\\n', '').split('\n')
        )
        for current_line, next_line in zip(content, content[1:]):
            # All parameter definitions in the Numpy parameters
            # section must be at the same indent level as the section
            # name.
            # Also, we ensure that the following line is indented,
            # and has some string, to ensure that the parameter actually
            # has a description.
            # This means, this is a parameter doc with some description
            if (
                (leading_space(current_line) == section_level_indent)
                and (
                    len(leading_space(next_line))
                    > len(leading_space(current_line))
                )
                and next_line.strip()
            ):
                # In case the parameter has type definitions, it
                # will have a colon
                if "":"" in current_line:
                    parameters, parameter_type = current_line.split("":"", 1)
                # Else, we simply have the list of parameters defined
                # on the current line.
                else:
                    parameters = current_line.strip()
                # Numpy allows grouping of multiple parameters of same
                # type in the same line. They are comma separated.
                parameter_list = parameters.split("","")
                for parameter in parameter_list:
                    docstring_args.add(parameter.strip())
        yield from ConventionChecker._check_missing_args(
            docstring_args, definition
        )",_131913.py,42,"for parameter in parameter_list:
    docstring_args.add(parameter.strip())",docstring_args |= {parameter.strip() for parameter in parameter_list},1,nan,nan
https://github.com/redhat-performance/tuned/tree/master/tuned/plugins/plugin_scsi_host.py,"def _init_devices(self):
		super(SCSIHostPlugin, self)._init_devices()
		self._devices_supported = True
		self._free_devices = set()
		for device in self._hardware_inventory.get_devices(""scsi""):
			if self._device_is_supported(device):
				self._free_devices.add(device.sys_name)

		self._assigned_devices = set()",_136053.py,5,"for device in self._hardware_inventory.get_devices('scsi'):
    if self._device_is_supported(device):
        self._free_devices.add(device.sys_name)",self._free_devices = {device.sys_name for device in self._hardware_inventory.get_devices('scsi') if self._device_is_supported(device)},1,nan,nan
https://github.com/openstack/swift/tree/master/test/unit/obj/test_ssync.py,"def test_handoff_fragment_revert(self):
        # test that a sync_revert type job does send the correct frag archives
        # to the receiver
        policy = POLICIES.default
        rx_node_index = 0
        tx_node_index = 1
        # for a revert job we iterate over frag index that belongs on
        # remote node
        frag_index = rx_node_index

        # create sender side diskfiles...
        tx_objs = {}
        rx_objs = {}
        tx_tombstones = {}
        tx_df_mgr = self.daemon._df_router[policy]
        rx_df_mgr = self.rx_controller._diskfile_router[policy]
        # o1 has primary and handoff fragment archives
        t1 = next(self.ts_iter)
        tx_objs['o1'] = self._create_ondisk_files(
            tx_df_mgr, 'o1', policy, t1, (rx_node_index, tx_node_index))
        # o2 only has primary
        t2 = next(self.ts_iter)
        tx_objs['o2'] = self._create_ondisk_files(
            tx_df_mgr, 'o2', policy, t2, (tx_node_index,))
        # o3 only has handoff, rx has other frag index
        t3 = next(self.ts_iter)
        tx_objs['o3'] = self._create_ondisk_files(
            tx_df_mgr, 'o3', policy, t3, (rx_node_index,))
        rx_objs['o3'] = self._create_ondisk_files(
            rx_df_mgr, 'o3', policy, t3, (13,))
        # o4 primary and handoff fragment archives on tx, handoff in sync on rx
        t4 = next(self.ts_iter)
        tx_objs['o4'] = self._create_ondisk_files(
            tx_df_mgr, 'o4', policy, t4, (tx_node_index, rx_node_index,))
        rx_objs['o4'] = self._create_ondisk_files(
            rx_df_mgr, 'o4', policy, t4, (rx_node_index,))
        # o5 is a tombstone, missing on receiver
        t5 = next(self.ts_iter)
        tx_tombstones['o5'] = self._create_ondisk_files(
            tx_df_mgr, 'o5', policy, t5, (tx_node_index,))
        tx_tombstones['o5'][0].delete(t5)

        suffixes = set()
        for diskfiles in list(tx_objs.values()) + list(tx_tombstones.values()):
            for df in diskfiles:
                suffixes.add(os.path.basename(os.path.dirname(df._datadir)))

        # create ssync sender instance...
        job = {'device': self.device,
               'partition': self.partition,
               'policy': policy,
               'frag_index': frag_index}
        node = dict(self.rx_node)
        sender = ssync_sender.Sender(self.daemon, node, job, suffixes)
        # wrap connection from tx to rx to capture ssync messages...
        sender.connect, trace = self.make_connect_wrapper(sender)

        # run the sync protocol...
        sender()

        # verify protocol
        results = self._analyze_trace(trace)
        # sender has handoff frags for o1, o3 and o4 and ts for o5
        self.assertEqual(4, len(results['tx_missing']))
        # receiver is missing frags for o1, o3 and ts for o5
        self.assertEqual(3, len(results['rx_missing']))
        self.assertEqual(3, len(results['tx_updates']))
        self.assertFalse(results['rx_updates'])
        sync_paths = []
        for subreq in results.get('tx_updates'):
            if subreq.get('method') == 'PUT':
                self.assertTrue(
                    'X-Object-Sysmeta-Ec-Frag-Index: %s' % rx_node_index
                    in subreq.get('headers'))
                expected_body = self._get_object_data(subreq['path'],
                                                      rx_node_index)
                self.assertEqual(expected_body, subreq['body'])
            elif subreq.get('method') == 'DELETE':
                self.assertEqual('/a/c/o5', subreq['path'])
            sync_paths.append(subreq.get('path'))
        self.assertEqual(['/a/c/o1', '/a/c/o3', '/a/c/o5'], sorted(sync_paths))

        # verify on disk files...
        self._verify_ondisk_files(
            tx_objs, policy, frag_index, rx_node_index)
        self._verify_tombstones(tx_tombstones, policy)",_139784.py,44,"for diskfiles in list(tx_objs.values()) + list(tx_tombstones.values()):
    for df in diskfiles:
        suffixes.add(os.path.basename(os.path.dirname(df._datadir)))",suffixes = {os.path.basename(os.path.dirname(df._datadir)) for diskfiles in list(tx_objs.values()) + list(tx_tombstones.values()) for df in diskfiles},1,nan,nan
https://github.com/WeblateOrg/weblate/tree/master/weblate/checks/icu.py,"def check_bad_submessage(self, result, name, data, src_data, flags):
        """"""Detect any bad sub-message selectors.""""""
        if ""-submessage_selectors"" in flags:
            return

        bad = set()

        # We also want to check individual select choices.
        if (
            src_data
            and ""select"" in data[""types""]
            and ""select"" in src_data[""types""]
            and ""choices"" in data
            and ""choices"" in src_data
        ):
            choices = data[""choices""]
            src_choices = src_data[""choices""]

            for selector in choices:
                if selector not in src_choices:
                    bad.add(selector)

        if bad:
            result[""bad_submessage""].append([name, bad])",_145221.py,19,"for selector in choices:
    if selector not in src_choices:
        bad.add(selector)",bad = {selector for selector in choices if selector not in src_choices},1,nan,nan
https://github.com/saltstack/salt/tree/master/salt/platform/win.py,"def elevate_token(th):
    """"""
    Set all token privileges to enabled
    """"""
    # Get list of privileges this token contains
    privileges = win32security.GetTokenInformation(th, win32security.TokenPrivileges)

    # Create a set of all privileges to be enabled
    enable_privs = set()
    for luid, flags in privileges:
        enable_privs.add((luid, win32con.SE_PRIVILEGE_ENABLED))

    # Enable the privileges
    if win32security.AdjustTokenPrivileges(th, 0, enable_privs) == 0:
        raise OSError(win32api.FormatMessage(win32api.GetLastError()))",_145852.py,10,"for (luid, flags) in privileges:
    enable_privs.add((luid, win32con.SE_PRIVILEGE_ENABLED))","enable_privs = {(luid, win32con.SE_PRIVILEGE_ENABLED) for (luid, flags) in privileges}",1,nan,nan
https://github.com/Hironsan/anago/tree/master/tests/test_wrapper.py,"def test_train_vocab_init(self):
        vocab = set()
        for words in np.r_[self.x_train, self.x_test, self.x_test]:
            for word in words:
                vocab.add(word)
        model = anago.Sequence(initial_vocab=vocab, embeddings=self.embeddings)
        model.fit(self.x_train, self.y_train, self.x_test, self.y_test)",_146105.py,3,"for words in np.r_[self.x_train, self.x_test, self.x_test]:
    for word in words:
        vocab.add(word)","vocab = {word for words in np.r_[self.x_train, self.x_test, self.x_test] for word in words}",1,nan,nan
https://github.com/gitpython-developers/GitPython/tree/master/test/test_refs.py,"def test_refs(self):
        types_found = set()
        for ref in self.rorepo.refs:
            types_found.add(type(ref))
        assert len(types_found) >= 3",_146519.py,3,"for ref in self.rorepo.refs:
    types_found.add(type(ref))",types_found = {type(ref) for ref in self.rorepo.refs},1,nan,nan
https://github.com/open-mmlab/mmcv/tree/master/mmcv/runner/hooks/hook.py,"def get_triggered_stages(self):
        trigger_stages = set()
        for stage in Hook.stages:
            if is_method_overridden(stage, Hook, self):
                trigger_stages.add(stage)

        # some methods will be triggered in multi stages
        # use this dict to map method to stages.
        method_stages_map = {
            'before_epoch': ['before_train_epoch', 'before_val_epoch'],
            'after_epoch': ['after_train_epoch', 'after_val_epoch'],
            'before_iter': ['before_train_iter', 'before_val_iter'],
            'after_iter': ['after_train_iter', 'after_val_iter'],
        }

        for method, map_stages in method_stages_map.items():
            if is_method_overridden(method, Hook, self):
                trigger_stages.update(map_stages)

        return [stage for stage in Hook.stages if stage in trigger_stages]",_150457.py,3,"for stage in Hook.stages:
    if is_method_overridden(stage, Hook, self):
        trigger_stages.add(stage)","trigger_stages = {stage for stage in Hook.stages if is_method_overridden(stage, Hook, self)}",1,nan,nan
https://github.com/panchunguang/ccks_baidu_entity_link/tree/master/code/ER_match_input.py,"def get_entity_id():
    '''
    与data_util 里面得到大致entity_id相同，区别是 为了更全的匹配mention，将实体名字全部变成小写
    :return:
    '''
    new_entity_alias=new_alias()
    entity_id={}
    with open('original_data/kb_data', 'r') as f:
        for line in f:
            temDict = json.loads(line)
            subject=temDict['subject']
            subject_id=temDict['subject_id']
            alias = set()
            for a in temDict['alias']:
                alias.add(a.lower())
            alias.add(entity_clear(subject))
            if subject in new_entity_alias:
                alias=alias|new_entity_alias[subject]
            alias.add(subject.lower())
            entity_set=set()
            for en in alias:
                entity_set.add(en.lower())
            for n in entity_set:
                n=del_bookname(n)
                if n in entity_id:
                    entity_id[n].append(subject_id)
                else:
                    entity_id[n]=[]
                    entity_id[n].append(subject_id)
    return entity_id",_152110.py,14,"for a in temDict['alias']:
    alias.add(a.lower())",alias = {a.lower() for a in temDict['alias']},1,nan,nan
https://github.com/panchunguang/ccks_baidu_entity_link/tree/master/code/ER_match_input.py,"def get_entity_id():
    '''
    与data_util 里面得到大致entity_id相同，区别是 为了更全的匹配mention，将实体名字全部变成小写
    :return:
    '''
    new_entity_alias=new_alias()
    entity_id={}
    with open('original_data/kb_data', 'r') as f:
        for line in f:
            temDict = json.loads(line)
            subject=temDict['subject']
            subject_id=temDict['subject_id']
            alias = set()
            for a in temDict['alias']:
                alias.add(a.lower())
            alias.add(entity_clear(subject))
            if subject in new_entity_alias:
                alias=alias|new_entity_alias[subject]
            alias.add(subject.lower())
            entity_set=set()
            for en in alias:
                entity_set.add(en.lower())
            for n in entity_set:
                n=del_bookname(n)
                if n in entity_id:
                    entity_id[n].append(subject_id)
                else:
                    entity_id[n]=[]
                    entity_id[n].append(subject_id)
    return entity_id",_152110.py,21,"for en in alias:
    entity_set.add(en.lower())",entity_set = {en.lower() for en in alias},1,nan,nan
https://github.com/lektor/lektor/tree/master/lektor/reporter.py,"def get_recorded_dependencies(self):
        rv = set()
        for event, data in self.buffer:
            if event == ""debug-info"" and data[""key""] == ""dependency"":
                rv.add(data[""value""])
        return sorted(rv)",_152328.py,3,"for (event, data) in self.buffer:
    if event == 'debug-info' and data['key'] == 'dependency':
        rv.add(data['value'])","rv = {data['value'] for (event, data) in self.buffer if event == 'debug-info' and data['key'] == 'dependency'}",1,nan,nan
https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/contrib/slim/tests/test_quantization_pass.py,"def residual_block_quant(
        self,
        activation_quant_type,
        weight_quantize_type,
        quantizable_op_type,
        for_ci=True,
    ):
        main = fluid.Program()
        startup = fluid.Program()
        with fluid.program_guard(main, startup):
            loss = residual_block(2)
            opt = fluid.optimizer.Adam(learning_rate=0.001)
            opt.minimize(loss)
        place = fluid.CPUPlace()
        graph = IrGraph(core.Graph(main.desc), for_test=False)
        transform_pass = QuantizationTransformPass(
            scope=fluid.global_scope(),
            place=place,
            activation_quantize_type=activation_quant_type,
            weight_quantize_type=weight_quantize_type,
            quantizable_op_type=quantizable_op_type,
        )
        transform_pass.apply(graph)
        if not for_ci:
            marked_nodes = set()
            for op in graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            graph.draw(
                '.', 'quantize_residual_' + activation_quant_type, marked_nodes
            )
        program = graph.to_program()
        self.check_program(program)
        val_graph = IrGraph(core.Graph(program.desc), for_test=False)
        if not for_ci:
            val_marked_nodes = set()
            for op in val_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    val_marked_nodes.add(op)
            val_graph.draw(
                '.', 'val_residual_' + activation_quant_type, val_marked_nodes
            )",_152767.py,26,"for op in graph.all_op_nodes():
    if op.name().find('quantize') > -1:
        marked_nodes.add(op)",marked_nodes = {op for op in graph.all_op_nodes() if op.name().find('quantize') > -1},1,nan,nan
https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/contrib/slim/tests/test_quantization_pass.py,"def residual_block_quant(
        self,
        activation_quant_type,
        weight_quantize_type,
        quantizable_op_type,
        for_ci=True,
    ):
        main = fluid.Program()
        startup = fluid.Program()
        with fluid.program_guard(main, startup):
            loss = residual_block(2)
            opt = fluid.optimizer.Adam(learning_rate=0.001)
            opt.minimize(loss)
        place = fluid.CPUPlace()
        graph = IrGraph(core.Graph(main.desc), for_test=False)
        transform_pass = QuantizationTransformPass(
            scope=fluid.global_scope(),
            place=place,
            activation_quantize_type=activation_quant_type,
            weight_quantize_type=weight_quantize_type,
            quantizable_op_type=quantizable_op_type,
        )
        transform_pass.apply(graph)
        if not for_ci:
            marked_nodes = set()
            for op in graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            graph.draw(
                '.', 'quantize_residual_' + activation_quant_type, marked_nodes
            )
        program = graph.to_program()
        self.check_program(program)
        val_graph = IrGraph(core.Graph(program.desc), for_test=False)
        if not for_ci:
            val_marked_nodes = set()
            for op in val_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    val_marked_nodes.add(op)
            val_graph.draw(
                '.', 'val_residual_' + activation_quant_type, val_marked_nodes
            )",_152767.py,37,"for op in val_graph.all_op_nodes():
    if op.name().find('quantize') > -1:
        val_marked_nodes.add(op)",val_marked_nodes = {op for op in val_graph.all_op_nodes() if op.name().find('quantize') > -1},1,nan,nan
https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/util/random_map.py,"def get_island_outline():
		""""""
		@return: the points just off the island as a dict
		""""""
		result = set()
		for x, y in map_set:
			for offset_x, offset_y in all_moves.values():
				coords = (x + offset_x, y + offset_y)
				if coords not in map_set:
					result.add(coords)
		return result",_155382.py,6,"for (x, y) in map_set:
    for (offset_x, offset_y) in all_moves.values():
        coords = (x + offset_x, y + offset_y)
        if coords not in map_set:
            result.add(coords)","result = {(x + offset_x, y + offset_y) for (x, y) in map_set for (offset_x, offset_y) in all_moves.values() if (x + offset_x, y + offset_y) not in map_set}",1,nan,nan
https://github.com/aleju/imgaug/tree/master/test/augmenters/test_size.py,"def test_decrease_size_by_tuples_of_floats__one_per_side(self):
        image2d = self.image2d[0:4, 0:4]
        image3d = self.image3d[0:4, 0:4, :]
        aug = iaa.Resize({""height"": (0.76, 1.0), ""width"": (0.76, 1.0)})
        not_seen2d = set()
        not_seen3d = set()
        for hsize in sm.xrange(3, 4+1):
            for wsize in sm.xrange(3, 4+1):
                not_seen2d.add((hsize, wsize))
        for hsize in sm.xrange(3, 4+1):
            for wsize in sm.xrange(3, 4+1):
                not_seen3d.add((hsize, wsize, 3))
        possible2d = set(list(not_seen2d))
        possible3d = set(list(not_seen3d))
        for _ in sm.xrange(100):
            observed2d = aug.augment_image(image2d)
            observed3d = aug.augment_image(image3d)
            assert observed2d.shape in possible2d
            assert observed3d.shape in possible3d
            if observed2d.shape in not_seen2d:
                not_seen2d.remove(observed2d.shape)
            if observed3d.shape in not_seen3d:
                not_seen3d.remove(observed3d.shape)
            if not not_seen2d and not not_seen3d:
                break
        assert not not_seen2d
        assert not not_seen3d",_155579.py,7,"for hsize in sm.xrange(3, 4 + 1):
    for wsize in sm.xrange(3, 4 + 1):
        not_seen2d.add((hsize, wsize))","not_seen2d = {(hsize, wsize) for hsize in sm.xrange(3, 4 + 1) for wsize in sm.xrange(3, 4 + 1)}",1,nan,nan
https://github.com/aleju/imgaug/tree/master/test/augmenters/test_size.py,"def test_decrease_size_by_tuples_of_floats__one_per_side(self):
        image2d = self.image2d[0:4, 0:4]
        image3d = self.image3d[0:4, 0:4, :]
        aug = iaa.Resize({""height"": (0.76, 1.0), ""width"": (0.76, 1.0)})
        not_seen2d = set()
        not_seen3d = set()
        for hsize in sm.xrange(3, 4+1):
            for wsize in sm.xrange(3, 4+1):
                not_seen2d.add((hsize, wsize))
        for hsize in sm.xrange(3, 4+1):
            for wsize in sm.xrange(3, 4+1):
                not_seen3d.add((hsize, wsize, 3))
        possible2d = set(list(not_seen2d))
        possible3d = set(list(not_seen3d))
        for _ in sm.xrange(100):
            observed2d = aug.augment_image(image2d)
            observed3d = aug.augment_image(image3d)
            assert observed2d.shape in possible2d
            assert observed3d.shape in possible3d
            if observed2d.shape in not_seen2d:
                not_seen2d.remove(observed2d.shape)
            if observed3d.shape in not_seen3d:
                not_seen3d.remove(observed3d.shape)
            if not not_seen2d and not not_seen3d:
                break
        assert not not_seen2d
        assert not not_seen3d",_155579.py,10,"for hsize in sm.xrange(3, 4 + 1):
    for wsize in sm.xrange(3, 4 + 1):
        not_seen3d.add((hsize, wsize, 3))","not_seen3d = {(hsize, wsize, 3) for hsize in sm.xrange(3, 4 + 1) for wsize in sm.xrange(3, 4 + 1)}",1,nan,nan
https://github.com/jasonwei20/eda_nlp/tree/master/code/eda.py,"def get_synonyms(word):
	synonyms = set()
	for syn in wordnet.synsets(word): 
		for l in syn.lemmas(): 
			synonym = l.name().replace(""_"", "" "").replace(""-"", "" "").lower()
			synonym = """".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])
			synonyms.add(synonym) 
	if word in synonyms:
		synonyms.remove(word)
	return list(synonyms)",_155701.py,3,"for syn in wordnet.synsets(word):
    for l in syn.lemmas():
        synonym = l.name().replace('_', ' ').replace('-', ' ').lower()
        synonym = ''.join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])
        synonyms.add(synonym)","synonyms = {''.join([char for char in l.name().replace('_', ' ').replace('-', ' ').lower() if char in ' qwertyuiopasdfghjklzxcvbnm']) for syn in wordnet.synsets(word) for l in syn.lemmas()}",1,,
https://github.com/mathics/Mathics/tree/master/mathics/builtin/inout.py,"def apply(self, expr, moff, mon, evaluation):
        ""Quiet[expr_, moff_, mon_]""

        def get_msg_list(expr):
            if check_message(expr):
                expr = Expression(SymbolList, expr)
            if expr.get_name() == ""System`All"":
                all = True
                messages = []
            elif expr.get_name() == ""System`None"":
                all = False
                messages = []
            elif expr.has_form(""List"", None):
                all = False
                messages = []
                for item in expr.leaves:
                    if check_message(item):
                        messages.append(item)
                    else:
                        raise ValueError
            else:
                raise ValueError
            return all, messages

        old_quiet_all = evaluation.quiet_all
        old_quiet_messages = set(evaluation.get_quiet_messages())
        quiet_messages = old_quiet_messages.copy()
        try:
            quiet_expr = Expression(""Quiet"", expr, moff, mon)
            try:
                off_all, off_messages = get_msg_list(moff)
            except ValueError:
                evaluation.message(""Quiet"", ""anmlist"", 2, quiet_expr)
                return
            try:
                on_all, on_messages = get_msg_list(mon)
            except ValueError:
                evaluation.message(""Quiet"", ""anmlist"", 2, quiet_expr)
                return
            if off_all and on_all:
                evaluation.message(""Quiet"", ""allall"", quiet_expr)
                return
            evaluation.quiet_all = off_all
            conflict = []
            for off in off_messages:
                if off in on_messages:
                    conflict.append(off)
                    break
            if conflict:
                evaluation.message(
                    ""Quiet"", ""conflict"", quiet_expr, Expression(SymbolList, *conflict)
                )
                return
            for off in off_messages:
                quiet_messages.add(off)
            for on in on_messages:
                quiet_messages.discard(on)
            if on_all:
                quiet_messages = set()
            evaluation.set_quiet_messages(quiet_messages)

            return expr.evaluate(evaluation)
        finally:
            evaluation.quiet_all = old_quiet_all
            evaluation.set_quiet_messages(old_quiet_messages)",_156465.py,54,"for off in off_messages:
    quiet_messages.add(off)",quiet_messages |= {off for off in off_messages},,,
https://github.com/thonny/thonny/tree/master/thonny/codeview.py,"def get_breakpoint_line_numbers(self):
        result = set()
        for num_line in self._gutter.get(""1.0"", ""end"").splitlines():
            if BREAKPOINT_SYMBOL in num_line:
                result.add(int(num_line.replace(BREAKPOINT_SYMBOL, """")))
        return result",_161291.py,3,"for num_line in self._gutter.get('1.0', 'end').splitlines():
    if BREAKPOINT_SYMBOL in num_line:
        result.add(int(num_line.replace(BREAKPOINT_SYMBOL, '')))","result = {int(num_line.replace(BREAKPOINT_SYMBOL, '')) for num_line in self._gutter.get('1.0', 'end').splitlines() if BREAKPOINT_SYMBOL in num_line}",1,,