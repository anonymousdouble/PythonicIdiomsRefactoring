file_html,method_content,file_name,lineno,old_code,new_code
https://github.com/XanaduAI/strawberryfields/tree/master/strawberryfields/ops.py,"def _sun_compact_cmds(reg, parameters, global_phase):
    cmds = []
    n = len(reg)

    if global_phase is not None:
        cmds += [Command(Rgate(global_phase / n), mode) for mode in reg]

    for modes, params in parameters:

        md1, md2 = modes[0], modes[1]
        a, b, g = params[0], params[1], params[2]

        if md2 != md1 + 1:
            raise ValueError(
                f""Mode combination {md1},{md2} is invalid.\n""
                + ""Currently only transformations on adjacent modes are implemented.""
            )

        cmds += [Command(Rgate(a / 2), reg[md1]), Command(Rgate(-a / 2), reg[md2])]
        cmds.append(Command(BSgate(b / 2, 0), (reg[md1], reg[md2])))
        cmds += [Command(Rgate(g / 2), reg[md1]), Command(Rgate(-g / 2), reg[md2])]

    # note cmds have to be reversed as they are build in matrix multiplication order,
    # which is in opposite order to gate application
    return cmds[::-1]",_1678.py,8,"for (modes, params) in parameters:
    (md1, md2) = (modes[0], modes[1])
    (a, b, g) = (params[0], params[1], params[2])
    if md2 != md1 + 1:
        raise ValueError(f'Mode combination {md1},{md2} is invalid.\n' + 'Currently only transformations on adjacent modes are implemented.')
    cmds += [Command(Rgate(a / 2), reg[md1]), Command(Rgate(-a / 2), reg[md2])]
    cmds.append(Command(BSgate(b / 2, 0), (reg[md1], reg[md2])))
    cmds += [Command(Rgate(g / 2), reg[md1]), Command(Rgate(-g / 2), reg[md2])]","for ((md1, md2, *mdremain),(a, b, g, *paremain)) in parameters:
    if md2 != md1 + 1:
        raise ValueError(f'Mode combination {md1},{md2} is invalid.\n' + 'Currently only transformations on adjacent modes are implemented.')
    cmds += [Command(Rgate(a / 2), reg[md1]), Command(Rgate(-a / 2), reg[md2])]
    cmds.append(Command(BSgate(b / 2, 0), (reg[md1], reg[md2])))
    cmds += [Command(Rgate(g / 2), reg[md1]), Command(Rgate(-g / 2), reg[md2])]"
https://github.com/microsoft/CodeBERT/tree/master/GraphCodeBERT/clonedetection/parser/DFG.py,"def DFG_python(root_node,index_to_code,states):
    assignment=['assignment','augmented_assignment','for_in_clause']
    if_statement=['if_statement']
    for_statement=['for_statement']
    while_statement=['while_statement']
    do_first_statement=['for_in_clause'] 
    def_statement=['default_parameter']
    states=states.copy() 
    if (len(root_node.children)==0 or root_node.type=='string') and root_node.type!='comment':        
        idx,code=index_to_code[(root_node.start_point,root_node.end_point)]
        if root_node.type==code:
            return [],states
        elif code in states:
            return [(code,idx,'comesFrom',[code],states[code].copy())],states
        else:
            if root_node.type=='identifier':
                states[code]=[idx]
            return [(code,idx,'comesFrom',[],[])],states
    elif root_node.type in def_statement:
        name=root_node.child_by_field_name('name')
        value=root_node.child_by_field_name('value')
        DFG=[]
        if value is None:
            indexs=tree_to_variable_index(name,index_to_code)
            for index in indexs:
                idx,code=index_to_code[index]
                DFG.append((code,idx,'comesFrom',[],[]))
                states[code]=[idx]
            return sorted(DFG,key=lambda x:x[1]),states
        else:
            name_indexs=tree_to_variable_index(name,index_to_code)
            value_indexs=tree_to_variable_index(value,index_to_code)
            temp,states=DFG_python(value,index_to_code,states)
            DFG+=temp            
            for index1 in name_indexs:
                idx1,code1=index_to_code[index1]
                for index2 in value_indexs:
                    idx2,code2=index_to_code[index2]
                    DFG.append((code1,idx1,'comesFrom',[code2],[idx2]))
                states[code1]=[idx1]   
            return sorted(DFG,key=lambda x:x[1]),states        
    elif root_node.type in assignment:
        if root_node.type=='for_in_clause':
            right_nodes=[root_node.children[-1]]
            left_nodes=[root_node.child_by_field_name('left')]
        else:
            if root_node.child_by_field_name('right') is None:
                return [],states
            left_nodes=[x for x in root_node.child_by_field_name('left').children if x.type!=',']
            right_nodes=[x for x in root_node.child_by_field_name('right').children if x.type!=',']
            if len(right_nodes)!=len(left_nodes):
                left_nodes=[root_node.child_by_field_name('left')]
                right_nodes=[root_node.child_by_field_name('right')]
            if len(left_nodes)==0:
                left_nodes=[root_node.child_by_field_name('left')]
            if len(right_nodes)==0:
                right_nodes=[root_node.child_by_field_name('right')]
        DFG=[]
        for node in right_nodes:
            temp,states=DFG_python(node,index_to_code,states)
            DFG+=temp
            
        for left_node,right_node in zip(left_nodes,right_nodes):
            left_tokens_index=tree_to_variable_index(left_node,index_to_code)
            right_tokens_index=tree_to_variable_index(right_node,index_to_code)
            temp=[]
            for token1_index in left_tokens_index:
                idx1,code1=index_to_code[token1_index]
                temp.append((code1,idx1,'computedFrom',[index_to_code[x][1] for x in right_tokens_index],
                             [index_to_code[x][0] for x in right_tokens_index]))
                states[code1]=[idx1]
            DFG+=temp        
        return sorted(DFG,key=lambda x:x[1]),states
    elif root_node.type in if_statement:
        DFG=[]
        current_states=states.copy()
        others_states=[]
        tag=False
        if 'else' in root_node.type:
            tag=True
        for child in root_node.children:
            if 'else' in child.type:
                tag=True
            if child.type not in ['elif_clause','else_clause']:
                temp,current_states=DFG_python(child,index_to_code,current_states)
                DFG+=temp
            else:
                temp,new_states=DFG_python(child,index_to_code,states)
                DFG+=temp
                others_states.append(new_states)
        others_states.append(current_states)
        if tag is False:
            others_states.append(states)
        new_states={}
        for dic in others_states:
            for key in dic:
                if key not in new_states:
                    new_states[key]=dic[key].copy()
                else:
                    new_states[key]+=dic[key]
        for key in new_states:
            new_states[key]=sorted(list(set(new_states[key])))
        return sorted(DFG,key=lambda x:x[1]),new_states
    elif root_node.type in for_statement:
        DFG=[]
        for i in range(2):
            right_nodes=[x for x in root_node.child_by_field_name('right').children if x.type!=',']
            left_nodes=[x for x in root_node.child_by_field_name('left').children if x.type!=',']
            if len(right_nodes)!=len(left_nodes):
                left_nodes=[root_node.child_by_field_name('left')]
                right_nodes=[root_node.child_by_field_name('right')]
            if len(left_nodes)==0:
                left_nodes=[root_node.child_by_field_name('left')]
            if len(right_nodes)==0:
                right_nodes=[root_node.child_by_field_name('right')]
            for node in right_nodes:
                temp,states=DFG_python(node,index_to_code,states)
                DFG+=temp
            for left_node,right_node in zip(left_nodes,right_nodes):
                left_tokens_index=tree_to_variable_index(left_node,index_to_code)
                right_tokens_index=tree_to_variable_index(right_node,index_to_code)
                temp=[]
                for token1_index in left_tokens_index:
                    idx1,code1=index_to_code[token1_index]
                    temp.append((code1,idx1,'computedFrom',[index_to_code[x][1] for x in right_tokens_index],
                                 [index_to_code[x][0] for x in right_tokens_index]))
                    states[code1]=[idx1]
                DFG+=temp   
            if  root_node.children[-1].type==""block"":
                temp,states=DFG_python(root_node.children[-1],index_to_code,states)
                DFG+=temp 
        dic={}
        for x in DFG:
            if (x[0],x[1],x[2]) not in dic:
                dic[(x[0],x[1],x[2])]=[x[3],x[4]]
            else:
                dic[(x[0],x[1],x[2])][0]=list(set(dic[(x[0],x[1],x[2])][0]+x[3]))
                dic[(x[0],x[1],x[2])][1]=sorted(list(set(dic[(x[0],x[1],x[2])][1]+x[4])))
        DFG=[(x[0],x[1],x[2],y[0],y[1]) for x,y in sorted(dic.items(),key=lambda t:t[0][1])]
        return sorted(DFG,key=lambda x:x[1]),states
    elif root_node.type in while_statement:  
        DFG=[]
        for i in range(2):
            for child in root_node.children:
                temp,states=DFG_python(child,index_to_code,states)
                DFG+=temp    
        dic={}
        for x in DFG:
            if (x[0],x[1],x[2]) not in dic:
                dic[(x[0],x[1],x[2])]=[x[3],x[4]]
            else:
                dic[(x[0],x[1],x[2])][0]=list(set(dic[(x[0],x[1],x[2])][0]+x[3]))
                dic[(x[0],x[1],x[2])][1]=sorted(list(set(dic[(x[0],x[1],x[2])][1]+x[4])))
        DFG=[(x[0],x[1],x[2],y[0],y[1]) for x,y in sorted(dic.items(),key=lambda t:t[0][1])]
        return sorted(DFG,key=lambda x:x[1]),states        
    else:
        DFG=[]
        for child in root_node.children:
            if child.type in do_first_statement:
                temp,states=DFG_python(child,index_to_code,states)
                DFG+=temp
        for child in root_node.children:
            if child.type not in do_first_statement:
                temp,states=DFG_python(child,index_to_code,states)
                DFG+=temp
        
        return sorted(DFG,key=lambda x:x[1]),states",_2979.py,133,"for x in DFG:
    if (x[0], x[1], x[2]) not in dic:
        dic[x[0], x[1], x[2]] = [x[3], x[4]]
    else:
        dic[x[0], x[1], x[2]][0] = list(set(dic[x[0], x[1], x[2]][0] + x[3]))
        dic[x[0], x[1], x[2]][1] = sorted(list(set(dic[x[0], x[1], x[2]][1] + x[4])))","for x_0, x_1, x_2, x_3, x_4, *x_remain in DFG:
    if (x_0, x_1, x_2) not in dic:
        dic[x_0, x_1, x_2] = [x_3, x_4]
    else:
        dic[x_0, x_1, x_2][0] = list(set(dic[x_0, x_1, x_2][0] +x_3))
        dic[x_0, x_1, x_2][1] = sorted(list(set(dic[x_0, x_1, x_2][1] + x_4)))"
https://github.com/microsoft/CodeBERT/tree/master/GraphCodeBERT/clonedetection/parser/DFG.py,"def DFG_python(root_node,index_to_code,states):
    assignment=['assignment','augmented_assignment','for_in_clause']
    if_statement=['if_statement']
    for_statement=['for_statement']
    while_statement=['while_statement']
    do_first_statement=['for_in_clause'] 
    def_statement=['default_parameter']
    states=states.copy() 
    if (len(root_node.children)==0 or root_node.type=='string') and root_node.type!='comment':        
        idx,code=index_to_code[(root_node.start_point,root_node.end_point)]
        if root_node.type==code:
            return [],states
        elif code in states:
            return [(code,idx,'comesFrom',[code],states[code].copy())],states
        else:
            if root_node.type=='identifier':
                states[code]=[idx]
            return [(code,idx,'comesFrom',[],[])],states
    elif root_node.type in def_statement:
        name=root_node.child_by_field_name('name')
        value=root_node.child_by_field_name('value')
        DFG=[]
        if value is None:
            indexs=tree_to_variable_index(name,index_to_code)
            for index in indexs:
                idx,code=index_to_code[index]
                DFG.append((code,idx,'comesFrom',[],[]))
                states[code]=[idx]
            return sorted(DFG,key=lambda x:x[1]),states
        else:
            name_indexs=tree_to_variable_index(name,index_to_code)
            value_indexs=tree_to_variable_index(value,index_to_code)
            temp,states=DFG_python(value,index_to_code,states)
            DFG+=temp            
            for index1 in name_indexs:
                idx1,code1=index_to_code[index1]
                for index2 in value_indexs:
                    idx2,code2=index_to_code[index2]
                    DFG.append((code1,idx1,'comesFrom',[code2],[idx2]))
                states[code1]=[idx1]   
            return sorted(DFG,key=lambda x:x[1]),states        
    elif root_node.type in assignment:
        if root_node.type=='for_in_clause':
            right_nodes=[root_node.children[-1]]
            left_nodes=[root_node.child_by_field_name('left')]
        else:
            if root_node.child_by_field_name('right') is None:
                return [],states
            left_nodes=[x for x in root_node.child_by_field_name('left').children if x.type!=',']
            right_nodes=[x for x in root_node.child_by_field_name('right').children if x.type!=',']
            if len(right_nodes)!=len(left_nodes):
                left_nodes=[root_node.child_by_field_name('left')]
                right_nodes=[root_node.child_by_field_name('right')]
            if len(left_nodes)==0:
                left_nodes=[root_node.child_by_field_name('left')]
            if len(right_nodes)==0:
                right_nodes=[root_node.child_by_field_name('right')]
        DFG=[]
        for node in right_nodes:
            temp,states=DFG_python(node,index_to_code,states)
            DFG+=temp
            
        for left_node,right_node in zip(left_nodes,right_nodes):
            left_tokens_index=tree_to_variable_index(left_node,index_to_code)
            right_tokens_index=tree_to_variable_index(right_node,index_to_code)
            temp=[]
            for token1_index in left_tokens_index:
                idx1,code1=index_to_code[token1_index]
                temp.append((code1,idx1,'computedFrom',[index_to_code[x][1] for x in right_tokens_index],
                             [index_to_code[x][0] for x in right_tokens_index]))
                states[code1]=[idx1]
            DFG+=temp        
        return sorted(DFG,key=lambda x:x[1]),states
    elif root_node.type in if_statement:
        DFG=[]
        current_states=states.copy()
        others_states=[]
        tag=False
        if 'else' in root_node.type:
            tag=True
        for child in root_node.children:
            if 'else' in child.type:
                tag=True
            if child.type not in ['elif_clause','else_clause']:
                temp,current_states=DFG_python(child,index_to_code,current_states)
                DFG+=temp
            else:
                temp,new_states=DFG_python(child,index_to_code,states)
                DFG+=temp
                others_states.append(new_states)
        others_states.append(current_states)
        if tag is False:
            others_states.append(states)
        new_states={}
        for dic in others_states:
            for key in dic:
                if key not in new_states:
                    new_states[key]=dic[key].copy()
                else:
                    new_states[key]+=dic[key]
        for key in new_states:
            new_states[key]=sorted(list(set(new_states[key])))
        return sorted(DFG,key=lambda x:x[1]),new_states
    elif root_node.type in for_statement:
        DFG=[]
        for i in range(2):
            right_nodes=[x for x in root_node.child_by_field_name('right').children if x.type!=',']
            left_nodes=[x for x in root_node.child_by_field_name('left').children if x.type!=',']
            if len(right_nodes)!=len(left_nodes):
                left_nodes=[root_node.child_by_field_name('left')]
                right_nodes=[root_node.child_by_field_name('right')]
            if len(left_nodes)==0:
                left_nodes=[root_node.child_by_field_name('left')]
            if len(right_nodes)==0:
                right_nodes=[root_node.child_by_field_name('right')]
            for node in right_nodes:
                temp,states=DFG_python(node,index_to_code,states)
                DFG+=temp
            for left_node,right_node in zip(left_nodes,right_nodes):
                left_tokens_index=tree_to_variable_index(left_node,index_to_code)
                right_tokens_index=tree_to_variable_index(right_node,index_to_code)
                temp=[]
                for token1_index in left_tokens_index:
                    idx1,code1=index_to_code[token1_index]
                    temp.append((code1,idx1,'computedFrom',[index_to_code[x][1] for x in right_tokens_index],
                                 [index_to_code[x][0] for x in right_tokens_index]))
                    states[code1]=[idx1]
                DFG+=temp   
            if  root_node.children[-1].type==""block"":
                temp,states=DFG_python(root_node.children[-1],index_to_code,states)
                DFG+=temp 
        dic={}
        for x in DFG:
            if (x[0],x[1],x[2]) not in dic:
                dic[(x[0],x[1],x[2])]=[x[3],x[4]]
            else:
                dic[(x[0],x[1],x[2])][0]=list(set(dic[(x[0],x[1],x[2])][0]+x[3]))
                dic[(x[0],x[1],x[2])][1]=sorted(list(set(dic[(x[0],x[1],x[2])][1]+x[4])))
        DFG=[(x[0],x[1],x[2],y[0],y[1]) for x,y in sorted(dic.items(),key=lambda t:t[0][1])]
        return sorted(DFG,key=lambda x:x[1]),states
    elif root_node.type in while_statement:  
        DFG=[]
        for i in range(2):
            for child in root_node.children:
                temp,states=DFG_python(child,index_to_code,states)
                DFG+=temp    
        dic={}
        for x in DFG:
            if (x[0],x[1],x[2]) not in dic:
                dic[(x[0],x[1],x[2])]=[x[3],x[4]]
            else:
                dic[(x[0],x[1],x[2])][0]=list(set(dic[(x[0],x[1],x[2])][0]+x[3]))
                dic[(x[0],x[1],x[2])][1]=sorted(list(set(dic[(x[0],x[1],x[2])][1]+x[4])))
        DFG=[(x[0],x[1],x[2],y[0],y[1]) for x,y in sorted(dic.items(),key=lambda t:t[0][1])]
        return sorted(DFG,key=lambda x:x[1]),states        
    else:
        DFG=[]
        for child in root_node.children:
            if child.type in do_first_statement:
                temp,states=DFG_python(child,index_to_code,states)
                DFG+=temp
        for child in root_node.children:
            if child.type not in do_first_statement:
                temp,states=DFG_python(child,index_to_code,states)
                DFG+=temp
        
        return sorted(DFG,key=lambda x:x[1]),states",_2979.py,148,"for x in DFG:
    if (x[0], x[1], x[2]) not in dic:
        dic[x[0], x[1], x[2]] = [x[3], x[4]]
    else:
        dic[x[0], x[1], x[2]][0] = list(set(dic[x[0], x[1], x[2]][0] + x[3]))
        dic[x[0], x[1], x[2]][1] = sorted(list(set(dic[x[0], x[1], x[2]][1] + x[4])))","for x_0, x_1, x_2, x_3, x_4, *x_remain in DFG:
    if (x_0, x_1, x_2) not in dic:
        dic[x_0, x_1, x_2] = [x_3, x_4]
    else:
        dic[x_0, x_1, x_2][0] = list(set(dic[x_0, x_1, x_2][0] +x_3))
        dic[x_0, x_1, x_2][1] = sorted(list(set(dic[x_0, x_1, x_2][1] + x_4)))"
https://github.com/cantools/cantools/tree/master/cantools/database/can/formats/dbc.py,"def _load_choices(tokens):
    choices = defaultdict(dict)

    for choice in tokens.get('VAL_', []):
        if len(choice[1]) == 0:
            continue

        od = odict((int(v[0]), NamedSignalValue(v[0], v[1])) for v in choice[3])

        if len(od) == 0:
            continue

        frame_id = int(choice[1][0])
        choices[frame_id][choice[2]] = od

    return choices",_3970.py,4,"for choice in tokens.get('VAL_', []):
    if len(choice[1]) == 0:
        continue
    od = odict(((int(v[0]), NamedSignalValue(v[0], v[1])) for v in choice[3]))
    if len(od) == 0:
        continue
    frame_id = int(choice[1][0])
    choices[frame_id][choice[2]] = od","for _, choice_1, choice_2, choice_3, *remain in tokens.get('VAL_', []):
    if len(choice_1) == 0:
        continue
    od = odict(((int(v[0]), NamedSignalValue(v[0], v[1])) for v in choice_3)
    if len(od) == 0:
        continue
    frame_id = int(choice_1[0])
    choices[frame_id][choice_2] = od"
https://github.com/10se1ucgo/DisableWinTracking/tree/master//dwt_util.py,"def set_registry(keys):
    mask = winreg.KEY_WOW64_64KEY | winreg.KEY_ALL_ACCESS if is_64bit() else winreg.KEY_ALL_ACCESS

    for key_name, values in keys.items():
        try:
            key = winreg.CreateKeyEx(values[0], values[1], 0, mask)
            winreg.SetValueEx(key, values[2], 0, values[3], values[4])
            winreg.CloseKey(key)
            logger.info(""Registry: Successfully modified {key} key."".format(key=key_name))
        except OSError:
            logger.exception(""Registry: Unable to modify {key} key."".format(key=key_name))",_4902.py,4,"for (key_name, values) in keys.items():
    try:
        key = winreg.CreateKeyEx(values[0], values[1], 0, mask)
        winreg.SetValueEx(key, values[2], 0, values[3], values[4])
        winreg.CloseKey(key)
        logger.info('Registry: Successfully modified {key} key.'.format(key=key_name))
    except OSError:
        logger.exception('Registry: Unable to modify {key} key.'.format(key=key_name))","for (key_name, (values_0, values_1, values_2, values_3, values_4, *remain)) in keys.items():
    try:
        key = winreg.CreateKeyEx(values_0, values_1, 0, mask)
        winreg.SetValueEx(key, values_2, 0, values_3, values_4)
        winreg.CloseKey(key)
        logger.info('Registry: Successfully modified {key} key.'.format(key=key_name))
    except OSError:
        logger.exception('Registry: Unable to modify {key} key.'.format(key=key_name))"
https://github.com/pyqtgraph/pyqtgraph/tree/master/pyqtgraph/graphicsItems/PlotItem/PlotItem.py,"def writeCsv(self, fileName=None):
        if fileName is None:
            self._chooseFilenameDialog(handler=self.writeCsv)
            return

        fileName = str(fileName)
        PlotItem.lastFileDir = os.path.dirname(fileName)
        
        data = [c.getData() for c in self.curves]
        with open(fileName, 'w') as fd:
            i = 0
            while True:
                done = True
                for d in data:
                    if i < len(d[0]):
                        fd.write('%g,%g,' % (d[0][i], d[1][i]))
                        done = False
                    else:
                        fd.write(' , ,')
                fd.write('\n')
                if done:
                    break
                i += 1",_7767.py,14,"for d in data:
    if i < len(d[0]):
        fd.write('%g,%g,' % (d[0][i], d[1][i]))
        done = False
    else:
        fd.write(' , ,')","for d_0, d_1, *remain in data:
    if i < len(d_0):
        fd.write('%g,%g,' % (d_0[I], d_1[i]))
        done = False
    else:
        fd.write(' , ,')"
https://github.com/deluge-torrent/deluge/tree/master/deluge/ui/gtk3/path_combo_chooser.py,"def set_selected_value(self, value, select_first=False):
        """"""
        Select the row of the list with value

        :param value: the value to be selected
        :type  value: str
        :param select_first: if the first item should be selected if the value if not found.
        :type  select_first: boolean

        """"""
        for i, row in enumerate(self.tree_store):
            if row[0] == value:
                self.treeview.set_cursor((i))
                return
        # The value was not found
        if select_first:
            self.treeview.set_cursor((0,))
        else:
            self.treeview.get_selection().unselect_all()",_7965.py,11,"for (i, row) in enumerate(self.tree_store):
    if row[0] == value:
        self.treeview.set_cursor(i)
        return","for (i, (row_0, *remain)) in enumerate(self.tree_store):
    if row_0 == value:
        self.treeview.set_cursor(i)
        return"
https://github.com/EpistasisLab/tpot/tree/master/tests/tpot_tests.py,"def test_evaluate_individuals_2():
    """"""Assert that _evaluate_individuals returns operator_counts and CV scores in correct order with n_jobs=2""""""
    tpot_obj = TPOTClassifier(
        n_jobs=2,
        random_state=42,
        verbosity=0,
        config_dict='TPOT light'
    )
    tpot_obj._fit_init()
    def pareto_eq(ind1, ind2):
        return np.allclose(ind1.fitness.values, ind2.fitness.values)

    tpot_obj._pareto_front = ParetoFront(similar=pareto_eq)

    tpot_obj._pbar = tqdm(total=1, disable=True)
    pop = tpot_obj._toolbox.population(n=10)
    pop = tpot_obj._evaluate_individuals(pop, training_features, training_target)
    fitness_scores = [ind.fitness.values for ind in pop]

    for deap_pipeline, fitness_score in zip(pop, fitness_scores):
        operator_count = tpot_obj._operator_count(deap_pipeline)
        sklearn_pipeline = tpot_obj._toolbox.compile(expr=deap_pipeline)

        try:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore')
                cv_scores = model_selection.cross_val_score(sklearn_pipeline,
                                                            training_features,
                                                            training_target,
                                                            cv=5,
                                                            scoring='accuracy',
                                                            verbose=0,
                                                            error_score='raise')
            mean_cv_scores = np.mean(cv_scores)
        except Exception:
            mean_cv_scores = -float('inf')

        assert isinstance(deap_pipeline, creator.Individual)
        assert np.allclose(fitness_score[0], operator_count)
        assert np.allclose(fitness_score[1], mean_cv_scores)",_10265.py,20,"for (deap_pipeline, fitness_score) in zip(pop, fitness_scores):
    operator_count = tpot_obj._operator_count(deap_pipeline)
    sklearn_pipeline = tpot_obj._toolbox.compile(expr=deap_pipeline)
    try:
        with warnings.catch_warnings():
            warnings.simplefilter('ignore')
            cv_scores = model_selection.cross_val_score(sklearn_pipeline, training_features, training_target, cv=5, scoring='accuracy', verbose=0, error_score='raise')
        mean_cv_scores = np.mean(cv_scores)
    except Exception:
        mean_cv_scores = -float('inf')
    assert isinstance(deap_pipeline, creator.Individual)
    assert np.allclose(fitness_score[0], operator_count)
    assert np.allclose(fitness_score[1], mean_cv_scores)","for (deap_pipeline, (fitness_score_0, fitness_score_1, *remain)) in zip(pop, fitness_scores):
    operator_count = tpot_obj._operator_count(deap_pipeline)
    sklearn_pipeline = tpot_obj._toolbox.compile(expr=deap_pipeline)
    try:
        with warnings.catch_warnings():
            warnings.simplefilter('ignore')
            cv_scores = model_selection.cross_val_score(sklearn_pipeline, training_features, training_target, cv=5, scoring='accuracy', verbose=0, error_score='raise')
        mean_cv_scores = np.mean(cv_scores)
    except Exception:
        mean_cv_scores = -float('inf')
    assert isinstance(deap_pipeline, creator.Individual)
    assert np.allclose(fitness_score_0, operator_count)
    assert np.allclose(fitness_score_1, mean_cv_scores)"
https://github.com/MrGiovanni/UNetPlusPlus/tree/master/pytorch/nnunet/evaluation/model_selection/rank_candidates_cascade.py,"if __name__ == ""__main__"":
    # run collect_all_fold0_results_and_summarize_in_one_csv.py first
    summary_files_dir = join(network_training_output_dir, ""summary_jsons_fold0_new"")
    output_file = join(network_training_output_dir, ""summary_cascade.csv"")

    folds = (0, )
    folds_str = """"
    for f in folds:
        folds_str += str(f)

    plans = ""nnUNetPlansv2.1""

    overwrite_plans = {
        'nnUNetTrainerCascadeFullRes': ['nnUNetPlans'],
    }

    trainers = [
        'nnUNetTrainerCascadeFullRes',
        'nnUNetTrainerV2CascadeFullRes_EducatedGuess',
        'nnUNetTrainerV2CascadeFullRes_EducatedGuess2',
        'nnUNetTrainerV2CascadeFullRes_EducatedGuess3',
        'nnUNetTrainerV2CascadeFullRes_lowerLR',
        'nnUNetTrainerV2CascadeFullRes',
        'nnUNetTrainerV2CascadeFullRes_noConnComp',
        'nnUNetTrainerV2CascadeFullRes_shorter_lowerLR',
        'nnUNetTrainerV2CascadeFullRes_shorter',
        'nnUNetTrainerV2CascadeFullRes_smallerBinStrel',
        #'',
        #'',
        #'',
        #'',
        #'',
        #'',
    ]

    datasets = \
        {
        ""Task003_Liver"": (""3d_cascade_fullres"", ),
        ""Task006_Lung"": (""3d_cascade_fullres"", ),
        ""Task007_Pancreas"": (""3d_cascade_fullres"", ),
        ""Task008_HepaticVessel"": (""3d_cascade_fullres"", ),
        ""Task009_Spleen"": (""3d_cascade_fullres"", ),
        ""Task010_Colon"": (""3d_cascade_fullres"", ),
        ""Task017_AbdominalOrganSegmentation"": (""3d_cascade_fullres"", ),
        #""Task029_LITS"": (""3d_cascade_fullres"", ),
        ""Task048_KiTS_clean"": (""3d_cascade_fullres"", ),
        ""Task055_SegTHOR"": (""3d_cascade_fullres"", ),
        ""Task056_VerSe"": (""3d_cascade_fullres"", ),
        #"""": (""3d_cascade_fullres"", ),
        }

    expected_validation_folder = ""validation_raw""
    alternative_validation_folder = ""validation""
    alternative_alternative_validation_folder = ""validation_tiledTrue_doMirror_True""

    interested_in = ""mean""

    result_per_dataset = {}
    for d in datasets:
        result_per_dataset[d] = {}
        for c in datasets[d]:
            result_per_dataset[d][c] = []

    valid_trainers = []
    all_trainers = []

    with open(output_file, 'w') as f:
        f.write(""trainer,"")
        for t in datasets.keys():
            s = t[4:7]
            for c in datasets[t]:
                s1 = s + ""_"" + c[3]
                f.write(""%s,"" % s1)
        f.write(""\n"")

        for trainer in trainers:
            trainer_plans = [plans]
            if trainer in overwrite_plans.keys():
                trainer_plans = overwrite_plans[trainer]

            result_per_dataset_here = {}
            for d in datasets:
                result_per_dataset_here[d] = {}

            for p in trainer_plans:
                name = ""%s__%s"" % (trainer, p)
                all_present = True
                all_trainers.append(name)

                f.write(""%s,"" % name)
                for dataset in datasets.keys():
                    for configuration in datasets[dataset]:
                        summary_file = join(summary_files_dir, ""%s__%s__%s__%s__%s__%s.json"" % (dataset, configuration, trainer, p, expected_validation_folder, folds_str))
                        if not isfile(summary_file):
                            summary_file = join(summary_files_dir, ""%s__%s__%s__%s__%s__%s.json"" % (dataset, configuration, trainer, p, alternative_validation_folder, folds_str))
                            if not isfile(summary_file):
                                summary_file = join(summary_files_dir, ""%s__%s__%s__%s__%s__%s.json"" % (
                                dataset, configuration, trainer, p, alternative_alternative_validation_folder, folds_str))
                                if not isfile(summary_file):
                                    all_present = False
                                    print(name, dataset, configuration, ""has missing summary file"")
                        if isfile(summary_file):
                            result = load_json(summary_file)['results'][interested_in]['mean']['Dice']
                            result_per_dataset_here[dataset][configuration] = result
                            f.write(""%02.4f,"" % result)
                        else:
                            f.write(""NA,"")
                            result_per_dataset_here[dataset][configuration] = 0

                f.write(""\n"")

                if True:
                    valid_trainers.append(name)
                    for d in datasets:
                        for c in datasets[d]:
                            result_per_dataset[d][c].append(result_per_dataset_here[d][c])

    invalid_trainers = [i for i in all_trainers if i not in valid_trainers]

    num_valid = len(valid_trainers)
    num_datasets = len(datasets.keys())
    # create an array that is trainer x dataset. If more than one configuration is there then use the best metric across the two
    all_res = np.zeros((num_valid, num_datasets))
    for j, d in enumerate(datasets.keys()):
        ks = list(result_per_dataset[d].keys())
        tmp = result_per_dataset[d][ks[0]]
        for k in ks[1:]:
            for i in range(len(tmp)):
                tmp[i] = max(tmp[i], result_per_dataset[d][k][i])
        all_res[:, j] = tmp

    ranks_arr = np.zeros_like(all_res)
    for d in range(ranks_arr.shape[1]):
        temp = np.argsort(all_res[:, d])[::-1] # inverse because we want the highest dice to be rank0
        ranks = np.empty_like(temp)
        ranks[temp] = np.arange(len(temp))

        ranks_arr[:, d] = ranks

    mn = np.mean(ranks_arr, 1)
    for i in np.argsort(mn):
        print(mn[i], valid_trainers[i])

    print()
    print(valid_trainers[np.argmin(mn)])",_11494.py,71,"for c in datasets[t]:
    s1 = s + '_' + c[3]
    f.write('%s,' % s1)","for _, _, _, c_3, *remain in datasets[t]:
    s1 = s + '_' + c_3
    f.write('%s,' % s1)"
https://github.com/shadowmoose/RedditDownloader/tree/master/redditdownloader/static/console.py,"def prompt_list(prompt, options, allow_none=False):
	"""""" Prompts for the user to select an option from the list, and returns the index.
	 	If *options* is an array of tuples, they are prompted with [1], and [0] is returned.
	""""""
	if len(options) == 0:
		return None
	stringutil.print_color('cyan', prompt)
	is_tuple = isinstance(options[0], tuple)
	if not is_tuple:
		options = [(o, o) for o in options]
	if allow_none:
		options.append(('Cancel', None))
	for idx, opt in enumerate(options):
		print(""\t%s: %s"" % (idx+1, opt[1]))
	select = number(""Choose an option"", 1, len(options), round_val=True) - 1
	return options[select][0]",_11721.py,13,"for (idx, opt) in enumerate(options):
    print('\t%s: %s' % (idx + 1, opt[1]))","for (idx, (_, opt_1, *remain)) in enumerate(options):
    print('\t%s: %s' % (idx + 1, opt_1))"
https://github.com/mkleehammer/pyodbc/tree/master/tests3/pgtests.py,"def test_executemany(self):
        self.cursor.execute(""create table t1(a int, b varchar(10))"")

        params = [ (i, str(i)) for i in range(1, 6) ]

        self.cursor.executemany(""insert into t1(a, b) values (?,?)"", params)

        # REVIEW: Without the cast, we get the following error:
        # [07006] [unixODBC]Received an unsupported type from Postgres.;\nERROR:  table ""t2"" does not exist (14)

        count = self.cursor.execute(""select cast(count(*) as int) from t1"").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute(""select a, b from t1 order by a"")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])",_13895.py,18,"for (param, row) in zip(params, rows):
    self.assertEqual(param[0], row[0])
    self.assertEqual(param[1], row[1])","for ((param_0, param_1, *par_remain), (row_0, row_1, *row_remain)) in zip(params, rows):
    self.assertEqual(param_0, row_0)
    self.assertEqual(param_1, row_1)"
https://github.com/mysql/mysql-connector-python/tree/master/lib/mysql/connector/django/introspection.py,"def get_indexes(self, cursor, table_name):
        cursor.execute(""SHOW INDEX FROM {0}""
                       """".format(self.connection.ops.quote_name(table_name)))
        # Do a two-pass search for indexes: on first pass check which indexes
        # are multicolumn, on second pass check which single-column indexes
        # are present.
        rows = list(cursor.fetchall())
        multicol_indexes = set()
        for row in rows:
            if row[3] > 1:
                multicol_indexes.add(row[2])
        indexes = {}
        for row in rows:
            if row[2] in multicol_indexes:
                continue
            if row[4] not in indexes:
                indexes[row[4]] = {'primary_key': False, 'unique': False}
            # It's possible to have the unique and PK constraints in
            # separate indexes.
            if row[2] == 'PRIMARY':
                indexes[row[4]]['primary_key'] = True
            if not row[1]:
                indexes[row[4]]['unique'] = True
        return indexes",_14751.py,9,"for row in rows:
    if row[3] > 1:
        multicol_indexes.add(row[2])","for _, _, row_2, row_3, *remain in rows:
    if row_3 > 1:
        multicol_indexes.add(row_2)"
https://github.com/mysql/mysql-connector-python/tree/master/lib/mysql/connector/django/introspection.py,"def get_indexes(self, cursor, table_name):
        cursor.execute(""SHOW INDEX FROM {0}""
                       """".format(self.connection.ops.quote_name(table_name)))
        # Do a two-pass search for indexes: on first pass check which indexes
        # are multicolumn, on second pass check which single-column indexes
        # are present.
        rows = list(cursor.fetchall())
        multicol_indexes = set()
        for row in rows:
            if row[3] > 1:
                multicol_indexes.add(row[2])
        indexes = {}
        for row in rows:
            if row[2] in multicol_indexes:
                continue
            if row[4] not in indexes:
                indexes[row[4]] = {'primary_key': False, 'unique': False}
            # It's possible to have the unique and PK constraints in
            # separate indexes.
            if row[2] == 'PRIMARY':
                indexes[row[4]]['primary_key'] = True
            if not row[1]:
                indexes[row[4]]['unique'] = True
        return indexes",_14751.py,13,"for row in rows:
    if row[2] in multicol_indexes:
        continue
    if row[4] not in indexes:
        indexes[row[4]] = {'primary_key': False, 'unique': False}
    if row[2] == 'PRIMARY':
        indexes[row[4]]['primary_key'] = True
    if not row[1]:
        indexes[row[4]]['unique'] = True","for _, row_1, row_2, _, row_4, *remain in rows:
    if row_2 in multicol_indexes:
        continue
    if row_4 not in indexes:
        indexes[row_4] = {'primary_key': False, 'unique': False}
    if row_2 == 'PRIMARY':
        indexes[row[4]]['primary_key'] = True
    if not row_1:
        indexes[row_4]['unique'] = True"
https://github.com/kivy/kivy-designer/tree/master/tools/pep8checker/pep8.py,"def check_logical(self):
        """"""
        Build a line from tokens and run all logical checks on it.
        """"""
        self.build_tokens_line()
        self.report.increment_logical_line()
        first_line = self.lines[self.mapping[0][1][2][0] - 1]
        indent = first_line[:self.mapping[0][1][2][1]]
        self.previous_indent_level = self.indent_level
        self.indent_level = expand_indent(indent)
        if self.verbose >= 2:
            print((self.logical_line[:80].rstrip()))
        for name, check, argument_names in self._logical_checks:
            if self.verbose >= 4:
                print(('   ' + name))
            for result in self.run_check(check, argument_names):
                offset, text = result
                if isinstance(offset, tuple):
                    orig_number, orig_offset = offset
                else:
                    for token_offset, token in self.mapping:
                        if offset >= token_offset:
                            orig_number = token[2][0]
                            orig_offset = (token[2][1] + offset - token_offset)
                self.report_error(orig_number, orig_offset, text, check)
        self.previous_logical = self.logical_line",_14801.py,21,"for (token_offset, token) in self.mapping:
    if offset >= token_offset:
        orig_number = token[2][0]
        orig_offset = token[2][1] + offset - token_offset","for (token_offset,( _, _, (token2_0, token2_1, *remain))) in self.mapping:
    if offset >= token_offset:
        orig_number = token2_0
        orig_offset = token2_1 + offset - token_offset"
https://github.com/WyattBlue/auto-editor/tree/master/auto_editor/formats/premiere.py,"def handle_audio_clips(tracks, outfile, audioFile, clips, inp, timebase, sr, pathurls):
    for t in range(tracks):
        outfile.write('\t\t\t\t<track currentExplodedTrackIndex=""0"" premiereTrackType=""Stereo"">\n')
        total = 0
        for j, clip in enumerate(clips):

            my_start = int(total)
            total += (clip[1] - clip[0]) / (clip[2] / 100)
            my_end = int(total)

            if(audioFile):
                clip_item_num = j + 1
                master_id = '1'
            else:
                clip_item_num = len(clips) + 1 + j + (t * len(clips))
                master_id = '2'

            outfile.write(indent(5,
                '<clipitem id=""clipitem-{}"" premiereChannelType=""stereo"">'.format(clip_item_num),
                '\t<masterclipid>masterclip-{}</masterclipid>'.format(master_id),
                '\t<name>{}</name>'.format(inp.basename),
                '\t<start>{}</start>'.format(my_start),
                '\t<end>{}</end>'.format(my_end),
                '\t<in>{}</in>'.format(int(clip[0] / (clip[2] / 100))),
                '\t<out>{}</out>'.format(int(clip[1] / (clip[2] / 100)))))

            if((audioFile and j == 0) or (t > 0 and j == 0)):
                outfile.write(indent(6, '<file id=""file-{}"">'.format(t+1),
                    '\t<name>{}</name>'.format(inp.basename),
                    '\t<pathurl>{}</pathurl>'.format(pathurls[t]),
                    '\t<rate>',
                    '\t\t<timebase>{}</timebase>'.format(timebase),
                    '\t\t<ntsc>{}</ntsc>'.format(ntsc),
                    '\t</rate>',
                    '\t<media>',
                    '\t\t<audio>',
                    '\t\t\t<samplecharacteristics>',
                    '\t\t\t\t<depth>{}</depth>'.format(depth),
                    '\t\t\t\t<samplerate>{}</samplerate>'.format(sr),
                    '\t\t\t</samplecharacteristics>',
                    '\t\t\t<channelcount>2</channelcount>',
                    '\t\t</audio>', '\t</media>', '</file>'))
            else:
                outfile.write('\t\t\t\t\t\t<file id=""file-{}""/>\n'.format(t+1))

            outfile.write(indent(6, '<sourcetrack>',
                '\t<mediatype>audio</mediatype>',
                '\t<trackindex>1</trackindex>',
                '</sourcetrack>'))

            if(audioFile):
                outfile.write('\t\t\t\t\t</clipitem>\n')
            else:
                outfile.write(indent(6, '<labels>', '\t<label2>Iris</label2>', '</labels>'))

            # Add speed effect for audio blocks
            if(clip[2] != 100):
                outfile.write(speedup(clip[2]))

            outfile.write('\t\t\t\t\t</clipitem>\n')
        if(not audioFile):
            outfile.write('\t\t\t\t\t<outputchannelindex>1</outputchannelindex>\n')
        outfile.write('\t\t\t\t</track>\n')",_14930.py,5,"for (j, clip) in enumerate(clips):
    my_start = int(total)
    total += (clip[1] - clip[0]) / (clip[2] / 100)
    my_end = int(total)
    if audioFile:
        clip_item_num = j + 1
        master_id = '1'
    else:
        clip_item_num = len(clips) + 1 + j + t * len(clips)
        master_id = '2'
    outfile.write(indent(5, '<clipitem id=""clipitem-{}"" premiereChannelType=""stereo"">'.format(clip_item_num), '\t<masterclipid>masterclip-{}</masterclipid>'.format(master_id), '\t<name>{}</name>'.format(inp.basename), '\t<start>{}</start>'.format(my_start), '\t<end>{}</end>'.format(my_end), '\t<in>{}</in>'.format(int(clip[0] / (clip[2] / 100))), '\t<out>{}</out>'.format(int(clip[1] / (clip[2] / 100)))))
    if audioFile and j == 0 or (t > 0 and j == 0):
        outfile.write(indent(6, '<file id=""file-{}"">'.format(t + 1), '\t<name>{}</name>'.format(inp.basename), '\t<pathurl>{}</pathurl>'.format(pathurls[t]), '\t<rate>', '\t\t<timebase>{}</timebase>'.format(timebase), '\t\t<ntsc>{}</ntsc>'.format(ntsc), '\t</rate>', '\t<media>', '\t\t<audio>', '\t\t\t<samplecharacteristics>', '\t\t\t\t<depth>{}</depth>'.format(depth), '\t\t\t\t<samplerate>{}</samplerate>'.format(sr), '\t\t\t</samplecharacteristics>', '\t\t\t<channelcount>2</channelcount>', '\t\t</audio>', '\t</media>', '</file>'))
    else:
        outfile.write('\t\t\t\t\t\t<file id=""file-{}""/>\n'.format(t + 1))
    outfile.write(indent(6, '<sourcetrack>', '\t<mediatype>audio</mediatype>', '\t<trackindex>1</trackindex>', '</sourcetrack>'))
    if audioFile:
        outfile.write('\t\t\t\t\t</clipitem>\n')
    else:
        outfile.write(indent(6, '<labels>', '\t<label2>Iris</label2>', '</labels>'))
    if clip[2] != 100:
        outfile.write(speedup(clip[2]))
    outfile.write('\t\t\t\t\t</clipitem>\n')","for (j,(clip_0, clip_1, clip_2, *remain)) in enumerate(clips):
    my_start = int(total)
    total += (clip_1 - clip_0) / (clip_2 / 100)
    my_end = int(total)
    if audioFile:
        clip_item_num = j + 1
        master_id = '1'
    else:
        clip_item_num = len(clips) + 1 + j + t * len(clips)
        master_id = '2'
    outfile.write(indent(5, '<clipitem id=""clipitem-{}"" premiereChannelType=""stereo"">'.format(clip_item_num), '\t<masterclipid>masterclip-{}</masterclipid>'.format(master_id), '\t<name>{}</name>'.format(inp.basename), '\t<start>{}</start>'.format(my_start), '\t<end>{}</end>'.format(my_end), '\t<in>{}</in>'.format(int(clip[0] / (clip[2] / 100))), '\t<out>{}</out>'.format(int(clip[1] / (clip_2 / 100)))))
    if audioFile and j == 0 or (t > 0 and j == 0):
        outfile.write(indent(6, '<file id=""file-{}"">'.format(t + 1), '\t<name>{}</name>'.format(inp.basename), '\t<pathurl>{}</pathurl>'.format(pathurls[t]), '\t<rate>', '\t\t<timebase>{}</timebase>'.format(timebase), '\t\t<ntsc>{}</ntsc>'.format(ntsc), '\t</rate>', '\t<media>', '\t\t<audio>', '\t\t\t<samplecharacteristics>', '\t\t\t\t<depth>{}</depth>'.format(depth), '\t\t\t\t<samplerate>{}</samplerate>'.format(sr), '\t\t\t</samplecharacteristics>', '\t\t\t<channelcount>2</channelcount>', '\t\t</audio>', '\t</media>', '</file>'))
    else:
        outfile.write('\t\t\t\t\t\t<file id=""file-{}""/>\n'.format(t + 1))
    outfile.write(indent(6, '<sourcetrack>', '\t<mediatype>audio</mediatype>', '\t<trackindex>1</trackindex>', '</sourcetrack>'))
    if audioFile:
        outfile.write('\t\t\t\t\t</clipitem>\n')
    else:
        outfile.write(indent(6, '<labels>', '\t<label2>Iris</label2>', '</labels>'))
    if clip_2 != 100:
        outfile.write(speedup(clip_2))
    outfile.write('\t\t\t\t\t</clipitem>\n')"
https://github.com/ansible/ansible-modules-core/tree/master/files/stat.py,"def main():
    module = AnsibleModule(
        argument_spec=dict(
            path=dict(required=True, type='path'),
            follow=dict(default='no', type='bool'),
            get_md5=dict(default='yes', type='bool'),
            get_checksum=dict(default='yes', type='bool'),
            get_mime=dict(default=True, type='bool', aliases=['mime', 'mime_type', 'mime-type']),
            get_attributes=dict(default=True, type='bool', aliases=['attributes', 'attr']),
            checksum_algorithm=dict(default='sha1', type='str',
                                    choices=['sha1', 'sha224', 'sha256', 'sha384', 'sha512'],
                                    aliases=['checksum_algo', 'checksum']),
        ),
        supports_check_mode=True
    )

    path = module.params.get('path')
    b_path = to_bytes(path, errors='surrogate_or_strict')
    follow = module.params.get('follow')
    get_mime = module.params.get('get_mime')
    get_attr = module.params.get('get_attributes')
    get_md5 = module.params.get('get_md5')
    get_checksum = module.params.get('get_checksum')
    checksum_algorithm = module.params.get('checksum_algorithm')

    # main stat data
    try:
        if follow:
            st = os.stat(b_path)
        else:
            st = os.lstat(b_path)
    except OSError:
        e = get_exception()
        if e.errno == errno.ENOENT:
            output = {'exists': False}
            module.exit_json(changed=False, stat=output)

        module.fail_json(msg=e.strerror)

    # process base results
    output = format_output(module, path, st)

    # resolved permissions
    for perm in [('readable', os.R_OK), ('writeable', os.W_OK), ('executable', os.X_OK)]:
        output[perm[0]] = os.access(path, perm[1])

    # symlink info
    if output.get('islnk'):
        output['lnk_source'] = os.path.realpath(path)

    try: # user data
        pw = pwd.getpwuid(st.st_uid)
        output['pw_name'] = pw.pw_name
    except:
        pass

    try: # group data
        grp_info = grp.getgrgid(st.st_gid)
        output['gr_name'] = grp_info.gr_name
    except:
        pass

    # checksums
    if output.get('isreg') and output.get('readable'):
        if get_md5:
            # Will fail on FIPS-140 compliant systems
            try:
                output['md5'] = module.md5(path)
            except ValueError:
                output['md5'] = None

        if get_checksum:
            output['checksum'] = module.digest_from_file(path, checksum_algorithm)

    # try to get mime data if requested
    if get_mime:
        output['mimetype'] = output['charset'] = 'unknown'
        mimecmd = module.get_bin_path('file')
        if mimecmd:
            mimecmd = [mimecmd, '-i', path]
            try:
                rc, out, err = module.run_command(mimecmd)
                if rc == 0:
                    mimetype, charset = out.split(':')[1].split(';')
                    output['mimetype'] = mimetype.strip()
                    output['charset'] = charset.split('=')[1].strip()
            except:
                pass

    # try to get attr data
    if get_attr:
        output['version'] = None
        output['attributes'] = []
        output['attr_flags'] = ''
        out = module.get_file_attributes(path)
        for x in ('version', 'attributes', 'attr_flags'):
            if x in out:
                output[x] = out[x]

    module.exit_json(changed=False, stat=output)",_15856.py,44,"for perm in [('readable', os.R_OK), ('writeable', os.W_OK), ('executable', os.X_OK)]:
    output[perm[0]] = os.access(path, perm[1])","for perm_0, perm_1, *remain in [('readable', os.R_OK), ('writeable', os.W_OK), ('executable', os.X_OK)]:
    output[perm_0] = os.access(path, perm_1])"
https://github.com/DLLXW/data-science-competition/tree/master/else/Chick-Counting/detect/val.py,"def save_one_json(predn, jdict, path, class_map):
    # Save one JSON result {""image_id"": 42, ""category_id"": 18, ""bbox"": [258.15, 41.29, 348.26, 243.78], ""score"": 0.236}
    image_id = int(path.stem) if path.stem.isnumeric() else path.stem
    box = xyxy2xywh(predn[:, :4])  # xywh
    box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner
    for p, b in zip(predn.tolist(), box.tolist()):
        jdict.append({'image_id': image_id,
                      'category_id': class_map[int(p[5])],
                      'bbox': [round(x, 3) for x in b],
                      'score': round(p[4], 5)})",_16206.py,6,"for (p, b) in zip(predn.tolist(), box.tolist()):
    jdict.append({'image_id': image_id, 'category_id': class_map[int(p[5])], 'bbox': [round(x, 3) for x in b], 'score': round(p[4], 5)})","for ((_, _, _, _, p_4, p_5, *remain), b) in zip(predn.tolist(), box.tolist()):
    jdict.append({'image_id': image_id, 'category_id': class_map[int(p_5)], 'bbox': [round(x, 3) for x in b], 'score': round(p_4, 5)})"
https://github.com/open-mmlab/mmfashion/tree/master/mmfashion/utils/image.py,"def draw_landmarks(img_file, landmarks, r=2):
    img = Image.open(img_file)
    draw = ImageDraw.Draw(img)
    for i, lm in enumerate(landmarks):
        x = lm[0]
        y = lm[1]
        draw.ellipse([(x - r, y - r), (x + r, y + r)], fill=(255, 0, 0, 0))
    img.show()",_18213.py,4,"for (i, lm) in enumerate(landmarks):
    x = lm[0]
    y = lm[1]
    draw.ellipse([(x - r, y - r), (x + r, y + r)], fill=(255, 0, 0, 0))","for (i, (lm_0, lm_1, *remain)) in enumerate(landmarks):
    x = lm_0
    y = lm_1
    draw.ellipse([(x - r, y - r), (x + r, y + r)], fill=(255, 0, 0, 0))"
https://github.com/facebookresearch/detectron2/tree/master/detectron2/modeling/meta_arch/rcnn.py,"def _postprocess(instances, batched_inputs: List[Dict[str, torch.Tensor]], image_sizes):
        """"""
        Rescale the output instances to the target size.
        """"""
        # note: private function; subject to changes
        processed_results = []
        for results_per_image, input_per_image, image_size in zip(
            instances, batched_inputs, image_sizes
        ):
            height = input_per_image.get(""height"", image_size[0])
            width = input_per_image.get(""width"", image_size[1])
            r = detector_postprocess(results_per_image, height, width)
            processed_results.append({""instances"": r})
        return processed_results",_20833.py,7,"for (results_per_image, input_per_image, image_size) in zip(instances, batched_inputs, image_sizes):
    height = input_per_image.get('height', image_size[0])
    width = input_per_image.get('width', image_size[1])
    r = detector_postprocess(results_per_image, height, width)
    processed_results.append({'instances': r})","for (results_per_image, input_per_image, (image_size_0, image_size_1, *remain) )) in zip(instances, batched_inputs, image_sizes):
    height = input_per_image.get(‘height', image_size_0)
    width = input_per_image.get('width', image_size_1)
    r = detector_postprocess(results_per_image, height, width)
    processed_results.append({'instances': r})"
https://github.com/zhm-real/MotionPlanning/tree/master/Control/Rear_Wheel_Feedback.py,"def main():
    # generate path
    states = [(0, 0, 0), (20, 15, 0), (35, 20, 90), (40, 0, 180),
              (20, 0, 120), (5, -10, 180), (15, 5, 30)]
    #
    # states = [(-3, 3, 120), (10, -7, 30), (10, 13, 30), (20, 5, -25),
    #           (35, 10, 180), (30, -10, 160), (5, -12, 90)]

    x_ref, y_ref, yaw_ref, direct, curv, x_all, y_all = generate_path(states)

    maxTime = 100.0
    yaw_old = 0.0
    x0, y0, yaw0, direct0 = \
        x_ref[0][0], y_ref[0][0], yaw_ref[0][0], direct[0][0]

    x_rec, y_rec, yaw_rec, direct_rec = [], [], [], []

    for cx, cy, cyaw, cdirect, ccurv in zip(x_ref, y_ref, yaw_ref, direct, curv):
        t = 0.0
        node = Node(x=x0, y=y0, yaw=yaw0, v=0.0, direct=cdirect[0])
        ref_trajectory = PATH(cx, cy, cyaw, ccurv)

        while t < maxTime:
            if cdirect[0] > 0:
                speed_ref = 30.0 / 3.6
                C.Ld = 3.5
            else:
                speed_ref = 15.0 / 3.6
                C.Ld = 2.5

            delta, ind = rear_wheel_feedback_control(node, ref_trajectory)

            dist = math.hypot(node.x - cx[-1], node.y - cy[-1])

            acceleration = pid_control(speed_ref, node.v, dist, node.direct)
            node.update(acceleration, delta, node.direct)
            t += C.dt

            if dist <= C.dist_stop:
                break

            x_rec.append(node.x)
            y_rec.append(node.y)
            yaw_rec.append(node.yaw)
            direct_rec.append(node.direct)

            dy = (node.yaw - yaw_old) / (node.v * C.dt)
            steer = rs.pi_2_pi(-math.atan(C.WB * dy))

            yaw_old = node.yaw
            x0 = x_rec[-1]
            y0 = y_rec[-1]
            yaw0 = yaw_rec[-1]

            plt.cla()
            plt.plot(x_all, y_all, color='gray', linewidth=2.0)
            plt.plot(x_rec, y_rec, linewidth=2.0, color='darkviolet')
            plt.plot(cx[ind], cy[ind], '.r')
            draw.draw_car(node.x, node.y, node.yaw, steer, C)
            plt.axis(""equal"")
            plt.title(""RearWheelFeedback: v="" + str(node.v * 3.6)[:4] + ""km/h"")
            plt.gcf().canvas.mpl_connect('key_release_event',
                                         lambda event:
                                         [exit(0) if event.key == 'escape' else None])
            plt.pause(0.001)

    plt.show()",_20956.py,18,"for (cx, cy, cyaw, cdirect, ccurv) in zip(x_ref, y_ref, yaw_ref, direct, curv):
    t = 0.0
    node = Node(x=x0, y=y0, yaw=yaw0, v=0.0, direct=cdirect[0])
    ref_trajectory = PATH(cx, cy, cyaw, ccurv)
    while t < maxTime:
        if cdirect[0] > 0:
            speed_ref = 30.0 / 3.6
            C.Ld = 3.5
        else:
            speed_ref = 15.0 / 3.6
            C.Ld = 2.5
        (delta, ind) = rear_wheel_feedback_control(node, ref_trajectory)
        dist = math.hypot(node.x - cx[-1], node.y - cy[-1])
        acceleration = pid_control(speed_ref, node.v, dist, node.direct)
        node.update(acceleration, delta, node.direct)
        t += C.dt
        if dist <= C.dist_stop:
            break
        x_rec.append(node.x)
        y_rec.append(node.y)
        yaw_rec.append(node.yaw)
        direct_rec.append(node.direct)
        dy = (node.yaw - yaw_old) / (node.v * C.dt)
        steer = rs.pi_2_pi(-math.atan(C.WB * dy))
        yaw_old = node.yaw
        x0 = x_rec[-1]
        y0 = y_rec[-1]
        yaw0 = yaw_rec[-1]
        plt.cla()
        plt.plot(x_all, y_all, color='gray', linewidth=2.0)
        plt.plot(x_rec, y_rec, linewidth=2.0, color='darkviolet')
        plt.plot(cx[ind], cy[ind], '.r')
        draw.draw_car(node.x, node.y, node.yaw, steer, C)
        plt.axis('equal')
        plt.title('RearWheelFeedback: v=' + str(node.v * 3.6)[:4] + 'km/h')
        plt.gcf().canvas.mpl_connect('key_release_event', lambda event: [exit(0) if event.key == 'escape' else None])
        plt.pause(0.001)","for (cx, cy, cyaw, (cdirect_0, *remain), ccurv) in zip(x_ref, y_ref, yaw_ref, direct, curv):
    t = 0.0
    node = Node(x=x0, y=y0, yaw=yaw0, v=0.0, direct=cdirect_0)
    ref_trajectory = PATH(cx, cy, cyaw, ccurv)
    while t < maxTime:
        if cdirect_0 > 0:
            speed_ref = 30.0 / 3.6
            C.Ld = 3.5
        else:
            speed_ref = 15.0 / 3.6
            C.Ld = 2.5
        (delta, ind) = rear_wheel_feedback_control(node, ref_trajectory)
        dist = math.hypot(node.x - cx[-1], node.y - cy[-1])
        acceleration = pid_control(speed_ref, node.v, dist, node.direct)
        node.update(acceleration, delta, node.direct)
        t += C.dt
        if dist <= C.dist_stop:
            break
        x_rec.append(node.x)
        y_rec.append(node.y)
        yaw_rec.append(node.yaw)
        direct_rec.append(node.direct)
        dy = (node.yaw - yaw_old) / (node.v * C.dt)
        steer = rs.pi_2_pi(-math.atan(C.WB * dy))
        yaw_old = node.yaw
        x0 = x_rec[-1]
        y0 = y_rec[-1]
        yaw0 = yaw_rec[-1]
        plt.cla()
        plt.plot(x_all, y_all, color='gray', linewidth=2.0)
        plt.plot(x_rec, y_rec, linewidth=2.0, color='darkviolet')
        plt.plot(cx[ind], cy[ind], '.r')
        draw.draw_car(node.x, node.y, node.yaw, steer, C)
        plt.axis('equal')
        plt.title('RearWheelFeedback: v=' + str(node.v * 3.6)[:4] + 'km/h')
        plt.gcf().canvas.mpl_connect('key_release_event', lambda event: [exit(0) if event.key == 'escape' else None])
        plt.pause(0.001)"
https://github.com/napari/napari/tree/master/napari/layers/vectors/vectors.py,"def _update_thumbnail(self):
        """"""Update thumbnail with current vectors and colors.""""""
        # Set the default thumbnail to black, opacity 1
        colormapped = np.zeros(self._thumbnail_shape)
        colormapped[..., 3] = 1
        if len(self.data) == 0:
            self.thumbnail = colormapped
        else:
            # calculate min vals for the vertices and pad with 0.5
            # the offset is needed to ensure that the top left corner of the
            # vectors corresponds to the top left corner of the thumbnail
            de = self._extent_data
            offset = (
                np.array([de[0, d] for d in self._slice_input.displayed]) + 0.5
            )[-2:]
            # calculate range of values for the vertices and pad with 1
            # padding ensures the entire vector can be represented in the thumbnail
            # without getting clipped
            shape = np.ceil(
                [de[1, d] - de[0, d] + 1 for d in self._slice_input.displayed]
            ).astype(int)[-2:]
            zoom_factor = np.divide(self._thumbnail_shape[:2], shape).min()

            if self._view_data.shape[0] > self._max_vectors_thumbnail:
                thumbnail_indices = np.random.randint(
                    0, self._view_data.shape[0], self._max_vectors_thumbnail
                )
                vectors = copy(self._view_data[thumbnail_indices, :, -2:])
                thumbnail_color_indices = self._view_indices[thumbnail_indices]
            else:
                vectors = copy(self._view_data[:, :, -2:])
                thumbnail_color_indices = self._view_indices
            vectors[:, 1, :] = (
                vectors[:, 0, :] + vectors[:, 1, :] * self.length
            )
            downsampled = (vectors - offset) * zoom_factor
            downsampled = np.clip(
                downsampled, 0, np.subtract(self._thumbnail_shape[:2], 1)
            )
            edge_colors = self._edge.colors[thumbnail_color_indices]
            for v, ec in zip(downsampled, edge_colors):
                start = v[0]
                stop = v[1]
                step = int(np.ceil(np.max(abs(stop - start))))
                x_vals = np.linspace(start[0], stop[0], step)
                y_vals = np.linspace(start[1], stop[1], step)
                for x, y in zip(x_vals, y_vals):
                    colormapped[int(x), int(y), :] = ec
            colormapped[..., 3] *= self.opacity
            self.thumbnail = colormapped",_22128.py,41,"for (v, ec) in zip(downsampled, edge_colors):
    start = v[0]
    stop = v[1]
    step = int(np.ceil(np.max(abs(stop - start))))
    x_vals = np.linspace(start[0], stop[0], step)
    y_vals = np.linspace(start[1], stop[1], step)
    for (x, y) in zip(x_vals, y_vals):
        colormapped[int(x), int(y), :] = ec","for ((start, stop, *remain), ec) in zip(downsampled, edge_colors):
    step = int(np.ceil(np.max(abs(stop - start))))
    x_vals = np.linspace(start[0], stop[0], step)
    y_vals = np.linspace(start[1], stop[1], step)
    for (x, y) in zip(x_vals, y_vals):
        colormapped[int(x), int(y), :] = ec"
https://github.com/lyft/l5kit/tree/master/l5kit/l5kit/dataset/select_agents.py,"def select_agents(
        zarr_dataset: ChunkedDataset,
        th_agent_prob: float,
        th_yaw_degree: float,
        th_extent_ratio: float,
        th_distance_av: float,
) -> None:
    """"""
    Filter agents from zarr INPUT_FOLDER according to multiple thresholds and store a boolean array of the same shape.
    """"""
    agents_mask_path = Path(zarr_dataset.path) / f""agents_mask/{th_agent_prob}""

    if agents_mask_path.exists():
        raise FileExistsError(f""{th_agent_prob} exists already! only one is supported!"")

    frame_index_intervals = zarr_dataset.scenes[""frame_index_interval""]

    # build a partial with all args except the first one (will be passed by threads)
    get_valid_agents_partial = partial(
        get_valid_agents,
        dataset=zarr_dataset,
        th_agent_filter_probability_threshold=th_agent_prob,
        th_yaw_degree=th_yaw_degree,
        th_extent_ratio=th_extent_ratio,
        th_distance_av=th_distance_av,
    )

    try:
        root = zarr.open(zarr_dataset.path, mode=""a"")
        root.create_group(""agents_mask"")
    except ValueError:
        pass  # group is already there

    agents_mask = zarr.open_array(
        str(agents_mask_path),
        mode=""w"",
        shape=(len(zarr_dataset.agents), 2),
        chunks=(10000,),
        dtype=np.uint32,
        synchronizer=zarr.ProcessSynchronizer(Path(gettempdir()) / f""ag_mask_{str(uuid4())}.sync""),
    )

    report: Counter = Counter()
    print(""starting pool..."")
    with Pool(cpu_count()) as pool:
        tasks = tqdm(enumerate(pool.imap_unordered(get_valid_agents_partial, frame_index_intervals)))
        for idx, (mask, count, agents_range) in tasks:
            report += count
            agents_mask[agents_range[0]: agents_range[1]] = mask
            tasks.set_description(f""{idx + 1}/{len(frame_index_intervals)}"")
        print(""collecting results.."")

    agents_cfg = {
        ""th_agent_filter_probability_threshold"": th_agent_prob,
        ""th_yaw_degree"": th_yaw_degree,
        ""th_extent_ratio"": th_extent_ratio,
        ""th_distance_av"": th_distance_av,
    }
    # print report
    pp = pprint.PrettyPrinter(indent=4)
    print(f""start report for {zarr_dataset.path}"")
    pp.pprint({**agents_cfg, **report})

    future_steps = [0, 10, 30, 50]
    past_steps = [0, 10, 30, 50]
    agents_mask_np = np.asarray(agents_mask)

    table = PrettyTable(field_names=[""past/future""] + [str(step) for step in future_steps])
    for step_p in tqdm(past_steps, desc=""computing past/future table""):
        row = [step_p]
        for step_f in future_steps:
            past_mask = agents_mask_np[:, 0] >= step_p
            future_mask = agents_mask_np[:, 1] >= step_f
            row.append(np.sum(past_mask * future_mask))
        table.add_row(row)
    print(table)
    print(f""end report for {zarr_dataset.path}"")
    print(""=============================="")",_22369.py,47,"for (idx, (mask, count, agents_range)) in tasks:
    report += count
    agents_mask[agents_range[0]:agents_range[1]] = mask
    tasks.set_description(f'{idx + 1}/{len(frame_index_intervals)}')","for (idx, (mask, count, (agents_range_0, agents_range_1, *remain))) in tasks:
    report += count
    agents_mask[agents_range_0:agents_range_1] = mask
    tasks.set_description(f'{idx + 1}/{len(frame_index_intervals)}')"
https://github.com/kootenpv/gittyleaks/tree/master/gittyleaks/gittyleaks.py,"def print_matches(self):
        if self.matched_items:
            print('----------------------------------------')

        for k, v in self.matched_items.items():
            for appear in set([x[0] for x in v]):
                # 32 is green, 31 is red
                if not self.no_fancy_color:
                    fname = colorize(k[0], '36')
                    appear = appear.replace(k[1], colorize(k[1], '33'))
                    appear = appear.replace(k[2], colorize(k[2], '31'))
                else:
                    fname = k[0]

                print('{}: {}'.format(fname, appear))",_24801.py,5,"for (k, v) in self.matched_items.items():
    for appear in set([x[0] for x in v]):
        if not self.no_fancy_color:
            fname = colorize(k[0], '36')
            appear = appear.replace(k[1], colorize(k[1], '33'))
            appear = appear.replace(k[2], colorize(k[2], '31'))
        else:
            fname = k[0]
        print('{}: {}'.format(fname, appear))","for ((k_0, k_1, k_2, *remain), v) in self.matched_items.items():
    for appear in set([x[0] for x in v]):
        if not self.no_fancy_color:
            fname = colorize(k_0, '36')
            appear = appear.replace(k_1, colorize(k_1, '33'))
            appear = appear.replace(k_2, colorize(k_2, '31'))
        else:
            fname = k_0
        print('{}: {}'.format(fname, appear))"
https://github.com/HuberTRoy/leetCode/tree/master/Array/NumberOfIslands.py,"def helper(x, y):
            Xy = self.makeXY(x, y)
            for i in Xy:
                try:
                    if i[1] < 0 or i[0] < 0:
                        continue
                        
                    if court[i[1]][i[0]] == '1':
                        court[i[1]][i[0]] = '0'
                        t = helper(i[0], i[1])
                except IndexError:
                    continue
            else:
                return 1",_27150.py,3,"for i in Xy:
    try:
        if i[1] < 0 or i[0] < 0:
            continue
        if court[i[1]][i[0]] == '1':
            court[i[1]][i[0]] = '0'
            t = helper(i[0], i[1])
    except IndexError:
        continue
else:
    return 1","for i_0, i_1, *remain in Xy:
    try:
        if i[1] < 0 or i_0 < 0:
            continue
        if court[i_1][i_0] == '1':
            court[i_1][i_0] = '0'
            t = helper(i_0, i_1])
    except IndexError:
        continue
else:
    return 1"
https://github.com/datastax/python-driver/tree/master/tests/unit/test_util_types.py,"def test_version_parsing(self):
        versions = [
            ('2.0.0', (2, 0, 0, 0, 0)),
            ('3.1.0', (3, 1, 0, 0, 0)),
            ('2.4.54', (2, 4, 54, 0, 0)),
            ('3.1.1.12', (3, 1, 1, 12, 0)),
            ('3.55.1.build12', (3, 55, 1, 'build12', 0)),
            ('3.55.1.20190429-TEST', (3, 55, 1, 20190429, 'TEST')),
            ('4.0-SNAPSHOT', (4, 0, 0, 0, 'SNAPSHOT')),
            ('1.0.5.4.3', (1, 0, 5, 4, 0)),
            ('1-SNAPSHOT', (1, 0, 0, 0, 'SNAPSHOT')),
            ('4.0.1.2.3.4.5-ABC-123-SNAP-TEST.blah', (4, 0, 1, 2, 'ABC-123-SNAP-TEST.blah')),
            ('2.1.hello', (2, 1, 0, 0, 0)),
            ('2.test.1', (2, 0, 0, 0, 0)),
        ]

        for str_version, expected_result in versions:
            v = Version(str_version)
            self.assertEqual(str_version, str(v))
            self.assertEqual(v.major, expected_result[0])
            self.assertEqual(v.minor, expected_result[1])
            self.assertEqual(v.patch, expected_result[2])
            self.assertEqual(v.build, expected_result[3])
            self.assertEqual(v.prerelease, expected_result[4])

        # not supported version formats
        with self.assertRaises(ValueError):
            Version('test.1.0')",_27186.py,17,"for (str_version, expected_result) in versions:
    v = Version(str_version)
    self.assertEqual(str_version, str(v))
    self.assertEqual(v.major, expected_result[0])
    self.assertEqual(v.minor, expected_result[1])
    self.assertEqual(v.patch, expected_result[2])
    self.assertEqual(v.build, expected_result[3])
    self.assertEqual(v.prerelease, expected_result[4])","for (str_version, (expected_result_0, expected_result_1, expected_result_2, expected_result_3, *remain)) in versions:
    v = Version(str_version)
    self.assertEqual(str_version, str(v))
    self.assertEqual(v.major, expected_result_0)
    self.assertEqual(v.minor, expected_result_1)
    self.assertEqual(v.patch, expected_result_2)
    self.assertEqual(v.build, expected_result_3)
    self.assertEqual(v.prerelease, expected_result_4)"
https://github.com/automl/SMAC3/tree/master/smac/epm/gaussian_process.py,"def _optimize(self) -> np.ndarray:
        """"""
        Optimizes the marginal log likelihood and returns the best found
        hyperparameter configuration theta.

        Returns
        -------
        theta : np.ndarray(H)
            Hyperparameter vector that maximizes the marginal log likelihood
        """"""

        log_bounds = [(b[0], b[1]) for b in self.gp.kernel.bounds]

        # Start optimization from the previous hyperparameter configuration
        p0 = [self.gp.kernel.theta]
        if self.n_opt_restarts > 0:
            dim_samples = []

            prior = None  # type: typing.Optional[typing.Union[typing.List[Prior], Prior]]
            for dim, hp_bound in enumerate(log_bounds):
                prior = self._all_priors[dim]
                # Always sample from the first prior
                if isinstance(prior, list):
                    if len(prior) == 0:
                        prior = None
                    else:
                        prior = prior[0]
                prior = typing.cast(typing.Optional[Prior], prior)
                if prior is None:
                    try:
                        sample = self.rng.uniform(
                            low=hp_bound[0],
                            high=hp_bound[1],
                            size=(self.n_opt_restarts,),
                        )
                    except OverflowError:
                        raise ValueError('OverflowError while sampling from (%f, %f)' % (hp_bound[0], hp_bound[1]))
                    dim_samples.append(sample.flatten())
                else:
                    dim_samples.append(prior.sample_from_prior(self.n_opt_restarts).flatten())
            p0 += list(np.vstack(dim_samples).transpose())

        theta_star = None
        f_opt_star = np.inf
        for i, start_point in enumerate(p0):
            theta, f_opt, _ = optimize.fmin_l_bfgs_b(self._nll, start_point, bounds=log_bounds)
            if f_opt < f_opt_star:
                f_opt_star = f_opt
                theta_star = theta
        return theta_star",_27442.py,20,"for (dim, hp_bound) in enumerate(log_bounds):
    prior = self._all_priors[dim]
    if isinstance(prior, list):
        if len(prior) == 0:
            prior = None
        else:
            prior = prior[0]
    prior = typing.cast(typing.Optional[Prior], prior)
    if prior is None:
        try:
            sample = self.rng.uniform(low=hp_bound[0], high=hp_bound[1], size=(self.n_opt_restarts,))
        except OverflowError:
            raise ValueError('OverflowError while sampling from (%f, %f)' % (hp_bound[0], hp_bound[1]))
        dim_samples.append(sample.flatten())
    else:
        dim_samples.append(prior.sample_from_prior(self.n_opt_restarts).flatten())","for (dim, (hp_bound_0, hp_bound_1, *remain)) in enumerate(log_bounds):
    prior = self._all_priors[dim]
    if isinstance(prior, list):
        if len(prior) == 0:
            prior = None
        else:
            prior = prior[0]
    prior = typing.cast(typing.Optional[Prior], prior)
    if prior is None:
        try:
            sample = self.rng.uniform(low=hp_bound_0, high=hp_bound_1 size=(self.n_opt_restarts,))
        except OverflowError:
            raise ValueError('OverflowError while sampling from (%f, %f)' % (hp_bound_0, hp_bound_1))
        dim_samples.append(sample.flatten())
    else:
        dim_samples.append(prior.sample_from_prior(self.n_opt_restarts).flatten())"
https://github.com/lutris/lutris/tree/master/lutris/util/libretro.py,"def __setitem__(self, key, value):
        for index, conf in enumerate(self.config):
            if key == conf[0]:
                # self.config is read-only
                self._config[index] = (key, self.serialize_value(value))
                return
        self._config.append((key, self.serialize_value(value)))",_28069.py,2,"for (index, conf) in enumerate(self.config):
    if key == conf[0]:
        self._config[index] = (key, self.serialize_value(value))
        return","for (index, (conf_0, *remain)) in enumerate(self.config):
    if key == conf_0:
        self._config[index] = (key, self.serialize_value(value))
        return"
https://github.com/WyattBlue/auto-editor/tree/master/auto_editor/formats/premiere.py,"def handle_video_clips(outfile, clips, inp, timebase, duration, width, height, sr,
    pathurls):
    tracks = len(inp.audio_streams)
    total = 0
    for j, clip in enumerate(clips):
        my_start = int(total)
        total += (clip[1] - clip[0]) / (clip[2] / 100)
        my_end = int(total)

        outfile.write(indent(5, '<clipitem id=""clipitem-{}"">'.format(j+1),
            '\t<masterclipid>masterclip-2</masterclipid>',
            '\t<name>{}</name>'.format(inp.basename),
            '\t<start>{}</start>'.format(my_start),
            '\t<end>{}</end>'.format(my_end),
            '\t<in>{}</in>'.format(int(clip[0] / (clip[2] / 100))),
            '\t<out>{}</out>'.format(int(clip[1] / (clip[2] / 100)))))

        if(j == 0):
            outfile.write(indent(6, '<file id=""file-1"">',
                '\t<name>{}</name>'.format(inp.basename),
                '\t<pathurl>{}</pathurl>'.format(pathurls[0]),
                '\t<rate>',
                '\t\t<timebase>{}</timebase>'.format(timebase),
                '\t\t<ntsc>{}</ntsc>'.format(ntsc),
                '\t</rate>',
                '\t<duration>{}</duration>'.format(duration),
                '\t<media>', '\t\t<video>',
                '\t\t\t<samplecharacteristics>',
                '\t\t\t\t<rate>',
                '\t\t\t\t\t<timebase>{}</timebase>'.format(timebase),
                '\t\t\t\t\t<ntsc>{}</ntsc>'.format(ntsc),
                '\t\t\t\t</rate>',
                '\t\t\t\t<width>{}</width>'.format(width),
                '\t\t\t\t<height>{}</height>'.format(height),
                '\t\t\t\t<anamorphic>{}</anamorphic>'.format(ana),
                '\t\t\t\t<pixelaspectratio>{}</pixelaspectratio>'.format(pixelar),
                '\t\t\t\t<fielddominance>none</fielddominance>',
                '\t\t\t</samplecharacteristics>',
                '\t\t</video>', '\t\t<audio>',
                '\t\t\t<samplecharacteristics>',
                '\t\t\t\t<depth>{}</depth>'.format(depth),
                '\t\t\t\t<samplerate>{}</samplerate>'.format(sr),
                '\t\t\t</samplecharacteristics>',
                '\t\t\t<channelcount>2</channelcount>',
                '\t\t</audio>', '\t</media>', '</file>'))
        else:
            outfile.write('\t\t\t\t\t\t<file id=""file-1""/>\n')

        if(clip[2] != 100):
            outfile.write(speedup(clip[2]))

        # Linking for video blocks
        for i in range(max(3, tracks + 1)):
            outfile.write('\t\t\t\t\t\t<link>\n')
            outfile.write('\t\t\t\t\t\t\t<linkclipref>clipitem-{}</linkclipref>\n'.format((i*(len(clips)))+j+1))
            if(i == 0):
                outfile.write('\t\t\t\t\t\t\t<mediatype>video</mediatype>\n')
            else:
                outfile.write('\t\t\t\t\t\t\t<mediatype>audio</mediatype>\n')
            if(i == 2):
                outfile.write('\t\t\t\t\t\t\t<trackindex>2</trackindex>\n')
            else:
                outfile.write('\t\t\t\t\t\t\t<trackindex>1</trackindex>\n')
            outfile.write('\t\t\t\t\t\t\t<clipindex>{}</clipindex>\n'.format(j+1))
            if(i > 0):
                outfile.write('\t\t\t\t\t\t\t<groupindex>1</groupindex>\n')
            outfile.write('\t\t\t\t\t\t</link>\n')

        outfile.write('\t\t\t\t\t</clipitem>\n')
    outfile.write(indent(3, '\t</track>', '</video>'))",_28504.py,5,"for (j, clip) in enumerate(clips):
    my_start = int(total)
    total += (clip[1] - clip[0]) / (clip[2] / 100)
    my_end = int(total)
    outfile.write(indent(5, '<clipitem id=""clipitem-{}"">'.format(j + 1), '\t<masterclipid>masterclip-2</masterclipid>', '\t<name>{}</name>'.format(inp.basename), '\t<start>{}</start>'.format(my_start), '\t<end>{}</end>'.format(my_end), '\t<in>{}</in>'.format(int(clip[0] / (clip[2] / 100))), '\t<out>{}</out>'.format(int(clip[1] / (clip[2] / 100)))))
    if j == 0:
        outfile.write(indent(6, '<file id=""file-1"">', '\t<name>{}</name>'.format(inp.basename), '\t<pathurl>{}</pathurl>'.format(pathurls[0]), '\t<rate>', '\t\t<timebase>{}</timebase>'.format(timebase), '\t\t<ntsc>{}</ntsc>'.format(ntsc), '\t</rate>', '\t<duration>{}</duration>'.format(duration), '\t<media>', '\t\t<video>', '\t\t\t<samplecharacteristics>', '\t\t\t\t<rate>', '\t\t\t\t\t<timebase>{}</timebase>'.format(timebase), '\t\t\t\t\t<ntsc>{}</ntsc>'.format(ntsc), '\t\t\t\t</rate>', '\t\t\t\t<width>{}</width>'.format(width), '\t\t\t\t<height>{}</height>'.format(height), '\t\t\t\t<anamorphic>{}</anamorphic>'.format(ana), '\t\t\t\t<pixelaspectratio>{}</pixelaspectratio>'.format(pixelar), '\t\t\t\t<fielddominance>none</fielddominance>', '\t\t\t</samplecharacteristics>', '\t\t</video>', '\t\t<audio>', '\t\t\t<samplecharacteristics>', '\t\t\t\t<depth>{}</depth>'.format(depth), '\t\t\t\t<samplerate>{}</samplerate>'.format(sr), '\t\t\t</samplecharacteristics>', '\t\t\t<channelcount>2</channelcount>', '\t\t</audio>', '\t</media>', '</file>'))
    else:
        outfile.write('\t\t\t\t\t\t<file id=""file-1""/>\n')
    if clip[2] != 100:
        outfile.write(speedup(clip[2]))
    for i in range(max(3, tracks + 1)):
        outfile.write('\t\t\t\t\t\t<link>\n')
        outfile.write('\t\t\t\t\t\t\t<linkclipref>clipitem-{}</linkclipref>\n'.format(i * len(clips) + j + 1))
        if i == 0:
            outfile.write('\t\t\t\t\t\t\t<mediatype>video</mediatype>\n')
        else:
            outfile.write('\t\t\t\t\t\t\t<mediatype>audio</mediatype>\n')
        if i == 2:
            outfile.write('\t\t\t\t\t\t\t<trackindex>2</trackindex>\n')
        else:
            outfile.write('\t\t\t\t\t\t\t<trackindex>1</trackindex>\n')
        outfile.write('\t\t\t\t\t\t\t<clipindex>{}</clipindex>\n'.format(j + 1))
        if i > 0:
            outfile.write('\t\t\t\t\t\t\t<groupindex>1</groupindex>\n')
        outfile.write('\t\t\t\t\t\t</link>\n')
    outfile.write('\t\t\t\t\t</clipitem>\n')","for (j, (clip_0, clip_1, clip_2, *remain)) in enumerate(clips):
    my_start = int(total)
    total += (clip_1- clip_0) / (clip_2 / 100)
    my_end = int(total)
    outfile.write(indent(5, '<clipitem id=""clipitem-{}"">'.format(j + 1), '\t<masterclipid>masterclip-2</masterclipid>', '\t<name>{}</name>'.format(inp.basename), '\t<start>{}</start>'.format(my_start), '\t<end>{}</end>'.format(my_end), ‘\t<in>{}</in>'.format(int(clip_0 / (clip_2 / 100))), '\t<out>{}</out>'.format(int(clip_1 / (clip_2 / 100)))))
    if j == 0:
        outfile.write(indent(6, '<file id=""file-1"">', '\t<name>{}</name>'.format(inp.basename), '\t<pathurl>{}</pathurl>'.format(pathurls[0]), '\t<rate>', '\t\t<timebase>{}</timebase>'.format(timebase), '\t\t<ntsc>{}</ntsc>'.format(ntsc), '\t</rate>', '\t<duration>{}</duration>'.format(duration), '\t<media>', '\t\t<video>', '\t\t\t<samplecharacteristics>', '\t\t\t\t<rate>', '\t\t\t\t\t<timebase>{}</timebase>'.format(timebase), '\t\t\t\t\t<ntsc>{}</ntsc>'.format(ntsc), '\t\t\t\t</rate>', '\t\t\t\t<width>{}</width>'.format(width), '\t\t\t\t<height>{}</height>'.format(height), '\t\t\t\t<anamorphic>{}</anamorphic>'.format(ana), '\t\t\t\t<pixelaspectratio>{}</pixelaspectratio>'.format(pixelar), '\t\t\t\t<fielddominance>none</fielddominance>', '\t\t\t</samplecharacteristics>', '\t\t</video>', '\t\t<audio>', '\t\t\t<samplecharacteristics>', '\t\t\t\t<depth>{}</depth>'.format(depth), '\t\t\t\t<samplerate>{}</samplerate>'.format(sr), '\t\t\t</samplecharacteristics>', '\t\t\t<channelcount>2</channelcount>', '\t\t</audio>', '\t</media>', '</file>'))
    else:
        outfile.write('\t\t\t\t\t\t<file id=""file-1""/>\n')
    if clip_2 != 100:
        outfile.write(speedup(clip_2))
    for i in range(max(3, tracks + 1)):
        outfile.write('\t\t\t\t\t\t<link>\n')
        outfile.write('\t\t\t\t\t\t\t<linkclipref>clipitem-{}</linkclipref>\n'.format(i * len(clips) + j + 1))
        if i == 0:
            outfile.write('\t\t\t\t\t\t\t<mediatype>video</mediatype>\n')
        else:
            outfile.write('\t\t\t\t\t\t\t<mediatype>audio</mediatype>\n')
        if i == 2:
            outfile.write('\t\t\t\t\t\t\t<trackindex>2</trackindex>\n')
        else:
            outfile.write('\t\t\t\t\t\t\t<trackindex>1</trackindex>\n')
        outfile.write('\t\t\t\t\t\t\t<clipindex>{}</clipindex>\n'.format(j + 1))
        if i > 0:
            outfile.write('\t\t\t\t\t\t\t<groupindex>1</groupindex>\n')
        outfile.write('\t\t\t\t\t\t</link>\n')
    outfile.write('\t\t\t\t\t</clipitem>\n')"
https://github.com/microsoft/unilm/tree/master/xtune/src/transformers/data/processors/glue.py,"def _create_examples(self, lines, set_type):
        """"""Creates examples for the training and dev sets.""""""
        examples = []
        for (i, line) in enumerate(lines):
            if i == 0:
                continue
            guid = ""%s-%s"" % (set_type, i)
            text_a = line[3]
            text_b = line[4]
            label = line[0]
            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
        return examples",_28511.py,4,"for (i, line) in enumerate(lines):
    if i == 0:
        continue
    guid = '%s-%s' % (set_type, i)
    text_a = line[3]
    text_b = line[4]
    label = line[0]
    examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))","for (i, (line_0, _, _, line_3, line_4, *remain)) in enumerate(lines):
    if i == 0:
        continue
    guid = '%s-%s' % (set_type, i)
    text_a = line_3
    text_b = line_4
    label = line_0
    examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))"
https://github.com/Epiphqny/VisTR/tree/master/models/segmentation.py,"def forward(self, results, outputs, orig_target_sizes, max_target_sizes):
        assert len(orig_target_sizes) == len(max_target_sizes)
        max_h, max_w = max_target_sizes.max(0)[0].tolist()
        outputs_masks = outputs[""pred_masks""].squeeze(2)
        outputs_masks = F.interpolate(outputs_masks, size=(max_h, max_w), mode=""bilinear"", align_corners=False)
        outputs_masks = (outputs_masks.sigmoid() > self.threshold).cpu()

        for i, (cur_mask, t, tt) in enumerate(zip(outputs_masks, max_target_sizes, orig_target_sizes)):
            img_h, img_w = t[0], t[1]
            results[i][""masks""] = cur_mask[:, :img_h, :img_w].unsqueeze(1)
            results[i][""masks""] = F.interpolate(
                results[i][""masks""].float(), size=tuple(tt.tolist()), mode=""nearest""
            ).byte()

        return results",_28701.py,8,"for (i, (cur_mask, t, tt)) in enumerate(zip(outputs_masks, max_target_sizes, orig_target_sizes)):
    (img_h, img_w) = (t[0], t[1])
    results[i]['masks'] = cur_mask[:, :img_h, :img_w].unsqueeze(1)
    results[i]['masks'] = F.interpolate(results[i]['masks'].float(), size=tuple(tt.tolist()), mode='nearest').byte()","for (i, (cur_mask, (t_0, t_1, *remain), tt)) in enumerate(zip(outputs_masks, max_target_sizes, orig_target_sizes)):
    (img_h, img_w) = (t_0, t_1)
    results[i]['masks'] = cur_mask[:, :img_h, :img_w].unsqueeze(1)
    results[i]['masks'] = F.interpolate(results[i]['masks'].float(), size=tuple(tt.tolist()), mode=‘nearest').byte()"
https://github.com/jookies/jasmin/tree/master/tests/routing/test_routing_submit_sm_and_dlr.py,"def test_correct_args_dlr_level1(self):
        """"""Related to #71
        Will check for correct args when throwing a level1 ack
        """"""
        yield self.connect('127.0.0.1', self.pbPort)
        yield self.prepareRoutingsAndStartConnector()

        self.params['dlr-url'] = self.dlr_url
        self.params['dlr-level'] = 1
        baseurl = 'http://127.0.0.1:1401/send'

        # Send a MT
        # We should receive a msg id
        agent = Agent(reactor)
        client = HTTPClient(agent)
        response = yield client.post(baseurl, data=self.params)
        c = yield text_content(response)
        msgStatus = c[:7]
        msgId = c[9:45]

        # Wait 1 seconds for submit_sm_resp
        yield waitFor(1)

        yield self.stopSmppClientConnectors()

        # Run tests
        self.assertEqual(msgStatus, 'Success')
        # A DLR must be sent to dlr_url
        self.assertEqual(self.AckServerResource.render_POST.call_count, 1)
        # Args assertions
        callArgs = self.AckServerResource.render_POST.call_args_list[0][0][0].args
        self.assertEqual(len(callArgs), 4)
        self.assertTrue(b'id' in callArgs)
        self.assertTrue(b'message_status' in callArgs)
        self.assertTrue(b'level' in callArgs)
        self.assertTrue(b'connector' in callArgs)
        self.assertEqual(callArgs[b'level'][0], b'1')
        for k, v in callArgs.items():
            self.assertNotEqual(v[0], b'')",_29164.py,38,"for (k, v) in callArgs.items():
    self.assertNotEqual(v[0], b'')","for (k, (v_0, *remain)) in callArgs.items():
    self.assertNotEqual(v_0, b'')"
https://github.com/gnes-ai/gnes/tree/master/gnes/preprocessor/image/segmentation.py,"def apply(self, doc: 'gnes_pb2.Document'):
        super().apply(doc)
        if doc.raw_bytes:
            original_image = Image.open(io.BytesIO(doc.raw_bytes))
            all_subareas, index = get_all_subarea(original_image)
            image_tensor = torch_transform(original_image)
            if self.on_gpu:
                image_tensor = image_tensor.cuda()

            seg_output = self._model([image_tensor])

            weight = seg_output[0]['scores'].tolist()
            length = len(list(filter(lambda x: x >= 0.5, weight)))
            chunks = seg_output[0]['boxes'].tolist()[:length]
            weight = weight[:length]

            for ci, ele in enumerate(zip(chunks, weight)):
                c = doc.chunks.add()
                c.doc_id = doc.doc_id
                c.blob.CopyFrom(array2blob(self._crop(original_image, ele[0])))
                c.offset = ci
                c.offset_nd.extend(self._get_seg_offset_nd(all_subareas, index, ele[0]))
                c.weight = self._cal_area(ele[0]) / (original_image.size[0] * original_image.size[1])

            c = doc.chunks.add()
            c.doc_id = doc.doc_id
            c.blob.CopyFrom(array2blob(np.array(original_image)))
            c.offset = len(chunks)
            c.offset_nd.extend([100, 100])
            c.weight = 1.
        else:
            self.logger.error('bad document: ""raw_bytes"" is empty!')",_30429.py,17,"for (ci, ele) in enumerate(zip(chunks, weight)):
    c = doc.chunks.add()
    c.doc_id = doc.doc_id
    c.blob.CopyFrom(array2blob(self._crop(original_image, ele[0])))
    c.offset = ci
    c.offset_nd.extend(self._get_seg_offset_nd(all_subareas, index, ele[0]))
    c.weight = self._cal_area(ele[0]) / (original_image.size[0] * original_image.size[1])","for (ci, (ele_0, *remain)) in enumerate(zip(chunks, weight)):
    c = doc.chunks.add()
    c.doc_id = doc.doc_id
    c.blob.CopyFrom(array2blob(self._crop(original_image, ele_0)))
    c.offset = ci
    c.offset_nd.extend(self._get_seg_offset_nd(all_subareas, index, ele_0))
    c.weight = self._cal_area(ele_0) / (original_image.size[0] * original_image.size[1])"
https://github.com/google/capirca/tree/master/capirca/lib/junipermsmpc.py,"def __str__(self):
    # Verify platform specific terms. Skip whole term if platform does not
    # match.
    if self.term.platform:
      if self._PLATFORM not in self.term.platform:
        return ''
    if self.term.platform_exclude:
      if self._PLATFORM in self.term.platform_exclude:
        return ''

    if self.enable_dsmo:
      raise NotImplementedError('enable_dsmo not implemented for msmpc')

    ret_str = juniper.Config(indent=self._DEFAULT_INDENT)

    # COMMENTS
    # this deals just fine with multi line comments, but we could probably
    # output them a little cleaner; do things like make sure the
    # len(output) < 80, etc. Note, if 'noverbose' is set for the filter, skip
    # all comment processing.
    if not self.noverbose:
      if self.term.owner:
        self.term.comment.append('Owner: %s' % self.term.owner)
      if self.term.comment:
        ret_str.Append('/*')
        for comment in self.term.comment:
          for line in comment.split('\n'):
            ret_str.Append('** ' + line)
        ret_str.Append('*/')

    # Term verbatim output - this will skip over normal term creation
    # code.  Warning generated from policy.py if appropriate.
    if self.term.verbatim:
      for next_term in self.term.verbatim:
        if next_term[0] == self._PLATFORM:
          ret_str.Append(str(next_term[1]), verbatim=True)
      return str(ret_str)

    # Determine whether there are any match conditions for the term.
    has_match_criteria = (
        self.term.address or self.term.dscp_except or self.term.dscp_match or
        self.term.destination_address or self.term.destination_port or
        self.term.destination_prefix or self.term.destination_prefix_except or
        self.term.encapsulate or self.term.ether_type or
        self.term.flexible_match_range or self.term.forwarding_class or
        self.term.forwarding_class_except or self.term.fragment_offset or
        self.term.hop_limit or self.term.next_ip or self.term.port or
        self.term.precedence or self.term.protocol or
        self.term.protocol_except or self.term.source_address or
        self.term.source_port or self.term.source_prefix or
        self.term.source_prefix_except or self.term.traffic_type or
        self.term.ttl)

    suffixes = []
    duplicate_term = False
    has_icmp = 'icmp' in self.term.protocol
    has_icmpv6 = 'icmpv6' in self.term.protocol
    has_v4_ip = self.term.GetAddressOfVersion(
        'source_address',
        self.AF_MAP.get('inet')) or self.term.GetAddressOfVersion(
            'source_address_exclude',
            self.AF_MAP.get('inet')) or self.term.GetAddressOfVersion(
                'destination_address',
                self.AF_MAP.get('inet')) or self.term.GetAddressOfVersion(
                    'destination_address_exclude', self.AF_MAP.get('inet'))
    has_v6_ip = self.term.GetAddressOfVersion(
        'source_address',
        self.AF_MAP.get('inet6')) or self.term.GetAddressOfVersion(
            'source_address_exclude',
            self.AF_MAP.get('inet6')) or self.term.GetAddressOfVersion(
                'destination_address',
                self.AF_MAP.get('inet6')) or self.term.GetAddressOfVersion(
                    'destination_address_exclude', self.AF_MAP.get('inet6'))

    if self.term_type == 'mixed':
      if not (has_v4_ip or has_v6_ip):
        suffixes = ['inet']
      elif not has_v6_ip:
        suffixes = ['inet']
      elif not has_v4_ip:
        suffixes = ['inet6']
      else:
        suffixes = ['inet', 'inet6']
        duplicate_term = True
    if not suffixes and self.term_type in ['inet', 'inet6']:
      suffixes = [self.term_type]

    for suffix in suffixes:
      if self.term_type == 'mixed' and (not (has_icmp and has_icmpv6)) and (
          has_v4_ip and has_v6_ip):
        if (has_icmp and suffix != 'inet') or (has_icmpv6 and
                                               suffix != 'inet6'):
          continue
      source_address = self.term.GetAddressOfVersion('source_address',
                                                     self.AF_MAP.get(suffix))
      source_address_exclude = self.term.GetAddressOfVersion(
          'source_address_exclude', self.AF_MAP.get(suffix))
      source_address, source_address_exclude = self._MinimizePrefixes(
          source_address, source_address_exclude)
      destination_address = self.term.GetAddressOfVersion(
          'destination_address', self.AF_MAP.get(suffix))
      destination_address_exclude = self.term.GetAddressOfVersion(
          'destination_address_exclude', self.AF_MAP.get(suffix))
      destination_address, destination_address_exclude = self._MinimizePrefixes(
          destination_address, destination_address_exclude)
      if ((not source_address) and self.term.GetAddressOfVersion(
          'source_address', self.AF_MAP.get('mixed')) and
          not source_address_exclude) or (
              (not destination_address) and self.term.GetAddressOfVersion(
                  'destination_address', self.AF_MAP.get('mixed')) and
              not destination_address_exclude):
        continue
      if ((has_icmpv6 and not has_icmp and suffix == 'inet') or
          (has_icmp and not has_icmpv6 and
           suffix == 'inet6')) and self.term_type != 'mixed':
        logging.debug(
            self.NO_AF_LOG_PROTO.substitute(
                term=self.term.name,
                proto=', '.join(self.term.protocol),
                af=suffix))
        return ''

      # NAME
      # if the term is inactive we have to set the prefix
      if self.term.inactive:
        term_prefix = 'inactive:'
      else:
        term_prefix = ''

      ret_str.Append(
          '%s term %s%s {' %
          (term_prefix, self.term.name, '-' + suffix if duplicate_term else ''))

      # We only need a ""from {"" clause if there are any conditions to match.
      if has_match_criteria:
        ret_str.Append('from {')
        # SOURCE ADDRESS
        if source_address or source_address_exclude:
          ret_str.Append('source-address {')
          if source_address:
            for saddr in source_address:
              for comment in self._Comment(saddr):
                ret_str.Append('%s' % comment)
              if saddr.version == 6 and 0 < saddr.prefixlen < 16:
                for saddr2 in saddr.subnets(new_prefix=16):
                  ret_str.Append('%s;' % saddr2)
              else:
                if saddr == nacaddr.IPv6('0::0/0'):
                  saddr = 'any-ipv6'
                elif saddr == nacaddr.IPv4('0.0.0.0/0'):
                  saddr = 'any-ipv4'
                ret_str.Append('%s;' % saddr)

          # SOURCE ADDRESS EXCLUDE
          if source_address_exclude:
            for ex in source_address_exclude:
              for comment in self._Comment(ex):
                ret_str.Append('%s' % comment)
              if ex.version == 6 and 0 < ex.prefixlen < 16:
                for ex2 in ex.subnets(new_prefix=16):
                  ret_str.Append('%s except;' % ex2)
              else:
                if ex == nacaddr.IPv6('0::0/0'):
                  ex = 'any-ipv6'
                elif ex == nacaddr.IPv4('0.0.0.0/0'):
                  ex = 'any-ipv4'
                ret_str.Append('%s except;' % ex)
          ret_str.Append('}')  # source-address {...}

        # DESTINATION ADDRESS
        if destination_address or destination_address_exclude:
          ret_str.Append('destination-address {')
          if destination_address:
            for daddr in destination_address:
              for comment in self._Comment(daddr):
                ret_str.Append('%s' % comment)
              if daddr.version == 6 and 0 < daddr.prefixlen < 16:
                for daddr2 in daddr.subnets(new_prefix=16):
                  ret_str.Append('%s;' % daddr2)
              else:
                if daddr == nacaddr.IPv6('0::0/0'):
                  daddr = 'any-ipv6'
                elif daddr == nacaddr.IPv4('0.0.0.0/0'):
                  daddr = 'any-ipv4'
                ret_str.Append('%s;' % daddr)

          # DESTINATION ADDRESS EXCLUDE
          if destination_address_exclude:
            for ex in destination_address_exclude:
              for comment in self._Comment(ex):
                ret_str.Append('%s' % comment)
              if ex.version == 6 and 0 < ex.prefixlen < 16:
                for ex2 in ex.subnets(new_prefix=16):
                  ret_str.Append('%s except;' % ex2)
              else:
                if ex == nacaddr.IPv6('0::0/0'):
                  ex = 'any-ipv6'
                elif ex == nacaddr.IPv4('0.0.0.0/0'):
                  ex = 'any-ipv4'
                ret_str.Append('%s except;' % ex)
          ret_str.Append('}')  # destination-address {...}

        # source prefix <except> list
        if self.term.source_prefix or self.term.source_prefix_except:
          for pfx in self.term.source_prefix:
            ret_str.Append('source-prefix-list ' + pfx + ';')
          for epfx in self.term.source_prefix_except:
            ret_str.Append('source-prefix-list ' + epfx + ' except;')

        # destination prefix <except> list
        if self.term.destination_prefix or self.term.destination_prefix_except:
          for pfx in self.term.destination_prefix:
            ret_str.Append('destination-prefix-list ' + pfx + ';')
          for epfx in self.term.destination_prefix_except:
            ret_str.Append('destination-prefix-list ' + epfx + ' except;')

        # APPLICATION
        if (self.term.source_port or self.term.destination_port or
            self.term.icmp_type or self.term.protocol):
          if hasattr(self.term, 'replacement_application_name'):
            ret_str.Append('application-sets ' +
                           self.term.replacement_application_name + '-app;')
          else:
            ret_str.Append('application-sets ' +
                           self.filter_name[:((MAX_IDENTIFIER_LEN) // 2)] +
                           self.term.name[-((MAX_IDENTIFIER_LEN) // 2):] +
                           '-app;')
        ret_str.Append('}')  # from {...}

      ret_str.Append('then {')
      # ACTION
      for action in self.term.action:
        ret_str.Append(self._ACTIONS.get(str(action)) + ';')
      if self.term.logging and 'disable' not in [
          x.value for x in self.term.logging
      ]:
        ret_str.Append('syslog;')
      ret_str.Append('}')  # then {...}
      ret_str.Append('}')  # term {...}
    return str(ret_str)",_147.py,34,"for next_term in self.term.verbatim:
    if next_term[0] == self._PLATFORM:
        ret_str.Append(str(next_term[1]), verbatim=True)","for (next_term_0, next_term_1, *next_term_len) in self.term.verbatim:
    if next_term_0 == self._PLATFORM:
        ret_str.Append(str(next_term_1), verbatim=True)"
https://github.com/Blazemeter/taurus/tree/master/bzt/modules/monitoring.py,"def get_data(self):
        now = time.time()

        if now > self._last_check + self.interval:
            self._cached_data = []
            self._last_check = now
            json_list = self._get_response()
            data_line = [now]

            for element in json_list:
                item = {
                    'ts': now,
                    'source': '%s' % self.host_label}

                for datapoint in reversed(element['datapoints']):
                    if datapoint[0] is not None:
                        item[element['target']] = datapoint[0]
                        data_line.append(datapoint[0])
                        break

                self._cached_data.append(item)

            if self.logs_file:
                with open(self.logs_file, ""a"", newline='') as g_logs:
                    logs_writer = csv.writer(g_logs, delimiter=',')
                    logs_writer.writerow(data_line)

        return self._cached_data",_262.py,15,"for datapoint in reversed(element['datapoints']):
    if datapoint[0] is not None:
        item[element['target']] = datapoint[0]
        data_line.append(datapoint[0])
        break","for (datapoint_0, *datapoint_len) in reversed(element['datapoints']):
    if datapoint_0 is not None:
        item[element['target']] = datapoint_0
        data_line.append(datapoint_0)
        break"
https://github.com/holoviz/holoviews/tree/master/holoviews/plotting/util.py,"def get_directed_graph_paths(element, arrow_length):
    """"""
    Computes paths for a directed path which include an arrow to
    indicate the directionality of each edge.
    """"""
    edgepaths = element._split_edgepaths
    edges = edgepaths.split(datatype='array', dimensions=edgepaths.kdims)
    arrows = []
    for e in edges:
        sx, sy = e[0]
        ex, ey = e[1]
        rad = np.arctan2(ey-sy, ex-sx)
        xa0 = ex - np.cos(rad+np.pi/8)*arrow_length
        ya0 = ey - np.sin(rad+np.pi/8)*arrow_length
        xa1 = ex - np.cos(rad-np.pi/8)*arrow_length
        ya1 = ey - np.sin(rad-np.pi/8)*arrow_length
        arrow = np.array([(sx, sy), (ex, ey), (np.nan, np.nan),
                          (xa0, ya0), (ex, ey), (xa1, ya1)])
        arrows.append(arrow)
    return arrows",_460.py,9,"for e in edges:
    (sx, sy) = e[0]
    (ex, ey) = e[1]
    rad = np.arctan2(ey - sy, ex - sx)
    xa0 = ex - np.cos(rad + np.pi / 8) * arrow_length
    ya0 = ey - np.sin(rad + np.pi / 8) * arrow_length
    xa1 = ex - np.cos(rad - np.pi / 8) * arrow_length
    ya1 = ey - np.sin(rad - np.pi / 8) * arrow_length
    arrow = np.array([(sx, sy), (ex, ey), (np.nan, np.nan), (xa0, ya0), (ex, ey), (xa1, ya1)])
    arrows.append(arrow)","for (e_0, e_1, *e_len) in edges:
    (sx, sy) = e_0
    (ex, ey) = e_1
    rad = np.arctan2(ey - sy, ex - sx)
    xa0 = ex - np.cos(rad + np.pi / 8) * arrow_length
    ya0 = ey - np.sin(rad + np.pi / 8) * arrow_length
    xa1 = ex - np.cos(rad - np.pi / 8) * arrow_length
    ya1 = ey - np.sin(rad - np.pi / 8) * arrow_length
    arrow = np.array([(sx, sy), (ex, ey), (np.nan, np.nan), (xa0, ya0), (ex, ey), (xa1, ya1)])
    arrows.append(arrow)"
https://github.com/DataBiosphere/toil/tree/master/src/toil/fileStores/cachingFileStore.py,"def getCacheUsed(self):
        """"""
        Return the total number of bytes used in the cache.

        If no value is available, raises an error.
        """"""

        # Space never counts as used if caching is free
        if self.cachingIsFree():
            return 0

        for row in self.cur.execute('SELECT TOTAL(size) FROM files'):
            return row[0]

        raise RuntimeError('Unable to retrieve cache usage')",_523.py,12,"for row in self.cur.execute('SELECT TOTAL(size) FROM files'):
    return row[0]","for (row_0, *row_len) in self.cur.execute('SELECT TOTAL(size) FROM files'):
    return row_0"
https://github.com/openstack/nova/tree/master/nova/tests/unit/virt/libvirt/test_driver.py,"def test_next_min_version_deprecation_warning(self, mock_warning,
                                                  mock_get_libversion):
        # Skip test if there's no currently planned new min version
        if (versionutils.convert_version_to_int(
                libvirt_driver.NEXT_MIN_LIBVIRT_VERSION) ==
            versionutils.convert_version_to_int(
                libvirt_driver.MIN_LIBVIRT_VERSION)):
            self.skipTest(""NEXT_MIN_LIBVIRT_VERSION == MIN_LIBVIRT_VERSION"")

        # Test that a warning is logged if the libvirt version is less than
        # the next required minimum version.
        drvr = libvirt_driver.LibvirtDriver(fake.FakeVirtAPI(), True)
        drvr.init_host(""dummyhost"")
        # assert that the next min version is in a warning message
        expected_arg = {'version': versionutils.convert_version_to_str(
            versionutils.convert_version_to_int(
                libvirt_driver.NEXT_MIN_LIBVIRT_VERSION))}
        version_arg_found = False
        for call in mock_warning.call_args_list:
            if call[0][1] == expected_arg:
                version_arg_found = True
                break
        self.assertTrue(version_arg_found)",_1049.py,19,"for call in mock_warning.call_args_list:
    if call[0][1] == expected_arg:
        version_arg_found = True
        break","for ((call_0_0, call_0_1, *call_0_len), *call_len) in mock_warning.call_args_list:
    if call_0_1 == expected_arg:
        version_arg_found = True
        break"
https://github.com/3b1b/videos/tree/master/_2020/chess.py,"def construct(self):
        # Camera stuffs
        frame = self.camera.frame
        light = self.camera.light_source
        light.move_to([-25, -20, 20])

        # Setup cube
        colors = [RED, GREEN, BLUE_D, YELLOW]
        cube = self.get_hypercube()
        for n, vert in enumerate(cube.verts):
            code = boolian_linear_combo(int_to_bit_coords(n, 4))
            cube.verts[n].set_color(colors[code])

        # Create trees
        trees = Group()
        original_trees = Group()
        for vert in cube.verts:
            tree = Group(
                vert,
                vert.edges,
                vert.neighbors,
            ).copy()
            original = tree.copy()
            original[0].set_color(GREY)
            original[0].scale(0)
            original_trees.add(original)
            trees.add(tree)
        for tree in trees:
            tree[0].set_color(GREY)
            tree[0].rotate(90 * DEGREES, LEFT)
            sorted_verts = Group(*tree[2])
            sorted_verts.submobjects.sort(key=lambda m: m.get_color().hex)
            sorted_verts.arrange(DOWN, buff=SMALL_BUFF)
            sorted_verts.next_to(tree[0], RIGHT, buff=0.75)
            for edge, neighbor in zip(tree[1], tree[2]):
                edge.become(Line3D(
                    tree[0].get_center(),
                    neighbor.get_center(),
                    resolution=edge.resolution,
                ))
                neighbor.rotate(90 * DEGREES, LEFT)

        trees.arrange_in_grid(4, 4, buff=MED_LARGE_BUFF)
        for i in range(4):
            trees[i::4].shift(0.5 * i * RIGHT)
        trees.center()
        trees.set_height(6)
        trees.rotate(PI / 2, RIGHT)
        trees.move_to(10 * LEFT, LEFT)

        frame.set_phi(90 * DEGREES)
        frame.move_to(5 * LEFT)
        self.add(trees)
        self.wait()

        # Show transition
        anims = []
        for tree, original in zip(trees, original_trees):
            anims.append(Transform(tree, original))
        self.play(
            frame.set_euler_angles, 20 * DEGREES, 70 * DEGREES,
            frame.move_to, ORIGIN,
            LaggedStart(*anims, lag_ratio=0.2),
            run_time=8,
        )
        self.remove(trees)
        self.add(cube)
        frame.add_updater(lambda m, dt: m.increment_theta(2 * dt * DEGREES))
        self.wait(30)",_1137.py,28,"for tree in trees:
    tree[0].set_color(GREY)
    tree[0].rotate(90 * DEGREES, LEFT)
    sorted_verts = Group(*tree[2])
    sorted_verts.submobjects.sort(key=lambda m: m.get_color().hex)
    sorted_verts.arrange(DOWN, buff=SMALL_BUFF)
    sorted_verts.next_to(tree[0], RIGHT, buff=0.75)
    for (edge, neighbor) in zip(tree[1], tree[2]):
        edge.become(Line3D(tree[0].get_center(), neighbor.get_center(), resolution=edge.resolution))
        neighbor.rotate(90 * DEGREES, LEFT)","for (tree_0, tree_1, tree_2, *tree_len) in trees:
    tree_0.set_color(GREY)
    tree_0.rotate(90 * DEGREES, LEFT)
    sorted_verts = Group(*tree_2)
    sorted_verts.submobjects.sort(key=lambda m: m.get_color().hex)
    sorted_verts.arrange(DOWN, buff=SMALL_BUFF)
    sorted_verts.next_to(tree_0, RIGHT, buff=0.75)
    for (edge, neighbor) in zip(tree_1, tree_2):
        edge.become(Line3D(tree_0.get_center(), neighbor.get_center(), resolution=edge.resolution))
        neighbor.rotate(90 * DEGREES, LEFT)"
https://github.com/savoirfairelinux/num2words/tree/master/tests/test_es.py,"def test_currency_yum(self):
        for test in TEST_CASES_TO_CURRENCY_YUM:
            self.assertEqual(
                num2words(test[0], lang='es', to='currency', currency='YUM'),
                test[1]
            )",_1338.py,2,"for test in TEST_CASES_TO_CURRENCY_YUM:
    self.assertEqual(num2words(test[0], lang='es', to='currency', currency='YUM'), test[1])","for (test_0, test_1, *test_len) in TEST_CASES_TO_CURRENCY_YUM:
    self.assertEqual(num2words(test_0, lang='es', to='currency', currency='YUM'), test_1)"
https://github.com/sunpy/sunpy/tree/master/sunpy/timeseries/metadata.py,"def _truncate(self, timerange):
        """"""
        Removes metadata entries outside of the new (truncated)
        `sunpy.time.TimeRange`. Also adjusts start and end times of time ranges
        going outside of the truncated time range.

        Parameters
        ----------
        timerange : `sunpy.time.TimeRange`
            The time range to truncate to.
        """"""
        truncated = []
        for metatuple in self.metadata:
            # Get metadata time range parameters
            start = metatuple[0].start
            end = metatuple[0].end
            out_of_range = False

            # Find truncations
            if start < timerange.start and end > timerange.start:
                # Truncate the start
                start = timerange.start
            elif start > timerange.end:
                # Metadata time range starts after truncated data ends.
                out_of_range = True
            if end > timerange.end and start < timerange.end:
                # Truncate the end
                end = timerange.end
            elif end < timerange.start:
                # Metadata time range finishes before truncated data starts.
                out_of_range = True

            # Add the values if applicable
            if not out_of_range:
                truncated.append((TimeRange(start, end), metatuple[1], metatuple[2]))

        # Update the original list
        self.metadata = truncated",_3383.py,13,"for metatuple in self.metadata:
    start = metatuple[0].start
    end = metatuple[0].end
    out_of_range = False
    if start < timerange.start and end > timerange.start:
        start = timerange.start
    elif start > timerange.end:
        out_of_range = True
    if end > timerange.end and start < timerange.end:
        end = timerange.end
    elif end < timerange.start:
        out_of_range = True
    if not out_of_range:
        truncated.append((TimeRange(start, end), metatuple[1], metatuple[2]))","for (metatuple_0, metatuple_1, metatuple_2, *metatuple_len) in self.metadata:
    start = metatuple_0.start
    end = metatuple_0.end
    out_of_range = False
    if start < timerange.start and end > timerange.start:
        start = timerange.start
    elif start > timerange.end:
        out_of_range = True
    if end > timerange.end and start < timerange.end:
        end = timerange.end
    elif end < timerange.start:
        out_of_range = True
    if not out_of_range:
        truncated.append((TimeRange(start, end), metatuple_1, metatuple_2))"
https://github.com/keras-team/keras/tree/master/keras/optimizer_v2/optimizer_v2_test.py,"def identify_redundant_ops(graph):
  """"""Implements basic common subexpression elimination.

  This is not intended to replicate the graph semantics of TensorFlow Graphs
  (for instance it does not handle stateful op ordering), nor is it intended to
  replace the common subexpression elimination Grappler pass. Rather, it
  provides a high level sanity check that clearly redundant ops are not being
  created.

  Args:
    graph: The graph to be analyzed.

  Returns:
    A count of the duplicate ops and a description of the structure of each.
  """"""
  sorted_ops = topological_sort(graph)
  duplicates = collections.defaultdict(list)
  unified_node_defs = {}
  name_map = {}

  for op in sorted_ops:
    input_names = []
    for op_input, name in zip(*get_inputs(op)):
      input_def = op_input.node_def

      # Operations can have multiple outputs. We track which is used to prevent
      # overzealous elimination.
      input_def.name = name

      input_def.input[:] = [name_map.get(i, i) for i in input_def.input]
      strip_name(input_def)

      # NodeDef.SerializeToString() does not provide identical serialized
      # representations for identical NodeDefs, so we instead use string
      # representation as a dict key.
      key = repr(input_def)

      if key in unified_node_defs:
        input_names.append(unified_node_defs[key])

      else:
        unified_node_defs[key] = op_input.name
        input_names.append(name)

    node_def = op.node_def
    node_def.input[:] = input_names
    strip_name(node_def)

    key = repr(node_def)
    duplicates[key].append(op)
    name_map[op.name] = duplicates[key][0].name

  num_duplicates = 0
  duplicate_types = []
  for standard_def, op_defs in duplicates.items():
    # We are only interested in testing the apply method of the optimizer
    op_defs = [i for i in op_defs if APPLY_SCOPE in i.name]

    # We only check for per-apply redundant ops.
    if len(op_defs) < _NUM_LEARNERS:
      continue

    # Certain ops are simply not worth eliminating, and are instead simply
    # ignored.
    name, op_type = op_defs[0].name, op_defs[0].type
    if any(allowlisted_scope in name and op_type == allowlisted_type
           for allowlisted_scope, allowlisted_type in ALLOWLIST):
      continue

    num_duplicates += len(op_defs)
    traceback = []
    for level in op_defs[0].traceback:
      traceback.append('  {} {}:{}'.format(level[0], level[2], level[1]))

    duplicate_types.append(
        '# Example name: {}\n# Op creation stack:\n{}\n{}'.format(
            op_defs[0].name,
            '\n'.join(traceback),
            standard_def))

  return num_duplicates, duplicate_types",_3432.py,72,"for level in op_defs[0].traceback:
    traceback.append('  {} {}:{}'.format(level[0], level[2], level[1]))","for (level_0, level_1, level_2, *level_len) in op_defs[0].traceback:
    traceback.append('  {} {}:{}'.format(level_0, level_2, level_1))"
https://github.com/SFTtech/openage/tree/master/openage/convert/processor/conversion/de2/nyan_subprocessor.py,"def terrain_group_to_terrain(terrain_group: GenieTerrainGroup) -> None:
        """"""
        Creates raw API objects for a terrain group.

        :param terrain_group: Terrain group that gets converted to a tech.
        :type terrain_group: ..dataformat.converter_object.ConverterObjectGroup
        """"""
        terrain_index = terrain_group.get_id()

        dataset = terrain_group.data

        # name_lookup_dict = internal_name_lookups.get_entity_lookups(dataset.game_version)
        terrain_lookup_dict = internal_name_lookups.get_terrain_lookups(dataset.game_version)
        terrain_type_lookup_dict = internal_name_lookups.get_terrain_type_lookups(
            dataset.game_version)

        if terrain_index not in terrain_lookup_dict:
            # TODO: Not all terrains are used in DE2; filter out the unused terrains
            # in pre-processor
            return

        # Start with the Terrain object
        terrain_name = terrain_lookup_dict[terrain_index][1]
        raw_api_object = RawAPIObject(terrain_name, terrain_name,
                                      dataset.nyan_api_objects)
        raw_api_object.add_raw_parent(""engine.util.terrain.Terrain"")
        obj_location = f""data/terrain/{terrain_lookup_dict[terrain_index][2]}/""
        raw_api_object.set_location(obj_location)
        raw_api_object.set_filename(terrain_lookup_dict[terrain_index][2])
        terrain_group.add_raw_api_object(raw_api_object)

        # =======================================================================
        # Types
        # =======================================================================
        terrain_types = []

        for terrain_type in terrain_type_lookup_dict.values():
            if terrain_index in terrain_type[0]:
                type_name = f""util.terrain_type.types.{terrain_type[2]}""
                type_obj = dataset.pregen_nyan_objects[type_name].get_nyan_object()
                terrain_types.append(type_obj)

        raw_api_object.add_raw_member(""types"", terrain_types, ""engine.util.terrain.Terrain"")

        # =======================================================================
        # Name
        # =======================================================================
        name_ref = f""{terrain_name}.{terrain_name}Name""
        name_raw_api_object = RawAPIObject(name_ref,
                                           f""{terrain_name}Name"",
                                           dataset.nyan_api_objects)
        name_raw_api_object.add_raw_parent(""engine.util.language.translated.type.TranslatedString"")
        name_location = ForwardRef(terrain_group, terrain_name)
        name_raw_api_object.set_location(name_location)

        name_raw_api_object.add_raw_member(""translations"",
                                           [],
                                           ""engine.util.language.translated.type.TranslatedString"")

        name_forward_ref = ForwardRef(terrain_group, name_ref)
        raw_api_object.add_raw_member(""name"", name_forward_ref, ""engine.util.terrain.Terrain"")
        terrain_group.add_raw_api_object(name_raw_api_object)

        # =======================================================================
        # Sound
        # =======================================================================
        sound_name = f""{terrain_name}.Sound""
        sound_raw_api_object = RawAPIObject(sound_name, ""Sound"",
                                            dataset.nyan_api_objects)
        sound_raw_api_object.add_raw_parent(""engine.util.sound.Sound"")
        sound_location = ForwardRef(terrain_group, terrain_name)
        sound_raw_api_object.set_location(sound_location)

        # TODO: Sounds
        sounds = []

        sound_raw_api_object.add_raw_member(""play_delay"",
                                            0,
                                            ""engine.util.sound.Sound"")
        sound_raw_api_object.add_raw_member(""sounds"",
                                            sounds,
                                            ""engine.util.sound.Sound"")

        sound_forward_ref = ForwardRef(terrain_group, sound_name)
        raw_api_object.add_raw_member(""sound"",
                                      sound_forward_ref,
                                      ""engine.util.terrain.Terrain"")

        terrain_group.add_raw_api_object(sound_raw_api_object)

        # =======================================================================
        # Ambience
        # =======================================================================
        terrain = terrain_group.get_terrain()
        # ambients_count = terrain[""terrain_units_used_count""].value

        ambience = []
        # TODO: Ambience
# ===============================================================================
#         for ambient_index in range(ambients_count):
#             ambient_id = terrain[""terrain_unit_id""][ambient_index].value
#
#             if ambient_id == -1:
#                 continue
#
#             ambient_line = dataset.unit_ref[ambient_id]
#             ambient_name = name_lookup_dict[ambient_line.get_head_unit_id()][0]
#
#             ambient_ref = ""%s.Ambient%s"" % (terrain_name, str(ambient_index))
#             ambient_raw_api_object = RawAPIObject(ambient_ref,
#                                                   ""Ambient%s"" % (str(ambient_index)),
#                                                   dataset.nyan_api_objects)
#             ambient_raw_api_object.add_raw_parent(""engine.util.terrain.TerrainAmbient"")
#             ambient_location = ForwardRef(terrain_group, terrain_name)
#             ambient_raw_api_object.set_location(ambient_location)
#
#             # Game entity reference
#             ambient_line_forward_ref = ForwardRef(ambient_line, ambient_name)
#             ambient_raw_api_object.add_raw_member(""object"",
#                                                   ambient_line_forward_ref,
#                                                   ""engine.util.terrain.TerrainAmbient"")
#
#             # Max density
#             max_density = terrain[""terrain_unit_density""][ambient_index].value
#             ambient_raw_api_object.add_raw_member(""max_density"",
#                                                   max_density,
#                                                   ""engine.util.terrain.TerrainAmbient"")
#
#             terrain_group.add_raw_api_object(ambient_raw_api_object)
#             terrain_ambient_forward_ref = ForwardRef(terrain_group, ambient_ref)
#             ambience.append(terrain_ambient_forward_ref)
# ===============================================================================

        raw_api_object.add_raw_member(""ambience"", ambience, ""engine.util.terrain.Terrain"")

        # =======================================================================
        # Graphic
        # =======================================================================
        texture_id = terrain.get_id()

        # Create animation object
        graphic_name = f""{terrain_name}.TerrainTexture""
        graphic_raw_api_object = RawAPIObject(graphic_name, ""TerrainTexture"",
                                              dataset.nyan_api_objects)
        graphic_raw_api_object.add_raw_parent(""engine.util.graphics.Terrain"")
        graphic_location = ForwardRef(terrain_group, terrain_name)
        graphic_raw_api_object.set_location(graphic_location)

        if texture_id in dataset.combined_terrains.keys():
            terrain_graphic = dataset.combined_terrains[texture_id]

        else:
            terrain_graphic = CombinedTerrain(texture_id,
                                              f""texture_{terrain_lookup_dict[terrain_index][2]}"",
                                              dataset)
            dataset.combined_terrains.update({terrain_graphic.get_id(): terrain_graphic})

        terrain_graphic.add_reference(graphic_raw_api_object)

        graphic_raw_api_object.add_raw_member(""sprite"", terrain_graphic,
                                              ""engine.util.graphics.Terrain"")

        terrain_group.add_raw_api_object(graphic_raw_api_object)
        graphic_forward_ref = ForwardRef(terrain_group, graphic_name)
        raw_api_object.add_raw_member(""terrain_graphic"", graphic_forward_ref,
                                      ""engine.util.terrain.Terrain"")",_4097.py,37,"for terrain_type in terrain_type_lookup_dict.values():
    if terrain_index in terrain_type[0]:
        type_name = f'util.terrain_type.types.{terrain_type[2]}'
        type_obj = dataset.pregen_nyan_objects[type_name].get_nyan_object()
        terrain_types.append(type_obj)","for (terrain_type_0, terrain_type_1, terrain_type_2, *terrain_type_len) in terrain_type_lookup_dict.values():
    if terrain_index in terrain_type_0:
        type_name = f'util.terrain_type.types.{terrain_type_2}'
        type_obj = dataset.pregen_nyan_objects[type_name].get_nyan_object()
        terrain_types.append(type_obj)"
https://github.com/dortania/OpenCore-Legacy-Patcher/tree/master/resources/cli_menu.py,"def patcher_setting_debug(self):
        response = None
        while not (response and response == -1):
            title = [""Adjust Debug Settings""]
            menu = utilities.TUIMenu(title, ""Please select an option: "", auto_number=True, top_level=True)
            options = [
                [f""Enable Verbose Mode:\tCurrently {self.constants.verbose_debug}"", MenuOptions(self.constants.custom_model or self.constants.computer.real_model, self.constants).change_verbose],
                [f""Enable OpenCore DEBUG:\tCurrently {self.constants.opencore_debug}"", MenuOptions(self.constants.custom_model or self.constants.computer.real_model, self.constants).change_oc],
                [f""Enable Kext DEBUG:\t\tCurrently {self.constants.kext_debug}"", MenuOptions(self.constants.custom_model or self.constants.computer.real_model, self.constants).change_kext],
            ] + (
                [[f""Set SurPlus Settings:\tCurrently {self.constants.force_surplus}"", MenuOptions(self.constants.custom_model or self.constants.computer.real_model, self.constants).set_surplus]]
                if (smbios_data.smbios_dictionary[self.constants.custom_model or self.constants.computer.real_model][""CPU Generation""] <= cpu_data.cpu_data.sandy_bridge)
                else []
            )

            for option in options:
                menu.add_menu_option(option[0], function=option[1])

            response = menu.start()",_5077.py,16,"for option in options:
    menu.add_menu_option(option[0], function=option[1])","for (option_0, option_1, *option_len) in options:
    menu.add_menu_option(option_0, function=option_1)"
https://github.com/ansible/ansible/tree/master/test/lib/ansible_test/_util/controller/sanity/pylint/plugins/unwanted.py,"def _check_importfrom(self, node, modname, names):  # type: (astroid.node_classes.ImportFrom, str, t.List[str]) -> None
        """"""Check the imports on the specified import from node.""""""
        self._check_module_import(node, modname)

        entry = self.unwanted_imports.get(modname)

        if not entry:
            return

        for name in names:
            if entry.applies_to(self.linter.current_file, name[0]):
                self.add_message(self.BAD_IMPORT_FROM, args=(name[0], entry.alternative, modname), node=node)",_5101.py,10,"for name in names:
    if entry.applies_to(self.linter.current_file, name[0]):
        self.add_message(self.BAD_IMPORT_FROM, args=(name[0], entry.alternative, modname), node=node)","for (name_0, *name_len) in names:
    if entry.applies_to(self.linter.current_file, name_0):
        self.add_message(self.BAD_IMPORT_FROM, args=(name_0, entry.alternative, modname), node=node)"
https://github.com/kizniche/Mycodo/tree/master/mycodo/scripts/generate_manual_outputs.py,"if __name__ == ""__main__"":
    for output_id, output_data in parse_output_information(exclude_custom=True).items():
        name_str = """"
        if 'output_name' in output_data and output_data['output_name']:
            name_str += ""{}"".format(output_data['output_name'])
        if 'output_manufacturer' in output_data and output_data['output_manufacturer']:
            name_str += "": {}"".format(output_data['output_manufacturer'])
        if 'measurements_name' in output_data and output_data['measurements_name']:
            name_str += "": {}"".format(output_data['measurements_name'])
        if 'output_library' in output_data and output_data['output_library']:
            name_str += "": {}"".format(output_data['output_library'])

        if ('output_manufacturer' in output_data and
                output_data['output_manufacturer'] in ['Linux', 'Mycodo', 'Raspberry Pi', 'System']):

            if name_str in mycodo_info and 'dependencies_module' in mycodo_info[name_str]:
                # Multiple sets of dependencies, append library
                mycodo_info[name_str]['dependencies_module'].append(output_data['dependencies_module'])
            else:
                # Only one set of dependencies
                mycodo_info[name_str] = output_data
                if 'dependencies_module' in output_data:
                    mycodo_info[name_str]['dependencies_module'] = [output_data['dependencies_module']]  # turn into list
        else:
            if name_str in outputs_info and 'dependencies_module' in outputs_info[name_str]:
                # Multiple sets of dependencies, append library
                outputs_info[name_str]['dependencies_module'].append(output_data['dependencies_module'])
            else:
                # Only one set of dependencies
                outputs_info[name_str] = output_data
                if 'dependencies_module' in output_data:
                    outputs_info[name_str]['dependencies_module'] = [output_data['dependencies_module']]  # turn into list

    mycodo_info = dict(OrderedDict(sorted(mycodo_info.items(), key = lambda t: t[0])))
    outputs_info = dict(OrderedDict(sorted(outputs_info.items(), key = lambda t: t[0])))

    list_outputs = [
        (mycodo_info, ""Built-In Outputs (System)""),
        (outputs_info, ""Built-In Outputs (Devices)"")
    ]

    with open(save_path, 'w') as out_file:
        for each_list in list_outputs:
            out_file.write(""## {}\n\n"".format(each_list[1]))

            for each_id, each_data in each_list[0].items():
                if 'output_name' in each_data and each_data['output_name']:
                    out_file.write(""### {}\n\n"".format(each_data['output_name']))
                else:
                    out_file.write(""### {}\n\n"".format(each_id))

                if 'output_manufacturer' in each_data and each_data['output_manufacturer']:
                    out_file.write(""- Manufacturer: {}\n"".format(each_data['output_manufacturer']))

                if 'measurements_name' in each_data and each_data['measurements_name']:
                    out_file.write(""- Measurements: {}\n"".format(each_data['measurements_name']))

                if 'interfaces' in each_data and each_data['interfaces']:
                    list_interfaces = []
                    for each_type in each_data['interfaces']:
                        if each_type == 'I2C':
                            list_interfaces.append(""I<sup>2</sup>C"")
                        elif each_type == 'MYCODO':
                            list_interfaces.append(""Mycodo"")
                        elif each_type == '1WIRE':
                            list_interfaces.append(""1-Wire"")
                        elif each_type == 'HTTP':
                            list_interfaces.append(""HTTP"")
                        elif each_type == 'FTDI':
                            list_interfaces.append(""FTDI"")
                        elif each_type == 'UART':
                            list_interfaces.append(""UART"")
                        elif each_type == 'GPIO':
                            list_interfaces.append(""GPIO"")
                        elif each_type == 'PYTHON':
                            list_interfaces.append(""Python"")
                        elif each_type == 'SHELL':
                            list_interfaces.append(""Shell"")
                        else:
                            list_interfaces.append(each_type)
                    out_file.write(""- Interfaces: {}\n"".format("", "".join(list_interfaces)))

                if 'output_types' in each_data and each_data['output_types']:
                    list_output_types = []
                    for each_type in each_data['output_types']:
                        if each_type == 'on_off':
                            list_output_types.append(""On/Off"")
                        elif each_type == 'volume':
                            list_output_types.append(""Volume"")
                        elif each_type == 'pwm':
                            list_output_types.append(""PWM"")
                        elif each_type == 'value':
                            list_output_types.append(""Value"")
                    out_file.write(""- Output Types: {}\n"".format("", "".join(list_output_types)))

                if 'output_library' in each_data and each_data['output_library']:
                    out_file.write(""- Libraries: {}\n"".format(each_data['output_library']))

                generate_controller_doc(out_file, each_data)",_5899.py,43,"for each_list in list_outputs:
    out_file.write('## {}\n\n'.format(each_list[1]))
    for (each_id, each_data) in each_list[0].items():
        if 'output_name' in each_data and each_data['output_name']:
            out_file.write('### {}\n\n'.format(each_data['output_name']))
        else:
            out_file.write('### {}\n\n'.format(each_id))
        if 'output_manufacturer' in each_data and each_data['output_manufacturer']:
            out_file.write('- Manufacturer: {}\n'.format(each_data['output_manufacturer']))
        if 'measurements_name' in each_data and each_data['measurements_name']:
            out_file.write('- Measurements: {}\n'.format(each_data['measurements_name']))
        if 'interfaces' in each_data and each_data['interfaces']:
            list_interfaces = []
            for each_type in each_data['interfaces']:
                if each_type == 'I2C':
                    list_interfaces.append('I<sup>2</sup>C')
                elif each_type == 'MYCODO':
                    list_interfaces.append('Mycodo')
                elif each_type == '1WIRE':
                    list_interfaces.append('1-Wire')
                elif each_type == 'HTTP':
                    list_interfaces.append('HTTP')
                elif each_type == 'FTDI':
                    list_interfaces.append('FTDI')
                elif each_type == 'UART':
                    list_interfaces.append('UART')
                elif each_type == 'GPIO':
                    list_interfaces.append('GPIO')
                elif each_type == 'PYTHON':
                    list_interfaces.append('Python')
                elif each_type == 'SHELL':
                    list_interfaces.append('Shell')
                else:
                    list_interfaces.append(each_type)
            out_file.write('- Interfaces: {}\n'.format(', '.join(list_interfaces)))
        if 'output_types' in each_data and each_data['output_types']:
            list_output_types = []
            for each_type in each_data['output_types']:
                if each_type == 'on_off':
                    list_output_types.append('On/Off')
                elif each_type == 'volume':
                    list_output_types.append('Volume')
                elif each_type == 'pwm':
                    list_output_types.append('PWM')
                elif each_type == 'value':
                    list_output_types.append('Value')
            out_file.write('- Output Types: {}\n'.format(', '.join(list_output_types)))
        if 'output_library' in each_data and each_data['output_library']:
            out_file.write('- Libraries: {}\n'.format(each_data['output_library']))
        generate_controller_doc(out_file, each_data)","for (each_list_0, each_list_1, *each_list_len) in list_outputs:
    out_file.write('## {}\n\n'.format(each_list_1))
    for (each_id, each_data) in each_list_0.items():
        if 'output_name' in each_data and each_data['output_name']:
            out_file.write('### {}\n\n'.format(each_data['output_name']))
        else:
            out_file.write('### {}\n\n'.format(each_id))
        if 'output_manufacturer' in each_data and each_data['output_manufacturer']:
            out_file.write('- Manufacturer: {}\n'.format(each_data['output_manufacturer']))
        if 'measurements_name' in each_data and each_data['measurements_name']:
            out_file.write('- Measurements: {}\n'.format(each_data['measurements_name']))
        if 'interfaces' in each_data and each_data['interfaces']:
            list_interfaces = []
            for each_type in each_data['interfaces']:
                if each_type == 'I2C':
                    list_interfaces.append('I<sup>2</sup>C')
                elif each_type == 'MYCODO':
                    list_interfaces.append('Mycodo')
                elif each_type == '1WIRE':
                    list_interfaces.append('1-Wire')
                elif each_type == 'HTTP':
                    list_interfaces.append('HTTP')
                elif each_type == 'FTDI':
                    list_interfaces.append('FTDI')
                elif each_type == 'UART':
                    list_interfaces.append('UART')
                elif each_type == 'GPIO':
                    list_interfaces.append('GPIO')
                elif each_type == 'PYTHON':
                    list_interfaces.append('Python')
                elif each_type == 'SHELL':
                    list_interfaces.append('Shell')
                else:
                    list_interfaces.append(each_type)
            out_file.write('- Interfaces: {}\n'.format(', '.join(list_interfaces)))
        if 'output_types' in each_data and each_data['output_types']:
            list_output_types = []
            for each_type in each_data['output_types']:
                if each_type == 'on_off':
                    list_output_types.append('On/Off')
                elif each_type == 'volume':
                    list_output_types.append('Volume')
                elif each_type == 'pwm':
                    list_output_types.append('PWM')
                elif each_type == 'value':
                    list_output_types.append('Value')
            out_file.write('- Output Types: {}\n'.format(', '.join(list_output_types)))
        if 'output_library' in each_data and each_data['output_library']:
            out_file.write('- Libraries: {}\n'.format(each_data['output_library']))
        generate_controller_doc(out_file, each_data)"
https://github.com/idealo/imagededup/tree/master/imagededup/utils/general_utils.py,"def save_json(results: Dict, filename: str, float_scores: bool = False) -> None:
    """"""
    Save results with a filename.

    Args:
        results: Dictionary of results to be saved.
        filename: Name of the file to be saved.
        float_scores: boolean to indicate if scores are floats.
    """"""
    logger.info('Start: Saving duplicates as json!')

    if float_scores:
        for _file, dup_list in results.items():
            if dup_list:
                typecasted_dup_list = []
                for dup in dup_list:
                    typecasted_dup_list.append((dup[0], float(dup[1])))

                results[_file] = typecasted_dup_list

    with open(filename, 'w') as f:
        json.dump(results, f, indent=2, sort_keys=True)

    logger.info('End: Saving duplicates as json!')",_5908.py,16,"for dup in dup_list:
    typecasted_dup_list.append((dup[0], float(dup[1])))","for (dup_0, dup_1, *dup_len) in dup_list:
    typecasted_dup_list.append((dup_0, float(dup_1)))"
https://github.com/CellProfiler/CellProfiler/tree/master/tests/modules/test_classifyobjects.py,"def test_classify_single_even():
    m = numpy.array((0.5, 0, 1, 0.1))
    labels = numpy.zeros((20, 10), int)
    labels[2:5, 3:7] = 1
    labels[12:15, 1:4] = 2
    labels[6:11, 5:9] = 3
    labels[16:19, 5:9] = 4
    workspace, module = make_workspace(
        labels, cellprofiler.modules.classifyobjects.BY_SINGLE_MEASUREMENT, m
    )
    module.single_measurements[
        0
    ].bin_choice.value = cellprofiler.modules.classifyobjects.BC_EVEN
    module.single_measurements[0].low_threshold.value = 0.2
    module.single_measurements[0].high_threshold.value = 0.7
    module.single_measurements[0].bin_count.value = 1
    module.single_measurements[0].wants_low_bin.value = True
    module.single_measurements[0].wants_high_bin.value = True
    module.single_measurements[0].wants_images.value = True

    expected_obj = dict(
        Classify_Measurement1_Bin_1=(0, 1, 0, 1),
        Classify_Measurement1_Bin_2=(1, 0, 0, 0),
        Classify_Measurement1_Bin_3=(0, 0, 1, 0),
    )
    expected_img = dict(
        Classify_Measurement1_Bin_1_NumObjectsPerBin=2,
        Classify_Measurement1_Bin_2_NumObjectsPerBin=1,
        Classify_Measurement1_Bin_3_NumObjectsPerBin=1,
        Classify_Measurement1_Bin_1_PctObjectsPerBin=50.0,
        Classify_Measurement1_Bin_2_PctObjectsPerBin=25.0,
        Classify_Measurement1_Bin_3_PctObjectsPerBin=25.0,
    )
    module.run(workspace)
    for measurement, expected_values in list(expected_obj.items()):
        values = workspace.measurements.get_current_measurement(
            OBJECTS_NAME, measurement
        )
        assert len(values) == 4
        assert numpy.all(values == numpy.array(expected_values))
    for measurement, expected_values in list(expected_img.items()):
        values = workspace.measurements.get_current_measurement(
            ""Image"", measurement
        )
        assert values == expected_values

    image = workspace.image_set.get_image(IMAGE_NAME)
    pixel_data = image.pixel_data
    assert numpy.all(pixel_data[labels == 0, :] == 0)
    colors = [pixel_data[x, y, :] for x, y in ((2, 3), (12, 1), (6, 5))]
    for i, color in enumerate(colors + [colors[1]]):
        assert numpy.all(pixel_data[labels == i + 1, :] == color)

    columns = module.get_measurement_columns(None)
    assert len(columns) == 9
    assert len(set([column[1] for column in columns])) == 9  # no duplicates
    for column in columns:
        if column[0] != OBJECTS_NAME:  # Must be image
            assert column[0] == ""Image""
            assert column[1] in list(expected_img.keys())
            assert (
                column[2] == COLTYPE_INTEGER
                if column[1].endswith(
                    cellprofiler.modules.classifyobjects.F_NUM_PER_BIN
                )
                else COLTYPE_FLOAT
            )
        else:
            assert column[0] == OBJECTS_NAME
            assert column[1] in list(expected_obj.keys())
            assert column[2] == COLTYPE_INTEGER

    categories = module.get_categories(None, ""Image"")
    assert len(categories) == 1
    assert categories[0] == cellprofiler.modules.classifyobjects.M_CATEGORY
    names = module.get_measurements(None, ""Image"", ""foo"")
    assert len(names) == 0
    categories = module.get_categories(None, OBJECTS_NAME)
    assert len(categories) == 1
    assert categories[0] == cellprofiler.modules.classifyobjects.M_CATEGORY
    names = module.get_measurements(None, OBJECTS_NAME, ""foo"")
    assert len(names) == 0
    names = module.get_measurements(
        None, ""foo"", cellprofiler.modules.classifyobjects.M_CATEGORY
    )
    assert len(names) == 0
    names = module.get_measurements(
        None, OBJECTS_NAME, cellprofiler.modules.classifyobjects.M_CATEGORY
    )
    assert len(names) == 3
    assert len(set(names)) == 3
    assert all(
        [
            ""_"".join((cellprofiler.modules.classifyobjects.M_CATEGORY, name))
            in list(expected_obj.keys())
            for name in names
        ]
    )
    names = module.get_measurements(
        None,
        ""Image"",
        cellprofiler.modules.classifyobjects.M_CATEGORY,
    )
    assert len(names) == 6
    assert len(set(names)) == 6
    assert all(
        [
            ""_"".join((cellprofiler.modules.classifyobjects.M_CATEGORY, name))
            in list(expected_img.keys())
            for name in names
        ]
    )",_6384.py,57,"for column in columns:
    if column[0] != OBJECTS_NAME:
        assert column[0] == 'Image'
        assert column[1] in list(expected_img.keys())
        assert column[2] == COLTYPE_INTEGER if column[1].endswith(cellprofiler.modules.classifyobjects.F_NUM_PER_BIN) else COLTYPE_FLOAT
    else:
        assert column[0] == OBJECTS_NAME
        assert column[1] in list(expected_obj.keys())
        assert column[2] == COLTYPE_INTEGER","for (column_0, column_1, column_2, *column_len) in columns:
    if column_0 != OBJECTS_NAME:
        assert column_0 == 'Image'
        assert column_1 in list(expected_img.keys())
        assert column_2 == COLTYPE_INTEGER if column_1.endswith(cellprofiler.modules.classifyobjects.F_NUM_PER_BIN) else COLTYPE_FLOAT
    else:
        assert column_0 == OBJECTS_NAME
        assert column_1 in list(expected_obj.keys())
        assert column_2 == COLTYPE_INTEGER"
https://github.com/tecladocode/rest-apis-flask-python/tree/master/section5/item.py,"def get(self):
        connection = sqlite3.connect('data.db')
        cursor = connection.cursor()

        query = ""SELECT * FROM {table}"".format(table=self.TABLE_NAME)
        result = cursor.execute(query)
        items = []
        for row in result:
            items.append({'name': row[0], 'price': row[1]})
        connection.close()

        return {'items': items}",_7543.py,8,"for row in result:
    items.append({'name': row[0], 'price': row[1]})","for (row_0, row_1, *row_len) in result:
    items.append({'name': row_0, 'price': row_1})"
https://github.com/dingjiansw101/AerialDetection/tree/master/DOTA_devkit/utils.py,"def choose_best_begin_point(pre_result):
    final_result = []
    for coordinate in pre_result:
        x1 = coordinate[0][0]
        y1 = coordinate[0][1]
        x2 = coordinate[1][0]
        y2 = coordinate[1][1]
        x3 = coordinate[2][0]
        y3 = coordinate[2][1]
        x4 = coordinate[3][0]
        y4 = coordinate[3][1]
        xmin = min(x1, x2, x3, x4)
        ymin = min(y1, y2, y3, y4)
        xmax = max(x1, x2, x3, x4)
        ymax = max(y1, y2, y3, y4)
        combinate = [[[x1, y1], [x2, y2], [x3, y3], [x4, y4]], [[x2, y2], [x3, y3], [x4, y4], [x1, y1]], [[x3, y3], [x4, y4], [x1, y1], [x2, y2]], [[x4, y4], [x1, y1], [x2, y2], [x3, y3]]]
        dst_coordinate = [[xmin, ymin], [xmax, ymin], [xmax, ymax], [xmin, ymax]]
        force = 100000000.0
        force_flag = 0
        for i in range(4):
            temp_force = cal_line_length(combinate[i][0], dst_coordinate[0]) + cal_line_length(combinate[i][1], dst_coordinate[1]) + cal_line_length(combinate[i][2], dst_coordinate[2]) + cal_line_length(combinate[i][3], dst_coordinate[3])
            if temp_force < force:
                force = temp_force
                force_flag = i
        if force_flag != 0:
            print(""choose one direction!"")
        final_result.append(combinate[force_flag])
    return final_result",_7739.py,3,"for coordinate in pre_result:
    x1 = coordinate[0][0]
    y1 = coordinate[0][1]
    x2 = coordinate[1][0]
    y2 = coordinate[1][1]
    x3 = coordinate[2][0]
    y3 = coordinate[2][1]
    x4 = coordinate[3][0]
    y4 = coordinate[3][1]
    xmin = min(x1, x2, x3, x4)
    ymin = min(y1, y2, y3, y4)
    xmax = max(x1, x2, x3, x4)
    ymax = max(y1, y2, y3, y4)
    combinate = [[[x1, y1], [x2, y2], [x3, y3], [x4, y4]], [[x2, y2], [x3, y3], [x4, y4], [x1, y1]], [[x3, y3], [x4, y4], [x1, y1], [x2, y2]], [[x4, y4], [x1, y1], [x2, y2], [x3, y3]]]
    dst_coordinate = [[xmin, ymin], [xmax, ymin], [xmax, ymax], [xmin, ymax]]
    force = 100000000.0
    force_flag = 0
    for i in range(4):
        temp_force = cal_line_length(combinate[i][0], dst_coordinate[0]) + cal_line_length(combinate[i][1], dst_coordinate[1]) + cal_line_length(combinate[i][2], dst_coordinate[2]) + cal_line_length(combinate[i][3], dst_coordinate[3])
        if temp_force < force:
            force = temp_force
            force_flag = i
    if force_flag != 0:
        print('choose one direction!')
    final_result.append(combinate[force_flag])","for ((coordinate_0_0, coordinate_0_1, *coordinate_0_len), (coordinate_1_0, coordinate_1_1, *coordinate_1_len), (coordinate_2_0, coordinate_2_1, *coordinate_2_len), (coordinate_3_0, coordinate_3_1, *coordinate_3_len), *coordinate_len) in pre_result:
    x1 = coordinate_0_0
    y1 = coordinate_0_1
    x2 = coordinate_1_0
    y2 = coordinate_1_1
    x3 = coordinate_2_0
    y3 = coordinate_2_1
    x4 = coordinate_3_0
    y4 = coordinate_3_1
    xmin = min(x1, x2, x3, x4)
    ymin = min(y1, y2, y3, y4)
    xmax = max(x1, x2, x3, x4)
    ymax = max(y1, y2, y3, y4)
    combinate = [[[x1, y1], [x2, y2], [x3, y3], [x4, y4]], [[x2, y2], [x3, y3], [x4, y4], [x1, y1]], [[x3, y3], [x4, y4], [x1, y1], [x2, y2]], [[x4, y4], [x1, y1], [x2, y2], [x3, y3]]]
    dst_coordinate = [[xmin, ymin], [xmax, ymin], [xmax, ymax], [xmin, ymax]]
    force = 100000000.0
    force_flag = 0
    for i in range(4):
        temp_force = cal_line_length(combinate[i][0], dst_coordinate[0]) + cal_line_length(combinate[i][1], dst_coordinate[1]) + cal_line_length(combinate[i][2], dst_coordinate[2]) + cal_line_length(combinate[i][3], dst_coordinate[3])
        if temp_force < force:
            force = temp_force
            force_flag = i
    if force_flag != 0:
        print('choose one direction!')
    final_result.append(combinate[force_flag])"
https://github.com/microsoft/playwright-python/tree/master/playwright/_impl/_wait_helper.py,"def _cleanup(self) -> None:
        for task in self._pending_tasks:
            if not task.done():
                task.cancel()
        for listener in self._registered_listeners:
            listener[0].remove_listener(listener[1], listener[2])",_7780.py,5,"for listener in self._registered_listeners:
    listener[0].remove_listener(listener[1], listener[2])","for (listener_0, listener_1, listener_2, *listener_len) in self._registered_listeners:
    listener_0.remove_listener(listener_1, listener_2)"
https://github.com/Netflix/vmaf/tree/master/python/vmaf/core/noref_feature_extractor.py,"def _generate_result(self, asset):
        # routine to generate feature scores in the log file.

        quality_w, quality_h = asset.quality_width_height
        yuv_type = self._get_workfile_yuv_type(asset)
        assert yuv_type in YuvReader.SUPPORTED_YUV_8BIT_TYPES, '{} only work with 8 bit for now.'.format(self.__class__.__name__)
        with YuvReader(filepath=asset.dis_procfile_path, width=quality_w,
                       height=quality_h,
                       yuv_type=yuv_type) \
                as dis_yuv_reader:
            scores_mtx_list = []
            i = 0
            for dis_yuv in dis_yuv_reader:
                dis_y = dis_yuv[0].astype('int32')
                mag = self.sobel_filt(dis_y)
                si = np.std(mag)
                if i == 0:
                    ti = 0
                else:
                    ti = np.std(dis_y - dis_y_prev)
                dis_y_prev = copy.deepcopy(dis_y)
                scores_mtx_list.append(np.hstack(([si], [ti])))
                i += 1
            scores_mtx = np.vstack(scores_mtx_list)

            # write scores_mtx to log file
        log_file_path = self._get_log_file_path(asset)
        with open(log_file_path, ""wb"") as log_file:
            np.save(log_file, scores_mtx)",_8940.py,13,"for dis_yuv in dis_yuv_reader:
    dis_y = dis_yuv[0].astype('int32')
    mag = self.sobel_filt(dis_y)
    si = np.std(mag)
    if i == 0:
        ti = 0
    else:
        ti = np.std(dis_y - dis_y_prev)
    dis_y_prev = copy.deepcopy(dis_y)
    scores_mtx_list.append(np.hstack(([si], [ti])))
    i += 1","for (dis_yuv_0, *dis_yuv_len) in dis_yuv_reader:
    dis_y = dis_yuv_0.astype('int32')
    mag = self.sobel_filt(dis_y)
    si = np.std(mag)
    if i == 0:
        ti = 0
    else:
        ti = np.std(dis_y - dis_y_prev)
    dis_y_prev = copy.deepcopy(dis_y)
    scores_mtx_list.append(np.hstack(([si], [ti])))
    i += 1"
https://github.com/dbcli/mssql-cli/tree/master/mssqlcli/mssqlcliclient.py,"def get_foreign_keys(self):
        """""" Yields (parent_schema, parent_table, parent_column, child_schema, child_table,
            child_column) typles""""""
        query = mssqlqueries.get_foreignkeys()
        logger.info(u'Foreign keys query: %s', query)
        for tabular_result in self.execute_query(query):
            for row in tabular_result[0]:
                yield ForeignKey(*row)",_10292.py,6,"for tabular_result in self.execute_query(query):
    for row in tabular_result[0]:
        yield ForeignKey(*row)","for (tabular_result_0, *tabular_result_len) in self.execute_query(query):
    for row in tabular_result_0:
        yield ForeignKey(*row)"
https://github.com/openstack/nova/tree/master/nova/policy.py,"def _warning_for_deprecated_user_based_rules(rules):
    """"""Warning user based policy enforcement used in the rule but the rule
    doesn't support it.
    """"""
    for rule in rules:
        # We will skip the warning for the resources which support user based
        # policy enforcement.
        if [resource for resource in USER_BASED_RESOURCES
                if resource in rule[0]]:
            continue
        if 'user_id' in KEY_EXPR.findall(rule[1]):
            LOG.warning(
                ""The user_id attribute isn't supported in the rule '%s'. ""
                ""All the user_id based policy enforcement will be removed in ""
                ""the future."",
                rule[0]
            )",_11151.py,5,"for rule in rules:
    if [resource for resource in USER_BASED_RESOURCES if resource in rule[0]]:
        continue
    if 'user_id' in KEY_EXPR.findall(rule[1]):
        LOG.warning(""The user_id attribute isn't supported in the rule '%s'. All the user_id based policy enforcement will be removed in the future."", rule[0])","for (rule_0, rule_1, *rule_len) in rules:
    if [resource for resource in USER_BASED_RESOURCES if resource in rule_0]:
        continue
    if 'user_id' in KEY_EXPR.findall(rule_1):
        LOG.warning(""The user_id attribute isn't supported in the rule '%s'. All the user_id based policy enforcement will be removed in the future."", rule_0)"
https://github.com/ricsinaruto/Seq2seqChatbots/tree/master/t2t_csaky/scripts/frequencies.py,"def main():
  parser = argparse.ArgumentParser()
  parser.add_argument('-n', '--ntokens', type=int,
                      help='number of tokens to show', default=20)
  parser.add_argument('-o', '--output', type=str,
                      help='name of the output file', default='top_tokens.txt')
  parser.add_argument('-i', '--input', type=str, help='name of the input file')

  args = parser.parse_args()
  tokens = {}
  with open(args.input, 'r') as fin:
    for line in fin:
      for token in line.strip().split():
        tokens[token] = tokens.get(token, 0) + 1

  freqs = sorted(
      tokens.items(), key=lambda x: x[1], reverse=True)[:args.ntokens]
  with open(args.output, 'w') as fou:
    for freq in freqs:
      fou.write(freq[0] + '\n')

  print(freqs)",_11263.py,19,"for freq in freqs:
    fou.write(freq[0] + '\n')","for (freq_0, *freq_len) in freqs:
    fou.write(freq_0 + '\n')"
https://github.com/IntelLabs/nlp-architect/tree/master/nlp_architect/models/absa/utils.py,"def load_opinion_lex(file_name: Union[str, PathLike]) -> dict:
    """"""Read opinion lexicon from CSV file.

    Returns:
        Dictionary of LexiconElements, each LexiconElement presents a row.
    """"""
    lexicon = {}
    with open(file_name, newline="""", encoding=""utf-8"") as csvfile:
        reader = csv.reader(csvfile, delimiter="","", quotechar=""|"")
        next(reader)
        for row in reader:
            term, score, polarity, is_acquired = row[0], row[1], row[2], row[3]
            score = float(score)
            # ignore terms with low score
            if score >= 0.5 and polarity in (Polarity.POS.value, Polarity.NEG.value):
                lexicon[term] = LexiconElement(
                    term.lower(),
                    score if polarity == Polarity.POS.value else -score,
                    polarity,
                    is_acquired,
                )
    return lexicon",_12577.py,11,"for row in reader:
    (term, score, polarity, is_acquired) = (row[0], row[1], row[2], row[3])
    score = float(score)
    if score >= 0.5 and polarity in (Polarity.POS.value, Polarity.NEG.value):
        lexicon[term] = LexiconElement(term.lower(), score if polarity == Polarity.POS.value else -score, polarity, is_acquired)","for (row_0, row_1, row_2, row_3, *row_len) in reader:
    (term, score, polarity, is_acquired) = (row_0, row_1, row_2, row_3)
    score = float(score)
    if score >= 0.5 and polarity in (Polarity.POS.value, Polarity.NEG.value):
        lexicon[term] = LexiconElement(term.lower(), score if polarity == Polarity.POS.value else -score, polarity, is_acquired)"
https://github.com/quay/quay/tree/master/buildman/manager/basemanager.py,"def __new__(cls, *args, **kwargs):
        """"""Hack to ensure method defined as async are implemented as such.""""""
        coroutines = inspect.getmembers(BaseManager, predicate=inspect.iscoroutinefunction)
        for coroutine in coroutines:
            implemented_method = getattr(cls, coroutine[0])
            if not inspect.iscoroutinefunction(implemented_method):
                raise RuntimeError(""The method %s must be a coroutine"" % implemented_method)

        return super().__new__(cls, *args, **kwargs)",_12980.py,4,"for coroutine in coroutines:
    implemented_method = getattr(cls, coroutine[0])
    if not inspect.iscoroutinefunction(implemented_method):
        raise RuntimeError('The method %s must be a coroutine' % implemented_method)","for (coroutine_0, *coroutine_len) in coroutines:
    implemented_method = getattr(cls, coroutine_0)
    if not inspect.iscoroutinefunction(implemented_method):
        raise RuntimeError('The method %s must be a coroutine' % implemented_method)"
https://github.com/cltk/cltk/tree/master/src/cltk/tokenizers/word.py,"def tokenize(self, text: str):
        """"""
        :rtype: list
        :param text: text to be tokenized into sentences
        :type text: str
        :param model: tokenizer object to used # Should be in init?
        :type model: object
        """"""
        for pattern in self.patterns:
            text = re.sub(pattern[0], pattern[1], text)
        return text.split()",_13820.py,9,"for pattern in self.patterns:
    text = re.sub(pattern[0], pattern[1], text)","for (pattern_0, pattern_1, *pattern_len) in self.patterns:
    text = re.sub(pattern_0, pattern_1, text)"
https://github.com/savoirfairelinux/num2words/tree/master/tests/test_es.py,"def test_currency_ang(self):
        for test in TEST_CASES_TO_CURRENCY_ANG:
            self.assertEqual(
                num2words(test[0], lang='es', to='currency', currency='ANG'),
                test[1]
            )",_14073.py,2,"for test in TEST_CASES_TO_CURRENCY_ANG:
    self.assertEqual(num2words(test[0], lang='es', to='currency', currency='ANG'), test[1])","for (test_0, test_1, *test_len) in TEST_CASES_TO_CURRENCY_ANG:
    self.assertEqual(num2words(test_0, lang='es', to='currency', currency='ANG'), test_1)"
https://github.com/qiucheng025/zao-/tree/master/lib/gui/display_analysis.py,"def tree_columns(self):
        """""" Add the columns to the totals treeview """"""
        logger.debug(""Adding Treeview columns"")
        columns = ((""session"", 40, ""#""),
                   (""start"", 130, None),
                   (""end"", 130, None),
                   (""elapsed"", 90, None),
                   (""batch"", 50, None),
                   (""iterations"", 90, None),
                   (""rate"", 60, ""EGs/sec""))
        self.tree[""columns""] = [column[0] for column in columns]

        for column in columns:
            text = column[2] if column[2] else column[0].title()
            logger.debug(""Adding heading: '%s'"", text)
            self.tree.heading(column[0], text=text)
            self.tree.column(column[0], width=column[1], anchor=tk.E, minwidth=40)
        self.tree.column(""#0"", width=40)
        self.tree.heading(""#0"", text=""Graphs"")

        return [column[0] for column in columns]",_14200.py,13,"for column in columns:
    text = column[2] if column[2] else column[0].title()
    logger.debug(""Adding heading: '%s'"", text)
    self.tree.heading(column[0], text=text)
    self.tree.column(column[0], width=column[1], anchor=tk.E, minwidth=40)","for (column_0, column_1, column_2, *column_len) in columns:
    text = column_2 if column_2 else column_0.title()
    logger.debug(""Adding heading: '%s'"", text)
    self.tree.heading(column_0, text=text)
    self.tree.column(column_0, width=column_1, anchor=tk.E, minwidth=40)"
https://github.com/pywebio/PyWebIO/tree/master/pywebio/utils.py,"def pyinstaller_datas(cli_args=False):
    """"""Return data files included in the PyWebIO to be added to pyinstaller bundle.""""""
    datas = [
        (STATIC_PATH, 'pywebio/html'),
        (normpath(STATIC_PATH + '/../platform/tpl'), 'pywebio/platform/tpl')
    ]
    if cli_args:
        args = ''
        for item in datas:
            args += ' --add-data %s%s%s' % (item[0], os.pathsep, item[1])
        return args
    return datas",_14382.py,9,"for item in datas:
    args += ' --add-data %s%s%s' % (item[0], os.pathsep, item[1])","for (item_0, item_1, *item_len) in datas:
    args += ' --add-data %s%s%s' % (item_0, os.pathsep, item_1)"
https://github.com/tribe29/checkmk/tree/master/cmk/gui/plugins/views/perfometers/check_mk.py,"def perfometer_nfsiostat(
    row: Row, check_command: str, perf_data: Perfdata
) -> LegacyPerfometerResult:
    for pd in perf_data:
        if pd[0] == ""op_s"":
            ops = float(pd[1])
            color = ""#ff6347""
            return ""%d op/s"" % ops, perfometer_linear(ops, color)
    return None",_14744.py,4,"for pd in perf_data:
    if pd[0] == 'op_s':
        ops = float(pd[1])
        color = '#ff6347'
        return ('%d op/s' % ops, perfometer_linear(ops, color))","for (pd_0, pd_1, *pd_len) in perf_data:
    if pd_0 == 'op_s':
        ops = float(pd_1)
        color = '#ff6347'
        return ('%d op/s' % ops, perfometer_linear(ops, color))"
https://github.com/dbcli/mssql-cli/tree/master/tests/test_sqlcompletion.py,"def test_named_query_completion():
        test_args = [
            ('\\ns abc SELECT ', 'SELECT ', [
                Column(table_refs=(), qualifiable=True),
                Function(schema=None),
                Keyword('SELECT')
            ]),
            ('\\ns abc SELECT foo ', 'SELECT foo ', (Keyword(),)),
            ('\\ns abc SELECT t1. FROM tabl1 t1', 'SELECT t1.', [
                Table(schema='t1'),
                View(schema='t1'),
                Column(table_refs=((None, 'tabl1', 't1', False),)),
                Function(schema='t1')
            ])
        ]
        for arg in test_args:
            text = arg[0]
            before = arg[1]
            expected = arg[2]
            suggestions = suggest_type(text, before)
            assert set(expected) == set(suggestions)",_15467.py,16,"for arg in test_args:
    text = arg[0]
    before = arg[1]
    expected = arg[2]
    suggestions = suggest_type(text, before)
    assert set(expected) == set(suggestions)","for (arg_0, arg_1, arg_2, *arg_len) in test_args:
    text = arg_0
    before = arg_1
    expected = arg_2
    suggestions = suggest_type(text, before)
    assert set(expected) == set(suggestions)"
https://github.com/k4m4/kickthemout/tree/master//kickthemout.py,"def main():

    # display heading
    heading()

    if interactive:

        print(""\n{}Using interface '{}{}{}' with MAC address '{}{}{}'.\nGateway IP: '{}{}{}' --> {}{}{} hosts are up.{}"".format(
            GREEN, RED, defaultInterface, GREEN, RED, defaultInterfaceMac, GREEN, RED, defaultGatewayIP, GREEN, RED, str(len(hostsList)), GREEN, END))
        # display warning in case of no active hosts
        if len(hostsList) == 0 or len(hostsList) == 1:
            if len(hostsList) == 1:
                if hostsList[0][0] == defaultGatewayIP:
                    print(""\n{}{}WARNING: There are {}0 hosts up{} on you network except your gateway.\n\tYou can't kick anyone off {}:/{}\n"".format(
                        GREEN, RED, GREEN, RED, GREEN, END))
                    os._exit(1)
            else:
                print(
                ""\n{}{}WARNING: There are {}0 hosts{} up on you network.\n\tIt looks like something went wrong {}:/{}"".format(
                    GREEN, RED, GREEN, RED, GREEN, END))
                print(
                ""\n{}If you are experiencing this error multiple times, please submit an issue here:\n\t{}https://github.com/k4m4/kickthemout/issues\n{}"".format(
                    RED, BLUE, END))
                os._exit(1)

    else:
        print(""\n{}Using interface '{}{}{}' with MAC address '{}{}{}'.\nGateway IP: '{}{}{}' --> Target(s): '{}{}{}'.{}"".format(
            GREEN, RED, defaultInterface, GREEN, RED, defaultInterfaceMac, GREEN, RED, defaultGatewayIP, GREEN, RED, "", "".join(options.targets), GREEN, END))

    if options.targets is None and options.scan is False:
        try:

            while True:
                optionBanner()

                header = ('{}kickthemout{}> {}'.format(BLUE, WHITE, END))
                choice = input(header)

                if choice.upper() == 'E' or choice.upper() == 'EXIT':
                    shutdown()

                elif choice == '1':
                    kickoneoff()

                elif choice == '2':
                    kicksomeoff()

                elif choice == '3':
                    kickalloff()

                elif choice.upper() == 'CLEAR':
                    os.system(""clear||cls"")
                else:
                    print(""\n{}ERROR: Please select a valid option.{}\n"".format(RED, END))

        except KeyboardInterrupt:
            shutdown()

    elif options.scan is not False:
        stopAnimation = False
        t = threading.Thread(target=scanningAnimation, args=('Scanning your network, hang on...',))
        t.daemon = True
        t.start()
    
        # commence scanning process
        try:
            scanNetwork()
        except KeyboardInterrupt:
            shutdown()
        stopAnimation = True
    
        print(""\nOnline IPs: "")
        for i in range(len(onlineIPs)):
            mac = """"
            for host in hostsList:
                if host[0] == onlineIPs[i]:
                    mac = host[1]
            try:
                hostname = utils.socket.gethostbyaddr(onlineIPs[i])[0]
            except:
                hostname = ""N/A""
            vendor = resolveMac(mac)
            print(""  [{}{}{}] {}{}{}\t{}{}\t{} ({}{}{}){}"".format(YELLOW, str(i), WHITE, RED, str(onlineIPs[i]), BLUE, mac, GREEN, vendor, YELLOW, hostname, GREEN, END))

    else:
        nonInteractiveAttack()",_16285.py,75,"for host in hostsList:
    if host[0] == onlineIPs[i]:
        mac = host[1]","for (host_0, host_1, *host_len) in hostsList:
    if host_0 == onlineIPs[i]:
        mac = host_1"
https://github.com/getsentry/sentry/tree/master/tests/sentry/api/endpoints/test_project_stats.py,"def test_simple(self):
        self.login_as(user=self.user)

        project1 = self.create_project(name=""foo"")
        project2 = self.create_project(name=""bar"")

        project_key1 = self.create_project_key(project=project1)
        self.store_outcomes(
            {
                ""org_id"": project1.organization.id,
                ""timestamp"": before_now(minutes=1),
                ""project_id"": project1.id,
                ""key_id"": project_key1.id,
                ""outcome"": Outcome.ACCEPTED,
                ""reason"": ""none"",
                ""category"": DataCategory.ERROR,
                ""quantity"": 3,
            },
            1,
        )
        project_key2 = self.create_project_key(project=project2)
        self.store_outcomes(
            {
                ""org_id"": project2.organization.id,
                ""timestamp"": before_now(minutes=1),
                ""project_id"": project2.id,
                ""key_id"": project_key2.id,
                ""outcome"": Outcome.ACCEPTED,
                ""reason"": ""none"",
                ""category"": DataCategory.ERROR,
                ""quantity"": 5,
            },
            1,
        )

        url = reverse(
            ""sentry-api-0-project-stats"",
            kwargs={""organization_slug"": project1.organization.slug, ""project_slug"": project1.slug},
        )
        response = self.client.get(url, format=""json"")

        assert response.status_code == 200, response.content
        assert response.data[-1][1] == 3, response.data
        for point in response.data[:-1]:
            assert point[1] == 0
        assert len(response.data) == 24",_16504.py,44,"for point in response.data[:-1]:
    assert point[1] == 0","for (point_0, point_1, *point_len) in response.data[:-1]:
    assert point_1 == 0"
https://github.com/dabeaz-course/practical-python/tree/master/Solutions/2_7/report.py,"def read_portfolio(filename):
    '''
    Read a stock portfolio file into a list of dictionaries with keys
    name, shares, and price.
    '''
    portfolio = []
    with open(filename) as f:
        rows = csv.reader(f)
        headers = next(rows)

        for row in rows:
            stock = {
                 'name'   : row[0],
                 'shares' : int(row[1]),
                 'price'   : float(row[2])
            }
            portfolio.append(stock)

    return portfolio",_17099.py,11,"for row in rows:
    stock = {'name': row[0], 'shares': int(row[1]), 'price': float(row[2])}
    portfolio.append(stock)","for (row_0, row_1, row_2, *row_len) in rows:
    stock = {'name': row_0, 'shares': int(row_1), 'price': float(row_2)}
    portfolio.append(stock)"
https://github.com/google/deepvariant/tree/master/deepvariant/postprocess_variants.py,"def reindex_allele_indexed_fields(self, variant, fields):
    """"""Updates variant.call fields indexed by ref + alt_alleles.

    Args:
      variant: Variant proto. We will update the info fields of the Variant.call
        protos.
      fields: Iterable of string. Each string should provide a key to an
        alternative allele indexed field in VariantCall.info fields. Each field
        specified here will be updated to remove values associated with alleles
        no longer wanted according to this remapper object.
    """"""
    for field_info in fields:
      field = field_info[0]
      ref_is_zero = field_info[1]
      for call in variant.calls:
        if field in call.info:
          entry = call.info[field]
          updated = [
              v for i, v in enumerate(entry.values)
              if self.keep_index(i, ref_is_zero=ref_is_zero)
          ]
          # We cannot do entry.values[:] = updated as the ListValue type ""does
          # not support assignment"" so we have to do this grossness.
          del entry.values[:]
          entry.values.extend(updated)",_17191.py,12,"for field_info in fields:
    field = field_info[0]
    ref_is_zero = field_info[1]
    for call in variant.calls:
        if field in call.info:
            entry = call.info[field]
            updated = [v for (i, v) in enumerate(entry.values) if self.keep_index(i, ref_is_zero=ref_is_zero)]
            del entry.values[:]
            entry.values.extend(updated)","for (field_info_0, field_info_1, *field_info_len) in fields:
    field = field_info_0
    ref_is_zero = field_info_1
    for call in variant.calls:
        if field in call.info:
            entry = call.info[field]
            updated = [v for (i, v) in enumerate(entry.values) if self.keep_index(i, ref_is_zero=ref_is_zero)]
            del entry.values[:]
            entry.values.extend(updated)"
https://github.com/astropy/astropy/tree/master/astropy/io/fits/column.py,"def _init_from_table(self, table):
        hdr = table._header
        nfields = hdr[""TFIELDS""]

        # go through header keywords to pick out column definition keywords
        # definition dictionaries for each field
        col_keywords = [{} for i in range(nfields)]
        for keyword in hdr:
            key = TDEF_RE.match(keyword)
            try:
                label = key.group(""label"")
            except Exception:
                continue  # skip if there is no match
            if label in KEYWORD_NAMES:
                col = int(key.group(""num""))
                if 0 < col <= nfields:
                    attr = KEYWORD_TO_ATTRIBUTE[label]
                    value = hdr[keyword]
                    if attr == ""format"":
                        # Go ahead and convert the format value to the
                        # appropriate ColumnFormat container now
                        value = self._col_format_cls(value)
                    col_keywords[col - 1][attr] = value

        # Verify the column keywords and display any warnings if necessary;
        # we only want to pass on the valid keywords
        for idx, kwargs in enumerate(col_keywords):
            valid_kwargs, invalid_kwargs = Column._verify_keywords(**kwargs)
            for val in invalid_kwargs.values():
                warnings.warn(
                    f""Invalid keyword for column {idx + 1}: {val[1]}"", VerifyWarning
                )
            # Special cases for recformat and dim
            # TODO: Try to eliminate the need for these special cases
            del valid_kwargs[""recformat""]
            if ""dim"" in valid_kwargs:
                valid_kwargs[""dim""] = kwargs[""dim""]
            col_keywords[idx] = valid_kwargs

        # data reading will be delayed
        for col in range(nfields):
            col_keywords[col][""array""] = Delayed(table, col)

        # now build the columns
        self.columns = [Column(**attrs) for attrs in col_keywords]

        # Add the table HDU is a listener to changes to the columns
        # (either changes to individual columns, or changes to the set of
        # columns (add/remove/etc.))
        self._add_listener(table)",_17195.py,29,"for val in invalid_kwargs.values():
    warnings.warn(f'Invalid keyword for column {idx + 1}: {val[1]}', VerifyWarning)","for (val_0, val_1, *val_len) in invalid_kwargs.values():
    warnings.warn(f'Invalid keyword for column {idx + 1}: {val_1}', VerifyWarning)"
https://github.com/astropy/astropy/tree/master/astropy/table/tests/test_column.py,"def test_unicode_sandwich_masked_compare():
    """"""Test the fix for #6839 from #6899.""""""
    c1 = table.MaskedColumn([""a"", ""b"", ""c"", ""d""], mask=[True, False, True, False])
    c2 = table.MaskedColumn([b""a"", b""b"", b""c"", b""d""], mask=[True, True, False, False])

    for cmp in ((c1 == c2), (c2 == c1)):
        assert cmp[0] is np.ma.masked
        assert cmp[1] is np.ma.masked
        assert cmp[2] is np.ma.masked
        assert cmp[3]

    for cmp in ((c1 != c2), (c2 != c1)):
        assert cmp[0] is np.ma.masked
        assert cmp[1] is np.ma.masked
        assert cmp[2] is np.ma.masked
        assert not cmp[3]",_17719.py,6,"for cmp in (c1 == c2, c2 == c1):
    assert cmp[0] is np.ma.masked
    assert cmp[1] is np.ma.masked
    assert cmp[2] is np.ma.masked
    assert cmp[3]","for (cmp_0, cmp_1, cmp_2, cmp_3, *cmp_len) in (c1 == c2, c2 == c1):
    assert cmp_0 is np.ma.masked
    assert cmp_1 is np.ma.masked
    assert cmp_2 is np.ma.masked
    assert cmp_3"
https://github.com/astropy/astropy/tree/master/astropy/table/tests/test_column.py,"def test_unicode_sandwich_masked_compare():
    """"""Test the fix for #6839 from #6899.""""""
    c1 = table.MaskedColumn([""a"", ""b"", ""c"", ""d""], mask=[True, False, True, False])
    c2 = table.MaskedColumn([b""a"", b""b"", b""c"", b""d""], mask=[True, True, False, False])

    for cmp in ((c1 == c2), (c2 == c1)):
        assert cmp[0] is np.ma.masked
        assert cmp[1] is np.ma.masked
        assert cmp[2] is np.ma.masked
        assert cmp[3]

    for cmp in ((c1 != c2), (c2 != c1)):
        assert cmp[0] is np.ma.masked
        assert cmp[1] is np.ma.masked
        assert cmp[2] is np.ma.masked
        assert not cmp[3]",_17719.py,12,"for cmp in (c1 != c2, c2 != c1):
    assert cmp[0] is np.ma.masked
    assert cmp[1] is np.ma.masked
    assert cmp[2] is np.ma.masked
    assert not cmp[3]","for (cmp_0, cmp_1, cmp_2, cmp_3, *cmp_len) in (c1 != c2, c2 != c1):
    assert cmp_0 is np.ma.masked
    assert cmp_1 is np.ma.masked
    assert cmp_2 is np.ma.masked
    assert not cmp_3"
https://github.com/CalciferZh/SMPL/tree/master//smpl_torch.py,"def write_obj(self, verts, file_name):
    with open(file_name, 'w') as fp:
      for v in verts:
        fp.write('v %f %f %f\n' % (v[0], v[1], v[2]))

      for f in self.faces + 1:
        fp.write('f %d %d %d\n' % (f[0], f[1], f[2]))",_18098.py,3,"for v in verts:
    fp.write('v %f %f %f\n' % (v[0], v[1], v[2]))","for (v_0, v_1, v_2, *v_len) in verts:
    fp.write('v %f %f %f\n' % (v_0, v_1, v_2))"
https://github.com/CalciferZh/SMPL/tree/master//smpl_torch.py,"def write_obj(self, verts, file_name):
    with open(file_name, 'w') as fp:
      for v in verts:
        fp.write('v %f %f %f\n' % (v[0], v[1], v[2]))

      for f in self.faces + 1:
        fp.write('f %d %d %d\n' % (f[0], f[1], f[2]))",_18098.py,6,"for f in self.faces + 1:
    fp.write('f %d %d %d\n' % (f[0], f[1], f[2]))","for (f_0, f_1, f_2, *f_len) in self.faces + 1:
    fp.write('f %d %d %d\n' % (f_0, f_1, f_2))"
https://github.com/NeymarL/ChineseChess-AlphaZero/tree/master/cchess_alphazero/agent/player.py,"def receiver(self):
        '''
        receive policy and value from neural network
        '''
        while not self.job_done:
            if self.pipe.poll(0.001):
                rets = self.pipe.recv()
            else:
                continue
            k = 0
            with self.q_lock:
                for ret in rets:
                    # logger.debug(f""NN ret, update tree buffer_history = {self.buffer_history}"")
                    self.executor.submit(self.update_tree, ret[0], ret[1], self.buffer_history[k])
                    # self.update_tree(ret[0], ret[1], self.buffer_history[k])
                    k = k + 1
                self.buffer_planes = self.buffer_planes[k:]
                self.buffer_history = self.buffer_history[k:]
            self.run_lock.release()",_18671.py,12,"for ret in rets:
    self.executor.submit(self.update_tree, ret[0], ret[1], self.buffer_history[k])
    k = k + 1","for (ret_0, ret_1, *ret_len) in rets:
    self.executor.submit(self.update_tree, ret_0, ret_1, self.buffer_history[k])
    k = k + 1"
https://github.com/lazzyfu/YaSQL/tree/master/yasql/apps/sqlorders/tasks.py,"def dbms_sync_clickhouse_schema(row):
    ignored_schemas = ('_temporary_and_external_tables', 'system', 'default')
    query = f""select name from system.databases where name not in {ignored_schemas}""
    config = {
        'host': row.host,
        'port': row.port,
        'database': 'default',
        'connect_timeout': 5,
        'send_receive_timeout': 5,
    }
    # 请在clickhouse创建好用户
    config.update(REOMOTE_USER)
    cnx = Client(**config)
    result = cnx.execute(query)
    for i in result:
        schema = i[0]
        models.DbSchemas.objects.update_or_create(
            cid_id=row.id,
            schema=schema,
            defaults={'schema': schema}
        )
    cnx.disconnect()",_19316.py,15,"for i in result:
    schema = i[0]
    models.DbSchemas.objects.update_or_create(cid_id=row.id, schema=schema, defaults={'schema': schema})","for (i_0, *i_len) in result:
    schema = i_0
    models.DbSchemas.objects.update_or_create(cid_id=row.id, schema=schema, defaults={'schema': schema})"
https://github.com/readbeyond/aeneas/tree/master/aeneas/tests/test_globalfunctions.py,"def test_is_unicode(self):
        tests = [
            (None, False),
            (u"""", True),
            (u""foo"", True),
            (u""fox99"", True),
            (b""foo"", False),
            ([], False),
            ([u""foo""], False),
            ({u""foo"": u""baz""}, False),
        ]
        if gf.PY2:
            tests.extend([
                ("""", False),
                (""foo"", False),
                (""fox99"", False),
            ])
        else:
            tests.extend([
                ("""", True),
                (""foo"", True),
                (""fox99"", True),
            ])
        for test in tests:
            self.assertEqual(gf.is_unicode(test[0]), test[1])",_19907.py,24,"for test in tests:
    self.assertEqual(gf.is_unicode(test[0]), test[1])","for (test_0, test_1, *test_len) in tests:
    self.assertEqual(gf.is_unicode(test_0), test_1)"
https://github.com/soimort/you-get/tree/master/src/you_get/extractors/embed.py,"def embed_download(url, output_dir = '.', merge = True, info_only = False, **kwargs):
    content = get_content(url, headers=fake_headers)
    found = False
    title = match1(content, '<title>([^<>]+)</title>')

    vids = matchall(content, youku_embed_patterns)
    for vid in set(vids):
        found = True
        youku_download_by_vid(vid, title=title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)

    vids = matchall(content, tudou_embed_patterns)
    for vid in set(vids):
        found = True
        tudou_download_by_id(vid, title=title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)

    vids = matchall(content, iqiyi_embed_patterns)
    for vid in vids:
        found = True
        iqiyi_download_by_vid((vid[1], vid[0]), title=title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)

    urls = matchall(content, netease_embed_patterns)
    for url in urls:
        found = True
        netease_download(url, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)

    urls = matchall(content, vimeo_embed_patters)
    for url in urls:
        found = True
        vimeo_download_by_id(url, title=title, output_dir=output_dir, merge=merge, info_only=info_only, referer=url, **kwargs)

    urls = matchall(content, dailymotion_embed_patterns)
    for url in urls:
        found = True
        dailymotion_download(url, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)

    aids = matchall(content, bilibili_embed_patterns)
    for aid in aids:
        found = True
        url = 'http://www.bilibili.com/video/av%s/' % aid
        bilibili_download(url, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)

    iqiyi_urls = matchall(content, iqiyi_patterns)
    for url in iqiyi_urls:
        found = True
        iqiyi.download(url, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)

    bokecc_metas = matchall(content, bokecc_patterns)
    for meta in bokecc_metas:
        found = True
        bokecc.bokecc_download_by_id(meta[1], output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)

    if found:
        return True

    # Try harder, check all iframes
    if 'recur_lv' not in kwargs or kwargs['recur_lv'] < recur_limit:
        r = kwargs.get('recur_lv')
        if r is None:
            r = 1
        else:
            r += 1
        iframes = matchall(content, [r'<iframe.+?src=(?:\""|\')(.*?)(?:\""|\')'])
        for iframe in iframes:
            if not iframe.startswith('http'):
                src = urllib.parse.urljoin(url, iframe)
            else:
                src = iframe
            found = embed_download(src, output_dir=output_dir, merge=merge, info_only=info_only, recur_lv=r, **kwargs)
            if found:
                return True

    if not found and 'recur_lv' not in kwargs:
        raise NotImplementedError(url)
    else:
        return found",_20380.py,17,"for vid in vids:
    found = True
    iqiyi_download_by_vid((vid[1], vid[0]), title=title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)","for (vid_0, vid_1, *vid_len) in vids:
    found = True
    iqiyi_download_by_vid((vid_1, vid_0), title=title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)"
https://github.com/soimort/you-get/tree/master/src/you_get/extractors/embed.py,"def embed_download(url, output_dir = '.', merge = True, info_only = False, **kwargs):
    content = get_content(url, headers=fake_headers)
    found = False
    title = match1(content, '<title>([^<>]+)</title>')

    vids = matchall(content, youku_embed_patterns)
    for vid in set(vids):
        found = True
        youku_download_by_vid(vid, title=title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)

    vids = matchall(content, tudou_embed_patterns)
    for vid in set(vids):
        found = True
        tudou_download_by_id(vid, title=title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)

    vids = matchall(content, iqiyi_embed_patterns)
    for vid in vids:
        found = True
        iqiyi_download_by_vid((vid[1], vid[0]), title=title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)

    urls = matchall(content, netease_embed_patterns)
    for url in urls:
        found = True
        netease_download(url, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)

    urls = matchall(content, vimeo_embed_patters)
    for url in urls:
        found = True
        vimeo_download_by_id(url, title=title, output_dir=output_dir, merge=merge, info_only=info_only, referer=url, **kwargs)

    urls = matchall(content, dailymotion_embed_patterns)
    for url in urls:
        found = True
        dailymotion_download(url, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)

    aids = matchall(content, bilibili_embed_patterns)
    for aid in aids:
        found = True
        url = 'http://www.bilibili.com/video/av%s/' % aid
        bilibili_download(url, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)

    iqiyi_urls = matchall(content, iqiyi_patterns)
    for url in iqiyi_urls:
        found = True
        iqiyi.download(url, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)

    bokecc_metas = matchall(content, bokecc_patterns)
    for meta in bokecc_metas:
        found = True
        bokecc.bokecc_download_by_id(meta[1], output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)

    if found:
        return True

    # Try harder, check all iframes
    if 'recur_lv' not in kwargs or kwargs['recur_lv'] < recur_limit:
        r = kwargs.get('recur_lv')
        if r is None:
            r = 1
        else:
            r += 1
        iframes = matchall(content, [r'<iframe.+?src=(?:\""|\')(.*?)(?:\""|\')'])
        for iframe in iframes:
            if not iframe.startswith('http'):
                src = urllib.parse.urljoin(url, iframe)
            else:
                src = iframe
            found = embed_download(src, output_dir=output_dir, merge=merge, info_only=info_only, recur_lv=r, **kwargs)
            if found:
                return True

    if not found and 'recur_lv' not in kwargs:
        raise NotImplementedError(url)
    else:
        return found",_20380.py,48,"for meta in bokecc_metas:
    found = True
    bokecc.bokecc_download_by_id(meta[1], output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)","for (meta_0, meta_1, *meta_len) in bokecc_metas:
    found = True
    bokecc.bokecc_download_by_id(meta_1, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)"
https://github.com/Boris-code/feapder/tree/master/feapder/commands/create/create_item.py,"def create_item(self, item_template, columns, table_name, support_dict):
        table_name_hump_format = self.convert_table_name_to_hump(table_name)
        # 组装 类名
        item_template = item_template.replace(""${item_name}"", table_name_hump_format)
        if support_dict:
            item_template = item_template.replace(""${command}"", table_name + "" 1"")
        else:
            item_template = item_template.replace(""${command}"", table_name)
        item_template = item_template.replace(""${table_name}"", table_name)

        # 组装 属性
        propertys = """"
        for column in columns:
            column_name = column[0]
            column_type = column[1]
            is_nullable = column[2]
            column_default = column[3]
            column_extra = column[4]
            column_key = column[5]
            column_comment = column[6]

            try:
                column_default = None if column_default == ""NULL"" else column_default
                value = (
                    ""kwargs.get('{column_name}')"".format(column_name=column_name)
                    if support_dict
                    else (
                        column_default != ""CURRENT_TIMESTAMP"" and column_default or None
                    )
                    and eval(column_default)
                )
            except:
                value = (
                    ""kwargs.get('{column_name}')"".format(column_name=column_name)
                    if support_dict
                    else (
                        column_default != ""CURRENT_TIMESTAMP"" and column_default or None
                    )
                    and column_default
                )

            if column_extra == ""auto_increment"" or column_default is not None:
                propertys += f""# self.{column_name} = {value}""

            else:
                if value is None or isinstance(value, (float, int)) or support_dict:
                    propertys += f""self.{column_name} = {value}""
                else:
                    propertys += f""self.{column_name} = '{value}'""

            if column_comment:
                propertys += f""  # {column_comment}""
            propertys += ""\n"" + "" "" * 8

        item_template = item_template.replace(""${propertys}"", propertys.strip())
        item_template = deal_file_info(item_template)

        return item_template",_20629.py,13,"for column in columns:
    column_name = column[0]
    column_type = column[1]
    is_nullable = column[2]
    column_default = column[3]
    column_extra = column[4]
    column_key = column[5]
    column_comment = column[6]
    try:
        column_default = None if column_default == 'NULL' else column_default
        value = ""kwargs.get('{column_name}')"".format(column_name=column_name) if support_dict else (column_default != 'CURRENT_TIMESTAMP' and column_default or None) and eval(column_default)
    except:
        value = ""kwargs.get('{column_name}')"".format(column_name=column_name) if support_dict else (column_default != 'CURRENT_TIMESTAMP' and column_default or None) and column_default
    if column_extra == 'auto_increment' or column_default is not None:
        propertys += f'# self.{column_name} = {value}'
    elif value is None or isinstance(value, (float, int)) or support_dict:
        propertys += f'self.{column_name} = {value}'
    else:
        propertys += f""self.{column_name} = '{value}'""
    if column_comment:
        propertys += f'  # {column_comment}'
    propertys += '\n' + ' ' * 8","for (column_0, column_1, column_2, column_3, column_4, column_5, column_6, *column_len) in columns:
    column_name = column_0
    column_type = column_1
    is_nullable = column_2
    column_default = column_3
    column_extra = column_4
    column_key = column_5
    column_comment = column_6
    try:
        column_default = None if column_default == 'NULL' else column_default
        value = ""kwargs.get('{column_name}')"".format(column_name=column_name) if support_dict else (column_default != 'CURRENT_TIMESTAMP' and column_default or None) and eval(column_default)
    except:
        value = ""kwargs.get('{column_name}')"".format(column_name=column_name) if support_dict else (column_default != 'CURRENT_TIMESTAMP' and column_default or None) and column_default
    if column_extra == 'auto_increment' or column_default is not None:
        propertys += f'# self.{column_name} = {value}'
    elif value is None or isinstance(value, (float, int)) or support_dict:
        propertys += f'self.{column_name} = {value}'
    else:
        propertys += f""self.{column_name} = '{value}'""
    if column_comment:
        propertys += f'  # {column_comment}'
    propertys += '\n' + ' ' * 8"
https://github.com/chubin/cheat.sh/tree/master/lib/fmt/comments.py,"def _commenting_script(lines_blocks, filetype):
    script_lines = []
    block_start = 1
    for block in lines_blocks:
        lines = list(block[1])

        block_end = block_start + len(lines)-1

        if block[0] == 0:
            comment_type = 'sexy'
            if block_end - block_start < 1 or filetype == 'ruby':
                comment_type = 'comment'

            script_lines.insert(0, ""%s,%s call NERDComment(1, '%s')""
                                % (block_start, block_end, comment_type))
            script_lines.insert(0, ""%s,%s call NERDComment(1, 'uncomment')""
                                % (block_start, block_end))

        block_start = block_end + 1

    script_lines.insert(0, ""set ft=%s"" % _language_name(filetype))
    script_lines.append(""wq"")

    return script_lines",_20768.py,4,"for block in lines_blocks:
    lines = list(block[1])
    block_end = block_start + len(lines) - 1
    if block[0] == 0:
        comment_type = 'sexy'
        if block_end - block_start < 1 or filetype == 'ruby':
            comment_type = 'comment'
        script_lines.insert(0, ""%s,%s call NERDComment(1, '%s')"" % (block_start, block_end, comment_type))
        script_lines.insert(0, ""%s,%s call NERDComment(1, 'uncomment')"" % (block_start, block_end))
    block_start = block_end + 1","for (block_0, block_1, *block_len) in lines_blocks:
    lines = list(block_1)
    block_end = block_start + len(lines) - 1
    if block_0 == 0:
        comment_type = 'sexy'
        if block_end - block_start < 1 or filetype == 'ruby':
            comment_type = 'comment'
        script_lines.insert(0, ""%s,%s call NERDComment(1, '%s')"" % (block_start, block_end, comment_type))
        script_lines.insert(0, ""%s,%s call NERDComment(1, 'uncomment')"" % (block_start, block_end))
    block_start = block_end + 1"
https://github.com/sobhe/hazm/tree/master/hazm/DadeganReader.py,"def chunked_trees(self):
		""""""درخت وابستگی‌های جملات را برمی‌گرداند.

		Examples:
			>>> from hazm.Chunker import tree2brackets
			>>> tree2brackets(next(dadegan.chunked_trees()))
			'[این میهمانی NP] [به PP] [منظور آشنایی هم‌تیمی‌های او NP] [با PP] [غذاهای ایرانی NP] [ترتیب داده_شد VP] .'

		Yields:
			(str): درخت وابستگی‌های جملهٔ بعدی.
		""""""		
		for tree in self.trees():
			chunks = []
			for node in word_nodes(tree):
				n = node['address']
				item = (node['word'], node['mtag'])
				appended = False
				if node['ctag'] in {'PREP', 'POSTP'}:
					for d in node_deps(node):
						label = 'PP'
						if node['ctag'] == 'POSTP':
							label = 'POSTP'
						if d == n - 1 and type(chunks[-1]) == Tree and chunks[-1].label() == label:
							chunks[-1].append(item)
							appended = True
					if node['head'] == n - 1 and len(chunks) > 0 and type(chunks[-1]) == Tree and chunks[
						-1].label() == label:
						chunks[-1].append(item)
						appended = True
					if not appended:
						chunks.append(Tree(label, [item]))
				elif node['ctag'] in {'PUNC', 'CONJ', 'SUBR', 'PART'}:
					if item[0] in {""'"", '""', '(', ')', '{', '}', '[', ']', '-', '#', '«', '»'} and len(chunks) > 0 and type(chunks[-1]) == Tree:
						for l in chunks[-1].leaves():
							if l[1] == item[1]:
								chunks[-1].append(item)
								appended = True
								break
					if appended is not True:
						chunks.append(item)
				elif node['ctag'] in {'N', 'PREM', 'ADJ', 'PR', 'ADR', 'PRENUM', 'IDEN', 'POSNUM', 'SADV'}:
					if node['rel'] in {'MOZ', 'NPOSTMOD'}:
						if len(chunks) > 0:
							if type(chunks[-1]) == Tree:
								j = n - len(chunks[-1].leaves())
								chunks[-1].append(item)
							else:
								j = n - 1
								treeNode = Tree('NP', [chunks.pop(), item])
								chunks.append(treeNode)
							while j > node['head']:
								leaves = chunks.pop().leaves()
								if len(chunks) < 1:
									chunks.append(Tree('NP', leaves))
									j -= 1
								elif type(chunks[-1]) == Tree:
									j -= len(chunks[-1])
									for l in leaves:
										chunks[-1].append(l)
								else:
									leaves.insert(0, chunks.pop())
									chunks.append(Tree('NP', leaves))
									j -= 1
							continue
					elif node['rel'] == 'POSDEP' and tree.nodes[node['head']]['rel'] in {'NCONJ', 'AJCONJ'}:
						conj = tree.nodes[node['head']]
						if tree.nodes[conj['head']]['rel'] in {'MOZ', 'NPOSTMOD', 'AJCONJ', 'POSDEP'}:
							label = 'NP'
							leaves = [item]
							j = n - 1
							while j >= conj['head']:
								if type(chunks[-1]) is Tree:
									j -= len(chunks[-1].leaves())
									label = chunks[-1].label()
									leaves = chunks.pop().leaves() + leaves
								else:
									leaves.insert(0, chunks.pop())
									j -= 1
							chunks.append(Tree(label, leaves))
							appended = True
					elif node['head'] == n - 1 and len(chunks) > 0 and type(chunks[-1]) == Tree and not chunks[
						-1].label() == 'PP':
						chunks[-1].append(item)
						appended = True
					elif node['rel'] == 'AJCONJ' and tree.nodes[node['head']]['rel'] in {'NPOSTMOD', 'AJCONJ'}:
						np_nodes = [item]
						label = 'ADJP'
						i = n - node['head']
						while i > 0:
							if type(chunks[-1]) == Tree:
								label = chunks[-1].label()
								leaves = chunks.pop().leaves()
								i -= len(leaves)
								np_nodes = leaves + np_nodes
							else:
								i -= 1
								np_nodes.insert(0, chunks.pop())
						chunks.append(Tree(label, np_nodes))
						appended = True
					elif node['ctag'] == 'ADJ' and node['rel'] == 'POSDEP' and tree.nodes[node['head']]['ctag'] != 'CONJ':
						np_nodes = [item]
						i = n - node['head']
						while i > 0:
							label = 'ADJP'
							if type(chunks[-1]) == Tree:
								label = chunks[-1].label()
								leaves = chunks.pop().leaves()
								i -= len(leaves)
								np_nodes = leaves + np_nodes
							else:
								i -= 1
								np_nodes.insert(0, chunks.pop())
						chunks.append(Tree(label, np_nodes))
						appended = True
					for d in node_deps(node):
						if d == n - 1 and type(chunks[-1]) == Tree and chunks[
							-1].label() != 'PP' and appended is not True:
							label = chunks[-1].label()
							if node['rel'] == 'ADV':
								label = 'ADVP'
							elif label in {'ADJP', 'ADVP'}:
								if node['ctag'] == 'N':
									label = 'NP'
								elif node['ctag'] == 'ADJ':
									label = 'ADJP'
							leaves = chunks.pop().leaves()
							leaves.append(item)
							chunks.append(Tree(label, leaves))
							appended = True
						elif tree.nodes[d]['rel'] == 'NPREMOD' and appended is not True:
							np_nodes = [item]
							i = n - d
							while i > 0:
								if type(chunks[-1]) == Tree:
									leaves = chunks.pop().leaves()
									i -= len(leaves)
									np_nodes = leaves + np_nodes
								else:
									i -= 1
									np_nodes.insert(0, chunks.pop())
							chunks.append(Tree('NP', np_nodes))
							appended = True
					if not appended:
						label = 'NP'
						if node['ctag'] == 'ADJ':
							label = 'ADJP'
						elif node['rel'] == 'ADV':
							label = 'ADVP'
						chunks.append(Tree(label, [item]))
				elif node['ctag'] in {'V'}:
					appended = False
					for d in node_deps(node):
						if d == n - 1 and type(chunks[-1]) == Tree and tree.nodes[d]['rel'] in {'NVE', 'ENC'} and appended is not True:
							leaves = chunks.pop().leaves()
							leaves.append(item)
							chunks.append(Tree('VP', leaves))
							appended = True
						elif tree.nodes[d]['rel'] in {'VPRT', 'NVE'}:
							vp_nodes = [item]
							i = n - d
							while i > 0:
								if type(chunks[-1]) == Tree:
									leaves = chunks.pop().leaves()
									i -= len(leaves)
									vp_nodes = leaves + vp_nodes
								else:
									i -= 1
									vp_nodes.insert(0, chunks.pop())
							chunks.append(Tree('VP', vp_nodes))
							appended = True
							break
					if not appended:
						chunks.append(Tree('VP', [item]))
				elif node['ctag'] in {'PSUS'}:
					if node['rel'] == 'ADV':
						chunks.append(Tree('ADVP', [item]))
					else:
						chunks.append(Tree('VP', [item]))
				elif node['ctag'] in {'ADV', 'SADV'}:
					appended = False
					for d in node_deps(node):
						if d == n - 1 and type(chunks[-1]) == Tree:
							leaves = chunks.pop().leaves()
							leaves.append(item)
							chunks.append(Tree('ADVP', leaves))
							appended = True
					if not appended:
						chunks.append(Tree('ADVP', [item]))

			yield Tree('S', chunks)",_21331.py,34,"for l in chunks[-1].leaves():
    if l[1] == item[1]:
        chunks[-1].append(item)
        appended = True
        break","for (l_0, l_1, *l_len) in chunks[-1].leaves():
    if l_1 == item[1]:
        chunks[-1].append(item)
        appended = True
        break"
https://github.com/savoirfairelinux/num2words/tree/master/tests/test_es.py,"def test_currency_crc(self):
        for test in TEST_CASES_TO_CURRENCY_CRC:
            self.assertEqual(
                num2words(test[0], lang='es', to='currency', currency='CRC'),
                test[1]
            )",_21594.py,2,"for test in TEST_CASES_TO_CURRENCY_CRC:
    self.assertEqual(num2words(test[0], lang='es', to='currency', currency='CRC'), test[1])","for (test_0, test_1, *test_len) in TEST_CASES_TO_CURRENCY_CRC:
    self.assertEqual(num2words(test_0, lang='es', to='currency', currency='CRC'), test_1)"
https://github.com/openstates/openstates-scrapers/tree/master/scrapers/va/csv_bills.py,"def load_members(self):
        resp = self.get(self._url_base + ""Members.csv"").text

        reader = csv.reader(resp.splitlines(), delimiter="","")
        # ['MBR_HOU', 'MBR_MBRNO', 'MBR_NAME']
        for row in reader:
            self._members[row[1]].append(
                {""chamber"": row[0], ""member_id"": row[1], ""name"": row[2].strip()}
            )
        self.warning(""Total Members Loaded: "" + str(len(self._members)))
        return True",_22933.py,6,"for row in reader:
    self._members[row[1]].append({'chamber': row[0], 'member_id': row[1], 'name': row[2].strip()})","for (row_0, row_1, row_2, *row_len) in reader:
    self._members[row_1].append({'chamber': row_0, 'member_id': row_1, 'name': row_2.strip()})"
https://github.com/anchore/anchore-engine/tree/master/anchore_engine/services/policy_engine/engine/vulnerabilities.py,"def merge_nvd_metadata(
    dbsession, vulnerability_objs, nvd_cls, cpe_cls, already_loaded_nvds=None
):
    """"""
    Return a list of tuples of (vuln obj, list(nvd records)

    :param dbsession active db session to use for query
    :param vulnerability_objs: a list of Vulnerability objects
    :param nvd_cls the class of nvd object to use for query
    :param cpe_cls the class of nvd object to use for query
    :return: list of tuples of (Vulnerability, list(NVD objects)) tuples
    """"""

    if already_loaded_nvds is None:
        already_loaded_nvds = []

    result_list = [
        [
            x,
            x.get_nvd_identifiers(nvd_cls, cpe_cls)
            if isinstance(x, Vulnerability)
            else [],
        ]
        for x in vulnerability_objs
    ]
    nvd_ids = []

    # Zip the ids into the master query list
    for id in result_list:
        nvd_ids.extend(id[1])

    # Dedup
    nvd_ids = list(set(nvd_ids).difference({rec.name for rec in already_loaded_nvds}))

    # Do the db lookup for all of them
    nvd_records = dbsession.query(nvd_cls).filter(nvd_cls.name.in_(nvd_ids)).all()
    nvd_records.extend(already_loaded_nvds)

    id_map = {x.name: x for x in nvd_records}

    # Map back to the records
    for entry in result_list:
        entry[1] = [id_map[id] for id in entry[1] if id in id_map]

    return result_list",_23143.py,29,"for id in result_list:
    nvd_ids.extend(id[1])","for (id_0, id_1, *id_len) in result_list:
    nvd_ids.extend(id_1)"
https://github.com/SavMartin/TexTools-Blender/tree/master//utilities_uv.py,"def selection_restore(bm = None, uv_layers = None, restore_seams=False):
	mode = bpy.context.object.mode
	if mode != 'EDIT':
		bpy.ops.object.mode_set(mode = 'EDIT')
	if bm is None:
		bm = bmesh.from_edit_mesh(bpy.context.active_object.data)
		uv_layers = bm.loops.layers.uv.verify()

	bpy.context.scene.tool_settings.use_uv_select_sync = settings.use_uv_sync
	bpy.context.scene.tool_settings.uv_select_mode = settings.selection_uv_mode

	contextViewUV = utilities_ui.GetContextViewUV()
	if contextViewUV:
		contextViewUV['area'].spaces[0].pivot_point = settings.selection_uv_pivot
		bpy.ops.uv.cursor_set(contextViewUV, location=settings.selection_uv_pivot_pos)

	#Restore seams
	if restore_seams:
		bpy.ops.mesh.select_all(action='SELECT')
		bpy.ops.mesh.mark_seam(clear=True)
		for edge in settings.seam_edges:
			edge.seam = True

	bpy.ops.mesh.select_all(action='DESELECT')

	#Selection Mode
	bpy.context.scene.tool_settings.mesh_select_mode = settings.selection_mode
	
	if settings.selection_mode[0]:
		bm.verts.ensure_lookup_table()
		for index in settings.selection_vert_indexies:
			if index < len(bm.verts):
				bm.verts[index].select = True
	if settings.selection_mode[1]:
		bm.edges.ensure_lookup_table()
		for index in settings.selection_edge_indexies:
			if index < len(bm.edges):
				bm.edges[index].select = True
	bm.faces.ensure_lookup_table()
	for index in settings.selection_face_indexies:
		if index < len(bm.faces):
			bm.faces[index].select = True

	#UV Face-UV Selections (Loops)
	if contextViewUV:
		bpy.ops.uv.select_all(contextViewUV, action='DESELECT')
	else:
		for face in bm.faces:
			for loop in face.loops:
				loop[uv_layers].select = False
	for uv_set in settings.selection_uv_loops:
		for loop in bm.faces[uv_set[0]].loops:
			if loop.vert.index == uv_set[1]:
				loop[uv_layers].select = True
				break

	bpy.context.view_layer.update()
	bpy.ops.object.mode_set(mode=mode)",_24065.py,51,"for uv_set in settings.selection_uv_loops:
    for loop in bm.faces[uv_set[0]].loops:
        if loop.vert.index == uv_set[1]:
            loop[uv_layers].select = True
            break","for (uv_set_0, uv_set_1, *uv_set_len) in settings.selection_uv_loops:
    for loop in bm.faces[uv_set_0].loops:
        if loop.vert.index == uv_set_1:
            loop[uv_layers].select = True
            break"
https://github.com/bit-team/backintime/tree/master/qt/app.py,"def updatePlaces(self):
        self.places.clear()
        self.addPlace(_('Global'), '', '')
        self.addPlace(_('Root'), '/', 'computer')
        self.addPlace(_('Home'), os.path.expanduser('~'), 'user-home')

        #add backup folders
        include_folders = self.config.include()
        if include_folders:
            folders = []
            for item in include_folders:
                if item[1] == 0:
                    folders.append(item[0])

            if folders:
                sortColumn = self.places.header().sortIndicatorSection()
                sortOrder  = self.places.header().sortIndicatorOrder()
                if not sortColumn:
                    folders.sort(key = lambda v: (v.upper(), v[0].islower()), reverse = sortOrder)
                self.addPlace(_('Backup folders'), '', '')
                for folder in folders:
                    self.addPlace(folder, folder, 'document-save')",_24277.py,11,"for item in include_folders:
    if item[1] == 0:
        folders.append(item[0])","for (item_0, item_1, *item_len) in include_folders:
    if item_1 == 0:
        folders.append(item_0)"
https://github.com/modin-project/modin/tree/master/scripts/doc_checker.py,"def check_docstring_indention(doc: Docstring) -> list:
    """"""
    Check indention of docstring since numpydoc reports weird results.

    Parameters
    ----------
    doc : numpydoc.validate.Docstring
        Docstring handler.

    Returns
    -------
    list
        List of tuples with Modin error code and its description.
    """"""
    from modin.utils import _get_indent

    numpy_docstring = NumpyDocString(doc.clean_doc)
    numpy_docstring._doc.reset()
    numpy_docstring._parse_summary()
    sections = list(numpy_docstring._read_sections())
    errors = []
    for section in sections:
        description = ""\n"".join(section[1])
        if _get_indent(description) != 0:
            errors.append(
                (""MD03"", MODIN_ERROR_CODES[""MD03""].format(section=section[0]))
            )
    return errors",_24334.py,22,"for section in sections:
    description = '\n'.join(section[1])
    if _get_indent(description) != 0:
        errors.append(('MD03', MODIN_ERROR_CODES['MD03'].format(section=section[0])))","for (section_0, section_1, *section_len) in sections:
    description = '\n'.join(section_1)
    if _get_indent(description) != 0:
        errors.append(('MD03', MODIN_ERROR_CODES['MD03'].format(section=section_0)))"
https://github.com/DataBiosphere/toil/tree/master/src/toil/fileStores/cachingFileStore.py,"def _readGlobalFileWithCache(self, fileStoreID, localFilePath, symlink, readerID):
        """"""
        Read a file, putting it into the cache if possible.

        :param toil.fileStores.FileID or str fileStoreID: job store id for the file
        :param str localFilePath: absolute destination path. Already known not to exist.
        :param bool symlink: Whether a symlink is acceptable.
        :param str readerID: Job ID of the job reading the file.
        :return: An absolute path to a local, temporary copy of or link to the file keyed by fileStoreID.
        :rtype: str
        """"""

        # Now we know to use the cache, and that we don't require a mutable copy.

        # Work out who we are
        me = get_process_name(self.coordination_dir)

        # Work out where to cache the file if it isn't cached already
        cachedPath = self._getNewCachingPath(fileStoreID)

        # Start a loop until we can do one of these
        while True:
            # Try and create a downloading entry if no entry exists.
            # Make sure to create a reference at the same time if it succeeds, to bill it against our job's space.
            # Don't create the mutable reference yet because we might not necessarily be able to clear that space.
            logger.debug('Trying to make file downloading file record and reference for id %s', fileStoreID)
            self._write([('INSERT OR IGNORE INTO files VALUES (?, ?, ?, ?, ?)',
                (fileStoreID, cachedPath, self.getGlobalFileSize(fileStoreID), 'downloading', me)),
                ('INSERT INTO refs SELECT ?, id, ?, ? FROM files WHERE id = ? AND state = ? AND owner = ?',
                (localFilePath, readerID, 'immutable', fileStoreID, 'downloading', me))])

            # See if we won the race
            self.cur.execute('SELECT COUNT(*) FROM files WHERE id = ? AND state = ? AND owner = ?', (fileStoreID, 'downloading', me))
            if self.cur.fetchone()[0] > 0:
                # We are responsible for downloading the file (and we have the reference)
                logger.debug('We are now responsible for downloading file %s', fileStoreID)

                # Make sure we have space for this download.
                self._freeUpSpace()

                # Do the download into the cache.
                self._downloadToCache(fileStoreID, cachedPath)

                # Try and make the link before we let the file go to cached state.
                # If we fail we may end up having to give away the file we just downloaded.
                if self._createLinkFromCache(cachedPath, localFilePath, symlink):
                    # We made the link!

                    # Change file state from downloading to cached so other people can use it
                    self._write([('UPDATE files SET state = ?, owner = NULL WHERE id = ?',
                        ('cached', fileStoreID))])

                    # Now we're done!
                    return localFilePath
                else:
                    # We could not make a link. We need to make a copy.

                    # Change the reference to copying.
                    self._write([('UPDATE refs SET state = ? WHERE path = ? AND file_id = ?', ('copying', localFilePath, fileStoreID))])

                    # Fulfill it with a full copy or by giving away the cached copy
                    self._fulfillCopyingReference(fileStoreID, cachedPath, localFilePath)

                    # Now we're done
                    return localFilePath

            else:
                logger.debug('We already have an entry in the cache database for file %s', fileStoreID)

                # A record already existed for this file.
                # Try and create an immutable reference to an entry that
                # is in 'cached' or 'uploadable' or 'uploading' state.
                # It might be uploading because *we* are supposed to be uploading it.
                logger.debug('Trying to make reference to file %s', fileStoreID)
                self._write([('INSERT INTO refs SELECT ?, id, ?, ? FROM files WHERE id = ? AND (state = ? OR state = ? OR state = ?)',
                    (localFilePath, readerID, 'immutable', fileStoreID, 'cached', 'uploadable', 'uploading'))])

                # See if we got it
                self.cur.execute('SELECT COUNT(*) FROM refs WHERE path = ? and file_id = ?', (localFilePath, fileStoreID))
                if self.cur.fetchone()[0] > 0:
                    # The file is cached and we can copy or link it
                    logger.debug('Obtained reference to file %s', fileStoreID)

                    # Get the path it is actually at in the cache, instead of where we wanted to put it
                    for row in self.cur.execute('SELECT path FROM files WHERE id = ?', (fileStoreID,)):
                        cachedPath = row[0]

                    if self._createLinkFromCache(cachedPath, localFilePath, symlink):
                        # We managed to make the link
                        return localFilePath
                    else:
                        # We can't make the link. We need a copy instead.

                        # We could change the reference to copying, see if
                        # there's space, make the copy, try and get ahold of
                        # the file if there isn't space, and give it away, but
                        # we already have code for that for mutable downloads,
                        # so just clear the reference and download mutably.

                        self._write([('DELETE FROM refs WHERE path = ? AND file_id = ?', (localFilePath, fileStoreID))])

                        return self._readGlobalFileMutablyWithCache(fileStoreID, localFilePath, readerID)
                else:
                    logger.debug('Could not obtain reference to file %s', fileStoreID)

                    # If we didn't get a download or a reference, adopt and do work from dead workers and loop again.
                    # We may have to wait for someone else's download or delete to
                    # finish. If they die, we will notice.
                    self._removeDeadJobs(self.coordination_dir, self.con)
                    self._stealWorkFromTheDead()
                    # We may have acquired ownership of partially-downloaded
                    # files, now in deleting state, that we need to delete
                    # before we can download them.
                    self._executePendingDeletions(self.coordination_dir, self.con, self.cur)

                    # Wait for other people's downloads to progress.
                    time.sleep(self.contentionBackoff)",_25609.py,85,"for row in self.cur.execute('SELECT path FROM files WHERE id = ?', (fileStoreID,)):
    cachedPath = row[0]","for (row_0, *row_len) in self.cur.execute('SELECT path FROM files WHERE id = ?', (fileStoreID,)):
    cachedPath = row_0"
https://github.com/quay/quay/tree/master/test/test_ldap.py,"def test_ldap_superuser_and_restricted_user_invalid_filter(self):
        valid_user_filter = ""(filterField=somevalue)""
        invalid_superuser_filter = ""(filterField=notsuperuser)""
        invalid_restricted_user_filter = ""(filterField=notrestricted)""

        with mock_ldap(user_filter=valid_user_filter) as ldap:
            # Verify we can login.
            (response, _) = ldap.verify_and_link_user(""someuser"", ""somepass"")
            self.assertEqual(response.username, ""someuser"")

        with mock_ldap(
            user_filter=valid_user_filter,
            superuser_filter=invalid_superuser_filter,
            restricted_user_filter=invalid_restricted_user_filter,
        ) as ldap:
            (it, err) = ldap.iterate_group_members(
                {""group_dn"": ""cn=AwesomeFolk""}, disable_pagination=True
            )
            self.assertIsNone(err)

            results = list(it)
            self.assertEqual(4, len(results))

            for u in results:
                user = u[0]

                is_superuser = ldap.is_superuser(user.username)
                is_restricted_user = ldap.is_restricted_user(user.username)
                self.assertFalse(is_superuser)
                self.assertFalse(is_restricted_user)

            self.assertFalse(ldap.has_superusers())
            self.assertFalse(ldap.has_restricted_users())",_25890.py,24,"for u in results:
    user = u[0]
    is_superuser = ldap.is_superuser(user.username)
    is_restricted_user = ldap.is_restricted_user(user.username)
    self.assertFalse(is_superuser)
    self.assertFalse(is_restricted_user)","for (u_0, *u_len) in results:
    user = u_0
    is_superuser = ldap.is_superuser(user.username)
    is_restricted_user = ldap.is_restricted_user(user.username)
    self.assertFalse(is_superuser)
    self.assertFalse(is_restricted_user)"
https://github.com/openstack/swift/tree/master/test/functional/tests.py,"def _scenario_generator(self):
        paths = ((None, None), ('c', None), ('c', 'o'))
        for path in paths:
            for method in ('PUT', 'POST', 'HEAD', 'GET', 'OPTIONS'):
                yield method, path[0], path[1]
        for path in reversed(paths):
            yield 'DELETE', path[0], path[1]",_27602.py,3,"for path in paths:
    for method in ('PUT', 'POST', 'HEAD', 'GET', 'OPTIONS'):
        yield (method, path[0], path[1])","for (path_0, path_1, *path_len) in paths:
    for method in ('PUT', 'POST', 'HEAD', 'GET', 'OPTIONS'):
        yield (method, path_0, path_1)"
https://github.com/openstack/swift/tree/master/test/functional/tests.py,"def _scenario_generator(self):
        paths = ((None, None), ('c', None), ('c', 'o'))
        for path in paths:
            for method in ('PUT', 'POST', 'HEAD', 'GET', 'OPTIONS'):
                yield method, path[0], path[1]
        for path in reversed(paths):
            yield 'DELETE', path[0], path[1]",_27602.py,6,"for path in reversed(paths):
    yield ('DELETE', path[0], path[1])","for (path_0, path_1, *path_len) in reversed(paths):
    yield ('DELETE', path_0, path_1)"
https://github.com/nicolas-chaulet/torch-points3d/tree/master/torch_points3d/datasets/registration/testeth.py,"def asl_to_pcd(folder_name):
    pattern = re.compile(""PointCloud(\d*).csv"")

    for filename in os.listdir(folder_name):
        matched_string = pattern.match(filename)
        full_filename = folder_name+""/""+filename
        if matched_string:
            points = []
            with open(full_filename) as csv_cloud:
                csv_reader = csv.reader(csv_cloud, delimiter=',')
                line = 0
                out_filename = folder_name+""/""+""PointCloud""+matched_string.group(1)+"".pcd""
                for row in csv_reader:
                    if line != 0:
                        points.append([float(row[1]),float(row[2]),float(row[3])])
                    else:
                        line=line+1
            with open(out_filename, ""w"") as out_file:
                out_file.write(""# .PCD v.7 - Point Cloud Data file format\nVERSION 0.7\nFIELDS x y z\nSIZE 4 4 4\nTYPE F F F\nCOUNT 1 1 1\nWIDTH ""+str(len(points))+""\nHEIGHT 1\nVIEWPOINT 0 0 0 1 0 0 0\nPOINTS ""+str(len(points))+""\nDATA ascii"")
                for point in points:
                    out_file.write(""\n""+str(point[0])+"" ""+str(point[1])+"" ""+str(point[2]))",_28028.py,13,"for row in csv_reader:
    if line != 0:
        points.append([float(row[1]), float(row[2]), float(row[3])])
    else:
        line = line + 1","for (row_0, row_1, row_2, row_3, *row_len) in csv_reader:
    if line != 0:
        points.append([float(row_1), float(row_2), float(row_3)])
    else:
        line = line + 1"
https://github.com/nicolas-chaulet/torch-points3d/tree/master/torch_points3d/datasets/registration/testeth.py,"def asl_to_pcd(folder_name):
    pattern = re.compile(""PointCloud(\d*).csv"")

    for filename in os.listdir(folder_name):
        matched_string = pattern.match(filename)
        full_filename = folder_name+""/""+filename
        if matched_string:
            points = []
            with open(full_filename) as csv_cloud:
                csv_reader = csv.reader(csv_cloud, delimiter=',')
                line = 0
                out_filename = folder_name+""/""+""PointCloud""+matched_string.group(1)+"".pcd""
                for row in csv_reader:
                    if line != 0:
                        points.append([float(row[1]),float(row[2]),float(row[3])])
                    else:
                        line=line+1
            with open(out_filename, ""w"") as out_file:
                out_file.write(""# .PCD v.7 - Point Cloud Data file format\nVERSION 0.7\nFIELDS x y z\nSIZE 4 4 4\nTYPE F F F\nCOUNT 1 1 1\nWIDTH ""+str(len(points))+""\nHEIGHT 1\nVIEWPOINT 0 0 0 1 0 0 0\nPOINTS ""+str(len(points))+""\nDATA ascii"")
                for point in points:
                    out_file.write(""\n""+str(point[0])+"" ""+str(point[1])+"" ""+str(point[2]))",_28028.py,20,"for point in points:
    out_file.write('\n' + str(point[0]) + ' ' + str(point[1]) + ' ' + str(point[2]))","for (point_0, point_1, point_2, *point_len) in points:
    out_file.write('\n' + str(point_0) + ' ' + str(point_1) + ' ' + str(point_2))"
https://github.com/PyThaiNLP/pythainlp/tree/master/pythainlp/transliterate/royin.py,"def _replace_vowels(word: str) -> str:
    for vowel in _VOWELS:
        word = re.sub(vowel[0], vowel[1], word)

    return word",_28076.py,2,"for vowel in _VOWELS:
    word = re.sub(vowel[0], vowel[1], word)","for (vowel_0, vowel_1, *vowel_len) in _VOWELS:
    word = re.sub(vowel_0, vowel_1, word)"
https://github.com/axcore/tartube/tree/master/tartube/config.py,"def on_add_timetable_button_clicked(self, button, liststore, combo, \
    spinbutton, spinbutton2):

        """"""Called by callback in self.setup_start_tab().

        Args:

            button (Gtk.Button): The widget clicked

            liststore (Gtk.ListStore): The treeview's model

            combo (Gtk.ComboBox): A widget to modify

            spinbutton, spinbutton2 (Gtk.SpinButton): Other widgets to modify

        """"""

        tree_iter = combo.get_active_iter()
        model = combo.get_model()
        day_str = model[tree_iter][1]

        hours = int(spinbutton.get_value())
        minutes = int(spinbutton2.get_value())

        # Each 'mini_list' is in the form [ day_string, time_string ]
        timetable_list = self.retrieve_val('timetable_list')
        mini_list = [
            day_str,
            '{:02d}'.format(hours) + ':' + '{:02d}'.format(minutes),
        ]
        # Check for duplicates
        for other_list in timetable_list:
            if other_list[0] == mini_list[0] \
            and other_list[1] == mini_list[1]:
                return

        # No duplicates found
        timetable_list.append(mini_list)
        self.edit_dict['timetable_list'] = timetable_list
        self.setup_start_tab_update_treeview(liststore)",_28814.py,32,"for other_list in timetable_list:
    if other_list[0] == mini_list[0] and other_list[1] == mini_list[1]:
        return","for (other_list_0, other_list_1, *other_list_len) in timetable_list:
    if other_list_0 == mini_list[0] and other_list_1 == mini_list[1]:
        return"
https://github.com/mushorg/conpot/tree/master/conpot/protocols/IEC104/DeviceDataController.py,"def inro_response(sorted_reg, asdu_type):
    resp_list = []
    resp = i_frame() / asdu_head(SQ=0, COT=20)
    max_frame_size = conpot_core.get_databus().get_value(""MaxFrameSize"")
    counter = 0
    asdu_infobj_type = ""asdu_infobj_"" + str(asdu_type)
    calls_dict = {
        ""asdu_infobj_1"": asdu_infobj_1,
        ""asdu_infobj_3"": asdu_infobj_3,
        ""asdu_infobj_5"": asdu_infobj_5,
        ""asdu_infobj_7"": asdu_infobj_7,
        ""asdu_infobj_9"": asdu_infobj_9,
        ""asdu_infobj_11"": asdu_infobj_11,
        ""asdu_infobj_13"": asdu_infobj_13,
    }
    call = calls_dict[asdu_infobj_type]
    for dev in sorted_reg:
        if dev[1].category_id == asdu_type:
            # 12 is length i_frame = 6 + length asdu_head = 6
            if counter >= int((max_frame_size - 12) / len(call())):
                resp_list.append(resp)
                counter = 0
                resp = i_frame() / asdu_head(SQ=0, COT=20)
            xaddr = addr_in_hex(dev[1].addr)

            add_info_obj = call(IOA=xaddr)  # SQ = 0
            val = dev[1].val
            if asdu_type == 1:
                add_info_obj.SIQ = SIQ(SPI=val)
                # Other possibility for allocation (certain value for whole field)
                # add_info_obj.SIQ = struct.pack(""B"", val)
            elif asdu_type == 3:
                add_info_obj.DIQ = DIQ(DPI=val)
            elif asdu_type == 5:
                add_info_obj.VTI = VTI(Value=val)
            elif asdu_type == 7:
                add_info_obj.BSI = val
            elif asdu_type == 9:
                add_info_obj.NVA = val
            elif asdu_type == 11:
                add_info_obj.SVA = val
            elif asdu_type == 13:
                add_info_obj.FPNumber = val
            resp /= add_info_obj
            counter += 1
            resp.NoO = counter
    if counter > 0:
        resp_list.append(resp)
    return resp_list",_28865.py,17,"for dev in sorted_reg:
    if dev[1].category_id == asdu_type:
        if counter >= int((max_frame_size - 12) / len(call())):
            resp_list.append(resp)
            counter = 0
            resp = i_frame() / asdu_head(SQ=0, COT=20)
        xaddr = addr_in_hex(dev[1].addr)
        add_info_obj = call(IOA=xaddr)
        val = dev[1].val
        if asdu_type == 1:
            add_info_obj.SIQ = SIQ(SPI=val)
        elif asdu_type == 3:
            add_info_obj.DIQ = DIQ(DPI=val)
        elif asdu_type == 5:
            add_info_obj.VTI = VTI(Value=val)
        elif asdu_type == 7:
            add_info_obj.BSI = val
        elif asdu_type == 9:
            add_info_obj.NVA = val
        elif asdu_type == 11:
            add_info_obj.SVA = val
        elif asdu_type == 13:
            add_info_obj.FPNumber = val
        resp /= add_info_obj
        counter += 1
        resp.NoO = counter","for (dev_0, dev_1, *dev_len) in sorted_reg:
    if dev_1.category_id == asdu_type:
        if counter >= int((max_frame_size - 12) / len(call())):
            resp_list.append(resp)
            counter = 0
            resp = i_frame() / asdu_head(SQ=0, COT=20)
        xaddr = addr_in_hex(dev_1.addr)
        add_info_obj = call(IOA=xaddr)
        val = dev_1.val
        if asdu_type == 1:
            add_info_obj.SIQ = SIQ(SPI=val)
        elif asdu_type == 3:
            add_info_obj.DIQ = DIQ(DPI=val)
        elif asdu_type == 5:
            add_info_obj.VTI = VTI(Value=val)
        elif asdu_type == 7:
            add_info_obj.BSI = val
        elif asdu_type == 9:
            add_info_obj.NVA = val
        elif asdu_type == 11:
            add_info_obj.SVA = val
        elif asdu_type == 13:
            add_info_obj.FPNumber = val
        resp /= add_info_obj
        counter += 1
        resp.NoO = counter"
https://github.com/slackapi/python-rtmbot/tree/master/rtmbot/core.py,"def output(self):
        for plugin in self.bot_plugins:
            limiter = False
            for output in plugin.do_output():
                destination = output[0]
                message = output[1]
                # things that start with U are users. convert to an IM channel.
                if destination.startswith('U'):
                    try:
                        result = json.loads(self.slack_client.api_call('im.open', user=destination))
                    except ValueError:
                        self._dbg(""Parse error on im.open call results!"")
                    channel = self.slack_client.server.channels.find(
                        result.get(u'channel', {}).get(u'id', None))
                elif destination.startswith('G'):
                    result = self.slack_client.api_call('groups.open', channel=destination)
                    channel = self.slack_client.server.channels.find(destination)
                else:
                    channel = self.slack_client.server.channels.find(destination)
                if channel is not None and message is not None:
                    if limiter:
                        time.sleep(.1)
                        limiter = False
                    channel.send_message(message)
                    limiter = True",_29013.py,4,"for output in plugin.do_output():
    destination = output[0]
    message = output[1]
    if destination.startswith('U'):
        try:
            result = json.loads(self.slack_client.api_call('im.open', user=destination))
        except ValueError:
            self._dbg('Parse error on im.open call results!')
        channel = self.slack_client.server.channels.find(result.get(u'channel', {}).get(u'id', None))
    elif destination.startswith('G'):
        result = self.slack_client.api_call('groups.open', channel=destination)
        channel = self.slack_client.server.channels.find(destination)
    else:
        channel = self.slack_client.server.channels.find(destination)
    if channel is not None and message is not None:
        if limiter:
            time.sleep(0.1)
            limiter = False
        channel.send_message(message)
        limiter = True","for (output_0, output_1, *output_len) in plugin.do_output():
    destination = output_0
    message = output_1
    if destination.startswith('U'):
        try:
            result = json.loads(self.slack_client.api_call('im.open', user=destination))
        except ValueError:
            self._dbg('Parse error on im.open call results!')
        channel = self.slack_client.server.channels.find(result.get(u'channel', {}).get(u'id', None))
    elif destination.startswith('G'):
        result = self.slack_client.api_call('groups.open', channel=destination)
        channel = self.slack_client.server.channels.find(destination)
    else:
        channel = self.slack_client.server.channels.find(destination)
    if channel is not None and message is not None:
        if limiter:
            time.sleep(0.1)
            limiter = False
        channel.send_message(message)
        limiter = True"
https://github.com/pimutils/khal/tree/master/khal/khalendar/backend.py,"def get_localized_calendars(self, start: dt.datetime, end: dt.datetime) -> Iterable[str]:
        assert start.tzinfo is not None
        assert end.tzinfo is not None
        start_u = utils.to_unix_time(start)
        end_u = utils.to_unix_time(end)
        sql_s = (
            'SELECT events.calendar FROM '
            'recs_loc JOIN events ON '
            'recs_loc.href = events.href AND '
            'recs_loc.calendar = events.calendar WHERE '
            '(dtstart >= ? AND dtstart <= ? OR '
            'dtend > ? AND dtend <= ? OR '
            'dtstart <= ? AND dtend >= ?) AND events.calendar in ({0}) '
            'ORDER BY dtstart')
        stuple = tuple(
            [start_u, end_u, start_u, end_u, start_u, end_u] + list(self.calendars))  # type: ignore
        result = self.sql_ex(sql_s.format(','.join([""?""] * len(self.calendars))), stuple)
        for calendar in result:
            yield calendar[0]",_29252.py,18,"for calendar in result:
    yield calendar[0]","for (calendar_0, *calendar_len) in result:
    yield calendar_0"
https://github.com/pythonarcade/arcade/tree/master/arcade/experimental/examples/shapes_perf.py,"def do_draw_line(self):
        for ln in self.single_lines_calls:
            arcade.draw_line(ln[0], ln[1], ln[2], ln[3], ln[4], 10)",_30278.py,2,"for ln in self.single_lines_calls:
    arcade.draw_line(ln[0], ln[1], ln[2], ln[3], ln[4], 10)","for (ln_0, ln_1, ln_2, ln_3, ln_4, *ln_len) in self.single_lines_calls:
    arcade.draw_line(ln_0, ln_1, ln_2, ln_3, ln_4, 10)"
https://github.com/quodlibet/quodlibet/tree/master/quodlibet/qltk/cbes.py,"def __finish(self, cbes):
        cbes_model = cbes.get_model()
        iter = cbes_model.get_iter_first()
        while cbes_model[iter][2] is None:
            cbes_model.remove(iter)
            iter = cbes_model.get_iter_first()
        for row in self.model:
            cbes_model.insert_before(iter, row=[row[0], row[1], None])
        cbes.write()",_30484.py,7,"for row in self.model:
    cbes_model.insert_before(iter, row=[row[0], row[1], None])","for (row_0, row_1, *row_len) in self.model:
    cbes_model.insert_before(iter, row=[row_0, row_1, None])"
https://github.com/chrismaddalena/ODIN/tree/master/lib/htmlreporter.py,"def create_subdomains_page(self):
        """"""Create the subdomains.html page in the report directory.""""""
        with open(self.report_path + ""subdomains.html"",""w"") as report:
            self.c.execute(""SELECT domain,subdomain,ip_address FROM subdomains ORDER BY ip_address,domain,subdomain ASC"")
            subdomains = self.c.fetchall()
            self.c.execute(""SELECT domain,subdomain,domain_frontable FROM subdomains WHERE domain_frontable <> 0"")
            frontable = self.c.fetchall()
            self.c.execute(""SELECT domain,subdomain,domain_takeover FROM subdomains WHERE domain_takeover <> 0"")
            takeovers = self.c.fetchall()
            content = """"""
            <html>
            <head><link rel=""stylesheet"" href=""styles.css""></head>
            <title>Subdomains</title>
            <body>
            <h1>Subdomains</h1>
            """"""
            if frontable:
                content += """"""
                <h2>Frontable Subdomains</h2>
                <p>This table contains domains and subdomains that may be used for domain fronting:
                <table style=""width:100%"" border=""1"">
                <tr>
                <th>Base Domain</th>
                <th>Domain</th>
                <th>CDN Information</th>
                </tr>
                """"""
                for row in frontable:
                    content += ""<tr><td>{}</td><td>{}</td><td>{}</td></tr>"".format(row[0],row[1],row[2])
                content += ""</table><p><br /></p>""
            if takeovers:
                content += """"""
                <h2>Possible Domain Takeovers</h2>
                <p>This table contains domains and subdomains that may be vulnerable to a domain takeover:
                <table style=""width:100%"" border=""1"">
                <tr>
                <th>Base Domain</th>
                <th>Domain</th>
                <th>Takeover Information</th>
                </tr>
                """"""
                for row in takeovers:
                    content += ""<tr><td>{}</td><td>{}</td><td>{}</td></tr>"".format(row[0],row[1],row[2])
                content += ""</table><p><br /></p>""
            content += """"""
            <h2>Discovered Subdomains</h2>
            <p>This table contains all of the subdomains ODIN identified and the IP address of the subdomain:
            <table style=""width:100%"" border=""1"">
            <tr>
            <th>Base Domain</th>
            <th>Subdomain</th>
            <th>IP Address</th>
            </tr>
            """"""
            for row in subdomains:
                content += ""<tr><td>{}</td><td>{}</td><td>{}</td></tr>"".format(row[0],row[1],row[2])
            content += ""</table><p><br /></p>""
            content += """"""
            </body>
            </html>
            """"""
            report.write(content)",_30752.py,55,"for row in subdomains:
    content += '<tr><td>{}</td><td>{}</td><td>{}</td></tr>'.format(row[0], row[1], row[2])","for (row_0, row_1, row_2, *row_len) in subdomains:
    content += '<tr><td>{}</td><td>{}</td><td>{}</td></tr>'.format(row_0, row_1, row_2)"
https://github.com/chrismaddalena/ODIN/tree/master/lib/htmlreporter.py,"def create_subdomains_page(self):
        """"""Create the subdomains.html page in the report directory.""""""
        with open(self.report_path + ""subdomains.html"",""w"") as report:
            self.c.execute(""SELECT domain,subdomain,ip_address FROM subdomains ORDER BY ip_address,domain,subdomain ASC"")
            subdomains = self.c.fetchall()
            self.c.execute(""SELECT domain,subdomain,domain_frontable FROM subdomains WHERE domain_frontable <> 0"")
            frontable = self.c.fetchall()
            self.c.execute(""SELECT domain,subdomain,domain_takeover FROM subdomains WHERE domain_takeover <> 0"")
            takeovers = self.c.fetchall()
            content = """"""
            <html>
            <head><link rel=""stylesheet"" href=""styles.css""></head>
            <title>Subdomains</title>
            <body>
            <h1>Subdomains</h1>
            """"""
            if frontable:
                content += """"""
                <h2>Frontable Subdomains</h2>
                <p>This table contains domains and subdomains that may be used for domain fronting:
                <table style=""width:100%"" border=""1"">
                <tr>
                <th>Base Domain</th>
                <th>Domain</th>
                <th>CDN Information</th>
                </tr>
                """"""
                for row in frontable:
                    content += ""<tr><td>{}</td><td>{}</td><td>{}</td></tr>"".format(row[0],row[1],row[2])
                content += ""</table><p><br /></p>""
            if takeovers:
                content += """"""
                <h2>Possible Domain Takeovers</h2>
                <p>This table contains domains and subdomains that may be vulnerable to a domain takeover:
                <table style=""width:100%"" border=""1"">
                <tr>
                <th>Base Domain</th>
                <th>Domain</th>
                <th>Takeover Information</th>
                </tr>
                """"""
                for row in takeovers:
                    content += ""<tr><td>{}</td><td>{}</td><td>{}</td></tr>"".format(row[0],row[1],row[2])
                content += ""</table><p><br /></p>""
            content += """"""
            <h2>Discovered Subdomains</h2>
            <p>This table contains all of the subdomains ODIN identified and the IP address of the subdomain:
            <table style=""width:100%"" border=""1"">
            <tr>
            <th>Base Domain</th>
            <th>Subdomain</th>
            <th>IP Address</th>
            </tr>
            """"""
            for row in subdomains:
                content += ""<tr><td>{}</td><td>{}</td><td>{}</td></tr>"".format(row[0],row[1],row[2])
            content += ""</table><p><br /></p>""
            content += """"""
            </body>
            </html>
            """"""
            report.write(content)",_30752.py,28,"for row in frontable:
    content += '<tr><td>{}</td><td>{}</td><td>{}</td></tr>'.format(row[0], row[1], row[2])","for (row_0, row_1, row_2, *row_len) in frontable:
    content += '<tr><td>{}</td><td>{}</td><td>{}</td></tr>'.format(row_0, row_1, row_2)"
https://github.com/chrismaddalena/ODIN/tree/master/lib/htmlreporter.py,"def create_subdomains_page(self):
        """"""Create the subdomains.html page in the report directory.""""""
        with open(self.report_path + ""subdomains.html"",""w"") as report:
            self.c.execute(""SELECT domain,subdomain,ip_address FROM subdomains ORDER BY ip_address,domain,subdomain ASC"")
            subdomains = self.c.fetchall()
            self.c.execute(""SELECT domain,subdomain,domain_frontable FROM subdomains WHERE domain_frontable <> 0"")
            frontable = self.c.fetchall()
            self.c.execute(""SELECT domain,subdomain,domain_takeover FROM subdomains WHERE domain_takeover <> 0"")
            takeovers = self.c.fetchall()
            content = """"""
            <html>
            <head><link rel=""stylesheet"" href=""styles.css""></head>
            <title>Subdomains</title>
            <body>
            <h1>Subdomains</h1>
            """"""
            if frontable:
                content += """"""
                <h2>Frontable Subdomains</h2>
                <p>This table contains domains and subdomains that may be used for domain fronting:
                <table style=""width:100%"" border=""1"">
                <tr>
                <th>Base Domain</th>
                <th>Domain</th>
                <th>CDN Information</th>
                </tr>
                """"""
                for row in frontable:
                    content += ""<tr><td>{}</td><td>{}</td><td>{}</td></tr>"".format(row[0],row[1],row[2])
                content += ""</table><p><br /></p>""
            if takeovers:
                content += """"""
                <h2>Possible Domain Takeovers</h2>
                <p>This table contains domains and subdomains that may be vulnerable to a domain takeover:
                <table style=""width:100%"" border=""1"">
                <tr>
                <th>Base Domain</th>
                <th>Domain</th>
                <th>Takeover Information</th>
                </tr>
                """"""
                for row in takeovers:
                    content += ""<tr><td>{}</td><td>{}</td><td>{}</td></tr>"".format(row[0],row[1],row[2])
                content += ""</table><p><br /></p>""
            content += """"""
            <h2>Discovered Subdomains</h2>
            <p>This table contains all of the subdomains ODIN identified and the IP address of the subdomain:
            <table style=""width:100%"" border=""1"">
            <tr>
            <th>Base Domain</th>
            <th>Subdomain</th>
            <th>IP Address</th>
            </tr>
            """"""
            for row in subdomains:
                content += ""<tr><td>{}</td><td>{}</td><td>{}</td></tr>"".format(row[0],row[1],row[2])
            content += ""</table><p><br /></p>""
            content += """"""
            </body>
            </html>
            """"""
            report.write(content)",_30752.py,42,"for row in takeovers:
    content += '<tr><td>{}</td><td>{}</td><td>{}</td></tr>'.format(row[0], row[1], row[2])","for (row_0, row_1, row_2, *row_len) in takeovers:
    content += '<tr><td>{}</td><td>{}</td><td>{}</td></tr>'.format(row_0, row_1, row_2)"
https://github.com/open-telemetry/opentelemetry-python/tree/master/propagator/opentelemetry-propagator-b3/tests/test_b3_format.py,"def test_fields(self):
        """"""Make sure the fields attribute returns the fields used in inject""""""

        propagator = self.get_propagator()
        tracer = trace.TracerProvider().get_tracer(""sdk_tracer_provider"")

        mock_setter = Mock()

        with tracer.start_as_current_span(""parent""):
            with tracer.start_as_current_span(""child""):
                propagator.inject({}, setter=mock_setter)

        inject_fields = set()

        for call in mock_setter.mock_calls:
            inject_fields.add(call[1][1])

        self.assertEqual(propagator.fields, inject_fields)",_30755.py,15,"for call in mock_setter.mock_calls:
    inject_fields.add(call[1][1])","for (call_0, (call_1_0, call_1_1, *call_1_len), *call_len) in mock_setter.mock_calls:
    inject_fields.add(call_1_1)"
https://github.com/ShiveryMoon/Imooc-Algorithm-PythonEdition/tree/master//repo.py,"def __contains__(self, vtx):
        for pair in self.heapArray:
            if pair[1] == vtx:
                return True
        return False",_31151.py,2,"for pair in self.heapArray:
    if pair[1] == vtx:
        return True","for (pair_0, pair_1, *pair_len) in self.heapArray:
    if pair_1 == vtx:
        return True"
https://github.com/rochacbruno/dynaconf/tree/master/dynaconf/vendor_src/ruamel/yaml/comments.py,"def update(self, vals):
        # type: (Any) -> None
        try:
            ordereddict.update(self, vals)
        except TypeError:
            # probably a dict that is used
            for x in vals:
                self[x] = vals[x]
        try:
            self._ok.update(vals.keys())  # type: ignore
        except AttributeError:
            # assume a list/tuple of two element lists/tuples
            for x in vals:
                self._ok.add(x[0])",_31290.py,13,"for x in vals:
    self._ok.add(x[0])","for (x_0, *x_len) in vals:
    self._ok.add(x_0)"
https://github.com/codebrainz/geany-themes/tree/master/scripts/autobump.py,"def write_log(log_file, entries):
  new_lines = []
  for ent in entries:
    new_lines.append('\t'.join((ent[1], ent[0])))
  open(log_file, 'w').write('\n'.join(new_lines) + '\n')",_31627.py,3,"for ent in entries:
    new_lines.append('\t'.join((ent[1], ent[0])))","for (ent_0, ent_1, *ent_len) in entries:
    new_lines.append('\t'.join((ent_1, ent_0)))"
https://github.com/PeizeSun/TransTrack/tree/master/models/save_track.py,"def save_track(results, out_root, video_to_images, video_names, data_split='val'):
    assert out_root is not None
    out_dir = os.path.join(out_root, data_split)
    if not os.path.exists(out_dir):
        os.mkdir(out_dir)

    # save it in standard mot format.
    track_dir = os.path.join(out_dir, ""tracks"")
    if not os.path.exists(track_dir):
        os.mkdir(track_dir)
    
    for video_id in video_to_images.keys():
        video_to_image_infos = video_to_images[video_id]
        video_name = video_names[video_id]
        file_path = os.path.join(track_dir, ""{}.txt"".format(video_name))
        f = open(file_path, ""w"")
        tracks = defaultdict(list)
        for image_info in video_to_image_infos:
            image_id, frame_id = image_info[""image_id""], image_info[""frame_id""]
            if image_id in results:
                result = results[image_id]
                for item in result:
                    if not (""tracking_id"" in item):
                        raise NotImplementedError
                    tracking_id = item[""tracking_id""]
                    bbox = item[""bbox""]
                    bbox = [bbox[0], bbox[1], bbox[2], bbox[3], item['score'], item['active']]
                    tracks[tracking_id].append([frame_id] + bbox)

        rename_track_id = 0
        for track_id in sorted(tracks):
            rename_track_id += 1
            for t in tracks[track_id]:
                if t[6] > 0:
                    f.write(""{},{},{:.2f},{:.2f},{:.2f},{:.2f},-1,-1,-1,-1\n"".format(
                        t[0], rename_track_id, t[1], t[2], t[3] - t[1], t[4] - t[2]))
        f.close()",_31637.py,33,"for t in tracks[track_id]:
    if t[6] > 0:
        f.write('{},{},{:.2f},{:.2f},{:.2f},{:.2f},-1,-1,-1,-1\n'.format(t[0], rename_track_id, t[1], t[2], t[3] - t[1], t[4] - t[2]))","for (t_0, t_1, t_2, t_3, t_4, t_5, t_6, *t_len) in tracks[track_id]:
    if t_6 > 0:
        f.write('{},{},{:.2f},{:.2f},{:.2f},{:.2f},-1,-1,-1,-1\n'.format(t_0, rename_track_id, t_1, t_2, t_3 - t_1, t_4 - t_2))"
https://github.com/turicas/rows/tree/master/tests/tests_plugin_html.py,"def test_preserve_html_and_not_skip_header(self, mocked_create_table):
        filename = ""tests/data/table-with-sections.html""

        # If `import_from_html` needs to identify field names, then it
        # should not preserve HTML inside first row
        table_1 = rows.import_from_html(filename, index=1, preserve_html=True)
        call_args = mocked_create_table.call_args_list.pop()
        data = list(call_args[0][0])
        kwargs = call_args[1]

        self.assertEqual(kwargs.get(""fields"", None), None)
        self.assertEqual(len(data), 6)
        self.assertNotIn(""<"", data[0][1])
        self.assertNotIn("">"", data[0][1])
        for row in data[1:]:
            # Second field has HTML
            self.assertIn(""<"", row[1])
            self.assertIn("">"", row[1])

        # If we provide fields and ask to preserve HTML and to don't skip
        # header then it should strip HTML from every row
        fields = OrderedDict(
            [
                (""first"", rows.fields.TextField),
                (""second"", rows.fields.TextField),
                (""third"", rows.fields.TextField),
                (""fourth"", rows.fields.TextField),
            ]
        )
        table_2 = rows.import_from_html(
            filename, index=1, fields=fields, preserve_html=True, skip_header=False
        )
        call_args = mocked_create_table.call_args_list.pop()
        data = list(call_args[0][0])
        kwargs = call_args[1]

        self.assertEqual(kwargs.get(""fields"", None), fields)
        self.assertEqual(len(data), 6)
        for row in data:
            # Second field has HTML and should not be stripped
            self.assertIn(""<"", row[1])
            self.assertIn("">"", row[1])",_32263.py,15,"for row in data[1:]:
    self.assertIn('<', row[1])
    self.assertIn('>', row[1])","for (row_0, row_1, *row_len) in data[1:]:
    self.assertIn('<', row_1)
    self.assertIn('>', row_1)"
https://github.com/turicas/rows/tree/master/tests/tests_plugin_html.py,"def test_preserve_html_and_not_skip_header(self, mocked_create_table):
        filename = ""tests/data/table-with-sections.html""

        # If `import_from_html` needs to identify field names, then it
        # should not preserve HTML inside first row
        table_1 = rows.import_from_html(filename, index=1, preserve_html=True)
        call_args = mocked_create_table.call_args_list.pop()
        data = list(call_args[0][0])
        kwargs = call_args[1]

        self.assertEqual(kwargs.get(""fields"", None), None)
        self.assertEqual(len(data), 6)
        self.assertNotIn(""<"", data[0][1])
        self.assertNotIn("">"", data[0][1])
        for row in data[1:]:
            # Second field has HTML
            self.assertIn(""<"", row[1])
            self.assertIn("">"", row[1])

        # If we provide fields and ask to preserve HTML and to don't skip
        # header then it should strip HTML from every row
        fields = OrderedDict(
            [
                (""first"", rows.fields.TextField),
                (""second"", rows.fields.TextField),
                (""third"", rows.fields.TextField),
                (""fourth"", rows.fields.TextField),
            ]
        )
        table_2 = rows.import_from_html(
            filename, index=1, fields=fields, preserve_html=True, skip_header=False
        )
        call_args = mocked_create_table.call_args_list.pop()
        data = list(call_args[0][0])
        kwargs = call_args[1]

        self.assertEqual(kwargs.get(""fields"", None), fields)
        self.assertEqual(len(data), 6)
        for row in data:
            # Second field has HTML and should not be stripped
            self.assertIn(""<"", row[1])
            self.assertIn("">"", row[1])",_32263.py,39,"for row in data:
    self.assertIn('<', row[1])
    self.assertIn('>', row[1])","for (row_0, row_1, *row_len) in data:
    self.assertIn('<', row_1)
    self.assertIn('>', row_1)"
https://github.com/sissbruecker/linkding/tree/master/bookmarks/tests/test_utils.py,"def test_humanize_absolute_date(self):
        test_cases = [
            (timezone.datetime(2021, 1, 1), timezone.datetime(2023, 1, 1), '01/01/2021'),
            (timezone.datetime(2021, 1, 1), timezone.datetime(2021, 2, 1), '01/01/2021'),
            (timezone.datetime(2021, 1, 1), timezone.datetime(2021, 1, 8), '01/01/2021'),
            (timezone.datetime(2021, 1, 1), timezone.datetime(2021, 1, 7), 'Friday'),
            (timezone.datetime(2021, 1, 1), timezone.datetime(2021, 1, 7, 23, 59), 'Friday'),
            (timezone.datetime(2021, 1, 1), timezone.datetime(2021, 1, 3), 'Friday'),
            (timezone.datetime(2021, 1, 1), timezone.datetime(2021, 1, 2), 'Yesterday'),
            (timezone.datetime(2021, 1, 1), timezone.datetime(2021, 1, 2, 23, 59), 'Yesterday'),
            (timezone.datetime(2021, 1, 1), timezone.datetime(2021, 1, 1), 'Today'),
        ]

        for test_case in test_cases:
            result = humanize_absolute_date(test_case[0], test_case[1])
            self.assertEqual(test_case[2], result)",_32384.py,14,"for test_case in test_cases:
    result = humanize_absolute_date(test_case[0], test_case[1])
    self.assertEqual(test_case[2], result)","for (test_case_0, test_case_1, test_case_2, *test_case_len) in test_cases:
    result = humanize_absolute_date(test_case_0, test_case_1)
    self.assertEqual(test_case_2, result)"
https://github.com/wujiyang/Face_Pytorch/tree/master//train_center.py,"def train(args):
    # gpu init
    multi_gpus = False
    if len(args.gpus.split(',')) > 1:
        multi_gpus = True
    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpus
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # log init
    save_dir = os.path.join(args.save_dir, args.model_pre + args.backbone.upper() + '_' + datetime.now().strftime('%Y%m%d_%H%M%S'))
    if os.path.exists(save_dir):
        raise NameError('model dir exists!')
    os.makedirs(save_dir)
    logging = init_log(save_dir)
    _print = logging.info

    # dataset loader
    transform = transforms.Compose([
        transforms.ToTensor(),  # range [0, 255] -> [0.0,1.0]
        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # range [0.0, 1.0] -> [-1.0,1.0]
    ])
    # validation dataset
    trainset = CASIAWebFace(args.train_root, args.train_file_list, transform=transform)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size,
                                              shuffle=True, num_workers=8, drop_last=False)
    # test dataset
    lfwdataset = LFW(args.lfw_test_root, args.lfw_file_list, transform=transform)
    lfwloader = torch.utils.data.DataLoader(lfwdataset, batch_size=128,
                                             shuffle=False, num_workers=4, drop_last=False)
    agedbdataset = AgeDB30(args.agedb_test_root, args.agedb_file_list, transform=transform)
    agedbloader = torch.utils.data.DataLoader(agedbdataset, batch_size=128,
                                            shuffle=False, num_workers=4, drop_last=False)
    cfpfpdataset = CFP_FP(args.cfpfp_test_root, args.cfpfp_file_list, transform=transform)
    cfpfploader = torch.utils.data.DataLoader(cfpfpdataset, batch_size=128,
                                              shuffle=False, num_workers=4, drop_last=False)

    # define backbone and margin layer
    if args.backbone == 'MobileFace':
        net = MobileFaceNet()
    elif args.backbone == 'Res50':
        net = ResNet50()
    elif args.backbone == 'Res101':
        net = ResNet101()
    elif args.backbone == 'Res50_IR':
        net = SEResNet_IR(50, feature_dim=args.feature_dim, mode='ir')
    elif args.backbone == 'SERes50_IR':
        net = SEResNet_IR(50, feature_dim=args.feature_dim, mode='se_ir')
    elif args.backbone == 'SphereNet':
        net = SphereNet(num_layers=64, feature_dim=args.feature_dim)
    else:
        print(args.backbone, ' is not available!')

    if args.margin_type == 'ArcFace':
        margin = ArcMarginProduct(args.feature_dim, trainset.class_nums, s=args.scale_size)
    elif args.margin_type == 'CosFace':
        pass
    elif args.margin_type == 'SphereFace':
        pass
    elif args.margin_type == 'InnerProduct':
        margin = InnerProduct(args.feature_dim, trainset.class_nums)
    else:
        print(args.margin_type, 'is not available!')

    if args.resume:
        print('resume the model parameters from: ', args.net_path, args.margin_path)
        net.load_state_dict(torch.load(args.net_path)['net_state_dict'])
        margin.load_state_dict(torch.load(args.margin_path)['net_state_dict'])

    # define optimizers for different layers
    criterion_classi = torch.nn.CrossEntropyLoss().to(device)
    optimizer_classi = optim.SGD([
        {'params': net.parameters(), 'weight_decay': 5e-4},
        {'params': margin.parameters(), 'weight_decay': 5e-4}
    ], lr=0.1, momentum=0.9, nesterov=True)

    #criterion_center = CenterLoss(trainset.class_nums, args.feature_dim).to(device)
    #optimizer_center = optim.SGD(criterion_center.parameters(), lr=0.5)

    scheduler_classi = lr_scheduler.MultiStepLR(optimizer_classi, milestones=[25, 50, 65], gamma=0.1)

    if multi_gpus:
        net = DataParallel(net).to(device)
        margin = DataParallel(margin).to(device)
    else:
        net = net.to(device)
        margin = margin.to(device)

    best_lfw_acc = 0.0
    best_lfw_iters = 0
    best_agedb30_acc = 0.0
    best_agedb30_iters = 0
    best_cfp_fp_acc = 0.0
    best_cfp_fp_iters = 0
    total_iters = 0
    #vis = Visualizer(env='softmax_center_xavier')
    for epoch in range(1, args.total_epoch + 1):
        scheduler_classi.step()
        # train model
        _print('Train Epoch: {}/{} ...'.format(epoch, args.total_epoch))
        net.train()

        since = time.time()
        for data in trainloader:
            img, label = data[0].to(device), data[1].to(device)
            feature = net(img)
            output = margin(feature)
            loss_classi = criterion_classi(output, label)
            #loss_center = criterion_center(feature, label)
            total_loss = loss_classi #+ loss_center * args.weight_center

            optimizer_classi.zero_grad()
            #optimizer_center.zero_grad()
            total_loss.backward()
            optimizer_classi.step()
            #optimizer_center.step()

            total_iters += 1
            # print train information
            if total_iters % 100 == 0:
                # current training accuracy
                _, predict = torch.max(output.data, 1)
                total = label.size(0)
                correct = (np.array(predict) == np.array(label.data)).sum()
                time_cur = (time.time() - since) / 100
                since = time.time()
                #vis.plot_curves({'softmax loss': loss_classi.item(), 'center loss': loss_center.item()}, iters=total_iters, title='train loss', xlabel='iters', ylabel='train loss')
                #vis.plot_curves({'train accuracy': correct / total}, iters=total_iters, title='train accuracy', xlabel='iters', ylabel='train accuracy')
                print(""Iters: {:0>6d}/[{:0>2d}], loss_classi: {:.4f}, loss_center: {:.4f}, train_accuracy: {:.4f}, time: {:.2f} s/iter, learning rate: {}"".format(total_iters,
                                                                                                                                          epoch,
                                                                                                                                          loss_classi.item(),
                                                                                                                                          loss_center.item(),
                                                                                                                                          correct/total,
                                                                                                                                          time_cur,
                                                                                                                                          scheduler_classi.get_lr()[
                                                                                                                                              0]))
            # save model
            if total_iters % args.save_freq == 0:
                msg = 'Saving checkpoint: {}'.format(total_iters)
                _print(msg)
                if multi_gpus:
                    net_state_dict = net.module.state_dict()
                    margin_state_dict = margin.module.state_dict()
                else:
                    net_state_dict = net.state_dict()
                    margin_state_dict = margin.state_dict()

                if not os.path.exists(save_dir):
                    os.mkdir(save_dir)
                torch.save({
                    'iters': total_iters,
                    'net_state_dict': net_state_dict},
                    os.path.join(save_dir, 'Iter_%06d_net.ckpt' % total_iters))
                torch.save({
                    'iters': total_iters,
                    'net_state_dict': margin_state_dict},
                    os.path.join(save_dir, 'Iter_%06d_margin.ckpt' % total_iters))
                #torch.save({
                #    'iters': total_iters,
                #    'net_state_dict': criterion_center.state_dict()},
                #    os.path.join(save_dir, 'Iter_%06d_center.ckpt' % total_iters))

            # test accuracy
            if total_iters % args.test_freq == 0:

                # test model on lfw
                net.eval()
                getFeatureFromTorch('./result/cur_lfw_result.mat', net, device, lfwdataset, lfwloader)
                lfw_accs = evaluation_10_fold('./result/cur_lfw_result.mat')
                _print('LFW Ave Accuracy: {:.4f}'.format(np.mean(lfw_accs) * 100))
                if best_lfw_acc < np.mean(lfw_accs) * 100:
                    best_lfw_acc = np.mean(lfw_accs) * 100
                    best_lfw_iters = total_iters

                # test model on AgeDB30
                getFeatureFromTorch('./result/cur_agedb30_result.mat', net, device, agedbdataset, agedbloader)
                age_accs = evaluation_10_fold('./result/cur_agedb30_result.mat')
                _print('AgeDB-30 Ave Accuracy: {:.4f}'.format(np.mean(age_accs) * 100))
                if best_agedb30_acc < np.mean(age_accs) * 100:
                    best_agedb30_acc = np.mean(age_accs) * 100
                    best_agedb30_iters = total_iters

                # test model on CFP-FP
                getFeatureFromTorch('./result/cur_cfpfp_result.mat', net, device, cfpfpdataset, cfpfploader)
                cfp_accs = evaluation_10_fold('./result/cur_cfpfp_result.mat')
                _print('CFP-FP Ave Accuracy: {:.4f}'.format(np.mean(cfp_accs) * 100))
                if best_cfp_fp_acc < np.mean(cfp_accs) * 100:
                    best_cfp_fp_acc = np.mean(cfp_accs) * 100
                    best_cfp_fp_iters = total_iters
                _print('Current Best Accuracy: LFW: {:.4f} in iters: {}, AgeDB-30: {:.4f} in iters: {} and CFP-FP: {:.4f} in iters: {}'.format(
                    best_lfw_acc, best_lfw_iters, best_agedb30_acc, best_agedb30_iters, best_cfp_fp_acc, best_cfp_fp_iters))

                #vis.plot_curves({'lfw': np.mean(lfw_accs), 'agedb-30': np.mean(age_accs), 'cfp-fp': np.mean(cfp_accs)}, iters=total_iters,
                #                title='test accuracy', xlabel='iters', ylabel='test accuracy')
                net.train()

    _print('Finally Best Accuracy: LFW: {:.4f} in iters: {}, AgeDB-30: {:.4f} in iters: {} and CFP-FP: {:.4f} in iters: {}'.format(
        best_lfw_acc, best_lfw_iters, best_agedb30_acc, best_agedb30_iters, best_cfp_fp_acc, best_cfp_fp_iters))
    print('finishing training')",_32634.py,103,"for data in trainloader:
    (img, label) = (data[0].to(device), data[1].to(device))
    feature = net(img)
    output = margin(feature)
    loss_classi = criterion_classi(output, label)
    total_loss = loss_classi
    optimizer_classi.zero_grad()
    total_loss.backward()
    optimizer_classi.step()
    total_iters += 1
    if total_iters % 100 == 0:
        (_, predict) = torch.max(output.data, 1)
        total = label.size(0)
        correct = (np.array(predict) == np.array(label.data)).sum()
        time_cur = (time.time() - since) / 100
        since = time.time()
        print('Iters: {:0>6d}/[{:0>2d}], loss_classi: {:.4f}, loss_center: {:.4f}, train_accuracy: {:.4f}, time: {:.2f} s/iter, learning rate: {}'.format(total_iters, epoch, loss_classi.item(), loss_center.item(), correct / total, time_cur, scheduler_classi.get_lr()[0]))
    if total_iters % args.save_freq == 0:
        msg = 'Saving checkpoint: {}'.format(total_iters)
        _print(msg)
        if multi_gpus:
            net_state_dict = net.module.state_dict()
            margin_state_dict = margin.module.state_dict()
        else:
            net_state_dict = net.state_dict()
            margin_state_dict = margin.state_dict()
        if not os.path.exists(save_dir):
            os.mkdir(save_dir)
        torch.save({'iters': total_iters, 'net_state_dict': net_state_dict}, os.path.join(save_dir, 'Iter_%06d_net.ckpt' % total_iters))
        torch.save({'iters': total_iters, 'net_state_dict': margin_state_dict}, os.path.join(save_dir, 'Iter_%06d_margin.ckpt' % total_iters))
    if total_iters % args.test_freq == 0:
        net.eval()
        getFeatureFromTorch('./result/cur_lfw_result.mat', net, device, lfwdataset, lfwloader)
        lfw_accs = evaluation_10_fold('./result/cur_lfw_result.mat')
        _print('LFW Ave Accuracy: {:.4f}'.format(np.mean(lfw_accs) * 100))
        if best_lfw_acc < np.mean(lfw_accs) * 100:
            best_lfw_acc = np.mean(lfw_accs) * 100
            best_lfw_iters = total_iters
        getFeatureFromTorch('./result/cur_agedb30_result.mat', net, device, agedbdataset, agedbloader)
        age_accs = evaluation_10_fold('./result/cur_agedb30_result.mat')
        _print('AgeDB-30 Ave Accuracy: {:.4f}'.format(np.mean(age_accs) * 100))
        if best_agedb30_acc < np.mean(age_accs) * 100:
            best_agedb30_acc = np.mean(age_accs) * 100
            best_agedb30_iters = total_iters
        getFeatureFromTorch('./result/cur_cfpfp_result.mat', net, device, cfpfpdataset, cfpfploader)
        cfp_accs = evaluation_10_fold('./result/cur_cfpfp_result.mat')
        _print('CFP-FP Ave Accuracy: {:.4f}'.format(np.mean(cfp_accs) * 100))
        if best_cfp_fp_acc < np.mean(cfp_accs) * 100:
            best_cfp_fp_acc = np.mean(cfp_accs) * 100
            best_cfp_fp_iters = total_iters
        _print('Current Best Accuracy: LFW: {:.4f} in iters: {}, AgeDB-30: {:.4f} in iters: {} and CFP-FP: {:.4f} in iters: {}'.format(best_lfw_acc, best_lfw_iters, best_agedb30_acc, best_agedb30_iters, best_cfp_fp_acc, best_cfp_fp_iters))
        net.train()","for (data_0, data_1, *data_len) in trainloader:
    (img, label) = (data_0.to(device), data_1.to(device))
    feature = net(img)
    output = margin(feature)
    loss_classi = criterion_classi(output, label)
    total_loss = loss_classi
    optimizer_classi.zero_grad()
    total_loss.backward()
    optimizer_classi.step()
    total_iters += 1
    if total_iters % 100 == 0:
        (_, predict) = torch.max(output.data, 1)
        total = label.size(0)
        correct = (np.array(predict) == np.array(label.data)).sum()
        time_cur = (time.time() - since) / 100
        since = time.time()
        print('Iters: {:0>6d}/[{:0>2d}], loss_classi: {:.4f}, loss_center: {:.4f}, train_accuracy: {:.4f}, time: {:.2f} s/iter, learning rate: {}'.format(total_iters, epoch, loss_classi.item(), loss_center.item(), correct / total, time_cur, scheduler_classi.get_lr()[0]))
    if total_iters % args.save_freq == 0:
        msg = 'Saving checkpoint: {}'.format(total_iters)
        _print(msg)
        if multi_gpus:
            net_state_dict = net.module.state_dict()
            margin_state_dict = margin.module.state_dict()
        else:
            net_state_dict = net.state_dict()
            margin_state_dict = margin.state_dict()
        if not os.path.exists(save_dir):
            os.mkdir(save_dir)
        torch.save({'iters': total_iters, 'net_state_dict': net_state_dict}, os.path.join(save_dir, 'Iter_%06d_net.ckpt' % total_iters))
        torch.save({'iters': total_iters, 'net_state_dict': margin_state_dict}, os.path.join(save_dir, 'Iter_%06d_margin.ckpt' % total_iters))
    if total_iters % args.test_freq == 0:
        net.eval()
        getFeatureFromTorch('./result/cur_lfw_result.mat', net, device, lfwdataset, lfwloader)
        lfw_accs = evaluation_10_fold('./result/cur_lfw_result.mat')
        _print('LFW Ave Accuracy: {:.4f}'.format(np.mean(lfw_accs) * 100))
        if best_lfw_acc < np.mean(lfw_accs) * 100:
            best_lfw_acc = np.mean(lfw_accs) * 100
            best_lfw_iters = total_iters
        getFeatureFromTorch('./result/cur_agedb30_result.mat', net, device, agedbdataset, agedbloader)
        age_accs = evaluation_10_fold('./result/cur_agedb30_result.mat')
        _print('AgeDB-30 Ave Accuracy: {:.4f}'.format(np.mean(age_accs) * 100))
        if best_agedb30_acc < np.mean(age_accs) * 100:
            best_agedb30_acc = np.mean(age_accs) * 100
            best_agedb30_iters = total_iters
        getFeatureFromTorch('./result/cur_cfpfp_result.mat', net, device, cfpfpdataset, cfpfploader)
        cfp_accs = evaluation_10_fold('./result/cur_cfpfp_result.mat')
        _print('CFP-FP Ave Accuracy: {:.4f}'.format(np.mean(cfp_accs) * 100))
        if best_cfp_fp_acc < np.mean(cfp_accs) * 100:
            best_cfp_fp_acc = np.mean(cfp_accs) * 100
            best_cfp_fp_iters = total_iters
        _print('Current Best Accuracy: LFW: {:.4f} in iters: {}, AgeDB-30: {:.4f} in iters: {} and CFP-FP: {:.4f} in iters: {}'.format(best_lfw_acc, best_lfw_iters, best_agedb30_acc, best_agedb30_iters, best_cfp_fp_acc, best_cfp_fp_iters))
        net.train()"