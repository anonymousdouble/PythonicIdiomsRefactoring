file_html,method_content,file_name,lineno,old_code,new_code,,,
https://github.com/frappe/frappe/tree/master/frappe/desk/notifications.py,"def get_notifications_for_targets(config, notification_percent):
	""""""Notifications for doc targets""""""
	can_read = frappe.get_user().get_can_read()
	doc_target_percents = {}

	# doc_target_percents = {
	# 	""Company"": {
	# 		""Acme"": 87,
	# 		""RobotsRUs"": 50,
	# 	}, {}...
	# }

	for doctype in config.targets:
		if doctype in can_read:
			if doctype in notification_percent:
				doc_target_percents[doctype] = notification_percent[doctype]
			else:
				doc_target_percents[doctype] = {}
				d = config.targets[doctype]
				condition = d[""filters""]
				target_field = d[""target_field""]
				value_field = d[""value_field""]
				try:
					if isinstance(condition, dict):
						doc_list = frappe.get_list(
							doctype,
							fields=[""name"", target_field, value_field],
							filters=condition,
							limit_page_length=100,
							ignore_ifnull=True,
						)

				except frappe.PermissionError:
					frappe.clear_messages()
					pass
				except Exception as e:
					if e.args[0] not in (1412, 1684):
						raise

				else:
					for doc in doc_list:
						value = doc[value_field]
						target = doc[target_field]
						doc_target_percents[doctype][doc.name] = (value / target * 100) if value < target else 100

	return doc_target_percents",_335.py,41,"for doc in doc_list:
    value = doc[value_field]
    target = doc[target_field]
    doc_target_percents[doctype][doc.name] = value / target * 100 if value < target else 100",doc_target_percents[doctype] = {doc.name: doc[value_field] / doc[target_field] * 100 if doc[value_field] < doc[target_field] else 100 for doc in doc_list},nan,,
https://github.com/anchore/anchore-engine/tree/master/anchore_engine/services/apiext/api/controllers/images.py,"def make_response_image(image_record, include_detail=True):
    ret = image_record

    image_content = {""metadata"": {}}
    for key in [
        ""arch"",
        ""distro"",
        ""distro_version"",
        ""dockerfile_mode"",
        ""image_size"",
        ""layer_count"",
    ]:
        val = image_record.pop(key, None)
        image_content[""metadata""][key] = val
    image_record[""image_content""] = image_content

    if image_record[""annotations""]:
        try:
            annotation_data = json.loads(image_record[""annotations""])
            image_record[""annotations""] = annotation_data
        except:
            pass

    # try to assemble full strings
    if image_record and ""image_detail"" in image_record:
        for image_detail in image_record[""image_detail""]:
            try:
                image_detail[""fulldigest""] = (
                    image_detail[""registry""]
                    + ""/""
                    + image_detail[""repo""]
                    + ""@""
                    + image_detail[""digest""]
                )
                image_detail[""fulltag""] = (
                    image_detail[""registry""]
                    + ""/""
                    + image_detail[""repo""]
                    + "":""
                    + image_detail[""tag""]
                )
            except:
                image_detail[""fulldigest""] = None
                image_detail[""fulltag""] = None

            for removekey in [""record_state_val"", ""record_state_key""]:
                image_detail.pop(removekey, None)

            for datekey in [""last_updated"", ""created_at"", ""tag_detected_at""]:
                try:
                    image_detail[datekey] = (
                        datetime.datetime.utcfromtimestamp(
                            image_detail[datekey]
                        ).isoformat()
                        + ""Z""
                    )
                except:
                    pass

    if not include_detail:
        image_record[""image_detail""] = []

    for datekey in [""last_updated"", ""created_at"", ""analyzed_at""]:
        try:
            image_record[datekey] = (
                datetime.datetime.utcfromtimestamp(image_record[datekey]).isoformat()
                + ""Z""
            )
        except:
            pass

    for removekey in [""record_state_val"", ""record_state_key""]:
        image_record.pop(removekey, None)

    return ret",_941.py,5,"for key in ['arch', 'distro', 'distro_version', 'dockerfile_mode', 'image_size', 'layer_count']:
    val = image_record.pop(key, None)
    image_content['metadata'][key] = val","image_content[‘metadata'] = {key: image_record.pop(key, None)}",nan,,
https://github.com/MilesCranmer/PySR/tree/master/pysr/sr.py,"def _validate_and_set_init_params(self):
        """"""
        Ensure parameters passed at initialization are valid.

        Also returns a dictionary of parameters to update from their
        values given at initialization.

        Returns
        -------
        packed_modified_params : dict
            Dictionary of parameters to modify from their initialized
            values. For example, default parameters are set here
            when a parameter is left set to `None`.
        """"""
        # Immutable parameter validation
        # Ensure instance parameters are allowable values:
        if self.tournament_selection_n > self.population_size:
            raise ValueError(
                ""tournament_selection_n parameter must be smaller than population_size.""
            )

        if self.maxsize > 40:
            warnings.warn(
                ""Note: Using a large maxsize for the equation search will be ""
                ""exponentially slower and use significant memory. You should consider ""
                ""turning `use_frequency` to False, and perhaps use `warmup_maxsize_by`.""
            )
        elif self.maxsize < 7:
            raise ValueError(""PySR requires a maxsize of at least 7"")

        if self.deterministic and not (
            self.multithreading in [False, None]
            and self.procs == 0
            and self.random_state is not None
        ):
            raise ValueError(
                ""To ensure deterministic searches, you must set `random_state` to a seed, ""
                ""`procs` to `0`, and `multithreading` to `False` or `None`.""
            )

        if self.random_state is not None and (
            not self.deterministic or self.procs != 0
        ):
            warnings.warn(
                ""Note: Setting `random_state` without also setting `deterministic` ""
                ""to True and `procs` to 0 will result in non-deterministic searches. ""
            )

        # NotImplementedError - Values that could be supported at a later time
        if self.optimizer_algorithm not in VALID_OPTIMIZER_ALGORITHMS:
            raise NotImplementedError(
                f""PySR currently only supports the following optimizer algorithms: {VALID_OPTIMIZER_ALGORITHMS}""
            )

        # 'Mutable' parameter validation
        buffer_available = ""buffer"" in sys.stdout.__dir__()
        # Params and their default values, if None is given:
        default_param_mapping = {
            ""binary_operators"": ""+ * - /"".split("" ""),
            ""unary_operators"": [],
            ""maxdepth"": self.maxsize,
            ""constraints"": {},
            ""multithreading"": self.procs != 0 and self.cluster_manager is None,
            ""batch_size"": 1,
            ""update_verbosity"": self.verbosity,
            ""progress"": buffer_available,
        }
        packed_modified_params = {}
        for parameter, default_value in default_param_mapping.items():
            parameter_value = getattr(self, parameter)
            if parameter_value is None:
                parameter_value = default_value
            else:
                # Special cases such as when binary_operators is a string
                if parameter in [""binary_operators"", ""unary_operators""] and isinstance(
                    parameter_value, str
                ):
                    parameter_value = [parameter_value]
                elif parameter == ""batch_size"" and parameter_value < 1:
                    warnings.warn(
                        ""Given `batch_size` must be greater than or equal to one. ""
                        ""`batch_size` has been increased to equal one.""
                    )
                    parameter_value = 1
                elif parameter == ""progress"" and not buffer_available:
                    warnings.warn(
                        ""Note: it looks like you are running in Jupyter. ""
                        ""The progress bar will be turned off.""
                    )
                    parameter_value = False
            packed_modified_params[parameter] = parameter_value

        assert (
            len(packed_modified_params[""binary_operators""])
            + len(packed_modified_params[""unary_operators""])
            > 0
        )

        julia_kwargs = {}
        if self.julia_kwargs is not None:
            for key, value in self.julia_kwargs.items():
                julia_kwargs[key] = value
        if ""optimize"" not in julia_kwargs:
            julia_kwargs[""optimize""] = 3
        if ""threads"" not in julia_kwargs and packed_modified_params[""multithreading""]:
            julia_kwargs[""threads""] = self.procs
        packed_modified_params[""julia_kwargs""] = julia_kwargs

        return packed_modified_params",_1496.py,101,"for (key, value) in self.julia_kwargs.items():
    julia_kwargs[key] = value","julia_kwargs = {key: value for (key, value) in self.julia_kwargs.items()}",nan,,
https://github.com/open-mmlab/mmgeneration/tree/master/mmgen/datasets/pipelines/augmentation.py,"def __call__(self, results):
        """"""Call function.

        Args:
            results (dict): A dict containing the necessary information and
                data for augmentation.

        Returns:
            dict: A dict containing the processed data and information.
        """"""

        for key in self.keys:
            img = results[key]
            img_height, img_width = img.shape[:2]
            crop_size = min(img_height, img_width)
            y1 = 0 if img_height == crop_size else \
                np.random.randint(0, img_height - crop_size)
            x1 = 0 if img_width == crop_size else \
                np.random.randint(0, img_width - crop_size)
            y2, x2 = y1 + crop_size - 1, x1 + crop_size - 1

            img = mmcv.imcrop(img, bboxes=np.array([x1, y1, x2, y2]))
            results[key] = img

        return results",_2055.py,12,"for key in self.keys:
    img = results[key]
    (img_height, img_width) = img.shape[:2]
    crop_size = min(img_height, img_width)
    y1 = 0 if img_height == crop_size else np.random.randint(0, img_height - crop_size)
    x1 = 0 if img_width == crop_size else np.random.randint(0, img_width - crop_size)
    (y2, x2) = (y1 + crop_size - 1, x1 + crop_size - 1)
    img = mmcv.imcrop(img, bboxes=np.array([x1, y1, x2, y2]))
    results[key] = img","results = {key: mmcv.imcrop(results[key], bboxes=np.array([0 if results[key].shape[:2][1] == min(results[key].shape[:2][0], results[key].shape[:2][1]) else np.random.randint(0, results[key].shape[:2][1] - min(results[key].shape[:2][0], results[key].shape[:2][1])), 0 if results[key].shape[:2][0] == min(results[key].shape[:2][0], results[key].shape[:2][1]) else np.random.randint(0, results[key].shape[:2][0] - min(results[key].shape[:2][0], results[key].shape[:2][1])), y1 + min(results[key].shape[:2][0], results[key].shape[:2][1]) - 1, x1 + crop_size - 1]))  for key in self.keys}",nan,,
https://github.com/esdalmaijer/PyGaze/tree/master/pygaze/_eyetracker/libtobiilegacy.py,"def doCalibration(self,calibrationPoints):
        
        """"""Performs a calibration; displaying points and the calibration
        menu and keyboard input are handled by PyGaze routines, calibration
        is handled by Tobii routines
        
        arguments
        calibrationPoints    --    a list of (x,y) typles, specifying the
                        coordinates for the calibration points
                        (coordinates should be in PyGaze notation,
                        where (0,0) is the topleft and coordinates
                        are specified in pixels, e.g. (1024,768))
        
        keyword arguments
        None
        
        returns
        None or retval    --    returns None if no tracker is connected
                        returns retval when a tracker is connected;
                        retval can be one of three string values:
                            'accept'
                            'retry'
                            'abort'
        """"""
        
        # immediately return when no eyetracker is connected
        if self.eyetracker is None:
            return
        
        # set some properties
        self.points = calibrationPoints
        self.point_index = -1
        
        # visuals
        img = Image.new('RGB',self.disp.dispsize)
        draw = ImageDraw.Draw(img)
        
        self.calin = {'colour':(0,0,0), 'pos':(int(self.disp.dispsize[0]/2),int(self.disp.dispsize[1]/2)), 'r':2}
        self.calout = {'colour':(128,255,128), 'pos':(int(self.disp.dispsize[0]/2),int(self.disp.dispsize[1]/2)), 'r':64}
        self.calresult = {'img':img}
        self.calresultmsg = {'text':"""",'pos':(int(self.disp.dispsize[0]/2),int(self.disp.dispsize[1]/4))}
        
        # start calibration
        self.initcalibration_completed = False
        print(""StartCalibration"")
        self.eyetracker.StartCalibration(lambda error, r: self.on_calib_start(error, r))
        while not self.initcalibration_completed:
            pass
        
        # draw central target
        self.screen.clear()
        self.screen.draw_circle(colour=self.calout['colour'], pos=self.calout['pos'], r=self.calout['r'], fill=False)
        self.screen.draw_circle(colour=self.calin['colour'], pos=self.calin['pos'], r=self.calin['r'], fill=True)
        self.disp.fill(self.screen)
        self.disp.show()
        # wait for start command
        self.kb.get_key(keylist=['space'],timeout=None)
        
        # run through all points
        for self.point_index in range(len(self.points)):
            # create tobii.eye_tracking_io.types 2D point
            px, py = self.points[self.point_index]
            p = Point2D()
            p.x, p.y = float(px)/self.disp.dispsize[0], float(py)/self.disp.dispsize[1]
            # recalculate to psycho coordinates
            self.calin['pos'] = (int(px),int(py))
            self.calout['pos'] = (int(px),int(py))

            # show target while decreasing its size for 1.5 seconds
            t0 = clock.get_time()
            currentTime = (clock.get_time() - t0) / 1000.0
            while currentTime < 1.5:
                # reduce size of the outer ring, as time passes
                self.calout['r'] = int(40*(1.5-(currentTime))+4)
                # check for input (should this even be here?)
                self.kb.get_key(keylist=None, timeout=1)
                # draw calibration point
                self.screen.clear()
                self.screen.draw_circle(colour=self.calout['colour'], pos=self.calout['pos'], r=self.calout['r'], fill=False)
                self.screen.draw_circle(colour=self.calin['colour'], pos=self.calin['pos'], r=self.calin['r'], fill=True)
                self.disp.fill(self.screen)
                self.disp.show()
                # get time
                currentTime = (clock.get_time() - t0) / 1000.0
            
            # wait for point calibration to succeed
            self.add_point_completed = False
            self.eyetracker.AddCalibrationPoint(p, lambda error, r: self.on_add_completed(error, r))
            while not self.add_point_completed:
                # TODO: why would you continuously show the same stuff and poll the keyboard without using the input?
#                psychopy.event.getKeys()
#                self.calout.draw()
#                self.calin.draw()
#                win.flip()
                pass
         
        # wait for calibration to be complete
        self.computeCalibration_completed = False
        self.computeCalibration_succeeded = False
        self.eyetracker.ComputeCalibration(lambda error, r: self.on_calib_compute(error, r))
        while not self.computeCalibration_completed:
            pass
        self.eyetracker.StopCalibration(None)

        # reset display (same seems to be done below: what's the use?)
        self.disp.show()
        
        # get calibration info
        self.getcalibration_completed = False
        self.calib = self.eyetracker.GetCalibration(lambda error, calib: self.on_calib_response(error, calib))
        while not self.getcalibration_completed:
            pass
        
        # fill screen with half-gray
        self.screen.clear(colour=(128,128,128))
        
        # show calibration info
        if not self.computeCalibration_succeeded:
            # computeCalibration failed.
            self.calresultmsg['text'] = 'Not enough data was collected (Retry:r/Abort:ESC)'
            
        elif self.calib == None:
            # no calibration data
            self.calresultmsg['text'] = 'No calibration data (Retry:r/Abort:ESC)'
            
        else:
            # show the calibration accuracy
            points = {}
            for data in self.calib.plot_data:
                points[data.true_point] = {'left':data.left, 'right':data.right}
            
            if len(points) == 0:
                self.calresultmsg['text'] = 'No ture calibration data (Retry:r/Abort:ESC)'
            
            else:
                for p,d in points.iteritems():
                    if d['left'].status == 1:
                        self.screen.draw_line(colour=(255,0,0), spos=(p.x*self.disp.dispsize[0],p.y*self.disp.dispsize[1]), epos=(d['left'].map_point.x*self.disp.dispsize[0],d['left'].map_point.y*self.disp.dispsize[1]), pw=3)
                    if d['right'].status == 1:
                        self.screen.draw_line(colour=(0,255,0), spos=(p.x*self.disp.dispsize[0],p.y*self.disp.dispsize[1]), epos=(d['right'].map_point.x*self.disp.dispsize[0],d['right'].map_point.y*self.disp.dispsize[1]), pw=3)
                    self.screen.draw_ellipse(colour=(0,0,0), x=p.x*self.disp.dispsize[0]-10, y=p.y*self.disp.dispsize[1]-10, w=20, h=20, pw=3, fill=False)

                self.calresultmsg['text'] = 'Accept calibration results (Accept:a/Retry:r/Abort:ESC)'
        
        # original approach (Sogo): draw an image, then show that image via PscyhoPy
        self.calresult['img'] = img
        
        # alternative approach (Dalmaijer): use PyGaze drawing operations on self.screen, then present self.screen
        self.screen.draw_text(text=self.calresultmsg['text'],pos=self.calresultmsg['pos'])
        self.disp.fill(self.screen)
        self.disp.show()
        
        # wait for keyboard input
        key, presstime = self.kb.get_key(keylist=['a','r','escape'], timeout=None)
        if key == 'a':
            retval = 'accept'
        elif key == 'r':
            retval = 'retry'
        elif key == 'escape':
            retval = 'abort'
        
        return retval",_2728.py,129,"for data in self.calib.plot_data:
    points[data.true_point] = {'left': data.left, 'right': data.right}","points = {data.true_point: {'left': data.left, 'right': data.right}  for data in self.calib.plot_data}",nan,,
https://github.com/Axelrod-Python/Axelrod/tree/master/axelrod/classifier.py,"def rebuild_classifier_table(
    classifiers: List[Classifier],
    players: List[Type[Player]],
    path: Text = ALL_CLASSIFIERS_PATH,
) -> None:
    """"""Builds the classifier table in data.

    Parameters
    ----------
    classifiers: A list of classifiers to calculate on the strategies
    players: A list of strategies (classes, not instances) to compute the
        classifiers for.
    path: Where to save the resulting yaml file.
    """"""
    # Get absolute path
    dirname = os.path.dirname(__file__)
    filename = os.path.join(dirname, path)

    all_player_dicts = dict()
    for p in players:
        new_player_dict = dict()
        for c in classifiers:
            new_player_dict[c.name] = c.classify_player(p)
        all_player_dicts[p.name] = new_player_dict

    with open(filename, ""w"") as f:
        yaml.dump(all_player_dicts, f)",_2748.py,20,"for p in players:
    new_player_dict = dict()
    for c in classifiers:
        new_player_dict[c.name] = c.classify_player(p)
    all_player_dicts[p.name] = new_player_dict",all_player_dicts = { p.name: {c.name: c.classify_player(p)  for c in classifiers} for p in players},nan,,
https://github.com/fizyr/keras-retinanet/tree/master/keras_retinanet/preprocessing/kitti.py,"def __init__(
        self,
        base_dir,
        subset='train',
        **kwargs
    ):
        """""" Initialize a KITTI data generator.

        Args
            base_dir: Directory w.r.t. where the files are to be searched (defaults to the directory containing the csv_data_file).
            subset: The subset to generate data for (defaults to 'train').
        """"""
        self.base_dir = base_dir

        label_dir = os.path.join(self.base_dir, subset, 'labels')
        image_dir = os.path.join(self.base_dir, subset, 'images')

        """"""
        1    type         Describes the type of object: 'Car', 'Van', 'Truck',
                             'Pedestrian', 'Person_sitting', 'Cyclist', 'Tram',
                             'Misc' or 'DontCare'
        1    truncated    Float from 0 (non-truncated) to 1 (truncated), where
                         truncated refers to the object leaving image boundaries
        1    occluded     Integer (0,1,2,3) indicating occlusion state:
                         0 = fully visible, 1 = partly occluded
                         2 = largely occluded, 3 = unknown
        1    alpha        Observation angle of object, ranging [-pi..pi]
        4    bbox         2D bounding box of object in the image (0-based index):
                         contains left, top, right, bottom pixel coordinates
        3    dimensions   3D object dimensions: height, width, length (in meters)
        3    location     3D object location x,y,z in camera coordinates (in meters)
        1    rotation_y   Rotation ry around Y-axis in camera coordinates [-pi..pi]
        """"""

        self.labels = {}
        self.classes = kitti_classes
        for name, label in self.classes.items():
            self.labels[label] = name

        self.image_data = dict()
        self.images = []
        for i, fn in enumerate(os.listdir(label_dir)):
            label_fp = os.path.join(label_dir, fn)
            image_fp = os.path.join(image_dir, fn.replace('.txt', '.png'))

            self.images.append(image_fp)

            fieldnames = ['type', 'truncated', 'occluded', 'alpha', 'left', 'top', 'right', 'bottom', 'dh', 'dw', 'dl',
                          'lx', 'ly', 'lz', 'ry']
            with open(label_fp, 'r') as csv_file:
                reader = csv.DictReader(csv_file, delimiter=' ', fieldnames=fieldnames)
                boxes = []
                for line, row in enumerate(reader):
                    label = row['type']
                    cls_id = kitti_classes[label]

                    annotation = {'cls_id': cls_id, 'x1': row['left'], 'x2': row['right'], 'y2': row['bottom'], 'y1': row['top']}
                    boxes.append(annotation)

                self.image_data[i] = boxes

        super(KittiGenerator, self).__init__(**kwargs)",_2791.py,37,"for (name, label) in self.classes.items():
    self.labels[label] = name","self.labels = {label: name  for (name, label) in self.classes.items()}",nan,,
https://github.com/PaddlePaddle/PGL/tree/master/legacy/pgl/graph_wrapper.py,"def __create_graph_attr(self, graph):
        """"""Create graph attributes for paddlepaddle.
        """"""
        src, dst, eid = graph.sorted_edges(sort_by=""dst"")
        indegree = graph.indegree()
        nodes = graph.nodes
        uniq_dst = nodes[indegree > 0]
        uniq_dst_count = indegree[indegree > 0]
        uniq_dst_count = np.cumsum(uniq_dst_count, dtype='int32')
        uniq_dst_count = np.insert(uniq_dst_count, 0, 0)
        graph_lod = graph.graph_lod
        num_graph = graph.num_graph

        num_edges = len(src)
        if num_edges == 0:
            # Fake Graph
            src = np.array([0], dtype=""int64"")
            dst = np.array([0], dtype=""int64"")
            eid = np.array([0], dtype=""int64"")
            uniq_dst_count = np.array([0, 1], dtype=""int32"")
            uniq_dst = np.array([0], dtype=""int64"")

        edge_feat = {}

        for key, value in graph.edge_feat.items():
            edge_feat[key] = value[eid]
        node_feat = graph.node_feat

        self.__create_graph_node_feat(node_feat, self._initializers)
        self.__create_graph_edge_feat(edge_feat, self._initializers)

        self._num_edges, init = paddle_helper.constant(
            dtype=""int64"",
            value=np.array(
                [num_edges], dtype=""int64""),
            name=self._data_name_prefix + '/num_edges')
        self._initializers.append(init)

        self._num_graph, init = paddle_helper.constant(
            dtype=""int64"",
            value=np.array(
                [num_graph], dtype=""int64""),
            name=self._data_name_prefix + '/num_graph')
        self._initializers.append(init)

        self._edges_src, init = paddle_helper.constant(
            dtype=""int64"",
            value=src,
            name=self._data_name_prefix + '/edges_src')
        self._initializers.append(init)

        self._edges_dst, init = paddle_helper.constant(
            dtype=""int64"",
            value=dst,
            name=self._data_name_prefix + '/edges_dst')
        self._initializers.append(init)

        self._num_nodes, init = paddle_helper.constant(
            dtype=""int64"",
            hide_batch_size=False,
            value=np.array([graph.num_nodes]),
            name=self._data_name_prefix + '/num_nodes')
        self._initializers.append(init)

        self._edge_uniq_dst, init = paddle_helper.constant(
            name=self._data_name_prefix + ""/uniq_dst"",
            dtype=""int64"",
            value=uniq_dst)
        self._initializers.append(init)

        self._edge_uniq_dst_count, init = paddle_helper.constant(
            name=self._data_name_prefix + ""/uniq_dst_count"",
            dtype=""int32"",
            value=uniq_dst_count)
        self._initializers.append(init)

        self._graph_lod, init = paddle_helper.constant(
            name=self._data_name_prefix + ""/graph_lod"",
            dtype=""int32"",
            value=graph_lod)
        self._initializers.append(init)

        self._indegree, init = paddle_helper.constant(
            name=self._data_name_prefix + ""/indegree"",
            dtype=""int64"",
            value=indegree)
        self._initializers.append(init)",_3821.py,25,"for (key, value) in graph.edge_feat.items():
    edge_feat[key] = value[eid]","edge_feat = {key: value[eid] for (key, value) in graph.edge_feat.items()}",nan,,
https://github.com/microsoft/nni/tree/master/nni/algorithms/hpo/networkmorphism_tuner/graph.py,"def get_main_chain(self):
        """"""Returns the main chain node ID list.""""""
        pre_node = {}
        distance = {}
        for i in range(self.n_nodes):
            distance[i] = 0
            pre_node[i] = i
        for i in range(self.n_nodes - 1):
            for u in range(self.n_nodes):
                for v, _ in self.adj_list[u]:
                    if distance[u] + 1 > distance[v]:
                        distance[v] = distance[u] + 1
                        pre_node[v] = u
        temp_id = 0
        for i in range(self.n_nodes):
            if distance[i] > distance[temp_id]:
                temp_id = i
        ret = []
        for i in range(self.n_nodes + 5):
            ret.append(temp_id)
            if pre_node[temp_id] == temp_id:
                break
            temp_id = pre_node[temp_id]
        assert temp_id == pre_node[temp_id]
        ret.reverse()
        return ret",_3873.py,5,"for i in range(self.n_nodes):
    distance[i] = 0
    pre_node[i] = i","distance = {i: 0 for i in range(self.n_nodes)}
pre_node = {i: i for i in range(self.n_nodes)}",nan,,
https://github.com/mwielgoszewski/doorman/tree/master/doorman/extract_ddl.py,"def extract_schema(filename):
    namespace = {
        'Column': Column,
        'schema': schema,
        'table_name': table_name,
        'extended_schema': extended_schema,
        'current_spec': {},
    }

    for fn in DUMMY_FUNCTIONS:
        namespace[fn] = lambda *args, **kwargs: None

    for ty in SQL_TYPES:
        namespace[ty] = ty

    with open(filename, 'rU') as f:
        tree = ast.parse(f.read())
        exec(compile(tree, '<string>', 'exec'), namespace)

    columns = ', '.join('%s %s' % (x[0], x[1]) for x in current_spec['schema'])
    statements = []
    statements = []
    statements.append('CREATE TABLE %s (%s);' % (current_spec['name'], columns))
    if 'extended_schema' in current_spec:
        statement = 'ALTER TABLE %s ADD %%s %%s;' % (current_spec['name'], )
        for column_name, column_definition in current_spec['extended_schema']:
            statements.append(statement % (column_name, column_definition))
        del current_spec['extended_schema']
    return '\n'.join(statements)",_4127.py,10,"for fn in DUMMY_FUNCTIONS:
    namespace[fn] = lambda *args, **kwargs: None","namespace.update( {fn: lambda *args, **kwargs: None for fn in DUMMY_FUNCTIONS})","namespace = {
        'Column': Column,
        'schema': schema,
        'table_name': table_name,
        'extended_schema': extended_schema,
        'current_spec': {},
    }",,
https://github.com/mwielgoszewski/doorman/tree/master/doorman/extract_ddl.py,"def extract_schema(filename):
    namespace = {
        'Column': Column,
        'schema': schema,
        'table_name': table_name,
        'extended_schema': extended_schema,
        'current_spec': {},
    }

    for fn in DUMMY_FUNCTIONS:
        namespace[fn] = lambda *args, **kwargs: None

    for ty in SQL_TYPES:
        namespace[ty] = ty

    with open(filename, 'rU') as f:
        tree = ast.parse(f.read())
        exec(compile(tree, '<string>', 'exec'), namespace)

    columns = ', '.join('%s %s' % (x[0], x[1]) for x in current_spec['schema'])
    statements = []
    statements = []
    statements.append('CREATE TABLE %s (%s);' % (current_spec['name'], columns))
    if 'extended_schema' in current_spec:
        statement = 'ALTER TABLE %s ADD %%s %%s;' % (current_spec['name'], )
        for column_name, column_definition in current_spec['extended_schema']:
            statements.append(statement % (column_name, column_definition))
        del current_spec['extended_schema']
    return '\n'.join(statements)",_4127.py,13,"for ty in SQL_TYPES:
    namespace[ty] = ty",namespace.update( {ty: ty for ty in SQL_TYPES}),nan,,
https://github.com/naspeh/mailur/tree/master/mailur/local.py,"def sync_flags(con=None, post_handler=None, timeout=None):
    @using(SRC, name='con_src', reuse=False)
    @using(ALL, name='con_all', readonly=False, reuse=False)
    def handler(res, con_src=None, con_all=None):
        cur_modseq = con.highestmodseq
        new_modseq = re.search(r'MODSEQ \((\d+)\)', res[0].decode()).group(1)
        new_modseq = int(new_modseq)
        if new_modseq < cur_modseq:
            return
        fields = '(UID FLAGS) (CHANGEDSINCE %s)' % cur_modseq
        res = con_src.fetch('1:*', fields)
        cur_modseq = new_modseq
        src_flags = {}
        for line in res:
            val = re.search(
                r'UID (\d+) FLAGS \(([^)]*)\) MODSEQ \(\d+\)',
                line.decode()
            )
            if not val:
                continue
            uid, flags = val.groups()
            src_flags[uid] = flags

        if not src_flags:
            return

        actions = {}
        parsed = data_msgs.get()
        pids = pair_origin_uids(src_flags)
        res = con_all.fetch(pids, '(UID FLAGS)')
        for line in res:
            pattern = r'UID (\d+) FLAGS \(([^)]*)\)'
            uid, flags = re.search(pattern, line.decode()).groups()
            flags = set(flags.split())
            orig_flags = set(src_flags[parsed[uid]['origin_uid']].split())
            val = sorted(orig_flags - flags)
            if val:
                key = ('+FLAGS.SILENT', ' '.join(val))
                actions.setdefault(key, [])
                actions[key].append(uid)
            val = sorted(flags - orig_flags)
            if val:
                key = ('-FLAGS.SILENT', ' '.join(val))
                actions.setdefault(key, [])
                actions[key].append(uid)
        log.debug('sync: MODSEQ=%s %s', cur_modseq, actions)
        for action, uids in actions.items():
            con_all.store(uids, *action)
        if post_handler:
            post_handler(res)

    log.info(
        '%s UIDVALIDITY=%s HIGHESTMODSEQ=%s',
        con, con.uidvalidity, con.highestmodseq
    )
    con.select(SRC)
    con.idle({'FETCH': handler}, timeout=timeout)",_4353.py,14,"for line in res:
    val = re.search('UID (\\d+) FLAGS \\(([^)]*)\\) MODSEQ \\(\\d+\\)', line.decode())
    if not val:
        continue
    (uid, flags) = val.groups()
    src_flags[uid] = flags","src_flags = { re.search('UID (\\d+) FLAGS \\(([^)]*)\\) MODSEQ \\(\\d+\\)', line.decode()).groups[0] : re.search(‘UID (\\d+) FLAGS \\(([^)]*)\\) MODSEQ \\(\\d+\\)', line.decode()).groups[1]
for line in res if re.search('UID (\\d+) FLAGS \\(([^)]*)\\) MODSEQ \\(\\d+\\)', line.decode())}",nan,,
https://github.com/Project-MONAI/MONAI/tree/master/monai/transforms/utility/dictionary.py,"def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:
        d = dict(data)
        for key, delay_time in self.key_iterator(d, self.delay_time):
            d[key] = self.delayer(d[key], delay_time=delay_time)
        return d",_5776.py,3,"for (key, delay_time) in self.key_iterator(d, self.delay_time):
    d[key] = self.delayer(d[key], delay_time=delay_time)","d.update( {key: self.delayer(d[key], delay_time=delay_time) for (key, delay_time) in self.key_iterator(d, self.delay_time)} )",dict merge,,
https://github.com/golemhq/golem/tree/master/golem/report/html_report.py,"def generate_html_report(project, execution, timestamp, destination_folder=None,
                         report_name=None, no_images=False):
    """"""Generate static HTML report.
    Report is generated in <report_directory>/<report_name>
    By default it's generated in <testdir>/projects/<project>/reports/<suite>/<timestamp>
    Default name is 'report.html' and 'report-no-images.html'
    """"""
    execution_directory = exec_report.execution_report_path(project, execution, timestamp)

    if destination_folder is None:
        destination_folder = execution_directory

    if not report_name:
        if no_images:
            report_name = 'report-no-images'
        else:
            report_name = 'report'

    formatted_date = utils.get_date_time_from_timestamp(timestamp)
    app = gui.create_app()
    static_folder = app.static_folder
    # css paths
    css_folder = os.path.join(static_folder, 'css')
    boostrap_css = os.path.join(css_folder, 'bootstrap', 'bootstrap.min.css')
    main_css = os.path.join(css_folder, 'main.css')
    report_css = os.path.join(css_folder, 'report.css')
    # js paths
    js_folder = os.path.join(static_folder, 'js')
    main_js = os.path.join(js_folder, 'main.js')
    jquery_js = os.path.join(js_folder, 'external', 'jquery.min.js')
    datatables_js = os.path.join(js_folder, 'external', 'datatable', 'datatables.min.js')
    bootstrap_js = os.path.join(js_folder, 'external', 'bootstrap.min.js')
    report_execution_js = os.path.join(js_folder, 'report_execution.js')

    css = {
        'bootstrap': open(boostrap_css, encoding='utf-8').read(),
        'main': open(main_css, encoding='utf-8').read(),
        'report': open(report_css, encoding='utf-8').read()
    }
    js = {
        'jquery': open(jquery_js, encoding='utf-8').read(),
        'datatables': open(datatables_js, encoding='utf-8').read(),
        'bootstrap': open(bootstrap_js, encoding='utf-8').read(),
        'main': open(main_js, encoding='utf-8').read(),
        'report_execution': open(report_execution_js).read()
    }

    execution_data = exec_report.get_execution_data(execution_directory)
    detail_test_data = {}
    for test in execution_data['tests']:
        test_detail = exec_report.function_test_execution_result(
            project, execution, timestamp, test['test_file'], test['test'], test['set_name'],
            no_screenshots=no_images, encode_screenshots=True
        )
        # testId is test_file + test + set_name
        test_id = f""{test['test_file']}.{test['test']}""
        if test['set_name']:
            test_id = f""{test_id}.{test['set_name']}""
        detail_test_data[test_id] = test_detail
    with app.app_context():
        html_string = render_template(
            'report/report_execution_static.html', project=project, execution=execution,
            timestamp=timestamp, execution_data=execution_data,
            detail_test_data=detail_test_data, formatted_date=formatted_date,
            css=css, js=js, static=True
        )
    _, file_extension = os.path.splitext(report_name)
    if not file_extension:
        report_name = f'{report_name}.html'
    destination = os.path.join(destination_folder, report_name)

    if not os.path.exists(os.path.dirname(destination)):
        os.makedirs(os.path.dirname(destination), exist_ok=True)

    try:
        with open(destination, 'w', encoding='utf-8') as f:
            f.write(html_string)
    except IOError as e:
        if e.errno == errno.EACCES:
            print(f'ERROR: cannot write to {destination}, PermissionError (Errno 13)')
        else:
            print(f'ERROR: There was an error writing to {destination}')

    return html_string",_5821.py,50,"for test in execution_data['tests']:
    test_detail = exec_report.function_test_execution_result(project, execution, timestamp, test['test_file'], test['test'], test['set_name'], no_screenshots=no_images, encode_screenshots=True)
    test_id = f""{test['test_file']}.{test['test']}""
    if test['set_name']:
        test_id = f""{test_id}.{test['set_name']}""
    detail_test_data[test_id] = test_detail","detail_test_data = {f""{test_id}.{test['set_name']}"" : exec_report.function_test_execution_result(project, execution, timestamp, test['test_file'], test['test'], test['set_name'], no_screenshots=no_images, encode_screenshots=True) if test[‘set_name'] else test_id: exec_report.function_test_execution_result(project, execution, timestamp, test['test_file'], test['test'], test['set_name'], no_screenshots=no_images, encode_screenshots=True)
for test in execution_data[‘tests']}",数据依赖,,
https://github.com/enkore/i3pystatus/tree/master/i3pystatus/file.py,"def run(self):
        cdict = {}

        for key, (component, file) in self.components.items():
            with open(join(self.base_path, file), ""r"") as f:
                cdict[key] = component(f.read().strip())

        for key, transform in self.transforms.items():
            cdict[key] = transform(cdict)

        self.data = cdict
        self.output = {
            ""full_text"": self.format.format(**cdict),
            ""color"": self.color
        }",_5826.py,8,"for (key, transform) in self.transforms.items():
    cdict[key] = transform(cdict)","cdict.update( {key: transform(cdict)  for (key, transform) in self.transforms.items()} )",dict merge,,
https://github.com/Neoteroi/BlackSheep/tree/master/blacksheep/server/openapi/v3.py,"def get_parameters(
        self, handler: Any
    ) -> Optional[List[Union[Parameter, Reference]]]:
        if not hasattr(handler, ""binders""):
            return None
        binders: List[Binder] = handler.binders
        parameters: Mapping[str, Union[Parameter, Reference]] = {}

        docs = self.get_handler_docs(handler)
        parameters_info = (docs.parameters if docs else None) or dict()

        for binder in binders:
            location = self.get_parameter_location_for_binder(binder)

            if not location:
                # the binder is used for something that is not a parameter
                # expressed in OpenAPI Docs (e.g. a DI service)
                continue

            if location == ParameterLocation.PATH:
                required = True
            else:
                required = binder.required and binder.default is empty

            # did the user specified information about the parameter?
            param_info = parameters_info.get(binder.parameter_name)

            parameters[binder.parameter_name] = Parameter(
                name=binder.parameter_name,
                in_=location,
                required=required or None,
                schema=self.get_schema_by_type(binder.expected_type),
                description=param_info.description if param_info else """",
                example=param_info.example if param_info else None,
            )

        for key, param_info in parameters_info.items():
            if key not in parameters:
                parameters[key] = Parameter(
                    name=key,
                    in_=self._parameter_source_to_openapi_obj(
                        param_info.source or ParameterSource.QUERY
                    ),
                    required=param_info.required,
                    schema=self.get_schema_by_type(param_info.value_type)
                    if param_info.value_type
                    else None,
                    description=param_info.description,
                    example=param_info.example,
                )

        return list(parameters.values())",_6315.py,12,"for binder in binders:
    location = self.get_parameter_location_for_binder(binder)
    if not location:
        continue
    if location == ParameterLocation.PATH:
        required = True
    else:
        required = binder.required and binder.default is empty
    param_info = parameters_info.get(binder.parameter_name)
    parameters[binder.parameter_name] = Parameter(name=binder.parameter_name, in_=location, required=required or None, schema=self.get_schema_by_type(binder.expected_type), description=param_info.description if param_info else '', example=param_info.example if param_info else None)","parameters = { binder.parameter_name: Parameter(name=binder.parameter_name, in_=location, required= True or None, schema=self.get_schema_by_type(binder.expected_type), description=param_info.description if param_info else '', example=param_info.example if param_info else None) if self.get_parameter_location_for_binder(binder) == ParameterLocation.PATH else binder.parameter_name: Parameter(name=binder.parameter_name, in_=location, required= binder.required and binder.default is empty or None, schema=self.get_schema_by_type(binder.expected_type), description=param_info.description if param_info else '', example=param_info.example if param_info else None)
for binder in binders if self.get_parameter_location_for_binder(binder)}",数据依赖,,
https://github.com/nextcord/nextcord/tree/master/nextcord/guild.py,"def _from_data(self, guild: GuildPayload) -> None:
        # according to Stan, this is always available even if the guild is unavailable
        # I don't have this guarantee when someone updates the guild.
        member_count = guild.get(""member_count"", None)
        if member_count is not None:
            self._member_count: int = member_count

        self.name: str = guild.get(""name"")
        self.region: VoiceRegion = try_enum(VoiceRegion, guild.get(""region""))
        self.verification_level: VerificationLevel = try_enum(
            VerificationLevel, guild.get(""verification_level"")
        )
        self.default_notifications: NotificationLevel = try_enum(
            NotificationLevel, guild.get(""default_message_notifications"")
        )
        self.explicit_content_filter: ContentFilter = try_enum(
            ContentFilter, guild.get(""explicit_content_filter"", 0)
        )
        self.afk_timeout: int = guild.get(""afk_timeout"")
        self._icon: Optional[str] = guild.get(""icon"")
        self._banner: Optional[str] = guild.get(""banner"")
        self.unavailable: bool = guild.get(""unavailable"", False)
        self.id: int = int(guild[""id""])
        self._roles: Dict[int, Role] = {}
        state = self._state  # speed up attribute access
        for r in guild.get(""roles"", []):
            role = Role(guild=self, data=r, state=state)
            self._roles[role.id] = role

        self.mfa_level: MFALevel = guild.get(""mfa_level"")
        self.emojis: Tuple[Emoji, ...] = tuple(
            map(lambda d: state.store_emoji(self, d), guild.get(""emojis"", []))
        )
        self.stickers: Tuple[GuildSticker, ...] = tuple(
            map(lambda d: state.store_sticker(self, d), guild.get(""stickers"", []))
        )
        self.features: List[GuildFeature] = guild.get(""features"", [])
        self._splash: Optional[str] = guild.get(""splash"")
        self._system_channel_id: Optional[int] = utils.get_as_snowflake(guild, ""system_channel_id"")
        self.description: Optional[str] = guild.get(""description"")
        self.max_presences: Optional[int] = guild.get(""max_presences"")
        self.max_members: Optional[int] = guild.get(""max_members"")
        self.max_video_channel_users: Optional[int] = guild.get(""max_video_channel_users"")
        self.premium_tier: int = guild.get(""premium_tier"", 0)
        self.premium_subscription_count: int = guild.get(""premium_subscription_count"") or 0
        self._system_channel_flags: int = guild.get(""system_channel_flags"", 0)
        self.preferred_locale: Optional[str] = guild.get(""preferred_locale"")
        self._discovery_splash: Optional[str] = guild.get(""discovery_splash"")
        self._rules_channel_id: Optional[int] = utils.get_as_snowflake(guild, ""rules_channel_id"")
        self._public_updates_channel_id: Optional[int] = utils.get_as_snowflake(
            guild, ""public_updates_channel_id""
        )
        self.nsfw_level: NSFWLevel = try_enum(NSFWLevel, guild.get(""nsfw_level"", 0))
        self.approximate_presence_count = guild.get(""approximate_presence_count"")
        self.approximate_member_count = guild.get(""approximate_member_count"")

        self._stage_instances: Dict[int, StageInstance] = {}
        for s in guild.get(""stage_instances"", []):
            stage_instance = StageInstance(guild=self, data=s, state=state)
            self._stage_instances[stage_instance.id] = stage_instance

        cache_joined = self._state.member_cache_flags.joined
        self_id = self._state.self_id
        for mdata in guild.get(""members"", []):
            member = Member(data=mdata, guild=self, state=state)  # type: ignore
            if cache_joined or member.id == self_id:
                self._add_member(member)

        self._sync(guild)
        self._large: Optional[bool] = None if member_count is None else self._member_count >= 250

        self.owner_id: Optional[int] = utils.get_as_snowflake(guild, ""owner_id"")
        self.afk_channel: Optional[VocalGuildChannel] = self.get_channel(utils.get_as_snowflake(guild, ""afk_channel_id""))  # type: ignore

        for obj in guild.get(""voice_states"", []):
            self._update_voice_state(obj, int(obj[""channel_id""]))

        for event in guild.get(""guild_scheduled_events"") or []:
            self._store_scheduled_event(event)",_6616.py,26,"for r in guild.get('roles', []):
    role = Role(guild=self, data=r, state=state)
    self._roles[role.id] = role","self._roles = {Role(guild=self, data=r, state=state).id : Role(guild=self, data=r, state=state)
for r in guild.get('roles', [])}",数据依赖,,
https://github.com/nextcord/nextcord/tree/master/nextcord/guild.py,"def _from_data(self, guild: GuildPayload) -> None:
        # according to Stan, this is always available even if the guild is unavailable
        # I don't have this guarantee when someone updates the guild.
        member_count = guild.get(""member_count"", None)
        if member_count is not None:
            self._member_count: int = member_count

        self.name: str = guild.get(""name"")
        self.region: VoiceRegion = try_enum(VoiceRegion, guild.get(""region""))
        self.verification_level: VerificationLevel = try_enum(
            VerificationLevel, guild.get(""verification_level"")
        )
        self.default_notifications: NotificationLevel = try_enum(
            NotificationLevel, guild.get(""default_message_notifications"")
        )
        self.explicit_content_filter: ContentFilter = try_enum(
            ContentFilter, guild.get(""explicit_content_filter"", 0)
        )
        self.afk_timeout: int = guild.get(""afk_timeout"")
        self._icon: Optional[str] = guild.get(""icon"")
        self._banner: Optional[str] = guild.get(""banner"")
        self.unavailable: bool = guild.get(""unavailable"", False)
        self.id: int = int(guild[""id""])
        self._roles: Dict[int, Role] = {}
        state = self._state  # speed up attribute access
        for r in guild.get(""roles"", []):
            role = Role(guild=self, data=r, state=state)
            self._roles[role.id] = role

        self.mfa_level: MFALevel = guild.get(""mfa_level"")
        self.emojis: Tuple[Emoji, ...] = tuple(
            map(lambda d: state.store_emoji(self, d), guild.get(""emojis"", []))
        )
        self.stickers: Tuple[GuildSticker, ...] = tuple(
            map(lambda d: state.store_sticker(self, d), guild.get(""stickers"", []))
        )
        self.features: List[GuildFeature] = guild.get(""features"", [])
        self._splash: Optional[str] = guild.get(""splash"")
        self._system_channel_id: Optional[int] = utils.get_as_snowflake(guild, ""system_channel_id"")
        self.description: Optional[str] = guild.get(""description"")
        self.max_presences: Optional[int] = guild.get(""max_presences"")
        self.max_members: Optional[int] = guild.get(""max_members"")
        self.max_video_channel_users: Optional[int] = guild.get(""max_video_channel_users"")
        self.premium_tier: int = guild.get(""premium_tier"", 0)
        self.premium_subscription_count: int = guild.get(""premium_subscription_count"") or 0
        self._system_channel_flags: int = guild.get(""system_channel_flags"", 0)
        self.preferred_locale: Optional[str] = guild.get(""preferred_locale"")
        self._discovery_splash: Optional[str] = guild.get(""discovery_splash"")
        self._rules_channel_id: Optional[int] = utils.get_as_snowflake(guild, ""rules_channel_id"")
        self._public_updates_channel_id: Optional[int] = utils.get_as_snowflake(
            guild, ""public_updates_channel_id""
        )
        self.nsfw_level: NSFWLevel = try_enum(NSFWLevel, guild.get(""nsfw_level"", 0))
        self.approximate_presence_count = guild.get(""approximate_presence_count"")
        self.approximate_member_count = guild.get(""approximate_member_count"")

        self._stage_instances: Dict[int, StageInstance] = {}
        for s in guild.get(""stage_instances"", []):
            stage_instance = StageInstance(guild=self, data=s, state=state)
            self._stage_instances[stage_instance.id] = stage_instance

        cache_joined = self._state.member_cache_flags.joined
        self_id = self._state.self_id
        for mdata in guild.get(""members"", []):
            member = Member(data=mdata, guild=self, state=state)  # type: ignore
            if cache_joined or member.id == self_id:
                self._add_member(member)

        self._sync(guild)
        self._large: Optional[bool] = None if member_count is None else self._member_count >= 250

        self.owner_id: Optional[int] = utils.get_as_snowflake(guild, ""owner_id"")
        self.afk_channel: Optional[VocalGuildChannel] = self.get_channel(utils.get_as_snowflake(guild, ""afk_channel_id""))  # type: ignore

        for obj in guild.get(""voice_states"", []):
            self._update_voice_state(obj, int(obj[""channel_id""]))

        for event in guild.get(""guild_scheduled_events"") or []:
            self._store_scheduled_event(event)",_6616.py,58,"for s in guild.get('stage_instances', []):
    stage_instance = StageInstance(guild=self, data=s, state=state)
    self._stage_instances[stage_instance.id] = stage_instance","self._stage_instances = {StageInstance(guild=self, data=s, state=state).id : StageInstance(guild=self, data=s, state=state)
for s in guild.get('stage_instances', [])}",数据依赖,,
https://github.com/archlinux/archinstall/tree/master/archinstall/lib/disk/helpers.py,"def sort_block_devices_based_on_performance(block_devices):
	result = {device: 0 for device in block_devices}

	for device, weight in result.items():
		if device.spinning:
			weight -= 10
		else:
			weight += 5

		if device.bus_type == 'nvme':
			weight += 20
		elif device.bus_type == 'sata':
			weight += 10

		result[device] = weight

	return result",_7520.py,4,"for (device, weight) in result.items():
    if device.spinning:
        weight -= 10
    else:
        weight += 5
    if device.bus_type == 'nvme':
        weight += 20
    elif device.bus_type == 'sata':
        weight += 10
    result[device] = weight","result.update({device: weight -10+20 if device.spinning and device.bus_type == ‘nvme' else weight -10+10 if device.spinning and device.bus_type == ‘sata' else weight + 5 +20 if not device.spinning and device.bus_type == ‘nvme' else weight + 5 +10 if not device.spinning and device.bus_type ==‘sata' else weight
for (device, weight) in result.items()})",更复杂,,
https://github.com/awslabs/sockeye/tree/master/sockeye/arguments.py,"def simple_dict() -> Callable:
    """"""
    A simple dictionary format that does not require spaces or quoting.

    Format: key1:value1,key2:value2,...

    Supported types: bool, int, float, str (that doesn't parse as other types).

    :return: A method that can be used as a type in argparse.
    """"""

    def parse(dict_str: str):

        def _parse(value: str):
            if value.lower() == ""true"":
                return True
            if value.lower() == ""false"":
                return False
            if value.isdigit() or (value.startswith('-') and value[1:].isdigit()):
                return int(value)
            try:
                return float(value)
            except:
                return value

        _dict = dict()
        try:
            for entry in dict_str.split("",""):
                key, value = entry.split("":"")
                _dict[key] = _parse(value)
        except ValueError:
            raise argparse.ArgumentTypeError(""Specify argument dictionary as key1:value1,key2:value2,..."")
        return _dict

    return parse",_8308.py,28,"for entry in dict_str.split(','):
    (key, value) = entry.split(':')
    _dict[key] = _parse(value)","_dict = {entry.split(‘:’)[0] : _parse(entry.split(‘:’)[1])
for entry in dict_str.split(',')}",数据依赖,,
https://github.com/ivre/ivre/tree/master/ivre/parser/netflow.py,"def parse_line(cls, line: bytes) -> Dict[str, Any]:
        fields: Dict[str, Any] = dict(
            (name[0], val.strip())
            for name, val in zip(cls.fields, line.decode().split("",""))
        )
        fields[""proto""] = fields[""proto""].lower()
        srv_idx = None
        if fields[""proto""] == ""icmp"":
            # Looks like an nfdump anomaly, keeping ""0.8"" leads to nonsense
            # flows, whereas switching to ""8.0"" makes it sane again.
            if fields[""port2""] == ""0.8"":
                fields[""port2""] = ""8.0""
            fields[""type""], fields[""code""] = [
                int(x) for x in fields.pop(""port2"").split(""."")
            ]
            # ICMP 0 is an answer to ICMP 8
            if fields[""type""] == 0:
                fields[""type""] = 8
                srv_idx = 1
            else:
                srv_idx = 2
            del fields[""port1""]
        else:
            for field in [""port1"", ""port2""]:
                fields[field] = int(fields[field])
        for field in [""start_time"", ""end_time""]:
            fields[field] = datetime.datetime.strptime(fields[field], cls.timefmt)
        if srv_idx is None:
            srv_idx = (
                1
                if utils.guess_srv_port(
                    fields[""port1""], fields[""port2""], proto=fields[""proto""]
                )
                >= 0
                else 2
            )
        cli_idx = 1 if srv_idx == 2 else 2
        fields[""src""] = fields.pop(""addr%d"" % cli_idx)
        fields[""dst""] = fields.pop(""addr%d"" % srv_idx)
        if ""port%s"" % cli_idx in fields:
            fields[""sport""] = fields.pop(""port%d"" % cli_idx)
        if ""port%s"" % srv_idx in fields:
            fields[""dport""] = fields.pop(""port%d"" % srv_idx)
            fields[""flow_name""] = ""%(proto)s %(dport)s"" % fields
        elif ""type"" in fields:
            fields[""flow_name""] = ""%(proto)s %(type)s"" % fields
        else:
            fields[""flow_name""] = fields[""proto""]
        fields[""scbytes""] = cls.str2int(fields.pop(""bytes%d"" % cli_idx))
        fields[""scpkts""] = cls.str2int(fields.pop(""pkts%d"" % cli_idx))
        fields[""csbytes""] = cls.str2int(fields.pop(""bytes%d"" % srv_idx))
        fields[""cspkts""] = cls.str2int(fields.pop(""pkts%d"" % srv_idx))
        return fields",_9075.py,26,"for field in ['start_time', 'end_time']:
    fields[field] = datetime.datetime.strptime(fields[field], cls.timefmt)","fields.update({field: datetime.datetime.strptime(fields[field], cls.timefmt)
for field in ['start_time', 'end_time']})",dict merge,,
https://github.com/ivre/ivre/tree/master/ivre/parser/netflow.py,"def parse_line(cls, line: bytes) -> Dict[str, Any]:
        fields: Dict[str, Any] = dict(
            (name[0], val.strip())
            for name, val in zip(cls.fields, line.decode().split("",""))
        )
        fields[""proto""] = fields[""proto""].lower()
        srv_idx = None
        if fields[""proto""] == ""icmp"":
            # Looks like an nfdump anomaly, keeping ""0.8"" leads to nonsense
            # flows, whereas switching to ""8.0"" makes it sane again.
            if fields[""port2""] == ""0.8"":
                fields[""port2""] = ""8.0""
            fields[""type""], fields[""code""] = [
                int(x) for x in fields.pop(""port2"").split(""."")
            ]
            # ICMP 0 is an answer to ICMP 8
            if fields[""type""] == 0:
                fields[""type""] = 8
                srv_idx = 1
            else:
                srv_idx = 2
            del fields[""port1""]
        else:
            for field in [""port1"", ""port2""]:
                fields[field] = int(fields[field])
        for field in [""start_time"", ""end_time""]:
            fields[field] = datetime.datetime.strptime(fields[field], cls.timefmt)
        if srv_idx is None:
            srv_idx = (
                1
                if utils.guess_srv_port(
                    fields[""port1""], fields[""port2""], proto=fields[""proto""]
                )
                >= 0
                else 2
            )
        cli_idx = 1 if srv_idx == 2 else 2
        fields[""src""] = fields.pop(""addr%d"" % cli_idx)
        fields[""dst""] = fields.pop(""addr%d"" % srv_idx)
        if ""port%s"" % cli_idx in fields:
            fields[""sport""] = fields.pop(""port%d"" % cli_idx)
        if ""port%s"" % srv_idx in fields:
            fields[""dport""] = fields.pop(""port%d"" % srv_idx)
            fields[""flow_name""] = ""%(proto)s %(dport)s"" % fields
        elif ""type"" in fields:
            fields[""flow_name""] = ""%(proto)s %(type)s"" % fields
        else:
            fields[""flow_name""] = fields[""proto""]
        fields[""scbytes""] = cls.str2int(fields.pop(""bytes%d"" % cli_idx))
        fields[""scpkts""] = cls.str2int(fields.pop(""pkts%d"" % cli_idx))
        fields[""csbytes""] = cls.str2int(fields.pop(""bytes%d"" % srv_idx))
        fields[""cspkts""] = cls.str2int(fields.pop(""pkts%d"" % srv_idx))
        return fields",_9075.py,24,"for field in ['port1', 'port2']:
    fields[field] = int(fields[field])","fields.update({field: int(fields[field])  for field in ['port1', 'port2']})",dict merge,,
https://github.com/cheshirekow/cmake_format/tree/master/cmakelang/format/__main__.py,"def get_argdict(args):
  """"""Return a dictionary representation of the argparser `namespace` object
     returned from parse_args(). The returned dictionary will be suitable
     as a configuration kwargs dict. Any command line options that aren't
     configuration options are removed.""""""
  out = {}
  for key, value in vars(args).items():
    if key.startswith(""_""):
      continue
    if hasattr(configuration.Configuration, key):
      continue
    # Remove common command line arguments
    if key in [""log_level"", ""outfile_path"", ""infilepaths"", ""config_files""]:
      continue
    # Remove --dump-config command line arguments
    if key in [""dump_config"", ""with_help"", ""with_defaults""]:
      continue
    # Remove cmake-format command line arguments
    if key in [""dump"", ""check"", ""in_place""]:
      continue
    # Remove cmake-lint command line arguments
    if key in [""suppress_decorations""]:
      continue
    if value is None:
      continue
    out[key] = value

  return out",_9966.py,7,"for (key, value) in vars(args).items():
    if key.startswith('_'):
        continue
    if hasattr(configuration.Configuration, key):
        continue
    if key in ['log_level', 'outfile_path', 'infilepaths', 'config_files']:
        continue
    if key in ['dump_config', 'with_help', 'with_defaults']:
        continue
    if key in ['dump', 'check', 'in_place']:
        continue
    if key in ['suppress_decorations']:
        continue
    if value is None:
        continue
    out[key] = value","out = {key: value
for (key, value) in vars(args).items() if not key.startswith(‘_') if not hasattr(configuration.Configuration, key) if key not in ['log_level', 'outfile_path', 'infilepaths', ‘config_files'] if key not in ['dump_config', 'with_help', ‘with_defaults'] if key not in ['dump', 'check', ‘in_place'] if key not in [‘suppress_decorations'] if value is not  None}",continue,,
https://github.com/nodejs/node-gyp/tree/master/gyp/pylib/gyp/xcode_emulation.py,"def _GetXcodeEnv(
    xcode_settings, built_products_dir, srcroot, configuration, additional_settings=None
):
    """"""Return the environment variables that Xcode would set. See
  http://developer.apple.com/library/mac/#documentation/DeveloperTools/Reference/XcodeBuildSettingRef/1-Build_Setting_Reference/build_setting_ref.html#//apple_ref/doc/uid/TP40003931-CH3-SW153
  for a full list.

  Args:
      xcode_settings: An XcodeSettings object. If this is None, this function
          returns an empty dict.
      built_products_dir: Absolute path to the built products dir.
      srcroot: Absolute path to the source root.
      configuration: The build configuration name.
      additional_settings: An optional dict with more values to add to the
          result.
  """"""

    if not xcode_settings:
        return {}

    # This function is considered a friend of XcodeSettings, so let it reach into
    # its implementation details.
    spec = xcode_settings.spec

    # These are filled in on an as-needed basis.
    env = {
        ""BUILT_FRAMEWORKS_DIR"": built_products_dir,
        ""BUILT_PRODUCTS_DIR"": built_products_dir,
        ""CONFIGURATION"": configuration,
        ""PRODUCT_NAME"": xcode_settings.GetProductName(),
        # For FULL_PRODUCT_NAME see:
        # /Developer/Platforms/MacOSX.platform/Developer/Library/Xcode/Specifications/MacOSX\ Product\ Types.xcspec  # noqa: E501
        ""SRCROOT"": srcroot,
        ""SOURCE_ROOT"": ""${SRCROOT}"",
        # This is not true for static libraries, but currently the env is only
        # written for bundles:
        ""TARGET_BUILD_DIR"": built_products_dir,
        ""TEMP_DIR"": ""${TMPDIR}"",
        ""XCODE_VERSION_ACTUAL"": XcodeVersion()[0],
    }
    if xcode_settings.GetPerConfigSetting(""SDKROOT"", configuration):
        env[""SDKROOT""] = xcode_settings._SdkPath(configuration)
    else:
        env[""SDKROOT""] = """"

    if xcode_settings.mac_toolchain_dir:
        env[""DEVELOPER_DIR""] = xcode_settings.mac_toolchain_dir

    if spec[""type""] in (
        ""executable"",
        ""static_library"",
        ""shared_library"",
        ""loadable_module"",
    ):
        env[""EXECUTABLE_NAME""] = xcode_settings.GetExecutableName()
        env[""EXECUTABLE_PATH""] = xcode_settings.GetExecutablePath()
        env[""FULL_PRODUCT_NAME""] = xcode_settings.GetFullProductName()
        mach_o_type = xcode_settings.GetMachOType()
        if mach_o_type:
            env[""MACH_O_TYPE""] = mach_o_type
        env[""PRODUCT_TYPE""] = xcode_settings.GetProductType()
    if xcode_settings._IsBundle():
        # xcodeproj_file.py sets the same Xcode subfolder value for this as for
        # FRAMEWORKS_FOLDER_PATH so Xcode builds will actually use FFP's value.
        env[""BUILT_FRAMEWORKS_DIR""] = os.path.join(
            built_products_dir + os.sep + xcode_settings.GetBundleFrameworksFolderPath()
        )
        env[""CONTENTS_FOLDER_PATH""] = xcode_settings.GetBundleContentsFolderPath()
        env[""EXECUTABLE_FOLDER_PATH""] = xcode_settings.GetBundleExecutableFolderPath()
        env[
            ""UNLOCALIZED_RESOURCES_FOLDER_PATH""
        ] = xcode_settings.GetBundleResourceFolder()
        env[""JAVA_FOLDER_PATH""] = xcode_settings.GetBundleJavaFolderPath()
        env[""FRAMEWORKS_FOLDER_PATH""] = xcode_settings.GetBundleFrameworksFolderPath()
        env[
            ""SHARED_FRAMEWORKS_FOLDER_PATH""
        ] = xcode_settings.GetBundleSharedFrameworksFolderPath()
        env[
            ""SHARED_SUPPORT_FOLDER_PATH""
        ] = xcode_settings.GetBundleSharedSupportFolderPath()
        env[""PLUGINS_FOLDER_PATH""] = xcode_settings.GetBundlePlugInsFolderPath()
        env[""XPCSERVICES_FOLDER_PATH""] = xcode_settings.GetBundleXPCServicesFolderPath()
        env[""INFOPLIST_PATH""] = xcode_settings.GetBundlePlistPath()
        env[""WRAPPER_NAME""] = xcode_settings.GetWrapperName()

    install_name = xcode_settings.GetInstallName()
    if install_name:
        env[""LD_DYLIB_INSTALL_NAME""] = install_name
    install_name_base = xcode_settings.GetInstallNameBase()
    if install_name_base:
        env[""DYLIB_INSTALL_NAME_BASE""] = install_name_base
    xcode_version, _ = XcodeVersion()
    if xcode_version >= ""0500"" and not env.get(""SDKROOT""):
        sdk_root = xcode_settings._SdkRoot(configuration)
        if not sdk_root:
            sdk_root = xcode_settings._XcodeSdkPath("""")
        if sdk_root is None:
            sdk_root = """"
        env[""SDKROOT""] = sdk_root

    if not additional_settings:
        additional_settings = {}
    else:
        # Flatten lists to strings.
        for k in additional_settings:
            if not isinstance(additional_settings[k], str):
                additional_settings[k] = "" "".join(additional_settings[k])
    additional_settings.update(env)

    for k in additional_settings:
        additional_settings[k] = _NormalizeEnvVarReferences(additional_settings[k])

    return additional_settings",_10173.py,110,"for k in additional_settings:
    additional_settings[k] = _NormalizeEnvVarReferences(additional_settings[k])","additional_settings.update({k: _NormalizeEnvVarReferences(additional_settings[k])
for k in additional_settings})",nan,,
https://github.com/forseti-security/forseti-security/tree/master/google/cloud/forseti/services/dao.py,"def define_model(model_name, dbengine, model_seed):
    """"""Defines table classes which point to the corresponding model.

        This means, for each model being accessed this function needs to
        be called in order to generate a full set of table definitions.

        Models are name spaced via a random model seed such that multiple
        models can exist within the same database. In order to implement
        the name spacing in an abstract way.

    Args:
        model_name (str): model handle
        dbengine (object): db engine
        model_seed (str): seed to get etag

    Returns:
        tuple: (sessionmaker, ModelAccess)
    """"""

    base = declarative_base()

    denormed_group_in_group = '{}_group_in_group'.format(model_name)
    bindings_tablename = '{}_bindings'.format(model_name)
    roles_tablename = '{}_roles'.format(model_name)
    permissions_tablename = '{}_permissions'.format(model_name)
    members_tablename = '{}_members'.format(model_name)
    resources_tablename = '{}_resources'.format(model_name)

    role_permissions = Table('{}_role_permissions'.format(model_name),
                             base.metadata,
                             Column(
                                 'roles_name', ForeignKey(
                                     '{}.name'.format(roles_tablename)),
                                 primary_key=True),
                             Column(
                                 'permissions_name', ForeignKey(
                                     '{}.name'.format(permissions_tablename)),
                                 primary_key=True), )

    binding_members = Table('{}_binding_members'.format(model_name),
                            base.metadata,
                            Column(
                                'bindings_id', ForeignKey(
                                    '{}.id'.format(bindings_tablename)),
                                primary_key=True),
                            Column(
                                'members_name', ForeignKey(
                                    '{}.name'.format(members_tablename)),
                                primary_key=True), )

    group_members = Table(
        '{}_group_members'.format(model_name),
        base.metadata,
        Column('group_name',
               ForeignKey('{}.name'.format(members_tablename)),
               primary_key=True),
        Column('members_name',
               ForeignKey('{}.name'.format(members_tablename)),
               primary_key=True),
    )

    groups_settings = Table(
        '{}_groups_settings'.format(model_name),
        base.metadata,
        Column('group_name',
               ForeignKey('{}.name'.format(members_tablename)),
               primary_key=True),
        Column('settings',
               Text(16777215)),
    )

    def get_string_by_dialect(db_dialect, column_size):
        """"""Get Sqlalchemy String by dialect.
        Sqlite doesn't support collation type, need to define different
        column types for different database engine.

        This is used to make MySQL column case sensitive by adding
        an encoding type.
        Args:
            db_dialect (str): The db dialect.
            column_size (int): The size of the column.

        Returns:
            String: Sqlalchemy String.
        """"""
        if db_dialect.lower() == 'sqlite':
            return String(column_size)
        return String(column_size, collation='utf8mb4_bin')

    class Resource(base):
        """"""Row entry for a GCP resource.""""""
        __tablename__ = resources_tablename

        cai_resource_name = Column(String(4096))
        cai_resource_type = Column(String(512))
        full_name = Column(String(2048), nullable=False)
        type_name = Column(get_string_by_dialect(dbengine.dialect.name, 700),
                           primary_key=True)
        parent_type_name = Column(
            get_string_by_dialect(dbengine.dialect.name, 700),
            ForeignKey('{}.type_name'.format(resources_tablename)))
        name = Column(String(512), nullable=False)
        type = Column(String(128), nullable=False)
        policy_update_counter = Column(Integer, default=0)
        display_name = Column(String(256), default='')
        email = Column(String(256), default='')
        data = Column(Text(16777215))

        parent = relationship('Resource', remote_side=[type_name])
        bindings = relationship('Binding', back_populates='resource')

        def increment_update_counter(self):
            """"""Increments counter for this object's db updates.
            """"""
            self.policy_update_counter += 1

        def get_etag(self):
            """"""Return the etag for this resource.

            Returns:
                str: etag to avoid race condition when set policy
            """"""
            serialized_ctr = struct.pack('>I', self.policy_update_counter)
            msg = binascii.hexlify(serialized_ctr)
            msg += self.full_name.encode()
            seed = (model_seed if isinstance(model_seed, bytes)
                    else model_seed.encode())
            return hmac.new(seed, msg).hexdigest()

        def __repr__(self):
            """"""String representation.

            Returns:
                str: Resource represented as
                    (full_name='{}', name='{}' type='{}')
            """"""
            return '<Resource(full_name={}, name={} type={})>'.format(
                self.full_name, self.name, self.type)

    Resource.children = relationship(
        'Resource', order_by=Resource.full_name, back_populates='parent')

    class Binding(base):
        """"""Row for a binding between resource, roles and members.""""""

        __tablename__ = bindings_tablename
        id = Column(Integer, Sequence('{}_id_seq'.format(bindings_tablename)),
                    primary_key=True)
        resource_type_name = Column(
            get_string_by_dialect(dbengine.dialect.name, 700),
            ForeignKey('{}.type_name'.format(resources_tablename)))

        role_name = Column(get_string_by_dialect(dbengine.dialect.name, 256),
                           ForeignKey('{}.name'.format(roles_tablename)))

        resource = relationship('Resource', remote_side=[resource_type_name])
        role = relationship('Role', remote_side=[role_name])

        members = relationship('Member',
                               secondary=binding_members,
                               back_populates='bindings')

        def __repr__(self):
            """"""String Representation

            Returns:
                str: Binding represented as
                    (id='{}', role='{}', resource='{}' members='{}')
            """"""
            fmt_s = '<Binding(id={}, role={}, resource={} members={})>'
            return fmt_s.format(
                self.id,
                self.role_name,
                self.resource_type_name,
                self.members)

    class Member(base):
        """"""Row entry for a policy member.""""""

        __tablename__ = members_tablename
        name = Column(String(256), primary_key=True)
        type = Column(String(64))
        member_name = Column(String(256))

        parents = relationship(
            'Member',
            secondary=group_members,
            primaryjoin=name == group_members.c.members_name,
            secondaryjoin=name == group_members.c.group_name)

        children = relationship(
            'Member',
            secondary=group_members,
            primaryjoin=name == group_members.c.group_name,
            secondaryjoin=name == group_members.c.members_name)

        bindings = relationship('Binding',
                                secondary=binding_members,
                                back_populates='members')

        def __repr__(self):
            """"""String representation.

            Returns:
                str: Member represented as (name='{}', type='{}')
            """"""
            return '<Member(name={}, type={})>'.format(
                self.name, self.type)

    class GroupInGroup(base):
        """"""Row for a group-in-group membership.""""""

        __tablename__ = denormed_group_in_group
        parent = Column(String(256), primary_key=True)
        member = Column(String(256), primary_key=True)

        def __repr__(self):
            """"""String representation.

            Returns:
                str: GroupInGroup represented as (parent='{}', member='{}')
            """"""
            return '<GroupInGroup(parent={}, member={})>'.format(
                self.parent,
                self.member)

    class Role(base):
        """"""Row entry for an IAM role.""""""

        __tablename__ = roles_tablename
        name = Column(get_string_by_dialect(dbengine.dialect.name, 256),
                      primary_key=True)
        title = Column(String(128), default='')
        stage = Column(String(128), default='')
        description = Column(String(1024), default='')
        custom = Column(Boolean, default=False)
        permissions = relationship('Permission',
                                   secondary=role_permissions,
                                   back_populates='roles')

        def __repr__(self):
            """"""String Representation

            Returns:
                str: Role represented by name
            """"""
            return '<Role(name=%s)>' % self.name

    class Permission(base):
        """"""Row entry for an IAM permission.""""""

        __tablename__ = permissions_tablename
        name = Column(get_string_by_dialect(dbengine.dialect.name, 256),
                      primary_key=True)
        roles = relationship('Role',
                             secondary=role_permissions,
                             back_populates='permissions')

        def __repr__(self):
            """"""String Representation

            Returns:
                str: Permission represented by name
            """"""
            return '<Permission(name=%s)>' % self.name

    # pylint: disable=too-many-public-methods
    class ModelAccess(object):
        """"""Data model facade, implement main API against database.""""""
        TBL_GROUP_IN_GROUP = GroupInGroup
        TBL_GROUPS_SETTINGS = groups_settings
        TBL_BINDING = Binding
        TBL_MEMBER = Member
        TBL_PERMISSION = Permission
        TBL_ROLE = Role
        TBL_RESOURCE = Resource
        TBL_MEMBERSHIP = group_members

        # Set of member binding types that expand like groups.
        GROUP_TYPES = {'group',
                       'projecteditor',
                       'projectowner',
                       'projectviewer'}

        # Members that represent all users
        ALL_USER_MEMBERS = ['allusers', 'allauthenticatedusers']

        @classmethod
        def delete_all(cls, engine):
            """"""Delete all data from the model.

            Args:
                engine (object): database engine
            """"""

            LOGGER.info('Deleting all data from the model.')
            role_permissions.drop(engine)
            binding_members.drop(engine)
            group_members.drop(engine)
            groups_settings.drop(engine)

            Binding.__table__.drop(engine)
            Permission.__table__.drop(engine)
            GroupInGroup.__table__.drop(engine)

            Role.__table__.drop(engine)
            Member.__table__.drop(engine)
            Resource.__table__.drop(engine)

        @classmethod
        def denorm_group_in_group(cls, session):
            """"""Denormalize group-in-group relation.

            This method will fill the GroupInGroup table with
            (parent, member) if parent is an ancestor of member,
            whenever adding or removing a new group or group-group
            relationship, this method should be called to re-denormalize

            Args:
                session (object): Database session to use.

            Returns:
                int: Number of iterations.

            Raises:
                Exception: dernomalize fail
            """"""

            tbl1 = aliased(GroupInGroup.__table__, name='alias1')
            tbl2 = aliased(GroupInGroup.__table__, name='alias2')
            tbl3 = aliased(GroupInGroup.__table__, name='alias3')

            if get_sql_dialect(session) != 'sqlite':
                # Lock tables for denormalization
                # including aliases 1-3
                locked_tables = [
                    '`{}`'.format(GroupInGroup.__tablename__),
                    '`{}` as {}'.format(
                        GroupInGroup.__tablename__,
                        tbl1.name),
                    '`{}` as {}'.format(
                        GroupInGroup.__tablename__,
                        tbl2.name),
                    '`{}` as {}'.format(
                        GroupInGroup.__tablename__,
                        tbl3.name),
                    '`{}`'.format(group_members.name)]
                lock_stmts = ['{} WRITE'.format(tbl) for tbl in locked_tables]
                query = 'LOCK TABLES {}'.format(', '.join(lock_stmts))
                session.execute(query)
            try:
                # Remove all existing rows in the denormalization
                session.execute(GroupInGroup.__table__.delete())

                # Select member relation into GroupInGroup
                qry = (GroupInGroup.__table__.insert().from_select(
                    ['parent', 'member'], group_members.select().where(
                        group_members.c.group_name.startswith('group/')
                    ).where(
                        group_members.c.members_name.startswith('group/')
                    )
                ))

                session.execute(qry)

                iterations = 0
                rows_affected = True
                while rows_affected:
                    # Join membership on its own to find transitive
                    expansion = tbl1.join(tbl2, tbl1.c.member == tbl2.c.parent)

                    # Left outjoin to find the entries that
                    # are already in the table to prevent
                    # inserting already existing entries
                    expansion = expansion.outerjoin(
                        tbl3,
                        and_(tbl1.c.parent == tbl3.c.parent,
                             tbl2.c.member == tbl3.c.member))

                    # Select only such elements that are not
                    # already in the table, indicated as NULL
                    # values through the outer-left-join
                    stmt = (
                        select([tbl1.c.parent,
                                tbl2.c.member])
                        .select_from(expansion)
                        # pylint: disable=singleton-comparison
                        .where(tbl3.c.parent == None)
                        .distinct()
                    )

                    # Execute the query and insert into the table
                    qry = (GroupInGroup.__table__
                           .insert()
                           .from_select(['parent', 'member'], stmt))

                    rows_affected = bool(session.execute(qry).rowcount)
                    iterations += 1
            except Exception as e:
                LOGGER.exception(e)
                session.rollback()
                raise
            finally:
                if get_sql_dialect(session) != 'sqlite':
                    session.execute('UNLOCK TABLES')
                session.commit()
            return iterations

        @classmethod
        def expand_special_members(cls, session):
            """"""Create dynamic groups for project(Editor|Owner|Viewer).

            Should be called after IAM bindings are added to the model.

            Args:
                session (object): Database session to use.
            """"""
            member_type_map = {
                'projecteditor': 'roles/editor',
                'projectowner': 'roles/owner',
                'projectviewer': 'roles/viewer'}
            for parent_member in cls.list_group_members(
                    session, '', member_types=list(member_type_map.keys())):
                member_type, project_id = parent_member.split('/')
                role = member_type_map[member_type]
                try:
                    iam_policy = cls.get_iam_policy(
                        session,
                        'project/{}'.format(project_id),
                        roles=[role])
                    LOGGER.info('iam_policy: %s', iam_policy)
                except NoResultFound:
                    LOGGER.warning('Found a non-existent project, or project '
                                   'outside of the organization, in an IAM '
                                   'binding: %s', parent_member)
                    continue
                members = iam_policy.get('bindings', {}).get(role, [])
                expanded_members = cls.expand_members(session, members)
                for member in expanded_members:
                    stmt = cls.TBL_MEMBERSHIP.insert(
                        {'group_name': parent_member,
                         'members_name': member.name})
                    session.execute(stmt)
                    if member.type == 'group' and member.name in members:
                        session.add(cls.TBL_GROUP_IN_GROUP(
                            parent=parent_member,
                            member=member.name))
            session.commit()

        @classmethod
        def explain_granted(cls, session, member_name, resource_type_name,
                            role, permission):
            """"""Provide info about how the member has access to the resource.

            For example, member m1 can access resource r1 with permission p
            it might be granted by binding (r2, rol, g1),
            r1 is a child resource in a project or folder r2,
            role rol contains permission p,
            m1 is a member in group g1.
            This method list bindings that grant the access, member relation
            and resource hierarchy

            Args:
                session (object): Database session.
                member_name (str): name of the member
                resource_type_name (str): type_name of the resource
                role (str): role to query
                permission (str): permission to query

            Returns:
                tuples: (bindings, member_graph, resource_type_names) bindings,
                    the bindings to grant the access member_graph, the graph to
                    have member included in the binding esource_type_names, the
                    resource tree

            Raises:
                Exception: not granted
            """"""
            members, member_graph = cls.reverse_expand_members(
                session, [member_name], request_graph=True)
            member_names = [m.name for m in members]
            resource_type_names = [r.type_name for r in
                                   cls.find_resource_path(session,
                                                          resource_type_name)]

            if role:
                roles = set([role])
                qry = session.query(Binding, Member).join(
                    binding_members).join(Member)
            else:
                roles = [r.name for r in
                         cls.get_roles_by_permission_names(
                             session,
                             [permission])]
                qry = session.query(Binding, Member)
                qry = qry.join(binding_members).join(Member)
                qry = qry.join(Role).join(role_permissions).join(Permission)

            qry = qry.filter(Binding.role_name.in_(roles))
            qry = qry.filter(Member.name.in_(member_names))
            qry = qry.filter(
                Binding.resource_type_name.in_(resource_type_names))
            result = qry.all()
            if not result:
                error_message = 'Grant not found: ({},{},{})'.format(
                    member_name,
                    resource_type_name,
                    role if role is not None else permission)
                LOGGER.error(error_message)
                raise Exception(error_message)
            else:
                bindings = [(b.resource_type_name, b.role_name, m.name)
                            for b, m in result]
                return bindings, member_graph, resource_type_names

        @classmethod
        def scanner_iter(cls, session, resource_type,
                         parent_type_name=None, stream_results=True):
            """"""Iterate over all resources with the specified type.

            Args:
                session (object): Database session.
                resource_type (str): type of the resource to scan
                parent_type_name (str): type_name of the parent resource
                stream_results (bool): Enable streaming in the query.

            Yields:
                Resource: resource that match the query.
            """"""
            query = (
                session.query(Resource)
                .filter(Resource.type == resource_type)
                .options(joinedload(Resource.parent))
                .enable_eagerloads(True))

            if parent_type_name:
                query = query.filter(
                    Resource.parent_type_name == parent_type_name)

            if stream_results:
                results = query.yield_per(PER_YIELD)
            else:
                results = page_query(query)

            for row in results:
                yield row

        @classmethod
        def scanner_fetch_groups_settings(cls, session, only_iam_groups):
            """"""Fetch Groups Settings.

            Args:
                session (object): Database session.
                only_iam_groups (bool): boolean indicating whether we want to
                only fetch groups settings for which there is at least 1 iam
                policy.

            Yields:
                Resource: resource that match the query
            """"""
            if only_iam_groups:
                query = (session.query(groups_settings)
                         .join(Member).join(binding_members)
                         .distinct().enable_eagerloads(True))
            else:
                query = (session.query(groups_settings).enable_eagerloads(True))
            for resource in query.yield_per(PER_YIELD):
                yield resource

        @classmethod
        def explain_denied(cls, session, member_name, resource_type_names,
                           permission_names, role_names):
            """"""Explain why an access is denied

            Provide information how to grant access to a member if such
            access is denied with current IAM policies.
            For example, member m1 cannot access resource r1 with permission
            p, this method shows the bindings with rol that covered the
            desired permission on the resource r1 and its ancestors.
            If adding this member to any of these bindings, such access
            can be granted. An overgranting level is also provided

            Args:
                session (object): Database session.
                member_name (str): name of the member
                resource_type_names (list): list of type_names of resources
                permission_names (list): list of permissions
                role_names (list): list of roles

            Returns:
                list: list of tuples,
                    (overgranting,[(role_name,member_name,resource_name)])

            Raises:
                Exception: No roles covering requested permission set,
                    Not possible
            """"""

            if not role_names:
                role_names = [r.name for r in
                              cls.get_roles_by_permission_names(
                                  session,
                                  permission_names)]
                if not role_names:
                    error_message = 'No roles covering requested permission set'
                    LOGGER.error(error_message)
                    raise Exception(error_message)

            resource_hierarchy = (
                cls.resource_ancestors(session,
                                       resource_type_names))

            def find_binding_candidates(resource_hierarchy):
                """"""Find the root node in the ancestors.

                    From there, walk down the resource tree and add
                    every node until a node has more than one child.
                    This is the set of nodes which grants access to
                    at least all of the resources requested.
                    There is always a chain with a single node root.

                Args:
                    resource_hierarchy (dict): graph of the resource hierarchy

                Returns:
                    list: candidates to add to bindings that potentially grant
                        access
                """"""

                root = None
                for parent in resource_hierarchy.keys():
                    is_root = True
                    for children in resource_hierarchy.values():
                        if parent in children:
                            is_root = False
                            break
                    if is_root:
                        root = parent
                chain = [root]
                cur = root
                while len(resource_hierarchy[cur]) == 1:
                    cur = next(iter(resource_hierarchy[cur]))
                    chain.append(cur)
                return chain

            bind_res_candidates = find_binding_candidates(
                resource_hierarchy)

            bindings = (
                session.query(Binding, Member)
                .join(binding_members)
                .join(Member)
                .join(Role)
                .filter(Binding.resource_type_name.in_(
                    bind_res_candidates))
                .filter(Role.name.in_(role_names))
                .filter(or_(Member.type == 'group',
                            Member.name == member_name))
                .filter(and_((binding_members.c.bindings_id ==
                              Binding.id),
                             (binding_members.c.members_name ==
                              Member.name)))
                .filter(Role.name == Binding.role_name)
                .all())

            strategies = []
            for resource in bind_res_candidates:
                for role_name in role_names:
                    overgranting = (len(bind_res_candidates) -
                                    bind_res_candidates.index(resource) -
                                    1)
                    strategies.append(
                        (overgranting, [
                            (role, member_name, resource)
                            for role in [role_name]]))
            if bindings:
                for binding, member in bindings:
                    overgranting = (len(bind_res_candidates) - 1 -
                                    bind_res_candidates.index(
                                        binding.resource_type_name))
                    strategies.append(
                        (overgranting, [
                            (binding.role_name,
                             member.name,
                             binding.resource_type_name)]))

            return strategies

        @classmethod
        def query_access_by_member(cls, session, member_name, permission_names,
                                   expand_resources=False,
                                   reverse_expand_members=True):
            """"""Return the set of resources the member has access to.

            By default, this method expand group_member relation,
            so the result includes all resources can be accessed by the
            groups that the member is in.
            By default, this method does not expand resource hierarchy,
            so the result does not include a resource if such resource does
            not have a direct binding to allow access.

            Args:
                session (object): Database session.
                member_name (str): name of the member
                permission_names (list): list of names of permissions to query
                expand_resources (bool): whether to expand resources
                reverse_expand_members (bool): whether to expand members

            Returns:
                list: list of access tuples, (""role_name"", ""resource_type_name"")
            """"""

            if reverse_expand_members:
                member_names = [m.name for m in
                                cls.reverse_expand_members(session,
                                                           [member_name],
                                                           False)]
            else:
                member_names = [member_name]

            roles = cls.get_roles_by_permission_names(
                session, permission_names)

            qry = (
                session.query(Binding)
                .join(binding_members)
                .join(Member)
                .filter(Binding.role_name.in_([r.name for r in roles]))
                .filter(Member.name.in_(member_names))
            )

            bindings = qry.yield_per(1024)
            if not expand_resources:
                return [(binding.role_name,
                         [binding.resource_type_name]) for binding in bindings]

            r_type_names = [binding.resource_type_name for binding in bindings]
            expansion = cls.expand_resources_by_type_names(
                session,
                r_type_names)

            res_exp = {k.type_name: [v.type_name for v in values]
                       for k, values in expansion.items()}

            return [(binding.role_name,
                     res_exp[binding.resource_type_name])
                    for binding in bindings]

        @classmethod
        def query_access_by_permission(cls,
                                       session,
                                       role_name=None,
                                       permission_name=None,
                                       expand_groups=False,
                                       expand_resources=False):
            """"""Query access via the specified permission

            Return all the (Principal, Resource) combinations allowing
            satisfying access via the specified permission.
            By default, the group relation and resource hierarchy will not be
            expanded, so the results will only contains direct bindings
            filtered by permission. But the relations can be expanded

            Args:
                session (object): Database session.
                role_name (str): Role name to query for
                permission_name (str): Permission name to query for.
                expand_groups (bool): Whether or not to expand groups.
                expand_resources (bool): Whether or not to expand resources.

            Yields:
                obejct: A generator of access tuples.

            Raises:
                ValueError: If neither role nor permission is set.
            """"""

            if role_name:
                role_names = [role_name]
            elif permission_name:
                role_names = [p.name for p in
                              cls.get_roles_by_permission_names(
                                  session,
                                  [permission_name])]
            else:
                error_message = 'Either role or permission must be set'
                LOGGER.error(error_message)
                raise ValueError(error_message)

            if expand_resources:
                expanded_resources = aliased(Resource)
                qry = (
                    session.query(expanded_resources, Binding, Member)
                    .filter(binding_members.c.bindings_id == Binding.id)
                    .filter(binding_members.c.members_name == Member.name)
                    .filter(expanded_resources.full_name.startswith(
                        Resource.full_name))
                    .filter((Resource.type_name ==
                             Binding.resource_type_name))
                    .filter(Binding.role_name.in_(role_names))
                    .order_by(expanded_resources.name.asc(),
                              Binding.role_name.asc())
                )
            else:
                qry = (
                    session.query(Resource, Binding, Member)
                    .filter(binding_members.c.bindings_id == Binding.id)
                    .filter(binding_members.c.members_name == Member.name)
                    .filter((Resource.type_name ==
                             Binding.resource_type_name))
                    .filter(Binding.role_name.in_(role_names))
                    .order_by(Resource.name.asc(), Binding.role_name.asc())
                )

            if expand_groups:
                to_expand = set([m.name for _, _, m in
                                 qry.yield_per(PER_YIELD)])
                expansion = cls.expand_members_map(session, to_expand,
                                                   show_group_members=False,
                                                   member_contain_self=True)

            qry = qry.distinct()

            cur_resource = None
            cur_role = None
            cur_members = set()
            for resource, binding, member in qry.yield_per(PER_YIELD):
                if cur_resource != resource.type_name:
                    if cur_resource is not None:
                        yield cur_role, cur_resource, cur_members
                    cur_resource = resource.type_name
                    cur_role = binding.role_name
                    cur_members = set()
                if expand_groups:
                    for member_name in expansion[member.name]:
                        cur_members.add(member_name)
                else:
                    cur_members.add(member.name)
            if cur_resource is not None:
                yield cur_role, cur_resource, cur_members

        @classmethod
        def query_access_by_resource(cls, session, resource_type_name,
                                     permission_names, expand_groups=False):
            """"""Query access by resource

            Return members who have access to the given resource.
            The resource hierarchy will always be expanded, so even if the
            current resource does not have that binding, if its ancestors
            have the binding, the access will be shown
            By default, the group relationship will not be expanded

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource to query
                permission_names (list): list of strs, names of the permissions
                    to query
                expand_groups (bool): whether to expand groups

            Returns:
                dict: role_member_mapping, <""role_name"", ""member_names"">
            """"""

            roles = cls.get_roles_by_permission_names(
                session, permission_names)
            resources = cls.find_resource_path(session, resource_type_name)

            res = (session.query(Binding, Member)
                   .filter(
                       Binding.role_name.in_([r.name for r in roles]),
                       Binding.resource_type_name.in_(
                           [r.type_name for r in resources]))
                   .join(binding_members).join(Member))

            role_member_mapping = collections.defaultdict(set)
            for binding, member in res:
                role_member_mapping[binding.role_name].add(member.name)

            if expand_groups:
                for role in role_member_mapping:
                    role_member_mapping[role] = (
                        [m.name for m in cls.expand_members(
                            session,
                            role_member_mapping[role])])

            return role_member_mapping

        @classmethod
        def query_permissions_by_roles(cls, session, role_names, role_prefixes,
                                       _=1024):
            """"""Resolve permissions for the role.

            Args:
                session (object): db session
                role_names (list): list of strs, names of the roles
                role_prefixes (list): list of strs, prefixes of the roles
                _ (int): place occupation

            Returns:
                list: list of (Role, Permission)

            Raises:
                Exception: No roles or role prefixes specified
            """"""

            if not role_names and not role_prefixes:
                error_message = 'No roles or role prefixes specified'
                LOGGER.error(error_message)
                raise Exception(error_message)
            qry = session.query(Role, Permission).join(
                role_permissions).join(Permission)
            if role_names:
                qry = qry.filter(Role.name.in_(role_names))
            if role_prefixes:
                qry = qry.filter(
                    or_(*[Role.name.startswith(prefix)
                          for prefix in role_prefixes]))
            return qry.all()

        @classmethod
        def set_iam_policy(cls,
                           session,
                           resource_type_name,
                           policy,
                           update_members=False):
            """"""Set IAM policy

            Sets an IAM policy for the resource, check the etag when setting
            new policy and reassign new etag.
            Check etag to avoid race condition

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource
                policy (dict): the policy to set on the resource
                update_members (bool): If true, then add new members to Member
                    table. This must be set when the call to set_iam_policy
                    happens outside of the model InventoryImporter class. Tests
                    or users that manually add an IAM policy need to mark this
                    as true to ensure the model remains consistent.

            Raises:
                Exception: Etag doesn't match
            """"""

            LOGGER.info('Setting IAM policy, resource_type_name = %s, policy'
                        ' = %s, session = %s',
                        resource_type_name, policy, session)
            old_policy = cls.get_iam_policy(session, resource_type_name)
            if policy['etag'] != old_policy['etag']:
                error_message = 'Etags distinct, stored={}, provided={}'.format(
                    old_policy['etag'], policy['etag'])
                LOGGER.error(error_message)
                raise Exception(error_message)

            old_policy = old_policy['bindings']
            policy = policy['bindings']

            def filter_etag(policy):
                """"""Filter etag key/value out of policy map.

                Args:
                    policy (dict): the policy to filter

                Returns:
                    dict: policy without etag, <""bindings"":[<role, members>]>

                Raises:
                """"""

                return {k: v for k, v in policy.items() if k != 'etag'}

            def calculate_diff(policy, old_policy):
                """"""Calculate the grant/revoke difference between policies.
                   The diff = policy['bindings'] - old_policy['bindings']

                Args:
                    policy (dict): the new policy in dict format
                    old_policy (dict): the old policy in dict format

                Returns:
                    dict: <role, members> diff of bindings
                """"""

                diff = collections.defaultdict(list)
                for role, members in filter_etag(policy).items():
                    if role in old_policy:
                        for member in members:
                            if member not in old_policy[role]:
                                diff[role].append(member)
                    else:
                        diff[role] = members
                return diff

            grants = calculate_diff(policy, old_policy)
            revocations = calculate_diff(old_policy, policy)

            for role, members in revocations.items():
                bindings = (
                    session.query(Binding)
                    .filter((Binding.resource_type_name ==
                             resource_type_name))
                    .filter(Binding.role_name == role)
                    .join(binding_members).join(Member)
                    .filter(Member.name.in_(members)).all())

                for binding in bindings:
                    session.delete(binding)

            for role, members in grants.items():
                inserted = False
                existing_bindings = (
                    session.query(Binding)
                    .filter((Binding.resource_type_name ==
                             resource_type_name))
                    .filter(Binding.role_name == role)
                    .all())

                if update_members:
                    for member in members:
                        if not cls.get_member(session, member):
                            try:
                                # This is the default case, e.g. 'group/foobar'
                                m_type, name = member.split('/', 1)
                            except ValueError:
                                # Special groups like 'allUsers'
                                m_type, name = member, member
                            session.add(cls.TBL_MEMBER(
                                name=member,
                                type=m_type,
                                member_name=name))

                for binding in existing_bindings:
                    if binding.role_name == role:
                        inserted = True
                        for member in members:
                            binding.members.append(
                                session.query(Member).filter(
                                    Member.name == member).one())
                if not inserted:
                    binding = Binding(
                        resource_type_name=resource_type_name,
                        role=session.query(Role).filter(
                            Role.name == role).one())
                    binding.members = session.query(Member).filter(
                        Member.name.in_(members)).all()
                    session.add(binding)
            resource = session.query(Resource).filter(
                Resource.type_name == resource_type_name).one()
            resource.increment_update_counter()
            session.commit()

        @classmethod
        def get_iam_policy(cls, session, resource_type_name, roles=None):
            """"""Return the IAM policy for a resource.

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource to query
                roles (list): An optional list of roles to limit the results to

            Returns:
                dict: the IAM policy
            """"""

            resource = session.query(Resource).filter(
                Resource.type_name == resource_type_name).one()
            policy = {'etag': resource.get_etag(),
                      'bindings': {},
                      'resource': resource.type_name}
            bindings = session.query(Binding).filter(
                Binding.resource_type_name == resource_type_name)
            if roles:
                bindings = bindings.filter(Binding.role_name.in_(roles))
            for binding in bindings.all():
                role = binding.role_name
                members = [m.name for m in binding.members]
                policy['bindings'][role] = members
            return policy

        @classmethod
        def check_iam_policy(cls, session, resource_type_name, permission_name,
                             member_name):
            """"""Check access according to the resource IAM policy.

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource to check
                permission_name (str): name of the permission to check
                member_name (str): name of the member to check

            Returns:
                bool: whether such access is allowed

            Raises:
                Exception: member or resource not found
            """"""

            member_names = [m.name for m in
                            cls.reverse_expand_members(
                                session,
                                [member_name])]
            resource_type_names = [r.type_name for r in cls.find_resource_path(
                session,
                resource_type_name)]

            if not member_names:
                error_message = 'Member not found: {}'.format(member_name)
                LOGGER.error(error_message)
                raise Exception(error_message)
            if not resource_type_names:
                error_message = 'Resource not found: {}'.format(
                    resource_type_name)
                LOGGER.error(error_message)
                raise Exception(error_message)

            return (session.query(Permission)
                    .filter(Permission.name == permission_name)
                    .join(role_permissions).join(Role).join(Binding)
                    .filter(Binding.resource_type_name.in_(resource_type_names))
                    .join(binding_members).join(Member)
                    .filter(Member.name.in_(member_names)).first() is not None)

        @classmethod
        def list_roles_by_prefix(cls, session, role_prefix):
            """"""Provides a list of roles matched via name prefix.

            Args:
                session (object): db session
                role_prefix (str): prefix of the role_name

            Returns:
                list: list of role_names that match the query
            """"""

            return [r.name for r in session.query(Role).filter(
                Role.name.startswith(role_prefix)).all()]

        @classmethod
        def add_role_by_name(cls, session, role_name, permission_names):
            """"""Creates a new role.

            Args:
                session (object): db session
                role_name (str): name of the role to add
                permission_names (list): list of permissions in the role
            """"""

            LOGGER.info('Creating a new role, role_name = %s, permission_names'
                        ' = %s, session = %s',
                        role_name, permission_names, session)
            permission_names = set(permission_names)
            existing_permissions = session.query(Permission).filter(
                Permission.name.in_(permission_names)).all()
            for existing_permission in existing_permissions:
                try:
                    permission_names.remove(existing_permission.name)
                except KeyError:
                    LOGGER.warning('existing_permissions.name = %s, KeyError',
                                   existing_permission.name)

            new_permissions = [Permission(name=n) for n in permission_names]
            for perm in new_permissions:
                session.add(perm)
            cls.add_role(session, role_name,
                         existing_permissions + new_permissions)
            session.commit()

        @classmethod
        def add_group_member(cls,
                             session,
                             member_type_name,
                             parent_type_names,
                             denorm=False):
            """"""Add member, optionally with parent relationship.

            Args:
                session (object): db session
                member_type_name (str): type_name of the member to add
                parent_type_names (list): type_names of the parents
                denorm (bool): whether to denorm the groupingroup table after
                    addition
            """"""

            LOGGER.info('Adding a member, member_type_name = %s,'
                        ' parent_type_names = %s, denorm = %s, session = %s',
                        member_type_name, parent_type_names, denorm, session)

            cls.add_member(session,
                           member_type_name,
                           parent_type_names,
                           denorm)
            session.commit()

        @classmethod
        def list_group_members(cls,
                               session,
                               member_name_prefix,
                               member_types=None):
            """"""Returns members filtered by prefix.

            Args:
                session (object): db session
                member_name_prefix (str): the prefix of the member_name
                member_types (list): an optional list of member types to filter
                    the results by.

            Returns:
                list: list of Members that match the query
            """"""

            qry = session.query(Member).filter(
                Member.member_name.startswith(member_name_prefix))
            if member_types:
                qry = qry.filter(Member.type.in_(member_types))
            return [m.name for m in qry.all()]

        @classmethod
        def iter_groups(cls, session):
            """"""Returns iterator of all groups in model.

            Args:
                session (object): db session

            Yields:
                Member: group in the model
            """"""

            qry = session.query(Member).filter(Member.type == 'group')
            for group in qry.yield_per(1024):
                yield group

        @classmethod
        def iter_resources_by_prefix(cls,
                                     session,
                                     full_resource_name_prefix=None,
                                     type_name_prefix=None,
                                     type_prefix=None,
                                     name_prefix=None):
            """"""Returns iterator to resources filtered by prefix.

            Args:
                session (object): db session
                full_resource_name_prefix (str): the prefix of the
                    full_resource_name
                type_name_prefix (str): the prefix of the type_name
                type_prefix (str): the prefix of the type
                name_prefix (ste): the prefix of the name

            Yields:
                Resource: that match the query

            Raises:
                Exception: No prefix given
            """"""

            if not any([arg is not None for arg in [full_resource_name_prefix,
                                                    type_name_prefix,
                                                    type_prefix,
                                                    name_prefix]]):
                error_message = 'At least one prefix must be set'
                LOGGER.error(error_message)
                raise Exception(error_message)

            qry = session.query(Resource)
            if full_resource_name_prefix:
                qry = qry.filter(Resource.full_name.startswith(
                    full_resource_name_prefix))
            if type_name_prefix:
                qry = qry.filter(Resource.type_name.startswith(
                    type_name_prefix))
            if type_prefix:
                qry = qry.filter(Resource.type.startswith(
                    type_prefix))
            if name_prefix:
                qry = qry.filter(Resource.name.startswith(
                    name_prefix))

            for resource in qry.yield_per(1024):
                yield resource

        @classmethod
        def list_resources_by_prefix(cls,
                                     session,
                                     full_resource_name_prefix=None,
                                     type_name_prefix=None,
                                     type_prefix=None,
                                     name_prefix=None):
            """"""Returns resources filtered by prefix.

            Args:
                session (object): db session
                full_resource_name_prefix (str): the prefix of the
                    full_resource_name
                type_name_prefix (str): the prefix of the type_name
                type_prefix (str): the prefix of the type
                name_prefix (ste): the prefix of the name

            Returns:
                list: list of Resources match the query

            Raises:
            """"""

            return list(
                cls.iter_resources_by_prefix(session,
                                             full_resource_name_prefix,
                                             type_name_prefix,
                                             type_prefix,
                                             name_prefix))

        @classmethod
        def add_resource_by_name(cls,
                                 session,
                                 resource_type_name,
                                 parent_type_name,
                                 no_require_parent):
            """"""Adds resource specified via full name.

            Args:
                session (object): db session
                resource_type_name (str): name of the resource
                parent_type_name (str): name of the parent resource
                no_require_parent (bool): if this resource has a parent

            Returns:
                Resource: Created new resource
            """"""

            LOGGER.info('Adding resource via full name, resource_type_name'
                        ' = %s, parent_type_name = %s, no_require_parent = %s,'
                        ' session = %s', resource_type_name,
                        parent_type_name, no_require_parent, session)
            if not no_require_parent:
                parent = session.query(Resource).filter(
                    Resource.type_name == parent_type_name).one()
            else:
                parent = None
            return cls.add_resource(session, resource_type_name, parent)

        @classmethod
        def add_resource(cls, session, resource_type_name, parent=None):
            """"""Adds resource by name.

            Args:
                session (object): db session
                resource_type_name (str): name of the resource
                parent (Resource): parent of the resource

            Returns:
                Resource: Created new resource
            """"""

            LOGGER.info('Adding resource by name, resource_type_name = %s,'
                        ' session = %s', resource_type_name, session)
            res_type, res_name = resource_type_name.split('/')
            parent_full_resource_name = (
                '' if parent is None else parent.full_name)

            full_resource_name = to_full_resource_name(
                parent_full_resource_name,
                resource_type_name)

            resource = Resource(full_name=full_resource_name,
                                type_name=resource_type_name,
                                name=res_name,
                                type=res_type,
                                parent=parent)
            session.add(resource)
            return resource

        @classmethod
        def add_role(cls, session, name, permissions=None):
            """"""Add role by name.

            Args:
                session (object): db session
                name (str): name of the role to add
                permissions (list): permissions to add in the role

            Returns:
                Role: The created role
            """"""

            LOGGER.info('Adding role, name = %s, permissions = %s,'
                        ' session = %s', name, permissions, session)
            permissions = [] if permissions is None else permissions
            role = Role(name=name, permissions=permissions)
            session.add(role)
            return role

        @classmethod
        def add_permission(cls, session, name, roles=None):
            """"""Add permission by name.

            Args:
                session (object): db session
                name (str): name of the permission
                roles (list): list od roles to add the permission

            Returns:
                Permission: The created permission
            """"""

            LOGGER.info('Adding permission, name = %s, roles = %s'
                        ' session = %s', name, roles, session)
            roles = [] if roles is None else roles
            permission = Permission(name=name, roles=roles)
            session.add(permission)
            return permission

        @classmethod
        def add_binding(cls, session, resource, role, members):
            """"""Add a binding to the model.

            Args:
                session (object): db session
                resource (str): Resource to be added in the binding
                role (str): Role to be added in the binding
                members (list): members to be added in the binding

            Returns:
                Binding: the created binding
            """"""

            LOGGER.info('Adding a binding to the model, resource = %s,'
                        ' role = %s, members = %s, session = %s',
                        resource, role, members, session)
            binding = Binding(resource=resource, role=role, members=members)
            session.add(binding)
            return binding

        @classmethod
        def add_member(cls,
                       session,
                       type_name,
                       parent_type_names=None,
                       denorm=False):
            """"""Add a member to the model.

            Args:
                session (object): db session
                type_name (str): type_name of the resource to add
                parent_type_names (list): list of parent names to add
                denorm (bool): whether to denormalize the GroupInGroup relation

            Returns:
                Member: the created member

            Raises:
                Exception: parent not found
            """"""

            LOGGER.info('Adding a member to the model, type_name = %s,'
                        ' parent_type_names = %s, denorm = %s, session = %s',
                        type_name, parent_type_names, denorm, session)
            if not parent_type_names:
                parent_type_names = []
            res_type, name = type_name.split('/', 1)
            parents = session.query(Member).filter(
                Member.name.in_(parent_type_names)).all()
            if len(parents) != len(parent_type_names):
                msg = 'Parents: {}, expected: {}'.format(
                    parents, parent_type_names)
                error_message = 'Parent not found, {}'.format(msg)
                LOGGER.error(error_message)
                raise Exception(error_message)

            member = Member(name=type_name,
                            member_name=name,
                            type=res_type,
                            parents=parents)
            session.add(member)
            session.commit()
            if denorm and res_type == 'group' and parents:
                cls.denorm_group_in_group(session)
            return member

        @classmethod
        def expand_resources_by_type_names(cls, session, res_type_names):
            """"""Expand resources by type/name format.

            Args:
                session (object): db session
                res_type_names (list): list of resources in type_names

            Returns:
                dict: mapping in the form:
                      {res_type_name: Expansion(res_type_name), ... }
            """"""

            res_key = aliased(Resource, name='res_key')
            res_values = aliased(Resource, name='res_values')

            expressions = []
            for res_type_name in res_type_names:
                expressions.append(and_(
                    res_key.type_name == res_type_name))

            res = (
                session.query(res_key, res_values)
                .filter(res_key.type_name.in_(res_type_names))
                .filter(res_values.full_name.startswith(
                    res_key.full_name))
                .yield_per(1024)
            )

            mapping = collections.defaultdict(set)
            for k, value in res:
                mapping[k].add(value)
            return mapping

        @classmethod
        def reverse_expand_members(cls, session, member_names,
                                   request_graph=False):
            """"""Expand members to their groups.

            List all groups that contains these members. Also return
            the graph if requested.

            Args:
                session (object): db session
                member_names (list): list of members to expand
                request_graph (bool): wether the parent-child graph is provided

            Returns:
                object: set if graph not requested, set and graph if requested
            """"""
            member_names.extend(cls.ALL_USER_MEMBERS)
            members = session.query(Member).filter(
                Member.name.in_(member_names)).all()
            membership_graph = collections.defaultdict(set)
            member_set = set()
            new_member_set = set()

            def add_to_sets(members, child):
                """"""Adds the members & children to the sets.

                Args:
                    members (list): list of Members to be added
                    child (Member): child to be added
                """"""

                for member in members:
                    if request_graph and child:
                        membership_graph[child.name].add(member.name)
                    if request_graph and not child:
                        if member.name not in membership_graph:
                            membership_graph[member.name] = set()
                    if member not in member_set:
                        new_member_set.add(member)
                        member_set.add(member)

            add_to_sets(members, None)
            while new_member_set:
                members_to_walk = new_member_set
                new_member_set = set()
                for member in members_to_walk:
                    add_to_sets(member.parents, member)

            if request_graph:
                return member_set, membership_graph
            return member_set

        @classmethod
        def expand_members_map(cls,
                               session,
                               member_names,
                               show_group_members=True,
                               member_contain_self=True):
            """"""Expand group membership keyed by member.

            Args:
                session (object): db session
                member_names (set): Member names to expand
                show_group_members (bool): Whether to include subgroups
                member_contain_self (bool): Whether to include a parent
                    as its own member
            Returns:
                dict: <Member, set(Children)>
            """"""

            def separate_groups(member_names):
                """"""Separate groups and other members in two lists.

                This is a helper function. groups are needed to query on
                group_in_group table

                Args:
                    member_names (list): list of members to be separated

                Returns:
                    tuples: two lists of strs containing groups and others
                """"""
                groups = []
                others = []
                for name in member_names:
                    member_type = name.split('/')[0]
                    if member_type in cls.GROUP_TYPES:
                        groups.append(name)
                    else:
                        others.append(name)
                return groups, others

            selectables = []
            group_names, other_names = separate_groups(member_names)

            t_ging = GroupInGroup.__table__
            t_members = group_members

            # This resolves groups to its transitive non-group members.
            transitive_membership = (
                select([t_ging.c.parent, t_members.c.members_name])
                .select_from(t_ging.join(t_members,
                                         (t_ging.c.member ==
                                          t_members.c.group_name)))
            ).where(t_ging.c.parent.in_(group_names))

            if not show_group_members:
                transitive_membership = transitive_membership.where(
                    not_(t_members.c.members_name.startswith('group/')))

            selectables.append(
                transitive_membership.alias('transitive_membership'))

            direct_membership = (
                select([t_members.c.group_name,
                        t_members.c.members_name])
                .where(t_members.c.group_name.in_(group_names))
            )

            if not show_group_members:
                direct_membership = direct_membership.where(
                    not_(t_members.c.members_name.startswith('group/')))

            selectables.append(
                direct_membership.alias('direct_membership'))

            if show_group_members:
                # Show groups as members of other groups
                group_in_groups = (
                    select([t_ging.c.parent,
                            t_ging.c.member]).where(
                                t_ging.c.parent.in_(group_names))
                )
                selectables.append(
                    group_in_groups.alias('group_in_groups'))

            # Union all the queries
            qry = union(*selectables)

            # Build the result dict
            result = collections.defaultdict(set)
            for parent, child in session.execute(qry):
                result[parent].add(child)
            for parent in other_names:
                result[parent] = set()

            # Add each parent as its own member
            if member_contain_self:
                for name in member_names:
                    result[name].add(name)
            return result

        @classmethod
        def expand_members(cls, session, member_names):
            """"""Expand group membership towards the members.

            Args:
                session (object): db session
                member_names (list): list of strs of member names

            Returns:
                set: expanded group members
            """"""

            members = session.query(Member).filter(
                Member.name.in_(member_names)).all()

            def is_group(member):
                """"""Returns true iff the member is a group.

                Args:
                    member (Member): member to check

                Returns:
                    bool: whether the member is a group
                """"""
                return member.type in cls.GROUP_TYPES

            group_set = set()
            non_group_set = set()
            new_group_set = set()

            def add_to_sets(members):
                """"""Adds new members to the sets.

                Args:
                    members (list): members to be added
                """"""
                for member in members:
                    if is_group(member):
                        if member not in group_set:
                            new_group_set.add(member)
                        group_set.add(member)
                    else:
                        non_group_set.add(member)

            add_to_sets(members)

            while new_group_set:
                groups_to_walk = new_group_set
                new_group_set = set()
                for group in groups_to_walk:
                    add_to_sets(group.children)

            return group_set.union(non_group_set)

        @classmethod
        def resource_ancestors(cls, session, resource_type_names):
            """"""Resolve the transitive ancestors by type/name format.

            Given a group of resource and find out all their parents.
            Then this method group the pairs with parent. Used to determine
            resource candidates to grant access in explain denied.

            Args:
                session (object): db session
                resource_type_names (list): list of strs, resources to query

            Returns:
                dict: <parent, childs> graph of the resource hierarchy
            """"""

            resource_names = resource_type_names
            resource_graph = collections.defaultdict(set)

            res_childs = aliased(Resource, name='res_childs')
            res_anc = aliased(Resource, name='resource_parent')

            resources_set = set(resource_names)
            resources_new = set(resource_names)

            for resource in resources_new:
                resource_graph[resource] = set()

            while resources_new:
                resources_new = set()
                for parent, child in (
                        session.query(res_anc, res_childs)
                        .filter(res_childs.type_name.in_(resources_set))
                        .filter(res_childs.parent_type_name ==
                                res_anc.type_name)
                        .all()):

                    if parent.type_name not in resources_set:
                        resources_new.add(parent.type_name)

                    resources_set.add(parent.type_name)
                    resources_set.add(child.type_name)

                    resource_graph[parent.type_name].add(child.type_name)

            return resource_graph

        @classmethod
        def find_resource_path(cls, session, resource_type_name):
            """"""Find resource ancestors by type/name format.

            Find all ancestors of a resource and return them in order

            Args:
                session (object): db session
                resource_type_name (str): resource to query

            Returns:
                list: list of Resources, transitive ancestors for the given
                    resource
            """"""

            qry = (
                session.query(Resource).filter(
                    Resource.type_name == resource_type_name)
            )

            resources = qry.all()
            return cls._find_resource_path(session, resources)

        @classmethod
        def _find_resource_path(cls, _, resources):
            """"""Find the list of transitive ancestors for the given resource.

            Args:
                _ (object): position holder
                resources (list): list of the resources to query

            Returns:
                list: list of Resources, transitive ancestors for the given
                    resource
            """"""

            if not resources:
                return []

            path = []
            resource = resources[0]

            path.append(resource)
            while resource.parent:
                resource = resource.parent
                path.append(resource)

            return path

        @classmethod
        def get_roles_by_permission_names(cls, session, permission_names):
            """"""Return the list of roles covering the specified permissions.

            Args:
                session (object): db session
                permission_names (list): permissions to be covered by.

            Returns:
                set: roles set that cover the permissions
            """"""

            permission_set = set(permission_names)
            qry = session.query(Permission)
            if permission_set:
                qry = qry.filter(Permission.name.in_(permission_set))
            permissions = qry.all()

            roles = set()
            for permission in permissions:
                for role in permission.roles:
                    roles.add(role)

            result_set = set()
            for role in roles:
                role_permissions = set(
                    [p.name for p in role.permissions])
                if permission_set.issubset(role_permissions):
                    result_set.add(role)

            return result_set

        @classmethod
        def get_member(cls, session, name):
            """"""Get member by name.

            Args:
                session (object): db session
                name (str): the name the member to query

            Returns:
                list: Members from the query
            """"""

            return session.query(Member).filter(Member.name == name).all()

    base.metadata.create_all(dbengine)
    return sessionmaker(bind=dbengine), ModelAccess",_10354.py,1075,"for binding in bindings.all():
    role = binding.role_name
    members = [m.name for m in binding.members]
    policy['bindings'][role] = members","policy[‘bindings'] = {binding.role_name: [m.name for m in binding.members]
for binding in bindings.all()}",dict merge,,
https://github.com/PaddlePaddle/PaddleHub/tree/master/modules/text/text_generation/plato2_en_large/tasks/dialog_generation.py,"def _post_process_generation_output(self, predictions):
        """"""
        Post process generation output.

        Calculate repetion, reranking.
        """"""
        for info in predictions:
            tokens = post_process_context(info[""context_token_ids""], self.reader)
            pred_token_ids, pred_tokens = post_process_response(info[""response_token_ids""], self.reader)
            info[""context""] = "" [SEP] "".join("" "".join(u) for u in tokens)
            info[""response""] = "" "".join(pred_tokens)
            info[""num_token""] = len(pred_token_ids)
            info[""cross_turn_repetition""] = get_cross_turn_repetition(tokens, pred_tokens, self.reader.eos_id,
                                                                      self.is_cn)
            info[""in_turn_repetition""] = max(
                get_in_turn_repetition(pred_tokens, self.is_cn), get_in_turn_repetition(pred_token_ids))
        if self.nsp_predictor is not None:
            get_nsp_score_batch(self.nsp_predictor, predictions)

        group = defaultdict(list)
        for info in predictions:
            group[info[""data_id""]].append(info)

        predictions = []
        for data_id in group:
            infos = group[data_id]
            for info in infos:
                info[""score""] = info[self.ranking_score]
                if self.max_dec_len is not None and info[""num_token""] >= self.max_dec_len:  # not ending
                    info[""score""] -= 1e3
                elif info[""cross_turn_repetition""] > 0:
                    info[""score""] -= 1e3
                elif info[""in_turn_repetition""] > 0:
                    info[""score""] -= 1e3
            infos = sorted(infos, key=lambda info: -info[""score""])
            pred = infos[0]
            keep_attr = [""data_id"", ""score"", ""response""]
            pred = {k: pred[k] for k in keep_attr}
            predictions.append(pred)
        return predictions",_10530.py,27,"for info in infos:
    info['score'] = info[self.ranking_score]
    if self.max_dec_len is not None and info['num_token'] >= self.max_dec_len:
        info['score'] -= 1000.0
    elif info['cross_turn_repetition'] > 0:
        info['score'] -= 1000.0
    elif info['in_turn_repetition'] > 0:
        info['score'] -= 1000.0","info = {‘score’: info[self.ranking_score]-1000.0 if self.max_dec_len is not None and info['num_token'] >= self.max_dec_len else info[self.ranking_score]-1000.0  if info['cross_turn_repetition'] > 0 else info[self.ranking_score]-1000.0 if info['in_turn_repetition'] > 0 else info[self.ranking_score]
for info in infos }",dict merge,,
https://github.com/openstates/openstates-scrapers/tree/master/scrapers/ca/bills.py,"def scrape_bill_type(
        self,
        chamber,
        session,
        bill_type,
        type_abbr,
        committee_abbr_regex=get_committee_name_regex(),
    ):
        bills = (
            self.session.query(CABill)
            .filter_by(session_year=session)
            .filter_by(measure_type=type_abbr)
        )

        archive_year = int(session[0:4])
        not_archive_year = archive_year >= 2009

        for bill in bills:
            bill_session = session
            if bill.session_num != ""0"":
                bill_session += "" Special Session %s"" % bill.session_num

            bill_id = bill.short_bill_id
            if bill_id.strip() == ""SB77"" and session == ""20052006"":
                continue

            fsbill = Bill(bill_id, bill_session, title="""", chamber=chamber)
            if (bill_id.startswith(""S"") and chamber == ""lower"") or (
                bill_id.startswith(""A"") and chamber == ""upper""
            ):
                print(""!!!! BAD ID/CHAMBER PAIR !!!!"", bill)
                continue

            # Construct a fake source url
            source_url = (
                ""http://leginfo.legislature.ca.gov/faces/""
                ""billNavClient.xhtml?bill_id=%s""
            ) % bill.bill_id

            fsbill.add_source(source_url)
            fsbill.add_version_link(bill_id, source_url, media_type=""text/html"")

            title = """"
            type_ = [""bill""]
            subject = """"
            all_titles = set()
            summary = """"

            # Get digest test (aka ""summary"") from latest version.
            if bill.versions and not_archive_year:
                version = bill.versions[-1]
                nsmap = version.xml.nsmap
                xpath = ""//caml:DigestText/xhtml:p""
                els = version.xml.xpath(xpath, namespaces=nsmap)
                chunks = []
                for el in els:
                    t = etree_text_content(el)
                    t = re.sub(r""\s+"", "" "", t)
                    t = re.sub(r""\)(\S)"", lambda m: "") %s"" % m.group(1), t)
                    chunks.append(t)
                summary = ""\n\n"".join(chunks)

            for version in bill.versions:
                if not version.bill_xml:
                    continue

                version_date = self._tz.localize(version.bill_version_action_date)

                # create a version name to match the state's format
                # 02/06/17 - Enrolled
                version_date_human = version_date.strftime(""%m/%d/%y"")
                version_name = ""{} - {}"".format(
                    version_date_human, version.bill_version_action
                )

                version_base = ""https://leginfo.legislature.ca.gov/faces""

                version_url_pdf = ""{}/billPdf.xhtml?bill_id={}&version={}"".format(
                    version_base, version.bill_id, version.bill_version_id
                )

                fsbill.add_version_link(
                    version_name,
                    version_url_pdf,
                    media_type=""application/pdf"",
                    date=version_date.date(),
                )

                # CA is inconsistent in that some bills have a short title
                # that is longer, more descriptive than title.
                if bill.measure_type in (""AB"", ""SB""):
                    impact_clause = clean_title(version.title)
                    title = clean_title(version.short_title)
                else:
                    impact_clause = None
                    if len(version.title) < len(
                        version.short_title
                    ) and not version.title.lower().startswith(""an act""):
                        title = clean_title(version.short_title)
                    else:
                        title = clean_title(version.title)

                if title:
                    all_titles.add(title)

                type_ = [bill_type]

                if version.appropriation == ""Yes"":
                    type_.append(""appropriation"")

                tags = []
                if version.fiscal_committee == ""Yes"":
                    tags.append(""fiscal committee"")
                if version.local_program == ""Yes"":
                    tags.append(""local program"")
                if version.urgency == ""Yes"":
                    tags.append(""urgency"")
                if version.taxlevy == ""Yes"":
                    tags.append(""tax levy"")

                if version.subject:
                    subject = clean_title(version.subject)

            if not title:
                self.warning(""Couldn't find title for %s, skipping"" % bill_id)
                continue

            fsbill.title = title
            if summary:
                fsbill.add_abstract(summary, note=""summary"")
            fsbill.classification = type_
            fsbill.subject = [subject] if subject else []
            fsbill.extras[""impact_clause""] = impact_clause
            fsbill.extras[""tags""] = tags

            # We don't want the current title in alternate_titles
            all_titles.remove(title)

            for title in all_titles:
                fsbill.add_title(title)

            for author in version.authors:
                fsbill.add_sponsorship(
                    author.name,
                    classification=SPONSOR_TYPES[author.contribution],
                    primary=author.primary_author_flg == ""Y"",
                    entity_type=""person"",
                )
                # fsbill.sponsorships[-1]['extras'] = {'official_type': author.contribution}

            seen_actions = set()
            for action in bill.actions:
                if not action.action:
                    # NULL action text seems to be an error on CA's part,
                    # unless it has some meaning I'm missing
                    continue
                actor = action.actor or chamber
                actor = actor.strip()
                match = re.match(r""(Assembly|Senate)($| \(Floor)"", actor)
                if match:
                    actor = {""Assembly"": ""lower"", ""Senate"": ""upper""}[match.group(1)]
                elif actor.startswith(""Governor""):
                    actor = ""executive""
                else:

                    def replacer(matchobj):
                        if matchobj:
                            return {""Assembly"": ""lower"", ""Senate"": ""upper""}[
                                matchobj.group()
                            ]
                        else:
                            return matchobj.group()

                    actor = re.sub(r""^(Assembly|Senate)"", replacer, actor)

                type_ = []

                act_str = action.action
                act_str = re.sub(r""\s+"", "" "", act_str)

                attrs = self.categorizer.categorize(act_str)

                # Add in the committee strings of the related committees, if any.
                kwargs = attrs
                matched_abbrs = committee_abbr_regex.findall(action.action)

                if re.search(r""Com[s]?. on"", action.action) and not matched_abbrs:
                    msg = ""Failed to extract committee abbr from %r.""
                    self.logger.warning(msg % action.action)

                if matched_abbrs:
                    committees = []
                    for abbr in matched_abbrs:
                        try:
                            name = self.committee_abbr_to_name(chamber, abbr)
                            committees.append(name)
                        except KeyError:
                            msg = (
                                ""Mapping contains no committee name for ""
                                ""abbreviation %r. Action text was %r.""
                            )
                            args = (abbr, action.action)
                            self.warning(msg % args)

                    committees = filter(None, committees)
                    kwargs[""committees""] = committees

                    code = re.search(r""C[SXZ]\d+"", actor)
                    if code is not None:
                        code = code.group()
                        kwargs[""actor_info""] = {""committee_code"": code}
                    if not_archive_year:
                        assert len(list(committees)) == len(matched_abbrs)
                    for committee, abbr in zip(committees, matched_abbrs):
                        act_str = act_str.replace(""Coms. on "", """")
                        act_str = act_str.replace(""Com. on "" + abbr, committee)
                        act_str = act_str.replace(abbr, committee)
                        if not act_str.endswith("".""):
                            act_str = act_str + "".""

                # Determine which chamber the action originated from.
                changed = False
                for committee_chamber in [""upper"", ""lower"", ""legislature""]:
                    if actor.startswith(committee_chamber):
                        actor = committee_chamber
                        changed = True
                        break
                if not changed:
                    actor = ""legislature""

                if actor != action.actor:
                    actor_info = kwargs.get(""actor_info"", {})
                    actor_info[""details""] = action.actor
                    kwargs[""actor_info""] = actor_info

                # Add strings for related legislators, if any.
                rgx = r""(?:senator|assembly[mwp][^ .,:;]+)\s+[^ .,:;]+""
                legislators = re.findall(rgx, action.action, re.I)
                if legislators:
                    kwargs[""legislators""] = legislators

                date = action.action_date
                date = self._tz.localize(date)
                date = date.date()
                if (actor, act_str, date) in seen_actions:
                    continue

                kwargs.update(self.categorizer.categorize(act_str))

                action = fsbill.add_action(
                    act_str,
                    date.strftime(""%Y-%m-%d""),
                    chamber=actor,
                    classification=kwargs[""classification""],
                )
                for committee in kwargs.get(""committees"", []):
                    action.add_related_entity(committee, entity_type=""organization"")
                seen_actions.add((actor, act_str, date))

            source_url = (
                ""http://leginfo.legislature.ca.gov/faces/billVotesClient.xhtml?""
            )
            source_url += f""bill_id={session}{bill.session_num}{fsbill.identifier}""

            # Votes for non archived years
            if archive_year > 2009:
                for vote_num, vote in enumerate(bill.votes):
                    if vote.vote_result == ""(PASS)"":
                        result = True
                    else:
                        result = False

                    if not vote.location:
                        continue

                    full_loc = vote.location.description
                    first_part = full_loc.split("" "")[0].lower()
                    if first_part in [""asm"", ""assembly""]:
                        vote_chamber = ""lower""
                        # vote_location = ' '.join(full_loc.split(' ')[1:])
                    elif first_part.startswith(""sen""):
                        vote_chamber = ""upper""
                        # vote_location = ' '.join(full_loc.split(' ')[1:])
                    else:
                        # raise ScrapeError(""Bad location: %s"" % full_loc) # To uncomment
                        continue

                    if vote.motion:
                        motion = vote.motion.motion_text or """"
                    else:
                        motion = """"

                    if ""Third Reading"" in motion or ""3rd Reading"" in motion:
                        vtype = ""passage""
                    elif ""Do Pass"" in motion:
                        vtype = ""passage""
                    else:
                        vtype = []

                    motion = motion.strip()
                    motion = re.compile(
                        r""(\w+)( Extraordinary)? Session$"", re.IGNORECASE
                    ).sub("""", motion)
                    motion = re.compile(r""^(Senate|Assembly) "", re.IGNORECASE).sub(
                        """", motion
                    )
                    motion = re.sub(
                        r""^(SCR|SJR|SB|AB|AJR|ACR)\s?\d+ \w+\.?  "", """", motion
                    )
                    motion = re.sub(r"" \(\w+\)$"", """", motion)
                    motion = re.sub(r""(SCR|SB|AB|AJR|ACR)\s?\d+ \w+\.?$"", """", motion)
                    motion = re.sub(
                        r""(SCR|SJR|SB|AB|AJR|ACR)\s?\d+ \w+\.? "" r""Urgency Clause$"",
                        ""(Urgency Clause)"",
                        motion,
                    )
                    motion = re.sub(r""\s+"", "" "", motion)

                    if not motion:
                        self.warning(""Got blank motion on vote for %s"" % bill_id)
                        continue

                    # XXX this is responsible for all the CA 'committee' votes, not
                    # sure if that's a feature or bug, so I'm leaving it as is...
                    # vote_classification = chamber if (vote_location == 'Floor') else 'committee'
                    # org = {
                    # 'name': vote_location,
                    # 'classification': vote_classification
                    # }

                    fsvote = VoteEvent(
                        motion_text=motion,
                        start_date=self._tz.localize(vote.vote_date_time),
                        result=""pass"" if result else ""fail"",
                        classification=vtype,
                        # organization=org,
                        chamber=vote_chamber,
                        bill=fsbill,
                    )
                    fsvote.extras = {""threshold"": vote.threshold}

                    fsvote.add_source(source_url)
                    fsvote.dedupe_key = source_url + ""#"" + str(vote_num)

                    rc = {""yes"": [], ""no"": [], ""other"": []}
                    for record in vote.votes:
                        if record.vote_code == ""AYE"":
                            rc[""yes""].append(record.legislator_name)
                        elif record.vote_code.startswith(""NO""):
                            rc[""no""].append(record.legislator_name)
                        else:
                            rc[""other""].append(record.legislator_name)

                    # Handle duplicate votes
                    for key in rc:
                        rc[key] = list(set(rc[key]))

                    for key, voters in rc.items():
                        for voter in voters:
                            fsvote.vote(key, voter)
                        # Set counts by summed votes for accuracy
                        fsvote.set_count(key, len(voters))

                    yield fsvote
            if len(bill.votes) > 0 and archive_year <= 2009:
                vote_page_url = (
                    ""http://leginfo.legislature.ca.gov/faces/billVotesClient.xhtml?""
                )
                vote_page_url += (
                    f""bill_id={session}{bill.session_num}{fsbill.identifier}""
                )

                # parse the bill data page, finding the latest html text
                data = self.get(vote_page_url).content
                doc = html.fromstring(data)
                doc.make_links_absolute(vote_page_url)
                num_of_votes = len(doc.xpath(""//div[@class='status']""))
                for vote_section in range(1, num_of_votes + 1):
                    lines = doc.xpath(
                        f""//div[@class='status'][{vote_section}]//div[@class='statusRow']""
                    )
                    date, result, motion, vtype, location = """", """", """", """", """"
                    votes = {}
                    for line in lines:
                        line = line.text_content().split()
                        if line[0] == ""Date"":
                            date = line[1]
                            date = datetime.datetime.strptime(date, ""%m/%d/%y"")
                            date = self._tz.localize(date)
                        elif line[0] == ""Result"":
                            result = ""pass"" if ""PASS"" in line[1] else ""fail""
                        elif line[0] == ""Motion"":
                            motion = "" "".join(line[1:])
                        elif line[0] == ""Location"":
                            location = "" "".join(line[1:])
                        elif len(line) > 1:
                            if line[0] == ""Ayes"" and line[1] != ""Count"":
                                votes[""yes""] = line[1:]
                            elif line[0] == ""Noes"" and line[1] != ""Count"":
                                votes[""no""] = line[1:]
                            elif line[0] == ""NVR"" and line[1] != ""Count"":
                                votes[""not voting""] = line[1:]
                    # Determine chamber based on location
                    first_part = location.split("" "")[0].lower()
                    vote_chamber = """"
                    if first_part in [""asm"", ""assembly""]:
                        vote_chamber = ""lower""
                    elif first_part.startswith(""sen""):
                        vote_chamber = ""upper""

                    if ""Third Reading"" in motion or ""3rd Reading"" in motion:
                        vtype = ""passage""
                    elif ""Do Pass"" in motion:
                        vtype = ""passage""
                    else:
                        vtype = ""other""
                    if len(motion) > 0:
                        fsvote = VoteEvent(
                            motion_text=motion,
                            start_date=date,
                            result=result,
                            classification=vtype,
                            chamber=vote_chamber,
                            bill=fsbill,
                        )
                        fsvote.add_source(vote_page_url)
                        fsvote.dedupe_key = vote_page_url + ""#"" + str(vote_section)

                        for how_voted, voters in votes.items():
                            for voter in voters:
                                voter = voter.replace("","", """")
                                fsvote.vote(how_voted, voter)
                        yield fsvote

            for analysis in bill.analyses:
                analysis_date = self._tz.localize(analysis.analysis_date)
                # create an analysis name to match the state's format
                # 05/31/20- Assembly Appropriations
                analysis_date_human = analysis_date.strftime(""%m/%d/%y"")
                analysis_name = ""{}- {}"".format(
                    analysis_date_human, analysis.committee_name
                )

                analysis_base = ""https://leginfo.legislature.ca.gov/faces""

                # unfortunately this just brings you to the analysis list
                # storing analysisId and analyzingOffice for a future POST request?
                analysis_url_pdf = ""{}/billAnalysisClient.xhtml?bill_id={}&analysisId={}&analyzingOffice={}"".format(
                    analysis_base,
                    analysis.bill_id,
                    analysis.analysis_id,
                    analysis.committee_name.replace("" "", ""+""),
                )

                fsbill.add_document_link(
                    analysis_name,
                    analysis_url_pdf,
                    classification=""analysis"",
                    date=analysis_date.date(),
                    media_type=""application/pdf"",
                    on_duplicate=""ignore"",
                )

            yield fsbill
            self.session.expire_all()",_10831.py,355,"for key in rc:
    rc[key] = list(set(rc[key]))","rc.update({key: list(set(rc[key]))
for key in rc})",dict merge,,
https://github.com/dask/dask/tree/master/dask/highlevelgraph.py,"def layer_info_dict(self):
        info = {
            ""layer_type"": type(self).__name__,
            ""is_materialized"": self.is_materialized(),
            ""number of outputs"": f""{len(self.get_output_keys())}"",
        }
        if self.annotations is not None:
            for key, val in self.annotations.items():
                info[key] = html.escape(str(val))
        if self.collection_annotations is not None:
            for key, val in self.collection_annotations.items():
                # Hide verbose chunk details from the HTML table
                if key != ""chunks"":
                    info[key] = html.escape(str(val))
        return info",_10930.py,8,"for (key, val) in self.annotations.items():
    info[key] = html.escape(str(val))","info.update({key: html.escape(str(val)) for (key, val) in self.annotations.items()})",dict merge,,
https://github.com/open-mmlab/mmfashion/tree/master/data/prepare_in_shop.py,"def split_label():
    id2label = {}
    labelf = open(os.path.join(PREFIX, 'list_attr_items.txt')).readlines()
    for line in labelf[2:]:
        aline = line.strip('\n').split()
        id, label = aline[0], aline[1:]
        id2label[id] = label

    def get_label(fn, prefix):
        rf = open(fn).readlines()
        wf = open(os.path.join(PREFIX, '%s_labels.txt' % prefix), 'w')
        for line in rf:
            aline = line.strip('\n').split('/')
            id = aline[3]
            label = id2label[id]
            for element in label:
                if element == '1':
                    wf.write('1 ')
                else:
                    wf.write('0 ')
            wf.write('\n')
        wf.close()

    get_label(os.path.join(PREFIX, 'train_img.txt'), 'train')
    get_label(os.path.join(PREFIX, 'gallery_img.txt'), 'gallery')
    get_label(os.path.join(PREFIX, 'query_img.txt'), 'query')",_11319.py,4,"for line in labelf[2:]:
    aline = line.strip('\n').split()
    (id, label) = (aline[0], aline[1:])
    id2label[id] = label",id2label = {line.strip(‘\n’).split()[0]: line.strip('\n').split()[1:for line in labelf[2:]},数据依赖,,
https://github.com/cpnota/autonomous-learning-library/tree/master/scripts/continuous.py,"def main():
    parser = argparse.ArgumentParser(description=""Run a continuous actions benchmark."")
    parser.add_argument(""env"", help=""Name of the env (e.g. 'lander', 'cheetah')"")
    parser.add_argument(
        ""agent"", help=""Name of the agent (e.g. ddpg). See presets for available agents.""
    )
    parser.add_argument(
        ""--device"",
        default=""cuda"",
        help=""The name of the device to run the agent on (e.g. cpu, cuda, cuda:0)."",
    )
    parser.add_argument(
        ""--frames"", type=int, default=2e6, help=""The number of training frames.""
    )
    parser.add_argument(
        ""--render"", action=""store_true"", default=False, help=""Render the environment.""
    )
    parser.add_argument(
        ""--logdir"", default='runs', help=""The base logging directory.""
    )
    parser.add_argument(""--logger"", default='tensorboard', help=""The backend used for tracking experiment metrics."")
    parser.add_argument(
        '--hyperparameters',
        default=[],
        nargs='*',
        help=""Custom hyperparameters, in the format hyperparameter1=value1 hyperparameter2=value2 etc.""
    )
    args = parser.parse_args()

    if args.env in ENVS:
        env = GymEnvironment(ENVS[args.env], device=args.device)
    elif 'BulletEnv' in args.env or args.env in PybulletEnvironment.short_names:
        env = PybulletEnvironment(args.env, device=args.device)
    else:
        env = GymEnvironment(args.env, device=args.device)

    agent_name = args.agent
    agent = getattr(continuous, agent_name)
    agent = agent.device(args.device)

    # parse hyperparameters
    hyperparameters = {}
    for hp in args.hyperparameters:
        key, value = hp.split('=')
        hyperparameters[key] = type(agent.default_hyperparameters[key])(value)
    agent = agent.hyperparameters(**hyperparameters)

    run_experiment(
        agent,
        env,
        frames=args.frames,
        render=args.render,
        logdir=args.logdir,
        logger=args.logger,
    )",_11426.py,43,"for hp in args.hyperparameters:
    (key, value) = hp.split('=')
    hyperparameters[key] = type(agent.default_hyperparameters[key])(value)",hyperparameters = {hp.split('=') [0]: type(agent.default_hyperparameters[key])(hp.split(‘=‘)[1]) for hp in args.hyperparameters},数据依赖,,
https://github.com/MrGiovanni/UNetPlusPlus/tree/master/pytorch/nnunet/evaluation/model_selection/rank_candidates_cascade.py,"if __name__ == ""__main__"":
    # run collect_all_fold0_results_and_summarize_in_one_csv.py first
    summary_files_dir = join(network_training_output_dir, ""summary_jsons_fold0_new"")
    output_file = join(network_training_output_dir, ""summary_cascade.csv"")

    folds = (0, )
    folds_str = """"
    for f in folds:
        folds_str += str(f)

    plans = ""nnUNetPlansv2.1""

    overwrite_plans = {
        'nnUNetTrainerCascadeFullRes': ['nnUNetPlans'],
    }

    trainers = [
        'nnUNetTrainerCascadeFullRes',
        'nnUNetTrainerV2CascadeFullRes_EducatedGuess',
        'nnUNetTrainerV2CascadeFullRes_EducatedGuess2',
        'nnUNetTrainerV2CascadeFullRes_EducatedGuess3',
        'nnUNetTrainerV2CascadeFullRes_lowerLR',
        'nnUNetTrainerV2CascadeFullRes',
        'nnUNetTrainerV2CascadeFullRes_noConnComp',
        'nnUNetTrainerV2CascadeFullRes_shorter_lowerLR',
        'nnUNetTrainerV2CascadeFullRes_shorter',
        'nnUNetTrainerV2CascadeFullRes_smallerBinStrel',
        #'',
        #'',
        #'',
        #'',
        #'',
        #'',
    ]

    datasets = \
        {
        ""Task003_Liver"": (""3d_cascade_fullres"", ),
        ""Task006_Lung"": (""3d_cascade_fullres"", ),
        ""Task007_Pancreas"": (""3d_cascade_fullres"", ),
        ""Task008_HepaticVessel"": (""3d_cascade_fullres"", ),
        ""Task009_Spleen"": (""3d_cascade_fullres"", ),
        ""Task010_Colon"": (""3d_cascade_fullres"", ),
        ""Task017_AbdominalOrganSegmentation"": (""3d_cascade_fullres"", ),
        #""Task029_LITS"": (""3d_cascade_fullres"", ),
        ""Task048_KiTS_clean"": (""3d_cascade_fullres"", ),
        ""Task055_SegTHOR"": (""3d_cascade_fullres"", ),
        ""Task056_VerSe"": (""3d_cascade_fullres"", ),
        #"""": (""3d_cascade_fullres"", ),
        }

    expected_validation_folder = ""validation_raw""
    alternative_validation_folder = ""validation""
    alternative_alternative_validation_folder = ""validation_tiledTrue_doMirror_True""

    interested_in = ""mean""

    result_per_dataset = {}
    for d in datasets:
        result_per_dataset[d] = {}
        for c in datasets[d]:
            result_per_dataset[d][c] = []

    valid_trainers = []
    all_trainers = []

    with open(output_file, 'w') as f:
        f.write(""trainer,"")
        for t in datasets.keys():
            s = t[4:7]
            for c in datasets[t]:
                s1 = s + ""_"" + c[3]
                f.write(""%s,"" % s1)
        f.write(""\n"")

        for trainer in trainers:
            trainer_plans = [plans]
            if trainer in overwrite_plans.keys():
                trainer_plans = overwrite_plans[trainer]

            result_per_dataset_here = {}
            for d in datasets:
                result_per_dataset_here[d] = {}

            for p in trainer_plans:
                name = ""%s__%s"" % (trainer, p)
                all_present = True
                all_trainers.append(name)

                f.write(""%s,"" % name)
                for dataset in datasets.keys():
                    for configuration in datasets[dataset]:
                        summary_file = join(summary_files_dir, ""%s__%s__%s__%s__%s__%s.json"" % (dataset, configuration, trainer, p, expected_validation_folder, folds_str))
                        if not isfile(summary_file):
                            summary_file = join(summary_files_dir, ""%s__%s__%s__%s__%s__%s.json"" % (dataset, configuration, trainer, p, alternative_validation_folder, folds_str))
                            if not isfile(summary_file):
                                summary_file = join(summary_files_dir, ""%s__%s__%s__%s__%s__%s.json"" % (
                                dataset, configuration, trainer, p, alternative_alternative_validation_folder, folds_str))
                                if not isfile(summary_file):
                                    all_present = False
                                    print(name, dataset, configuration, ""has missing summary file"")
                        if isfile(summary_file):
                            result = load_json(summary_file)['results'][interested_in]['mean']['Dice']
                            result_per_dataset_here[dataset][configuration] = result
                            f.write(""%02.4f,"" % result)
                        else:
                            f.write(""NA,"")
                            result_per_dataset_here[dataset][configuration] = 0

                f.write(""\n"")

                if True:
                    valid_trainers.append(name)
                    for d in datasets:
                        for c in datasets[d]:
                            result_per_dataset[d][c].append(result_per_dataset_here[d][c])

    invalid_trainers = [i for i in all_trainers if i not in valid_trainers]

    num_valid = len(valid_trainers)
    num_datasets = len(datasets.keys())
    # create an array that is trainer x dataset. If more than one configuration is there then use the best metric across the two
    all_res = np.zeros((num_valid, num_datasets))
    for j, d in enumerate(datasets.keys()):
        ks = list(result_per_dataset[d].keys())
        tmp = result_per_dataset[d][ks[0]]
        for k in ks[1:]:
            for i in range(len(tmp)):
                tmp[i] = max(tmp[i], result_per_dataset[d][k][i])
        all_res[:, j] = tmp

    ranks_arr = np.zeros_like(all_res)
    for d in range(ranks_arr.shape[1]):
        temp = np.argsort(all_res[:, d])[::-1] # inverse because we want the highest dice to be rank0
        ranks = np.empty_like(temp)
        ranks[temp] = np.arange(len(temp))

        ranks_arr[:, d] = ranks

    mn = np.mean(ranks_arr, 1)
    for i in np.argsort(mn):
        print(mn[i], valid_trainers[i])

    print()
    print(valid_trainers[np.argmin(mn)])",_11494.py,59,"for d in datasets:
    result_per_dataset[d] = {}
    for c in datasets[d]:
        result_per_dataset[d][c] = []","result_per_dataset = {d: {c: [] for c in datasets[d]}
for d in datasets}",嵌套的情况,,
https://github.com/mysql/mysql-connector-python/tree/master/cpydist/utils.py,"def _parse_release_file(release_file):
    """"""Parse the contents of /etc/lsb-release or /etc/os-release file.

    Returns:
        A dictionary containing release information.
    """"""
    distro = {}
    if os.path.exists(release_file):
        with open(release_file) as file_obj:
            for line in file_obj:
                key_value = line.split(""="")
                if len(key_value) != 2:
                    continue
                key = key_value[0].lower()
                value = key_value[1].rstrip(""\n"").strip('""')
                distro[key] = value
    return distro",_12165.py,10,"for line in file_obj:
    key_value = line.split('=')
    if len(key_value) != 2:
        continue
    key = key_value[0].lower()
    value = key_value[1].rstrip('\n').strip('""')
    distro[key] = value","distro = {line.split(‘=‘)[0].lower(): line.split('=')[1].rstrip('\n').strip('""')
for line in file_obj if if len(line.split('=')) == 2}",数据依赖,,
https://github.com/openstack/swift/tree/master/swift/cli/dispersion_report.py,"def container_dispersion_report(coropool, connpool, account, container_ring,
                                retries, output_missing_partitions, policy):
    with connpool.item() as conn:
        containers = [c['name'] for c in conn.get_account(
            prefix='dispersion_%d' % policy.idx, full_listing=True)[1]]
    containers_listed = len(containers)
    if not containers_listed:
        print('No containers to query. Has '
              'swift-dispersion-populate been run?', file=stderr)
        stderr.flush()
        return
    retries_done = [0]
    containers_queried = [0]
    container_copies_missing = defaultdict(int)
    container_copies_found = [0]
    container_copies_expected = [0]
    begun = time()
    next_report = [time() + 2]

    def direct(container, part, nodes):
        found_count = 0
        for node in nodes:
            error_log = get_error_log(node_to_string(node))
            try:
                attempts, _junk = direct_client.retry(
                    direct_client.direct_head_container, node, part, account,
                    container, error_log=error_log, retries=retries)
                retries_done[0] += attempts - 1
                found_count += 1
            except ClientException as err:
                if err.http_status not in (404, 507):
                    error_log('Giving up on /%s/%s/%s: %s' % (part, account,
                              container, err))
            except (Exception, Timeout) as err:
                error_log('Giving up on /%s/%s/%s: %s' % (part, account,
                          container, err))
        if output_missing_partitions and \
                found_count < len(nodes):
            missing = len(nodes) - found_count
            print('\r\x1B[K', end='')
            stdout.flush()
            print('# Container partition %s missing %s cop%s' % (
                part, missing, 'y' if missing == 1 else 'ies'), file=stderr)
        container_copies_found[0] += found_count
        containers_queried[0] += 1
        container_copies_missing[len(nodes) - found_count] += 1
        if time() >= next_report[0]:
            next_report[0] = time() + 5
            eta, eta_unit = compute_eta(begun, containers_queried[0],
                                        containers_listed)
            if not json_output:
                print('\r\x1B[KQuerying containers: %d of %d, %d%s left, %d '
                      'retries' % (containers_queried[0], containers_listed,
                                   round(eta), eta_unit, retries_done[0]),
                      end='')
                stdout.flush()
    container_parts = {}
    for container in containers:
        part, nodes = container_ring.get_nodes(account, container)
        if part not in container_parts:
            container_copies_expected[0] += len(nodes)
            container_parts[part] = part
            coropool.spawn(direct, container, part, nodes)
    coropool.waitall()
    distinct_partitions = len(container_parts)
    copies_found = container_copies_found[0]
    copies_expected = container_copies_expected[0]
    value = 100.0 * copies_found / copies_expected
    elapsed, elapsed_unit = get_time_units(time() - begun)
    container_copies_missing.pop(0, None)
    if not json_output:
        print('\r\x1B[KQueried %d containers for dispersion reporting, '
              '%d%s, %d retries' % (containers_listed, round(elapsed),
                                    elapsed_unit, retries_done[0]))
        if containers_listed - distinct_partitions:
            print('There were %d overlapping partitions' % (
                  containers_listed - distinct_partitions))
        for missing_copies, num_parts in container_copies_missing.items():
            print(missing_string(num_parts, missing_copies,
                                 container_ring.replica_count))
        print('%.02f%% of container copies found (%d of %d)' % (
            value, copies_found, copies_expected))
        print('Sample represents %.02f%% of the container partition space' % (
            100.0 * distinct_partitions / container_ring.partition_count))
        stdout.flush()
        return None
    else:
        results = {'retries': retries_done[0],
                   'overlapping': containers_listed - distinct_partitions,
                   'pct_found': value,
                   'copies_found': copies_found,
                   'copies_expected': copies_expected}
        for missing_copies, num_parts in container_copies_missing.items():
            results['missing_%d' % (missing_copies)] = num_parts
        return results",_12351.py,93,"for (missing_copies, num_parts) in container_copies_missing.items():
    results['missing_%d' % missing_copies] = num_parts","results.update({'missing_%d' % missing_copies: num_parts for (missing_copies, num_parts) in container_copies_missing.items()})",dict merge,,
https://github.com/PaddlePaddle/PGL/tree/master/pgl/graph.py,"def _apply_to_numpy(self, key, value, inplace=True):
        if value is None:
            return value

        if key == '_is_tensor':
            # set is_tensor to True
            return False

        if isinstance(value, EdgeIndex):
            value = value.numpy(inplace=inplace)
        elif isinstance(value, dict):
            if inplace:
                for k, v in value.items():
                    value[k] = v.numpy()
            else:
                new_value = {}
                for k, v in value.items():
                    new_value[k] = v.numpy()
                value = new_value
        else:
            value = value.numpy()
        return value",_12640.py,13,"for (k, v) in value.items():
    value[k] = v.numpy()","value.update({k: v.numpy() for (k, v) in value.items()})","if dict
值得注意",,
https://github.com/PaddlePaddle/PGL/tree/master/pgl/graph.py,"def _apply_to_numpy(self, key, value, inplace=True):
        if value is None:
            return value

        if key == '_is_tensor':
            # set is_tensor to True
            return False

        if isinstance(value, EdgeIndex):
            value = value.numpy(inplace=inplace)
        elif isinstance(value, dict):
            if inplace:
                for k, v in value.items():
                    value[k] = v.numpy()
            else:
                new_value = {}
                for k, v in value.items():
                    new_value[k] = v.numpy()
                value = new_value
        else:
            value = value.numpy()
        return value",_12640.py,17,"for (k, v) in value.items():
    new_value[k] = v.numpy()","new_value.update({k: v.numpy() for (k, v) in value.items()})","if dict
值得注意",,
https://github.com/IndicoDataSolutions/finetune/tree/master/finetune/util/imbalance.py,"def _compute_ratios(counts, n_total, multilabel=False):
    computed_ratios = {}
    max_count = max(counts.values())
    for class_name, count in counts.items():
        if multilabel:
            ratio = (n_total - count) / count
        else:
            ratio = ratio = max_count / count
        computed_ratios[class_name] = ratio
    return computed_ratios",_12671.py,4,"for (class_name, count) in counts.items():
    if multilabel:
        ratio = (n_total - count) / count
    else:
        ratio = ratio = max_count / count
    computed_ratios[class_name] = ratio","computed_ratios = {class_name: (n_total - count) / count if multilabel else class_name: ratio = max_count / count
for (class_name, count) in counts.items()}",数据依赖,,
https://github.com/lovit/soynlp/tree/master/soynlp/utils/utils.py,"def _counting_from_sents(self, sents):
        check_corpus(sents)

        _counter = {}
        for i_sent, sent in enumerate(sents):
            sent = self.preprocess(sent)
            # filtering during eojeol counting
            if (self.min_count > 1 and
                self.filtering_checkpoint > 0 and
                i_sent > 0 and
                i_sent % self.filtering_checkpoint == 0):
                _counter = {k:v for k,v in _counter.items()
                            if v >= self.min_count}
            # add eojeol count
            for eojeol in sent.split():
                if (not eojeol) or (len(eojeol) > self.max_length):
                    continue
                _counter[eojeol] = _counter.get(eojeol, 0) + 1
            # print status
            if self.verbose and i_sent % 100000 == 99999:
                print('\r[EojeolCounter] n eojeol = {} from {} sents. mem={} Gb{}'.format(
                    len(_counter), i_sent + 1, '%.3f'%get_process_memory(), ' '*20), flush=True, end='')
        # final filtering
        _counter = {k:v for k,v in _counter.items() if v >= self.min_count}
        if self.verbose:
            print('\r[EojeolCounter] n eojeol = {} from {} sents. mem={} Gb{}'.format(
                len(_counter), i_sent + 1, '%.3f'%get_process_memory(), ' '*20), flush=True)
        return _counter",_12795.py,15,"for eojeol in sent.split():
    if not eojeol or len(eojeol) > self.max_length:
        continue
    _counter[eojeol] = _counter.get(eojeol, 0) + 1","_counter.update( {eojeol: _counter.get(eojeol, 0) + 1
for eojeol in sent.split() if eojeol and len(eojeol) > self.max_length})",continue,,
https://github.com/freeipa/freeipa/tree/master//pylint_plugins.py,"def open(self):
        self._dir = os.path.abspath(os.path.dirname(__file__))

        self._forbidden_imports = {self._dir: []}
        for forbidden_import in self.config.forbidden_imports:
            forbidden_import = forbidden_import.split(':')
            path = os.path.join(self._dir, forbidden_import[0])
            path = os.path.abspath(path)
            modules = forbidden_import[1:]
            self._forbidden_imports[path] = modules

        self._forbidden_imports_stack = []",_12889.py,5,"for forbidden_import in self.config.forbidden_imports:
    forbidden_import = forbidden_import.split(':')
    path = os.path.join(self._dir, forbidden_import[0])
    path = os.path.abspath(path)
    modules = forbidden_import[1:]
    self._forbidden_imports[path] = modules","self._forbidden_imports.update({os.path.abspath(os.path.join(self._dir, forbidden_import[0])): forbidden_import.split(':')[1:]
for forbidden_import in self.config.forbidden_imports})",数据依赖,,
https://github.com/zhunzhong07/Random-Erasing/tree/master/utils/logger.py,"def __init__(self, fpath, title=None, resume=False): 
        self.file = None
        self.resume = resume
        self.title = '' if title == None else title
        if fpath is not None:
            if resume: 
                self.file = open(fpath, 'r') 
                name = self.file.readline()
                self.names = name.rstrip().split('\t')
                self.numbers = {}
                for _, name in enumerate(self.names):
                    self.numbers[name] = []

                for numbers in self.file:
                    numbers = numbers.rstrip().split('\t')
                    for i in range(0, len(numbers)):
                        self.numbers[self.names[i]].append(numbers[i])
                self.file.close()
                self.file = open(fpath, 'a')  
            else:
                self.file = open(fpath, 'w')",_14006.py,11,"for (_, name) in enumerate(self.names):
    self.numbers[name] = []","self.numbers = {name: [] for (_, name) in enumerate(self.names)}",中间可能使用过self,,
https://github.com/zentralopensource/zentral/tree/master/zentral/core/secret_engines/backends/gcp_kms.py,"def _prepared_context(context):
        prepared_context = {}
        for k, v in context.items():
            if not isinstance(v, str):
                v = str(v)
            prepared_context[k] = v
        return json.dumps(prepared_context, ensure_ascii=False, sort_keys=True).encode(""utf-8"")",_14458.py,3,"for (k, v) in context.items():
    if not isinstance(v, str):
        v = str(v)
    prepared_context[k] = v","prepared_context ={k: str(v) if not isinstance(v, str) else k: v for (k, v) in context.items()}",if语句,,
https://github.com/Project-MONAI/MONAI/tree/master/monai/transforms/utility/dictionary.py,"def __call__(self, data: Mapping[Hashable, Any]) -> Dict[Hashable, Any]:
        d = dict(data)
        for key in self.key_iterator(d):
            d[key] = self.converter(d[key])
        return d",_14615.py,3,"for key in self.key_iterator(d):
    d[key] = self.converter(d[key])",d.update({key: self.converter(d[key]),"dict merge
 d = dict(data)",,
https://github.com/google/TensorNetwork/tree/master/tensornetwork/quantum/quantum.py,"def eliminate_identities(nodes: Collection[AbstractNode]) -> Tuple[dict, dict]:
  """"""Eliminates any connected CopyNodes that are identity matrices.

  This will modify the network represented by `nodes`.
  Only identities that are connected to other nodes are eliminated.

  Args:
    nodes: Collection of nodes to search.
  Returns:
    nodes_dict: Dictionary mapping remaining Nodes to any replacements.
    dangling_edges_dict: Dictionary specifying all dangling-edge replacements.
  """"""
  nodes_dict = {}
  dangling_edges_dict = {}
  for n in nodes:
    if isinstance(
        n, CopyNode) and n.get_rank() == 2 and not (n[0].is_dangling() and
                                                    n[1].is_dangling()):
      old_edges = [n[0], n[1]]
      _, new_edges = remove_node(n)
      if 0 in new_edges and 1 in new_edges:
        e = connect(new_edges[0], new_edges[1])
      elif 0 in new_edges:  # 1 was dangling
        dangling_edges_dict[old_edges[1]] = new_edges[0]
      elif 1 in new_edges:  # 0 was dangling
        dangling_edges_dict[old_edges[0]] = new_edges[1]
      else:
        # Trace of identity, so replace with a scalar node!
        d = n.get_dimension(0)
        # NOTE: Assume CopyNodes have numpy dtypes.
        nodes_dict[n] = Node(np.array(d, dtype=n.dtype), backend=n.backend)
    else:
      for e in n.get_all_dangling():
        dangling_edges_dict[e] = e
      nodes_dict[n] = n

  return nodes_dict, dangling_edges_dict",_154.py,33,"for e in n.get_all_dangling():
    dangling_edges_dict[e] = e",dangling_edges_dict.update({e: e for e in n.get_all_dangling()}),1,nan,nan
https://github.com/gavin66/proxy_list/tree/master/persistence/redis_impl.py,"def list(self, count=1, query=None, columns=None):
        query_list = list()
        if query:
            for k, v in query.items():
                if k in self._index_keys:
                    query_list.append('index_%s_%s' % (k, v))
            keys = list(self._client.sinter(query_list))
            keys.sort(key=lambda x: float(self._client.zscore('index_speed', x)))
            if isinstance(count, int):
                keys = keys[:count]
        else:
            start = 0
            if isinstance(count, str):
                count = None
                start = None
            keys = list(self._client.zrangebyscore('index_speed', '-inf', '+inf', start=start, num=count))
        proxies = []
        for key in keys:
            proxy = self._client.hgetall(key)
            if isinstance(columns, tuple) and len(columns):
                x = {}
                for k in columns:
                    if k.encode('utf-8') in proxy.keys():
                        x[k] = proxy[k.encode('utf-8')].decode('utf-8')
                proxies.append(x)
            elif isinstance(columns, str) and columns == 'all':
                # ip, port, country, address, anonymity, protocol, speed
                proxies.append({x.decode('utf-8'): y.decode('utf-8') for x, y in proxy.items()})
            else:
                proxies.append(
                    (proxy[b'ip'].decode('utf-8'), proxy[b'port'].decode('utf-8'))
                )
        return proxies",_565.py,22,"for k in columns:
    if k.encode('utf-8') in proxy.keys():
        x[k] = proxy[k.encode('utf-8')].decode('utf-8')",x = {k: proxy[k.encode('utf-8')].decode('utf-8') for k in columns if k.encode('utf-8') in proxy.keys()},1,nan,nan
https://github.com/gcollazo/BrowserRefresh-Sublime/tree/master/win/pywinauto/controls/Accessability HwndWrapper.py,"def GetProperties(self):
        ""Return the properties of the control as a dictionary""
        props = {}

        # for each of the properties that can be written out
        for propname in self.writable_props:
            # set the item in the props dictionary keyed on the propname
            props[propname] = getattr(self, propname)()

        if self._NeedsImageProp:
            props[""Image""] = self.CaptureAsImage()

        return props",_576.py,6,"for propname in self.writable_props:
    props[propname] = getattr(self, propname)()","props = {propname: getattr(self, propname)() for propname in self.writable_props}",1,nan,nan
https://github.com/imageio/imageio/tree/master/imageio/plugins/_freeimage.py,"def set_meta_data(self, metadata):

        # Create a dict mapping model_name to number
        models = {}
        for name, number in METADATA_MODELS.__dict__.items():
            if name.startswith(""FIMD_""):
                models[name[5:]] = number

        # Create a mapping from numpy.dtype to METADATA_DATATYPE
        def get_tag_type_number(dtype):
            for number, numpy_dtype in METADATA_DATATYPE.dtypes.items():
                if dtype == numpy_dtype:
                    return number
            else:
                return None

        with self._fi as lib:

            for model_name, subdict in metadata.items():

                # Get model number
                number = models.get(model_name, None)
                if number is None:
                    continue  # Unknown model, silent ignore

                for tag_name, tag_val in subdict.items():

                    # Create new tag
                    tag = lib.FreeImage_CreateTag()
                    tag = ctypes.c_void_p(tag)

                    try:
                        # Convert Python value to FI type, val
                        is_ascii = False
                        if isinstance(tag_val, str):
                            try:
                                tag_bytes = tag_val.encode(""ascii"")
                                is_ascii = True
                            except UnicodeError:
                                pass
                        if is_ascii:
                            tag_type = METADATA_DATATYPE.FIDT_ASCII
                            tag_count = len(tag_bytes)
                        else:
                            if not hasattr(tag_val, ""dtype""):
                                tag_val = numpy.array([tag_val])
                            tag_type = get_tag_type_number(tag_val.dtype)
                            if tag_type is None:
                                logger.warning(
                                    ""imageio.freeimage warning: Could not ""
                                    ""determine tag type of %r."" % tag_name
                                )
                                continue
                            tag_bytes = tag_val.tobytes()
                            tag_count = tag_val.size
                        # Set properties
                        lib.FreeImage_SetTagKey(tag, tag_name.encode(""utf-8""))
                        lib.FreeImage_SetTagType(tag, tag_type)
                        lib.FreeImage_SetTagCount(tag, tag_count)
                        lib.FreeImage_SetTagLength(tag, len(tag_bytes))
                        lib.FreeImage_SetTagValue(tag, tag_bytes)
                        # Store tag
                        tag_key = lib.FreeImage_GetTagKey(tag)
                        lib.FreeImage_SetMetadata(number, self._bitmap, tag_key, tag)

                    except Exception as err:  # pragma: no cover
                        logger.warning(
                            ""imagio.freeimage warning: Could not set tag ""
                            ""%r: %s, %s""
                            % (tag_name, self._fi._get_error_message(), str(err))
                        )
                    finally:
                        lib.FreeImage_DeleteTag(tag)",_610.py,5,"for (name, number) in METADATA_MODELS.__dict__.items():
    if name.startswith('FIMD_'):
        models[name[5:]] = number","models = {name[5:]: number for (name, number) in METADATA_MODELS.__dict__.items() if name.startswith('FIMD_')}",1,nan,nan
https://github.com/facebookresearch/ReAgent/tree/master/reagent/training/gradient_free/evolution_pool.py,"def __init__(
        self,
        seed: int,
        es_params: EvolutionParameters,
        tensor_sizes: Dict[str, List[int]],
    ) -> None:
        self.es_params = es_params
        self.tensor_sizes = tensor_sizes
        self.seed = seed
        assert self.seed < MAX_RNG_SEED, ""The random seed must be less than "" + str(
            MAX_RNG_SEED
        )
        logger.info(""Starting pool with RNG seed: "" + str(self.seed))

        # Fill the population with empty values: will populate later
        self.population_tensors: List[Dict[str, torch.Tensor]] = []
        for _ in range(es_params.population_size):
            individual = {}
            for tensor_name, tensor_size in self.tensor_sizes.items():
                individual[tensor_name] = torch.zeros(tensor_size, dtype=torch.float)
            self.population_tensors.append(individual)

        torch.manual_seed(self.seed)
        self.parent_tensors: Dict[str, torch.Tensor] = {}
        for tensor_name, tensor_size in self.tensor_sizes.items():
            self.parent_tensors[tensor_name] = torch.randn(
                tensor_size, dtype=torch.float
            )
            self.parent_tensors[tensor_name].grad = torch.randn(
                tensor_size, dtype=torch.float
            )

        self.optimizer = torch.optim.Adam(
            self.parent_tensors.values(), lr=self.es_params.learning_rate
        )

        self.populate_children(0)",_1484.py,19,"for (tensor_name, tensor_size) in self.tensor_sizes.items():
    individual[tensor_name] = torch.zeros(tensor_size, dtype=torch.float)","individual = {tensor_name: torch.zeros(tensor_size, dtype=torch.float) for (tensor_name, tensor_size) in self.tensor_sizes.items()}",1,nan,nan
https://github.com/saltstack/salt/tree/master/salt/modules/win_status.py,"def meminfo():
    """"""
    Return information about physical and virtual memory on the system

    Returns:
        dict: A dictionary of information about memory on the system

    CLI Example:

    .. code-block:: bash

        salt * status.meminfo
    """"""
    # Get physical memory
    vm_total, vm_available, vm_percent, vm_used, vm_free = psutil.virtual_memory()
    # Get swap memory
    swp_total, swp_used, swp_free, swp_percent, _, _ = psutil.swap_memory()

    def get_unit_value(memory):
        symbols = (""K"", ""M"", ""G"", ""T"", ""P"", ""E"", ""Z"", ""Y"")
        prefix = {}
        for i, s in enumerate(symbols):
            prefix[s] = 1 << (i + 1) * 10
        for s in reversed(symbols):
            if memory >= prefix[s]:
                value = float(memory) / prefix[s]
                return {""unit"": s, ""value"": value}
        return {""unit"": ""B"", ""value"": memory}

    return {
        ""VmallocTotal"": get_unit_value(vm_total),
        ""VmallocUsed"": get_unit_value(vm_used),
        ""VmallocFree"": get_unit_value(vm_free),
        ""VmallocAvail"": get_unit_value(vm_available),
        ""SwapTotal"": get_unit_value(swp_total),
        ""SwapUsed"": get_unit_value(swp_used),
        ""SwapFree"": get_unit_value(swp_free),
    }",_1583.py,22,"for (i, s) in enumerate(symbols):
    prefix[s] = 1 << (i + 1) * 10","prefix = {s: 1 << (i + 1) * 10 for (i, s) in enumerate(symbols)}",1,nan,nan
https://github.com/facebookresearch/xformers/tree/master/xformers/factory/model_factory.py,"def __init__(
        self,
        stack_configs: Union[List[Dict[str, Any]], Dict[str, Dict[str, Any]]],
        tie_embedding_weights: bool = False,
        weight_init: xFormerWeightInit = xFormerWeightInit.ViT,
    ):
        # Type all the configurations. Possible typos are caught here
        if isinstance(stack_configs, dict):
            self.stack_configs = {}
            for k, config in stack_configs.items():
                if config[""block_type""] == ""encoder"":
                    self.stack_configs[k] = xFormerEncoderConfig(**config)
                else:
                    self.stack_configs[k] = xFormerDecoderConfig(**config)
        else:
            self.stack_configs = []
            for config in stack_configs:
                if config[""block_type""] == ""encoder"":
                    self.stack_configs.append(xFormerEncoderConfig(**config))
                else:
                    self.stack_configs.append(xFormerDecoderConfig(**config))

        self.tie_embedding_weights = tie_embedding_weights
        self.weight_init = weight_init",_1762.py,10,"for (k, config) in stack_configs.items():
    if config['block_type'] == 'encoder':
        self.stack_configs[k] = xFormerEncoderConfig(**config)
    else:
        self.stack_configs[k] = xFormerDecoderConfig(**config)","self.stack_configs = {k: xFormerEncoderConfig(**config) if config['block_type'] == 'encoder' else xFormerDecoderConfig(**config) for (k, config) in stack_configs.items()}",1,nan,nan
https://github.com/panchunguang/ccks_baidu_entity_link/tree/master/code/evaluate.py,"def get_index_bio_dict():
    bio_dict=get_bio_dict()
    index_bio={}
    for k in bio_dict:
        index_bio[bio_dict[k]]=k
    index_bio[3]='O'
    index_bio[4] = 'O'
    return index_bio",_1800.py,4,"for k in bio_dict:
    index_bio[bio_dict[k]] = k",index_bio = {bio_dict[k]: k for k in bio_dict},1,nan,nan
https://github.com/gunthercox/ChatterBot/tree/master/chatterbot/storage/sql_storage.py,"def create_many(self, statements):
        """"""
        Creates multiple statement entries.
        """"""
        Statement = self.get_model('statement')
        Tag = self.get_model('tag')

        session = self.Session()

        create_statements = []
        create_tags = {}

        for statement in statements:

            statement_data = statement.serialize()
            tag_data = statement_data.pop('tags', [])

            statement_model_object = Statement(**statement_data)

            if not statement.search_text:
                statement_model_object.search_text = self.tagger.get_text_index_string(statement.text)

            if not statement.search_in_response_to and statement.in_response_to:
                statement_model_object.search_in_response_to = self.tagger.get_text_index_string(statement.in_response_to)

            new_tags = set(tag_data) - set(create_tags.keys())

            if new_tags:
                existing_tags = session.query(Tag).filter(
                    Tag.name.in_(new_tags)
                )

                for existing_tag in existing_tags:
                    create_tags[existing_tag.name] = existing_tag

            for tag_name in tag_data:
                if tag_name in create_tags:
                    tag = create_tags[tag_name]
                else:
                    # Create the tag if it does not exist
                    tag = Tag(name=tag_name)

                    create_tags[tag_name] = tag

                statement_model_object.tags.append(tag)
            create_statements.append(statement_model_object)

        session.add_all(create_statements)
        session.commit()",_1826.py,33,"for existing_tag in existing_tags:
    create_tags[existing_tag.name] = existing_tag",create_tags.update({existing_tag.name: existing_tag for existing_tag in existing_tags}),1,nan,nan
https://github.com/Ultimaker/Cura/tree/master/tests/TestThemes.py,"def test_deprecatedIconsExist(theme_path: str) -> None:
    icons_folder = os.path.join(theme_path, ""icons"")
    deprecated_icons_file = os.path.join(icons_folder, ""deprecated_icons.json"")
    if not os.path.exists(deprecated_icons_file):
        return  # No deprecated icons file, there is nothing to go wrong.

    # Find out which icons exist in this theme file.
    existing_icons = {}
    for size in [subfolder for subfolder in os.listdir(icons_folder) if os.path.isdir(os.path.join(icons_folder, subfolder))]:
        existing_icons[size] = set(os.path.splitext(fname)[0] for fname in os.listdir(os.path.join(icons_folder, size)))

    with open(deprecated_icons_file) as f:
        deprecated_icons = json.load(f)

    for entry in deprecated_icons.values():
        assert ""new_icon"" in entry  # For each deprecated icon we must know which icon replaced it.
        new_icon = entry[""new_icon""]
        assert ""size"" in entry
        size = entry[""size""]

        assert size in existing_icons  # The replacement icon must have a size that exists.
        assert new_icon in existing_icons[size]",_2162.py,9,"for size in [subfolder for subfolder in os.listdir(icons_folder) if os.path.isdir(os.path.join(icons_folder, subfolder))]:
    existing_icons[size] = set((os.path.splitext(fname)[0] for fname in os.listdir(os.path.join(icons_folder, size))))","existing_icons = {size: set((os.path.splitext(fname)[0] for fname in os.listdir(os.path.join(icons_folder, size)))) for size in [subfolder for subfolder in os.listdir(icons_folder) if os.path.isdir(os.path.join(icons_folder, subfolder))]}",1,nan,nan
https://github.com/psf/requests/tree/master/tests/test_requests.py,"def test_cookielib_cookiejar_on_redirect(self, httpbin):
        """"""Tests resolve_redirect doesn't fail when merging cookies
        with non-RequestsCookieJar cookiejar.

        See GH #3579
        """"""
        cj = cookiejar_from_dict({'foo': 'bar'}, cookielib.CookieJar())
        s = requests.Session()
        s.cookies = cookiejar_from_dict({'cookie': 'tasty'})

        # Prepare request without using Session
        req = requests.Request('GET', httpbin('headers'), cookies=cj)
        prep_req = req.prepare()

        # Send request and simulate redirect
        resp = s.send(prep_req)
        resp.status_code = 302
        resp.headers['location'] = httpbin('get')
        redirects = s.resolve_redirects(resp, prep_req)
        resp = next(redirects)

        # Verify CookieJar isn't being converted to RequestsCookieJar
        assert isinstance(prep_req._cookies, cookielib.CookieJar)
        assert isinstance(resp.request._cookies, cookielib.CookieJar)
        assert not isinstance(resp.request._cookies, requests.cookies.RequestsCookieJar)

        cookies = {}
        for c in resp.request._cookies:
            cookies[c.name] = c.value
        assert cookies['foo'] == 'bar'
        assert cookies['cookie'] == 'tasty'",_2542.py,28,"for c in resp.request._cookies:
    cookies[c.name] = c.value",cookies = {c.name: c.value for c in resp.request._cookies},1,nan,nan
https://github.com/sabeechen/hassio-google-drive-backup/tree/master/hassio-google-drive-backup/backup/model/model.py,"def _getPurgeStats(self):
        ret = {}
        for source in [self.source, self.dest]:
            ret[source.name()] = len(self._getPurgeList(source))
        return ret",_2556.py,3,"for source in [self.source, self.dest]:
    ret[source.name()] = len(self._getPurgeList(source))","ret = {source.name(): len(self._getPurgeList(source)) for source in [self.source, self.dest]}",1,nan,nan
https://github.com/Axelrod-Python/Axelrod/tree/master/axelrod/classifier.py,"def rebuild_classifier_table(
    classifiers: List[Classifier],
    players: List[Type[Player]],
    path: Text = ALL_CLASSIFIERS_PATH,
) -> None:
    """"""Builds the classifier table in data.

    Parameters
    ----------
    classifiers: A list of classifiers to calculate on the strategies
    players: A list of strategies (classes, not instances) to compute the
        classifiers for.
    path: Where to save the resulting yaml file.
    """"""
    # Get absolute path
    dirname = os.path.dirname(__file__)
    filename = os.path.join(dirname, path)

    all_player_dicts = dict()
    for p in players:
        new_player_dict = dict()
        for c in classifiers:
            new_player_dict[c.name] = c.classify_player(p)
        all_player_dicts[p.name] = new_player_dict

    with open(filename, ""w"") as f:
        yaml.dump(all_player_dicts, f)",_2748.py,22,"for c in classifiers:
    new_player_dict[c.name] = c.classify_player(p)",new_player_dict = {c.name: c.classify_player(p) for c in classifiers},1,nan,nan
https://github.com/huggingface/transformers/tree/master/tests/test_modeling_common.py,"def cast_to_device(dictionary, device):
                output = {}
                for k, v in dictionary.items():
                    if isinstance(v, torch.Tensor):
                        output[k] = v.to(device)
                    else:
                        output[k] = v

                return output",_3051.py,3,"for (k, v) in dictionary.items():
    if isinstance(v, torch.Tensor):
        output[k] = v.to(device)
    else:
        output[k] = v","output = {k: v.to(device) if isinstance(v, torch.Tensor) else v for (k, v) in dictionary.items()}",1,nan,nan
https://github.com/nlplab/brat/tree/master/server/src/verify_annotations.py,"def verify_equivs(ann_obj, projectconf):
    issues = []

    # shortcut
    def disp(s):
        return projectconf.preferred_display_form(s)

    for eq in ann_obj.get_equivs():
        # get the equivalent annotations
        equiv_anns = [ann_obj.get_ann_by_id(eid) for eid in eq.entities]

        # all pairs of entity types in the Equiv group must be allowed
        # to have an Equiv. Create type-level pairs to avoid N^2
        # search where N=entities.
        eq_type = {}
        for e in equiv_anns:
            eq_type[e.type] = True
        type_pairs = []
        for t1 in eq_type:
            for t2 in eq_type:
                type_pairs.append((t1, t2))

        # do avoid marking both (a1,a2) and (a2,a1), remember what's
        # already included
        marked = {}

        for t1, t2 in type_pairs:
            reltypes = projectconf.relation_types_from_to(t1, t2)
            # TODO: this is too convoluted; use projectconf directly
            equiv_type_found = False
            for rt in reltypes:
                if projectconf.is_equiv_type(rt):
                    equiv_type_found = True
            if not equiv_type_found:
                # Avoid redundant output
                if (t2, t1) in marked:
                    continue
                # TODO: mark this error on the Eq relation, not the entities
                for e in equiv_anns:
                    issues.append(
                        AnnotationIssue(
                            e.id, AnnotationError, ""Equivalence relation %s not allowed between %s and %s"" %
                            (eq.type, disp(t1), disp(t2))))
                marked[(t1, t2)] = True

    return issues",_3364.py,16,"for e in equiv_anns:
    eq_type[e.type] = True",eq_type = {e.type: True for e in equiv_anns},1,nan,nan
https://github.com/open-io/oio-sds/tree/master/oio/directory/admin.py,"def set_properties(self, params,
                       properties=None, system=None, **kwargs):
        """"""
        Set user or system properties in the admin table of an sqliterepo base.
        """"""
        data = dict()
        if properties:
            data['properties'] = properties
        if system:
            data['system'] = dict()
            for k, v in system:
                data['system'][k if k.startswith('sys.') else 'sys.' + k] = v
        self._request('POST', ""/set_properties"",
                      params=params, json=data, **kwargs)",_3424.py,11,"for (k, v) in system:
    data['system'][k if k.startswith('sys.') else 'sys.' + k] = v","data['system'] = {k if k.startswith('sys.') else 'sys.' + k: v for (k, v) in system}",1,nan,nan
https://github.com/xonsh/xonsh/tree/master/xonsh/ply/ply/yacc.py,"def infinite_cycles(self):
        terminates = {}

        # Terminals:
        for t in self.Terminals:
            terminates[t] = True

        terminates['$end'] = True

        # Nonterminals:

        # Initialize to false:
        for n in self.Nonterminals:
            terminates[n] = False

        # Then propagate termination until no change:
        while True:
            some_change = False
            for (n, pl) in self.Prodnames.items():
                # Nonterminal n terminates iff any of its productions terminates.
                for p in pl:
                    # Production p terminates iff all of its rhs symbols terminate.
                    for s in p.prod:
                        if not terminates[s]:
                            # The symbol s does not terminate,
                            # so production p does not terminate.
                            p_terminates = False
                            break
                    else:
                        # didn't break from the loop,
                        # so every symbol s terminates
                        # so production p terminates.
                        p_terminates = True

                    if p_terminates:
                        # symbol n terminates!
                        if not terminates[n]:
                            terminates[n] = True
                            some_change = True
                        # Don't need to consider any more productions for this n.
                        break

            if not some_change:
                break

        infinite = []
        for (s, term) in terminates.items():
            if not term:
                if s not in self.Prodnames and s not in self.Terminals and s != 'error':
                    # s is used-but-not-defined, and we've already warned of that,
                    # so it would be overkill to say that it's also non-terminating.
                    pass
                else:
                    infinite.append(s)

        return infinite",_3452.py,5,"for t in self.Terminals:
    terminates[t] = True",terminates = {t: True for t in self.Terminals},1,nan,nan
https://github.com/xonsh/xonsh/tree/master/xonsh/ply/ply/yacc.py,"def infinite_cycles(self):
        terminates = {}

        # Terminals:
        for t in self.Terminals:
            terminates[t] = True

        terminates['$end'] = True

        # Nonterminals:

        # Initialize to false:
        for n in self.Nonterminals:
            terminates[n] = False

        # Then propagate termination until no change:
        while True:
            some_change = False
            for (n, pl) in self.Prodnames.items():
                # Nonterminal n terminates iff any of its productions terminates.
                for p in pl:
                    # Production p terminates iff all of its rhs symbols terminate.
                    for s in p.prod:
                        if not terminates[s]:
                            # The symbol s does not terminate,
                            # so production p does not terminate.
                            p_terminates = False
                            break
                    else:
                        # didn't break from the loop,
                        # so every symbol s terminates
                        # so production p terminates.
                        p_terminates = True

                    if p_terminates:
                        # symbol n terminates!
                        if not terminates[n]:
                            terminates[n] = True
                            some_change = True
                        # Don't need to consider any more productions for this n.
                        break

            if not some_change:
                break

        infinite = []
        for (s, term) in terminates.items():
            if not term:
                if s not in self.Prodnames and s not in self.Terminals and s != 'error':
                    # s is used-but-not-defined, and we've already warned of that,
                    # so it would be overkill to say that it's also non-terminating.
                    pass
                else:
                    infinite.append(s)

        return infinite",_3452.py,13,"for n in self.Nonterminals:
    terminates[n] = False",terminates.update({n: False for n in self.Nonterminals}),1,nan,nan
https://github.com/ytorg/Yotter/tree/master/youtube/yt_data_extract/watch_extraction.py,"def _extract_watch_info_mobile(top_level):
    info = {}
    microformat = deep_get(top_level, 'playerResponse', 'microformat', 'playerMicroformatRenderer', default={})

    family_safe = microformat.get('isFamilySafe')
    if family_safe is None:
        info['age_restricted'] = None
    else:
        info['age_restricted'] = not family_safe
    info['allowed_countries'] = microformat.get('availableCountries', [])
    info['time_published'] = microformat.get('publishDate')

    response = top_level.get('response', {})

    # this renderer has the stuff visible on the page
    # check for playlist
    items, _ = extract_items(response,
        item_types={'singleColumnWatchNextResults'})
    if items:
        watch_next_results = items[0]['singleColumnWatchNextResults']
        playlist = deep_get(watch_next_results, 'playlist', 'playlist')
        if playlist is None:
            info['playlist'] = None
        else:
            info['playlist'] = {}
            info['playlist']['title'] = playlist.get('title')
            info['playlist']['author'] = extract_str(multi_get(playlist,
                'ownerName', 'longBylineText', 'shortBylineText', 'ownerText'))
            author_id = deep_get(playlist, 'longBylineText', 'runs', 0,
                'navigationEndpoint', 'browseEndpoint', 'browseId')
            info['playlist']['author_id'] = author_id
            info['playlist']['author_url'] = concat_or_none(
                'https://www.youtube.com/channel/', author_id)
            info['playlist']['id'] = playlist.get('playlistId')
            info['playlist']['url'] = concat_or_none(
                'https://www.youtube.com/playlist?list=',
                info['playlist']['id'])
            info['playlist']['video_count'] = playlist.get('totalVideos')
            info['playlist']['current_index'] = playlist.get('currentIndex')
            info['playlist']['items'] = [
                extract_item_info(i) for i in playlist.get('contents', ())]
    else:
        info['playlist'] = None

    # Holds the visible video info. It is inside singleColumnWatchNextResults
    # but use our convenience function instead
    items, _ = extract_items(response, item_types={'slimVideoMetadataRenderer'})
    if items:
        video_info = items[0]['slimVideoMetadataRenderer']
    else:
        print('Failed to extract video metadata')
        video_info = {}

    info.update(_extract_metadata_row_info(video_info))
    info['description'] = extract_str(video_info.get('description'), recover_urls=True)
    info['view_count'] = extract_int(extract_str(video_info.get('expandedSubtitle')))
    info['author'] = extract_str(deep_get(video_info, 'owner', 'slimOwnerRenderer', 'title'))
    info['author_id'] = deep_get(video_info, 'owner', 'slimOwnerRenderer', 'navigationEndpoint', 'browseEndpoint', 'browseId')
    info['title'] = extract_str(video_info.get('title'))
    info['live'] = 'watching' in extract_str(video_info.get('expandedSubtitle'), default='')
    info['unlisted'] = False
    for badge in video_info.get('badges', []):
        if deep_get(badge, 'metadataBadgeRenderer', 'label') == 'Unlisted':
            info['unlisted'] = True
    info['like_count'] = None
    info['dislike_count'] = None
    if not info['time_published']:
        info['time_published'] = extract_date(extract_str(video_info.get('dateText', None)))
    for button in video_info.get('buttons', ()):
        button_renderer = button.get('slimMetadataToggleButtonRenderer', {})

        # all the digits can be found in the accessibility data
        count = extract_int(deep_get(button_renderer, 'button', 'toggleButtonRenderer', 'defaultText', 'accessibility', 'accessibilityData', 'label'))

        # this count doesn't have all the digits, it's like 53K for instance
        dumb_count = extract_int(extract_str(deep_get(button_renderer, 'button', 'toggleButtonRenderer', 'defaultText')))

        # the accessibility text will be ""No likes"" or ""No dislikes"" or something like that, but dumb count will be 0
        if dumb_count == 0:
            count = 0

        if 'isLike' in button_renderer:
            info['like_count'] = count
        elif 'isDislike' in button_renderer:
            info['dislike_count'] = count

    # comment section info
    items, _ = extract_items(response, item_types={
        'commentSectionRenderer', 'commentsEntryPointHeaderRenderer'})
    if items:
        header_type = list(items[0])[0]
        comment_info = items[0][header_type]
        # This seems to be some kind of A/B test being done on mobile, where
        # this is present instead of the normal commentSectionRenderer. It can
        # be seen here:
        # https://www.androidpolice.com/2019/10/31/google-youtube-app-comment-section-below-videos/
        # https://www.youtube.com/watch?v=bR5Q-wD-6qo
        if header_type == 'commentsEntryPointHeaderRenderer':
            comment_count_text = extract_str(comment_info.get('headerText'))
        else:
            comment_count_text = extract_str(deep_get(comment_info,
                'header', 'commentSectionHeaderRenderer', 'countText'))
        if comment_count_text == 'Comments':    # just this with no number, means 0 comments
            info['comment_count'] = 0
        else:
            info['comment_count'] = extract_int(comment_count_text)
        info['comments_disabled'] = False
    else:   # no comment section present means comments are disabled
        info['comment_count'] = 0
        info['comments_disabled'] = True

    # check for limited state
    items, _ = extract_items(response, item_types={'limitedStateMessageRenderer'})
    if items:
        info['limited_state'] = True
    else:
        info['limited_state'] = False

    # related videos
    related, _ = extract_items(response)
    info['related_videos'] = [extract_item_info(renderer) for renderer in related]

    return info",_3887.py,62,"for badge in video_info.get('badges', []):
    if deep_get(badge, 'metadataBadgeRenderer', 'label') == 'Unlisted':
        info['unlisted'] = True","info.update({'unlisted': True for badge in video_info.get('badges', []) if deep_get(badge, 'metadataBadgeRenderer', 'label') == 'Unlisted'})",1,nan,nan
https://github.com/facebookresearch/SpanBERT/tree/master/code/run_mrqa.py,"def get_raw_scores(dataset, predictions):
    answers = {}
    for example in dataset:
        for qa in example['qas']:
            answers[qa['qid']] = qa['answers']
    exact_scores = {}
    f1_scores = {}
    for qid, ground_truths in answers.items():
        if qid not in predictions:
            print('Missing prediction for %s' % qid)
            continue
        prediction = predictions[qid]
        exact_scores[qid] = metric_max_over_ground_truths(
            exact_match_score, prediction, ground_truths)
        f1_scores[qid] = metric_max_over_ground_truths(
            f1_score, prediction, ground_truths)
    return exact_scores, f1_scores",_4738.py,3,"for example in dataset:
    for qa in example['qas']:
        answers[qa['qid']] = qa['answers']",answers = {qa['qid']: qa['answers'] for example in dataset for qa in example['qas']},1,nan,nan
https://github.com/nlp-uoregon/trankit/tree/master/trankit/adapter_transformers/data/metrics/squad_metrics.py,"def get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):
    """"""Project the tokenized prediction back to the original text.""""""

    # When we created the data, we kept track of the alignment between original
    # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So
    # now `orig_text` contains the span of our original text corresponding to the
    # span that we predicted.
    #
    # However, `orig_text` may contain extra characters that we don't want in
    # our prediction.
    #
    # For example, let's say:
    #   pred_text = steve smith
    #   orig_text = Steve Smith's
    #
    # We don't want to return `orig_text` because it contains the extra ""'s"".
    #
    # We don't want to return `pred_text` because it's already been normalized
    # (the SQuAD eval script also does punctuation stripping/lower casing but
    # our tokenizer does additional normalization like stripping accent
    # characters).
    #
    # What we really want to return is ""Steve Smith"".
    #
    # Therefore, we have to apply a semi-complicated alignment heuristic between
    # `pred_text` and `orig_text` to get a character-to-character alignment. This
    # can fail in certain cases in which case we just return `orig_text`.

    def _strip_spaces(text):
        ns_chars = []
        ns_to_s_map = collections.OrderedDict()
        for (i, c) in enumerate(text):
            if c == "" "":
                continue
            ns_to_s_map[len(ns_chars)] = i
            ns_chars.append(c)
        ns_text = """".join(ns_chars)
        return (ns_text, ns_to_s_map)

    # We first tokenize `orig_text`, strip whitespace from the result
    # and `pred_text`, and check if they are the same length. If they are
    # NOT the same length, the heuristic has failed. If they are the same
    # length, we assume the characters are one-to-one aligned.
    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)

    tok_text = "" "".join(tokenizer.tokenize(orig_text))

    start_position = tok_text.find(pred_text)
    if start_position == -1:
        if verbose_logging:
            logger.info(""Unable to find text: '%s' in '%s'"" % (pred_text, orig_text))
        return orig_text
    end_position = start_position + len(pred_text) - 1

    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)
    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)

    if len(orig_ns_text) != len(tok_ns_text):
        if verbose_logging:
            logger.info(""Length not equal after stripping spaces: '%s' vs '%s'"", orig_ns_text, tok_ns_text)
        return orig_text

    # We then project the characters in `pred_text` back to `orig_text` using
    # the character-to-character alignment.
    tok_s_to_ns_map = {}
    for (i, tok_index) in tok_ns_to_s_map.items():
        tok_s_to_ns_map[tok_index] = i

    orig_start_position = None
    if start_position in tok_s_to_ns_map:
        ns_start_position = tok_s_to_ns_map[start_position]
        if ns_start_position in orig_ns_to_s_map:
            orig_start_position = orig_ns_to_s_map[ns_start_position]

    if orig_start_position is None:
        if verbose_logging:
            logger.info(""Couldn't map start position"")
        return orig_text

    orig_end_position = None
    if end_position in tok_s_to_ns_map:
        ns_end_position = tok_s_to_ns_map[end_position]
        if ns_end_position in orig_ns_to_s_map:
            orig_end_position = orig_ns_to_s_map[ns_end_position]

    if orig_end_position is None:
        if verbose_logging:
            logger.info(""Couldn't map end position"")
        return orig_text

    output_text = orig_text[orig_start_position : (orig_end_position + 1)]
    return output_text",_5181.py,66,"for (i, tok_index) in tok_ns_to_s_map.items():
    tok_s_to_ns_map[tok_index] = i","tok_s_to_ns_map = {tok_index: i for (i, tok_index) in tok_ns_to_s_map.items()}",1,nan,nan
https://github.com/commaai/openpilot/tree/master/selfdrive/car/fw_versions.py,"def build_fw_dict(fw_versions):
  fw_versions_dict = {}
  for fw in fw_versions:
    addr = fw.address
    sub_addr = fw.subAddress if fw.subAddress != 0 else None
    fw_versions_dict[(addr, sub_addr)] = fw.fwVersion
  return fw_versions_dict",_5440.py,3,"for fw in fw_versions:
    addr = fw.address
    sub_addr = fw.subAddress if fw.subAddress != 0 else None
    fw_versions_dict[addr, sub_addr] = fw.fwVersion","fw_versions_dict = {(fw.address, fw.subAddress if fw.subAddress != 0 else None): fw.fwVersion for fw in fw_versions}",1,nan,nan
https://github.com/rwightman/pytorch-image-models/tree/master//avg_checkpoints.py,"def main():
    args = parser.parse_args()
    # by default use the EMA weights (if present)
    args.use_ema = not args.no_use_ema
    # by default sort by checkpoint metric (if present) and avg top n checkpoints
    args.sort = not args.no_sort

    if os.path.exists(args.output):
        print(""Error: Output filename ({}) already exists."".format(args.output))
        exit(1)

    pattern = args.input
    if not args.input.endswith(os.path.sep) and not args.filter.startswith(os.path.sep):
        pattern += os.path.sep
    pattern += args.filter
    checkpoints = glob.glob(pattern, recursive=True)

    if args.sort:
        checkpoint_metrics = []
        for c in checkpoints:
            metric = checkpoint_metric(c)
            if metric is not None:
                checkpoint_metrics.append((metric, c))
        checkpoint_metrics = list(sorted(checkpoint_metrics))
        checkpoint_metrics = checkpoint_metrics[-args.n:]
        print(""Selected checkpoints:"")
        [print(m, c) for m, c in checkpoint_metrics]
        avg_checkpoints = [c for m, c in checkpoint_metrics]
    else:
        avg_checkpoints = checkpoints
        print(""Selected checkpoints:"")
        [print(c) for c in checkpoints]

    avg_state_dict = {}
    avg_counts = {}
    for c in avg_checkpoints:
        new_state_dict = load_state_dict(c, args.use_ema)
        if not new_state_dict:
            print(""Error: Checkpoint ({}) doesn't exist"".format(args.checkpoint))
            continue

        for k, v in new_state_dict.items():
            if k not in avg_state_dict:
                avg_state_dict[k] = v.clone().to(dtype=torch.float64)
                avg_counts[k] = 1
            else:
                avg_state_dict[k] += v.to(dtype=torch.float64)
                avg_counts[k] += 1

    for k, v in avg_state_dict.items():
        v.div_(avg_counts[k])

    # float32 overflow seems unlikely based on weights seen to date, but who knows
    float32_info = torch.finfo(torch.float32)
    final_state_dict = {}
    for k, v in avg_state_dict.items():
        v = v.clamp(float32_info.min, float32_info.max)
        final_state_dict[k] = v.to(dtype=torch.float32)

    try:
        torch.save(final_state_dict, args.output, _use_new_zipfile_serialization=False)
    except:
        torch.save(final_state_dict, args.output)

    with open(args.output, 'rb') as f:
        sha_hash = hashlib.sha256(f.read()).hexdigest()
    print(""=> Saved state_dict to '{}, SHA256: {}'"".format(args.output, sha_hash))",_5723.py,56,"for (k, v) in avg_state_dict.items():
    v = v.clamp(float32_info.min, float32_info.max)
    final_state_dict[k] = v.to(dtype=torch.float32)","final_state_dict = {k: v.clamp(float32_info.min, float32_info.max).to(dtype=torch.float32) for (k, v) in avg_state_dict.items()}",1,nan,nan
https://github.com/digitalbazaar/pyld/tree/master/lib/pyld/jsonld.py,"def to_rdf(self, input_, options):
        """"""
        Outputs the RDF dataset found in the given JSON-LD object.

        :param input_: the JSON-LD input.
        :param options: the options to use.
          [base] the base IRI to use.
          [contextResolver] internal use only.
          [format] the format if input is a string:
            'application/n-quads' for N-Quads.
          [produceGeneralizedRdf] true to output generalized RDF, false
            to produce only standard RDF (default: false).
          [documentLoader(url, options)] the document loader
            (default: _default_document_loader).
          [rdfDirection] Only 'i18n-datatype' supported
            (default: None).

        :return: the resulting RDF dataset (or a serialization of it).
        """"""
        # set default options
        options = options.copy() if options else {}
        options.setdefault('base', input_ if _is_string(input_) else '')
        options.setdefault('produceGeneralizedRdf', False)
        options.setdefault('documentLoader', _default_document_loader)
        options.setdefault('contextResolver',
            ContextResolver(_resolved_context_cache, options['documentLoader']))
        options.setdefault('extractAllScripts', True)
        options.setdefault('processingMode', 'json-ld-1.1')

        try:
            # expand input
            expanded = self.expand(input_, options)
        except JsonLdError as cause:
            raise JsonLdError(
                'Could not expand input before serialization to '
                'RDF.', 'jsonld.RdfError', cause=cause)

        # create node map for default graph (and any named graphs)
        issuer = IdentifierIssuer('_:b')
        node_map = {'@default': {}}
        self._create_node_map(expanded, node_map, '@default', issuer)

        # output RDF dataset
        dataset = {}
        for graph_name, graph in sorted(node_map.items()):
            # skip relative IRIs
            if graph_name == '@default' or _is_absolute_iri(graph_name):
                dataset[graph_name] = self._graph_to_rdf(
                    graph, issuer, options)

        # convert to output format
        if 'format' in options:
            if (options['format'] == 'application/n-quads' or
                    options['format'] == 'application/nquads'):
                return self.to_nquads(dataset)
            raise JsonLdError(
                'Unknown output format.',
                'jsonld.UnknownFormat', {'format': options['format']})
        return dataset",_5757.py,45,"for (graph_name, graph) in sorted(node_map.items()):
    if graph_name == '@default' or _is_absolute_iri(graph_name):
        dataset[graph_name] = self._graph_to_rdf(graph, issuer, options)","dataset = {graph_name: self._graph_to_rdf(graph, issuer, options) for (graph_name, graph) in sorted(node_map.items()) if graph_name == '@default' or _is_absolute_iri(graph_name)}",1,nan,nan
https://github.com/tensorflow/ranking/tree/master/tensorflow_ranking/python/head.py,"def _labels_and_logits_metrics(self, labels, logits):
    """"""Returns metrics for labels and logits.""""""
    is_label_valid = tf.reshape(tf.greater_equal(labels, 0.), [-1])
    metrics_dict = {}
    for name, tensor in [('labels_mean', labels), ('logits_mean', logits)]:
      metrics_dict[name] = tf.compat.v1.metrics.mean(
          tf.boolean_mask(tensor=tf.reshape(tensor, [-1]), mask=is_label_valid))
    return metrics_dict",_6300.py,5,"for (name, tensor) in [('labels_mean', labels), ('logits_mean', logits)]:
    metrics_dict[name] = tf.compat.v1.metrics.mean(tf.boolean_mask(tensor=tf.reshape(tensor, [-1]), mask=is_label_valid))","metrics_dict = {name: tf.compat.v1.metrics.mean(tf.boolean_mask(tensor=tf.reshape(tensor, [-1]), mask=is_label_valid)) for (name, tensor) in [('labels_mean', labels), ('logits_mean', logits)]}",1,nan,nan
https://github.com/whyliam/whyliam.workflows.youdao/tree/master/workflow/workflow3.py,"def obj(self):
        """"""``alfredworkflow`` :class:`dict`.""""""
        o = {}
        if self:
            d2 = {}
            for k, v in list(self.items()):
                d2[k] = v
            o[""variables""] = d2

        if self.config:
            o[""config""] = self.config

        if self.arg is not None:
            o[""arg""] = self.arg

        return {""alfredworkflow"": o}",_6327.py,6,"for (k, v) in list(self.items()):
    d2[k] = v","d2 = {k: v for (k, v) in list(self.items())}",1,nan,nan
https://github.com/taigaio/taiga-back/tree/master/taiga/projects/history/models.py,"def values_diff(self):
        if self.values_diff_cache is not None:
            return self.values_diff_cache

        result = {}
        users_keys = [""assigned_to"", ""owner""]

        def resolve_diff_value(key):
            value = None
            diff = get_diff_of_htmls(
                self.diff[key][0] or """",
                self.diff[key][1] or """"
            )

            if diff:
                key = ""{}_diff"".format(key)
                value = (None, diff)

            return (key, value)

        def resolve_value(field, key):
            data = self.values[field]
            key = str(key)

            if key not in data:
                return None
            return data[key]

        for key in self.diff:
            value = None
            if key in IGNORE_DIFF_FIELDS:
                continue
            elif key in[""description"", ""content"", ""blocked_note""]:
                (key, value) = resolve_diff_value(key)
            elif key in users_keys:
                value = [resolve_value(""users"", x) for x in self.diff[key]]
            elif key == ""assigned_users"":
                diff_in, diff_out = self.diff[key]
                value_in = None
                value_out = None

                if diff_in:
                    users_list = [resolve_value(""users"", x) for x in diff_in if x]
                    value_in = "", "".join(filter(None, users_list))
                if diff_out:
                    users_list = [resolve_value(""users"", x) for x in diff_out if x]
                    value_out = "", "".join(filter(None, users_list))
                value = [value_in, value_out]
            elif key == ""points"":
                points = {}

                pointsold = self.diff[""points""][0]
                pointsnew = self.diff[""points""][1]
                # pointsold = pointsnew

                if pointsold is None:
                    for role_id, point_id in pointsnew.items():
                        role_name = resolve_value(""roles"", role_id)
                        points[role_name] = [None, resolve_value(""points"", point_id)]

                else:
                    for role_id, point_id in pointsnew.items():
                        role_name = resolve_value(""roles"", role_id)
                        oldpoint_id = pointsold.get(role_id, None)
                        points[role_name] = [resolve_value(""points"", oldpoint_id),
                                             resolve_value(""points"", point_id)]

                # Process that removes points entries with
                # duplicate value.
                for role in dict(points):
                    values = points[role]
                    if values[1] == values[0]:
                        del points[role]

                if points:
                    value = points

            elif key == ""attachments"":
                attachments = {
                    ""new"": [],
                    ""changed"": [],
                    ""deleted"": [],
                }

                oldattachs = {x[""id""]: x for x in self.diff[""attachments""][0]}
                newattachs = {x[""id""]: x for x in self.diff[""attachments""][1]}

                for aid in set(tuple(oldattachs.keys()) + tuple(newattachs.keys())):
                    if aid in oldattachs and aid in newattachs:
                        changes = make_diff_from_dicts(oldattachs[aid], newattachs[aid],
                                                       excluded_keys=(""filename"", ""url"", ""thumb_url""))

                        if changes:
                            change = {
                                ""filename"": newattachs.get(aid, {}).get(""filename"", """"),
                                ""url"": newattachs.get(aid, {}).get(""url"", """"),
                                ""thumb_url"": newattachs.get(aid, {}).get(""thumb_url"", """"),
                                ""changes"": changes
                            }
                            attachments[""changed""].append(change)
                    elif aid in oldattachs and aid not in newattachs:
                        attachments[""deleted""].append(oldattachs[aid])
                    elif aid not in oldattachs and aid in newattachs:
                        attachments[""new""].append(newattachs[aid])

                if attachments[""new""] or attachments[""changed""] or attachments[""deleted""]:
                    value = attachments

            elif key == ""custom_attributes"":
                custom_attributes = {
                    ""new"": [],
                    ""changed"": [],
                    ""deleted"": [],
                }

                oldcustattrs = {x[""id""]: x for x in self.diff[""custom_attributes""][0] or []}
                newcustattrs = {x[""id""]: x for x in self.diff[""custom_attributes""][1] or []}

                for aid in set(tuple(oldcustattrs.keys()) + tuple(newcustattrs.keys())):
                    if aid in oldcustattrs and aid in newcustattrs:
                        changes = make_diff_from_dicts(oldcustattrs[aid], newcustattrs[aid],
                                                       excluded_keys=(""name"", ""type""))
                        newcustattr = newcustattrs.get(aid, {})
                        if changes:
                            change_type = newcustattr.get(""type"", TEXT_TYPE)

                            if change_type in [NUMBER_TYPE, CHECKBOX_TYPE]:
                                old_value = oldcustattrs[aid].get(""value"")
                                new_value = newcustattrs[aid].get(""value"")
                                value_diff = [old_value, new_value]
                            else:
                                old_value = oldcustattrs[aid].get(""value"", """")
                                new_value = newcustattrs[aid].get(""value"", """")
                                value_diff = get_diff_of_htmls(old_value,
                                                               new_value)
                            change = {
                                ""name"": newcustattr.get(""name"", """"),
                                ""changes"": changes,
                                ""type"": change_type,
                                ""value_diff"": value_diff
                            }
                            custom_attributes[""changed""].append(change)
                    elif aid in oldcustattrs and aid not in newcustattrs:
                        custom_attributes[""deleted""].append(oldcustattrs[aid])
                    elif aid not in oldcustattrs and aid in newcustattrs:
                        newcustattr = newcustattrs.get(aid, {})
                        change_type = newcustattr.get(""type"", TEXT_TYPE)
                        if change_type in [NUMBER_TYPE, CHECKBOX_TYPE]:
                            old_value = None
                            new_value = newcustattrs[aid].get(""value"")
                            value_diff = [old_value, new_value]
                        else:
                            new_value = newcustattrs[aid].get(""value"", """")
                            value_diff = get_diff_of_htmls("""", new_value)
                        newcustattrs[aid][""value_diff""] = value_diff
                        custom_attributes[""new""].append(newcustattrs[aid])

                if custom_attributes[""new""] or custom_attributes[""changed""] or custom_attributes[""deleted""]:
                    value = custom_attributes

            elif key == ""user_stories"":
                user_stories = {
                    ""new"": [],
                    ""deleted"": [],
                }

                olduss = {x[""id""]: x for x in self.diff[""user_stories""][0]}
                newuss = {x[""id""]: x for x in self.diff[""user_stories""][1]}

                for usid in set(tuple(olduss.keys()) + tuple(newuss.keys())):
                    if usid in olduss and usid not in newuss:
                        user_stories[""deleted""].append(olduss[usid])
                    elif usid not in olduss and usid in newuss:
                        user_stories[""new""].append(newuss[usid])

                if user_stories[""new""] or user_stories[""deleted""]:
                    value = user_stories

            elif key in self.values:
                value = [resolve_value(key, x) for x in self.diff[key]]
            else:
                value = self.diff[key]

            if not value:
                continue

            result[key] = value

        self.values_diff_cache = result
        # Update values_diff_cache without dispatching signals
        HistoryEntry.objects.filter(pk=self.pk).update(values_diff_cache=self.values_diff_cache)
        return self.values_diff_cache",_6448.py,57,"for (role_id, point_id) in pointsnew.items():
    role_name = resolve_value('roles', role_id)
    points[role_name] = [None, resolve_value('points', point_id)]","points = {resolve_value('roles', role_id): [None, resolve_value('points', point_id)] for (role_id, point_id) in pointsnew.items()}",1,nan,nan
https://github.com/taigaio/taiga-back/tree/master/taiga/projects/history/models.py,"def values_diff(self):
        if self.values_diff_cache is not None:
            return self.values_diff_cache

        result = {}
        users_keys = [""assigned_to"", ""owner""]

        def resolve_diff_value(key):
            value = None
            diff = get_diff_of_htmls(
                self.diff[key][0] or """",
                self.diff[key][1] or """"
            )

            if diff:
                key = ""{}_diff"".format(key)
                value = (None, diff)

            return (key, value)

        def resolve_value(field, key):
            data = self.values[field]
            key = str(key)

            if key not in data:
                return None
            return data[key]

        for key in self.diff:
            value = None
            if key in IGNORE_DIFF_FIELDS:
                continue
            elif key in[""description"", ""content"", ""blocked_note""]:
                (key, value) = resolve_diff_value(key)
            elif key in users_keys:
                value = [resolve_value(""users"", x) for x in self.diff[key]]
            elif key == ""assigned_users"":
                diff_in, diff_out = self.diff[key]
                value_in = None
                value_out = None

                if diff_in:
                    users_list = [resolve_value(""users"", x) for x in diff_in if x]
                    value_in = "", "".join(filter(None, users_list))
                if diff_out:
                    users_list = [resolve_value(""users"", x) for x in diff_out if x]
                    value_out = "", "".join(filter(None, users_list))
                value = [value_in, value_out]
            elif key == ""points"":
                points = {}

                pointsold = self.diff[""points""][0]
                pointsnew = self.diff[""points""][1]
                # pointsold = pointsnew

                if pointsold is None:
                    for role_id, point_id in pointsnew.items():
                        role_name = resolve_value(""roles"", role_id)
                        points[role_name] = [None, resolve_value(""points"", point_id)]

                else:
                    for role_id, point_id in pointsnew.items():
                        role_name = resolve_value(""roles"", role_id)
                        oldpoint_id = pointsold.get(role_id, None)
                        points[role_name] = [resolve_value(""points"", oldpoint_id),
                                             resolve_value(""points"", point_id)]

                # Process that removes points entries with
                # duplicate value.
                for role in dict(points):
                    values = points[role]
                    if values[1] == values[0]:
                        del points[role]

                if points:
                    value = points

            elif key == ""attachments"":
                attachments = {
                    ""new"": [],
                    ""changed"": [],
                    ""deleted"": [],
                }

                oldattachs = {x[""id""]: x for x in self.diff[""attachments""][0]}
                newattachs = {x[""id""]: x for x in self.diff[""attachments""][1]}

                for aid in set(tuple(oldattachs.keys()) + tuple(newattachs.keys())):
                    if aid in oldattachs and aid in newattachs:
                        changes = make_diff_from_dicts(oldattachs[aid], newattachs[aid],
                                                       excluded_keys=(""filename"", ""url"", ""thumb_url""))

                        if changes:
                            change = {
                                ""filename"": newattachs.get(aid, {}).get(""filename"", """"),
                                ""url"": newattachs.get(aid, {}).get(""url"", """"),
                                ""thumb_url"": newattachs.get(aid, {}).get(""thumb_url"", """"),
                                ""changes"": changes
                            }
                            attachments[""changed""].append(change)
                    elif aid in oldattachs and aid not in newattachs:
                        attachments[""deleted""].append(oldattachs[aid])
                    elif aid not in oldattachs and aid in newattachs:
                        attachments[""new""].append(newattachs[aid])

                if attachments[""new""] or attachments[""changed""] or attachments[""deleted""]:
                    value = attachments

            elif key == ""custom_attributes"":
                custom_attributes = {
                    ""new"": [],
                    ""changed"": [],
                    ""deleted"": [],
                }

                oldcustattrs = {x[""id""]: x for x in self.diff[""custom_attributes""][0] or []}
                newcustattrs = {x[""id""]: x for x in self.diff[""custom_attributes""][1] or []}

                for aid in set(tuple(oldcustattrs.keys()) + tuple(newcustattrs.keys())):
                    if aid in oldcustattrs and aid in newcustattrs:
                        changes = make_diff_from_dicts(oldcustattrs[aid], newcustattrs[aid],
                                                       excluded_keys=(""name"", ""type""))
                        newcustattr = newcustattrs.get(aid, {})
                        if changes:
                            change_type = newcustattr.get(""type"", TEXT_TYPE)

                            if change_type in [NUMBER_TYPE, CHECKBOX_TYPE]:
                                old_value = oldcustattrs[aid].get(""value"")
                                new_value = newcustattrs[aid].get(""value"")
                                value_diff = [old_value, new_value]
                            else:
                                old_value = oldcustattrs[aid].get(""value"", """")
                                new_value = newcustattrs[aid].get(""value"", """")
                                value_diff = get_diff_of_htmls(old_value,
                                                               new_value)
                            change = {
                                ""name"": newcustattr.get(""name"", """"),
                                ""changes"": changes,
                                ""type"": change_type,
                                ""value_diff"": value_diff
                            }
                            custom_attributes[""changed""].append(change)
                    elif aid in oldcustattrs and aid not in newcustattrs:
                        custom_attributes[""deleted""].append(oldcustattrs[aid])
                    elif aid not in oldcustattrs and aid in newcustattrs:
                        newcustattr = newcustattrs.get(aid, {})
                        change_type = newcustattr.get(""type"", TEXT_TYPE)
                        if change_type in [NUMBER_TYPE, CHECKBOX_TYPE]:
                            old_value = None
                            new_value = newcustattrs[aid].get(""value"")
                            value_diff = [old_value, new_value]
                        else:
                            new_value = newcustattrs[aid].get(""value"", """")
                            value_diff = get_diff_of_htmls("""", new_value)
                        newcustattrs[aid][""value_diff""] = value_diff
                        custom_attributes[""new""].append(newcustattrs[aid])

                if custom_attributes[""new""] or custom_attributes[""changed""] or custom_attributes[""deleted""]:
                    value = custom_attributes

            elif key == ""user_stories"":
                user_stories = {
                    ""new"": [],
                    ""deleted"": [],
                }

                olduss = {x[""id""]: x for x in self.diff[""user_stories""][0]}
                newuss = {x[""id""]: x for x in self.diff[""user_stories""][1]}

                for usid in set(tuple(olduss.keys()) + tuple(newuss.keys())):
                    if usid in olduss and usid not in newuss:
                        user_stories[""deleted""].append(olduss[usid])
                    elif usid not in olduss and usid in newuss:
                        user_stories[""new""].append(newuss[usid])

                if user_stories[""new""] or user_stories[""deleted""]:
                    value = user_stories

            elif key in self.values:
                value = [resolve_value(key, x) for x in self.diff[key]]
            else:
                value = self.diff[key]

            if not value:
                continue

            result[key] = value

        self.values_diff_cache = result
        # Update values_diff_cache without dispatching signals
        HistoryEntry.objects.filter(pk=self.pk).update(values_diff_cache=self.values_diff_cache)
        return self.values_diff_cache",_6448.py,62,"for (role_id, point_id) in pointsnew.items():
    role_name = resolve_value('roles', role_id)
    oldpoint_id = pointsold.get(role_id, None)
    points[role_name] = [resolve_value('points', oldpoint_id), resolve_value('points', point_id)]","points.update({resolve_value('roles', role_id): [resolve_value('points', pointsold.get(role_id, None)), resolve_value('points', point_id)] for (role_id, point_id) in pointsnew.items()})",1,nan,nan
https://github.com/ansible/ansible/tree/master/lib/ansible/plugins/inventory/generator.py,"def parse(self, inventory, loader, path, cache=False):
        ''' parses the inventory file '''

        super(InventoryModule, self).parse(inventory, loader, path, cache=cache)

        config = self._read_config_data(path)

        template_inputs = product(*config['layers'].values())
        for item in template_inputs:
            template_vars = dict()
            for i, key in enumerate(config['layers'].keys()):
                template_vars[key] = item[i]
            host = self.template(config['hosts']['name'], template_vars)
            inventory.add_host(host)
            self.add_parents(inventory, host, config['hosts'].get('parents', []), template_vars)",_6477.py,11,"for (i, key) in enumerate(config['layers'].keys()):
    template_vars[key] = item[i]","template_vars = {key: item[i] for (i, key) in enumerate(config['layers'].keys())}",1,nan,nan
https://github.com/django/django/tree/master/tests/forms_tests/widget_tests/test_selectdatewidget.py,"def test_value_from_datadict(self):
        tests = [
            (('2000', '12', '1'), '2000-12-01'),
            (('', '12', '1'), '0-12-1'),
            (('2000', '', '1'), '2000-0-1'),
            (('2000', '12', ''), '2000-12-0'),
            (('', '', '', ''), None),
            ((None, '12', '1'), None),
            (('2000', None, '1'), None),
            (('2000', '12', None), None),
        ]
        for values, expected in tests:
            with self.subTest(values=values):
                data = {}
                for field_name, value in zip(('year', 'month', 'day'), values):
                    if value is not None:
                        data['field_%s' % field_name] = value
                self.assertEqual(self.widget.value_from_datadict(data, {}, 'field'), expected)",_6483.py,15,"for (field_name, value) in zip(('year', 'month', 'day'), values):
    if value is not None:
        data['field_%s' % field_name] = value","data = {'field_%s' % field_name: value for (field_name, value) in zip(('year', 'month', 'day'), values) if value is not None}",1,nan,nan
https://github.com/cerndb/dist-keras/tree/master/distkeras/transformers.py,"def clean_mean_keys(self, means):
        """"""Cleans the keys of the specified dictionary (mean).""""""
        new_means = {}

        for k in means:
            new_means[k[4:-1]] = means[k]

        return new_means",_6503.py,5,"for k in means:
    new_means[k[4:-1]] = means[k]",new_means = {k[4:-1]: means[k] for k in means},1,nan,nan
https://github.com/mementum/backtrader/tree/master/samples/psar/psar.py,"def runstrat(args=None):
    args = parse_args(args)

    cerebro = bt.Cerebro()

    # Data feed kwargs
    kwargs = dict()

    # Parse from/to-date
    dtfmt, tmfmt = '%Y-%m-%d', 'T%H:%M:%S'
    for a, d in ((getattr(args, x), x) for x in ['fromdate', 'todate']):
        if a:
            strpfmt = dtfmt + tmfmt * ('T' in a)
            kwargs[d] = datetime.datetime.strptime(a, strpfmt)

    # Data feed
    data0 = bt.feeds.BacktraderCSVData(dataname=args.data0, **kwargs)
    cerebro.adddata(data0)

    # Broker
    cerebro.broker = bt.brokers.BackBroker(**eval('dict(' + args.broker + ')'))

    # Sizer
    cerebro.addsizer(bt.sizers.FixedSize, **eval('dict(' + args.sizer + ')'))

    # Strategy
    cerebro.addstrategy(St, **eval('dict(' + args.strat + ')'))

    # Execute
    cerebro.run(**eval('dict(' + args.cerebro + ')'))

    if args.plot:  # Plot if requested to
        cerebro.plot(**eval('dict(' + args.plot + ')'))",_6719.py,11,"for (a, d) in ((getattr(args, x), x) for x in ['fromdate', 'todate']):
    if a:
        strpfmt = dtfmt + tmfmt * ('T' in a)
        kwargs[d] = datetime.datetime.strptime(a, strpfmt)","kwargs = {d: datetime.datetime.strptime(a, dtfmt + tmfmt * ('T' in a)) for (a, d) in ((getattr(args, x), x) for x in ['fromdate', 'todate']) if a}",1,nan,nan
https://github.com/vmware/pyvmomi/tree/master/pyVim/sso.py,"def __init__(self, *args, **kwargs):
        '''
        Initializer.  See httplib.HTTPConnection for other arguments
        '''
        tmpKwargs = {}
        httpConn = six.moves.http_client.HTTPConnection
        for key in httpConn.__init__.__code__.co_varnames:
            if key in kwargs and key != 'self':
                tmpKwargs[key] = kwargs[key]
        self.host = kwargs.pop('host')
        six.moves.http_client.HTTPConnection.__init__(self, *args, **tmpKwargs)",_7045.py,7,"for key in httpConn.__init__.__code__.co_varnames:
    if key in kwargs and key != 'self':
        tmpKwargs[key] = kwargs[key]",tmpKwargs = {key: kwargs[key] for key in httpConn.__init__.__code__.co_varnames if key in kwargs and key != 'self'},1,nan,nan
https://github.com/projecthamster/hamster/tree/master/waflib/TaskGen.py,"def post(self):
		""""""
		Creates tasks for this task generators. The following operations are performed:

		#. The body of this method is called only once and sets the attribute ``posted``
		#. The attribute ``features`` is used to add more methods in ``self.meths``
		#. The methods are sorted by the precedence table ``self.prec`` or `:waflib:attr:waflib.TaskGen.task_gen.prec`
		#. The methods are then executed in order
		#. The tasks created are added to :py:attr:`waflib.TaskGen.task_gen.tasks`
		""""""
		if getattr(self, 'posted', None):
			return False
		self.posted = True

		keys = set(self.meths)
		keys.update(feats['*'])

		# add the methods listed in the features
		self.features = Utils.to_list(self.features)
		for x in self.features:
			st = feats[x]
			if st:
				keys.update(st)
			elif not x in Task.classes:
				Logs.warn('feature %r does not exist - bind at least one method to it?', x)

		# copy the precedence table
		prec = {}
		prec_tbl = self.prec
		for x in prec_tbl:
			if x in keys:
				prec[x] = prec_tbl[x]

		# elements disconnected
		tmp = []
		for a in keys:
			for x in prec.values():
				if a in x:
					break
			else:
				tmp.append(a)

		tmp.sort(reverse=True)

		# topological sort
		out = []
		while tmp:
			e = tmp.pop()
			if e in keys:
				out.append(e)
			try:
				nlst = prec[e]
			except KeyError:
				pass
			else:
				del prec[e]
				for x in nlst:
					for y in prec:
						if x in prec[y]:
							break
					else:
						tmp.append(x)
						tmp.sort(reverse=True)

		if prec:
			buf = ['Cycle detected in the method execution:']
			for k, v in prec.items():
				buf.append('- %s after %s' % (k, [x for x in v if x in prec]))
			raise Errors.WafError('\n'.join(buf))
		self.meths = out

		# then we run the methods in order
		Logs.debug('task_gen: posting %s %d', self, id(self))
		for x in out:
			try:
				v = getattr(self, x)
			except AttributeError:
				raise Errors.WafError('%r is not a valid task generator method' % x)
			Logs.debug('task_gen: -> %s (%d)', x, id(self))
			v()

		Logs.debug('task_gen: posted %s', self.name)
		return True",_7200.py,30,"for x in prec_tbl:
    if x in keys:
        prec[x] = prec_tbl[x]",prec.update({x: prec_tbl[x] for x in prec_tbl if x in keys}),1,nan,nan
https://github.com/HaddyYang/django2.0-course/tree/master/35.部署准备（一）：Git/blog/views.py,"def get_blog_list_common_data(request, blogs_all_list):
    paginator = Paginator(blogs_all_list, settings.EACH_PAGE_BLOGS_NUMBER)
    page_num = request.GET.get('page', 1) # 获取url的页面参数（GET请求）
    page_of_blogs = paginator.get_page(page_num)
    currentr_page_num = page_of_blogs.number # 获取当前页码
    # 获取当前页码前后各2页的页码范围
    page_range = list(range(max(currentr_page_num - 2, 1), currentr_page_num)) + \
                 list(range(currentr_page_num, min(currentr_page_num + 2, paginator.num_pages) + 1))
    # 加上省略页码标记
    if page_range[0] - 1 >= 2:
        page_range.insert(0, '...')
    if paginator.num_pages - page_range[-1] >= 2:
        page_range.append('...')
    # 加上首页和尾页
    if page_range[0] != 1:
        page_range.insert(0, 1)
    if page_range[-1] != paginator.num_pages:
        page_range.append(paginator.num_pages)

    # 获取日期归档对应的博客数量
    blog_dates = Blog.objects.dates('created_time', 'month', order=""DESC"")
    blog_dates_dict = {}
    for blog_date in blog_dates:
        blog_count = Blog.objects.filter(created_time__year=blog_date.year, 
                                         created_time__month=blog_date.month).count()
        blog_dates_dict[blog_date] = blog_count

    context = {}
    context['blogs'] = page_of_blogs.object_list
    context['page_of_blogs'] = page_of_blogs
    context['page_range'] = page_range
    context['blog_types'] = BlogType.objects.annotate(blog_count=Count('blog'))
    context['blog_dates'] = blog_dates_dict
    return context",_7410.py,23,"for blog_date in blog_dates:
    blog_count = Blog.objects.filter(created_time__year=blog_date.year, created_time__month=blog_date.month).count()
    blog_dates_dict[blog_date] = blog_count","blog_dates_dict = {blog_date: Blog.objects.filter(created_time__year=blog_date.year, created_time__month=blog_date.month).count() for blog_date in blog_dates}",1,nan,nan
https://github.com/strawberry-graphql/strawberry/tree/master/strawberry/extensions/query_depth_limiter.py,"def __init__(self, validation_context: ValidationContext):
            document = validation_context.document
            definitions = document.definitions

            fragments = get_fragments(definitions)
            queries = get_queries_and_mutations(definitions)
            query_depths = {}

            for name in queries:
                query_depths[name] = determine_depth(
                    node=queries[name],
                    fragments=fragments,
                    depth_so_far=0,
                    max_depth=max_depth,
                    context=validation_context,
                    operation_name=name,
                    ignore=ignore,
                )

            if callable(callback):
                callback(query_depths)
            super().__init__(validation_context)",_7508.py,9,"for name in queries:
    query_depths[name] = determine_depth(node=queries[name], fragments=fragments, depth_so_far=0, max_depth=max_depth, context=validation_context, operation_name=name, ignore=ignore)","query_depths = {name: determine_depth(node=queries[name], fragments=fragments, depth_so_far=0, max_depth=max_depth, context=validation_context, operation_name=name, ignore=ignore) for name in queries}",1,nan,nan
https://github.com/Tencent/TFace/tree/master/tools/img2tfrecord.py,"def get_img2lmk(pts_file):
    img2lmk = {}
    with open(pts_file, 'r') as f:
        for line in f:
            line = line.rstrip().split(' ')
            lmk = np.array([float(x) for x in line[1: -1]], dtype=np.float32)
            lmk = lmk.reshape((5, 2))
            filename = line[0]
            img2lmk[filename] = lmk
    return img2lmk",_7523.py,4,"for line in f:
    line = line.rstrip().split(' ')
    lmk = np.array([float(x) for x in line[1:-1]], dtype=np.float32)
    lmk = lmk.reshape((5, 2))
    filename = line[0]
    img2lmk[filename] = lmk","img2lmk = {line.rstrip().split(' ')[0]: np.array([float(x) for x in line.rstrip().split(' ')[1:-1]], dtype=np.float32).reshape((5, 2)) for line in f}",1,nan,nan
https://github.com/pyro-ppl/pyro/tree/master/pyro/contrib/epidemiology/compartmental.py,"def reparam(self, model):
        """"""
        Wrap a model with ``poutine.reparam``.
        """"""
        # Transform to Haar coordinates.
        config = {}
        for name, dim in self.dims.items():
            config[name] = HaarReparam(dim=dim, flip=True)
        model = poutine.reparam(model, config)

        if self.split:
            # Split into low- and high-frequency parts.
            splits = [self.split, self.duration - self.split]
            config = {}
            for name, dim in self.dims.items():
                config[name + ""_haar""] = SplitReparam(splits, dim=dim)
            model = poutine.reparam(model, config)

        return model",_7787.py,7,"for (name, dim) in self.dims.items():
    config[name] = HaarReparam(dim=dim, flip=True)","config = {name: HaarReparam(dim=dim, flip=True) for (name, dim) in self.dims.items()}",1,nan,nan
https://github.com/pyro-ppl/pyro/tree/master/pyro/contrib/epidemiology/compartmental.py,"def reparam(self, model):
        """"""
        Wrap a model with ``poutine.reparam``.
        """"""
        # Transform to Haar coordinates.
        config = {}
        for name, dim in self.dims.items():
            config[name] = HaarReparam(dim=dim, flip=True)
        model = poutine.reparam(model, config)

        if self.split:
            # Split into low- and high-frequency parts.
            splits = [self.split, self.duration - self.split]
            config = {}
            for name, dim in self.dims.items():
                config[name + ""_haar""] = SplitReparam(splits, dim=dim)
            model = poutine.reparam(model, config)

        return model",_7787.py,15,"for (name, dim) in self.dims.items():
    config[name + '_haar'] = SplitReparam(splits, dim=dim)","config = {name + '_haar': SplitReparam(splits, dim=dim) for (name, dim) in self.dims.items()}",1,nan,nan
https://github.com/alteryx/evalml/tree/master/evalml/automl/automl_search.py,"def _post_evaluation_callback(self, pipeline, evaluation_results, job_log):
        job_log.write_to_logger(self.logger)
        training_time = evaluation_results[""training_time""]
        cv_data = evaluation_results[""cv_data""]
        cv_scores = evaluation_results[""cv_scores""]
        is_baseline = pipeline.model_family == ModelFamily.BASELINE
        cv_score = cv_scores.mean()
        cv_sd = cv_scores.std()

        percent_better_than_baseline = {}
        mean_cv_all_objectives = self._get_mean_cv_scores_for_all_objectives(
            cv_data, self.objective_name_to_class
        )
        if is_baseline:
            self._baseline_cv_scores = mean_cv_all_objectives
        for obj_name in mean_cv_all_objectives:
            objective_class = self.objective_name_to_class[obj_name]

            # In the event add_to_rankings is called before search _baseline_cv_scores will be empty so we will return
            # nan for the base score.
            percent_better = objective_class.calculate_percent_difference(
                mean_cv_all_objectives[obj_name],
                self._baseline_cv_scores.get(obj_name, np.nan),
            )
            percent_better_than_baseline[obj_name] = percent_better

        high_variance_cv = self._check_for_high_variance(pipeline, cv_scores)

        pipeline_id = len(self._results[""pipeline_results""])
        self._results[""pipeline_results""][pipeline_id] = {
            ""id"": pipeline_id,
            ""pipeline_name"": pipeline.name,
            ""pipeline_class"": pipeline.__class__,
            ""pipeline_summary"": pipeline.summary,
            ""parameters"": pipeline.parameters,
            ""mean_cv_score"": cv_score,
            ""standard_deviation_cv_score"": cv_sd,
            ""high_variance_cv"": high_variance_cv,
            ""training_time"": training_time,
            ""cv_data"": cv_data,
            ""percent_better_than_baseline_all_objectives"": percent_better_than_baseline,
            ""percent_better_than_baseline"": percent_better_than_baseline[
                self.objective.name
            ],
            ""validation_score"": cv_scores[0],
        }
        self._pipelines_searched.update({pipeline_id: pipeline.clone()})

        if pipeline.model_family == ModelFamily.ENSEMBLE:
            input_pipeline_ids = [
                self._automl_algorithm._best_pipeline_info[model_family][""id""]
                for model_family in self._automl_algorithm._best_pipeline_info
            ]
            self._results[""pipeline_results""][pipeline_id][
                ""input_pipeline_ids""
            ] = input_pipeline_ids

        self._results[""search_order""].append(pipeline_id)

        if not is_baseline:
            score_to_minimize = (
                -cv_score if self.objective.greater_is_better else cv_score
            )
            try:
                self._automl_algorithm.add_result(
                    score_to_minimize,
                    pipeline,
                    self._results[""pipeline_results""][pipeline_id],
                )
            except PipelineNotFoundError:
                pass

        # True when running in a jupyter notebook, else the plot is an instance of plotly.Figure
        if isinstance(self.search_iteration_plot, SearchIterationPlot):
            self.search_iteration_plot.update(self.results, self.objective)

        if self.add_result_callback:
            self.add_result_callback(
                self._results[""pipeline_results""][pipeline_id], pipeline, self
            )
        return pipeline_id",_8404.py,16,"for obj_name in mean_cv_all_objectives:
    objective_class = self.objective_name_to_class[obj_name]
    percent_better = objective_class.calculate_percent_difference(mean_cv_all_objectives[obj_name], self._baseline_cv_scores.get(obj_name, np.nan))
    percent_better_than_baseline[obj_name] = percent_better","percent_better_than_baseline = {obj_name: self.objective_name_to_class[obj_name].calculate_percent_difference(mean_cv_all_objectives[obj_name], self._baseline_cv_scores.get(obj_name, np.nan)) for obj_name in mean_cv_all_objectives}",1,nan,nan
https://github.com/darrenburns/ward/tree/master/ward/testing.py,"def _unpack_resolved(fixture_dict: Dict[str, Any]) -> Dict[str, Any]:
        resolved_vals = {}
        for (k, arg) in fixture_dict.items():
            if isinstance(arg, Fixture):
                resolved_vals[k] = arg.resolved_val
            else:
                resolved_vals[k] = arg
        return resolved_vals",_8546.py,3,"for (k, arg) in fixture_dict.items():
    if isinstance(arg, Fixture):
        resolved_vals[k] = arg.resolved_val
    else:
        resolved_vals[k] = arg","resolved_vals = {k: arg.resolved_val if isinstance(arg, Fixture) else arg for (k, arg) in fixture_dict.items()}",1,nan,nan
https://github.com/Kismuz/btgym/tree/master/btgym/server.py,"def run(self):
        """"""
        Server process runtime body. This method is invoked by env._start_server().
        """"""
        # Logging:
        from logbook import Logger, StreamHandler, WARNING
        import sys
        StreamHandler(sys.stdout).push_application()
        if self.log_level is None:
            self.log_level = WARNING
        self.log = Logger('BTgymServer_{}'.format(self.task), level=self.log_level)

        self.process = multiprocessing.current_process()
        self.log.info('PID: {}'.format(self.process.pid))

        # Runtime Housekeeping:
        cerebro = None
        episode_result = dict()
        episode_sample = None

        # How long to wait for data_master to reset data:
        self.wait_for_data_reset = 300  # seconds

        connect_timeout = 60  # in seconds

        # Set up a comm. channel for server as ZMQ socket
        # to carry both service and data signal
        # !! Reminder: Since we use REQ/REP - messages do go in pairs !!
        self.context = zmq.Context()
        self.socket = self.context.socket(zmq.REP)
        self.socket.setsockopt(zmq.RCVTIMEO, -1)
        self.socket.setsockopt(zmq.SNDTIMEO, connect_timeout * 1000)
        self.socket.bind(self.network_address)

        self.data_context = zmq.Context()
        self.data_socket = self.data_context.socket(zmq.REQ)
        self.data_socket.setsockopt(zmq.RCVTIMEO, connect_timeout * 1000)
        self.data_socket.setsockopt(zmq.SNDTIMEO, connect_timeout * 1000)
        self.data_socket.connect(self.data_network_address)

        # Check connection:
        self.log.debug('Pinging data_server at: {} ...'.format(self.data_network_address))

        data_server_response = self._comm_with_timeout(
            socket=self.data_socket,
            message={'ctrl': 'ping!'}
        )
        if data_server_response['status'] in 'ok':
            self.log.debug('Data_server seems ready with response: <{}>'.
                          format(data_server_response['message']))

        else:
            msg = 'Data_server unreachable with status: <{}>.'.\
                format(data_server_response['status'])
            self.log.error(msg)
            raise ConnectionError(msg)

        # Init renderer:
        self.render.initialize_pyplot()

        # Mandatory DrawDown and auxillary plotting observers to add to data-master strategy instance:
        # TODO: make plotters optional args
        if self.render.enabled:
            aux_obsrevers = [bt.observers.DrawDown, Reward, Position, NormPnL]

        else:
            aux_obsrevers = [bt.observers.DrawDown]

        # Server 'Control Mode' loop:
        for episode_number in itertools.count(0):
            while True:
                # Stuck here until '_reset' or '_stop':
                service_input = self.socket.recv_pyobj()
                msg = 'Control mode: received <{}>'.format(service_input)
                self.log.debug(msg)

                if 'ctrl' in service_input:
                    # It's time to exit:
                    if service_input['ctrl'] == '_stop':
                        # Server shutdown logic:
                        # send last run statistic, release comm channel and exit:
                        message = 'Exiting.'
                        self.log.info(message)
                        self.socket.send_pyobj(message)
                        self.socket.close()
                        self.context.destroy()
                        return None

                    # Start episode:
                    elif service_input['ctrl'] == '_reset':
                        message = 'Preparing new episode with kwargs: {}'.format(service_input['kwargs'])
                        self.log.debug(message)
                        self.socket.send_pyobj(message)  # pairs '_reset'
                        break

                    # Retrieve statistic:
                    elif service_input['ctrl'] == '_getstat':
                        self.socket.send_pyobj(episode_result)
                        self.log.debug('Episode statistic sent.')

                    # Send episode rendering:
                    elif service_input['ctrl'] == '_render' and 'mode' in service_input.keys():
                        # Just send what we got:
                        self.socket.send_pyobj(self.render.render(service_input['mode']))
                        self.log.debug('Episode rendering for [{}] sent.'.format(service_input['mode']))

                    # Serve data-dependent environment with trial instance:
                    elif service_input['ctrl'] == '_get_data':
                        message = 'Sending trial data to slave'
                        self.log.debug(message)
                        self.socket.send_pyobj(self.get_trial_message())

                    # Serve data-dependent environment with dataset statisitc:
                    elif service_input['ctrl'] == '_get_info':
                        message = 'Sending dataset statistic to slave'
                        self.log.debug(message)
                        self.socket.send_pyobj(self.get_dataset_stat())

                    else:  # ignore any other input
                        # NOTE: response string must include 'ctrl' key
                        # for env.reset(), env.get_stat(), env.close() correct operation.
                        message = {'ctrl': 'send control keys: <_reset>, <_getstat>, <_render>, <_stop>.'}
                        self.log.debug('Control mode: sent: ' + str(message))
                        self.socket.send_pyobj(message)  # pairs any other input

                else:
                    message = 'No <ctrl> key received:{}\nHint: forgot to call reset()?'.format(msg)
                    self.log.debug(message)
                    self.socket.send_pyobj(message)

            # Got '_reset' signal -> prepare Cerebro subclass and run episode:
            start_time = time.time()
            cerebro = copy.deepcopy(self.cerebro)
            cerebro._socket = self.socket
            cerebro._data_socket = self.data_socket
            cerebro._log = self.log
            cerebro._render = self.render

            # Pass methods for serving capabilities:
            cerebro._get_data = self.get_trial_message
            cerebro._get_info = self.get_dataset_stat

            # Add auxillary observers, if not already:
            for aux in aux_obsrevers:
                is_added = False
                for observer in cerebro.observers:
                    if aux in observer:
                        is_added = True
                if not is_added:
                    cerebro.addobserver(aux)

            # Add communication utility:
            cerebro.addanalyzer(_BTgymAnalyzer, _name='_env_analyzer',)

            # Data preparation:

            # Renew system state:
            current_timestamp, current_broadcast_message = self.get_broadcast_message()

            # Parse args we got with _reset call:
            sample_config = dict(
                episode_config=copy.deepcopy(DataSampleConfig),
                trial_config=copy.deepcopy(DataSampleConfig)
            )
            for key, config in sample_config.items():
                try:
                    config.update(service_input['kwargs'][key])

                except KeyError:
                    self.log.debug(
                        '_reset <{}> kwarg not found, using default values: {}'.format(key, config)
                    )
            sample_config['trial_config']['broadcast_message'] = current_broadcast_message
            sample_config['episode_config']['broadcast_message'] = current_broadcast_message

            # Get new Trial from data_server if requested,
            # despite bult-in new/reuse data object sampling option, perform checks here to avoid
            # redundant traffic:
            if sample_config['trial_config']['get_new'] or self.trial_sample is None:
                self.log.info(
                    'Requesting new Trial sample with args: {}'.format(sample_config['trial_config'])
                )
                self.trial_sample, self.trial_stat, self.dataset_stat, origin, current_timestamp =\
                    self.get_trial(**sample_config['trial_config'])

                if origin in 'data_server':
                    self.trial_sample.set_logger(self.log_level, self.task)

                self.log.debug('Got new Trial: <{}>'.format(self.trial_sample.filename))

            else:
                self.log.info('Reusing Trial <{}>'.format(self.trial_sample.filename))
                # current_timestamp = self.get_global_time()

            self.log.debug(
                'current global_time: {}'.format(datetime.datetime.fromtimestamp(current_timestamp))
            )
            # Get episode:
            if sample_config['episode_config']['timestamp'] is None or\
                    sample_config['episode_config']['timestamp'] < current_timestamp:
                sample_config['episode_config']['timestamp'] = current_timestamp

            self.log.info(
                'Requesting episode from <{}> with args: {}'.
                format(self.trial_sample.filename, sample_config['episode_config'])
            )

            episode_sample = self.trial_sample.sample(**sample_config['episode_config'])
            self.log.debug('Got new Episode: <{}>'.format(episode_sample.filename))

            # Get episode data statistic and pass it to strategy params:
            cerebro.strats[0][0][2]['trial_stat'] = self.trial_stat
            cerebro.strats[0][0][2]['trial_metadata'] = self.trial_sample.metadata
            cerebro.strats[0][0][2]['dataset_stat'] = self.dataset_stat
            cerebro.strats[0][0][2]['episode_stat'] = episode_sample.describe()
            cerebro.strats[0][0][2]['metadata'] = episode_sample.metadata

            cerebro.strats[0][0][2]['broadcast_message'] = current_broadcast_message

            # Set nice broker cash plotting:
            cerebro.broker.set_shortcash(False)

            # Convert and add data to engine:
            feed = episode_sample.to_btfeed()
            if isinstance(feed, dict):
                for key, stream in feed.items():
                    cerebro.adddata(stream, name=key)

            else:
                cerebro.adddata(feed, name='base_asset')

            # Finally:
            episode = cerebro.run(stdstats=True, preload=False, oldbuysell=True, tradehistory=True)[0]

            self.log.debug('Episode run finished.')

            # Update episode rendering:
            _ = self.render.render('just_render', cerebro=cerebro)
            _ = None

            # Recover that bloody analytics:
            analyzers_list = episode.analyzers.getnames()
            analyzers_list.remove('_env_analyzer')

            elapsed_time = timedelta(seconds=time.time() - start_time)
            self.log.debug('Episode elapsed time: {}.'.format(elapsed_time))

            episode_result['episode'] = episode_number
            episode_result['runtime'] = elapsed_time
            episode_result['length'] = len(episode.data.close)

            for name in analyzers_list:
                episode_result[name] = episode.analyzers.getbyname(name).get_analysis()

            gc.collect()

        # Just in case -- we actually shouldn't get there except by some error:
        return None",_8890.py,252,"for name in analyzers_list:
    episode_result[name] = episode.analyzers.getbyname(name).get_analysis()",episode_result.update({name: episode.analyzers.getbyname(name).get_analysis() for name in analyzers_list}),1,nan,nan
https://github.com/sphinx-doc/sphinx/tree/master/tests/test_ext_apidoc.py,"def apidoc_params(request):
    pargs = {}
    kwargs = {}

    for info in reversed(list(request.node.iter_markers(""apidoc""))):
        for i, a in enumerate(info.args):
            pargs[i] = a
        kwargs.update(info.kwargs)

    args = [pargs[i] for i in sorted(pargs.keys())]
    return args, kwargs",_9002.py,6,"for (i, a) in enumerate(info.args):
    pargs[i] = a","pargs.update({i: a for (i, a) in enumerate(info.args)})",1,nan,nan
https://github.com/openstack/nova/tree/master/nova/api/openstack/compute/images.py,"def _get_filters(self, req):
        """"""Return a dictionary of query param filters from the request.

        :param req: the Request object coming from the wsgi layer
        :retval a dict of key/value filters
        """"""
        filters = {}
        for param in req.params:
            if param in SUPPORTED_FILTERS or param.startswith('property-'):
                # map filter name or carry through if property-*
                filter_name = SUPPORTED_FILTERS.get(param, param)
                filters[filter_name] = req.params.get(param)

        # ensure server filter is the instance uuid
        filter_name = 'property-instance_uuid'
        try:
            filters[filter_name] = filters[filter_name].rsplit('/', 1)[1]
        except (AttributeError, IndexError, KeyError):
            pass

        filter_name = 'status'
        if filter_name in filters:
            # The Image API expects us to use lowercase strings for status
            filters[filter_name] = filters[filter_name].lower()

        return filters",_9565.py,8,"for param in req.params:
    if param in SUPPORTED_FILTERS or param.startswith('property-'):
        filter_name = SUPPORTED_FILTERS.get(param, param)
        filters[filter_name] = req.params.get(param)","filters = {SUPPORTED_FILTERS.get(param, param): req.params.get(param) for param in req.params if param in SUPPORTED_FILTERS or param.startswith('property-')}",1,nan,nan
https://github.com/QuantFans/quantdigger/tree/master/quantdigger/util/log.py,"def __init__(self, color=True, fmt=DEFAULT_FORMAT,
                 datefmt=DEFAULT_DATE_FORMAT, colors=DEFAULT_COLORS):
        r""""""
        :arg bool color: Enables color support.
        :arg string fmt: Log message format.
          It will be applied to the attributes dict of log records. The
          text between ``%(color)s`` and ``%(end_color)s`` will be colored
          depending on the level if color support is on.
        :arg dict colors: color mappings from logging level to terminal color
          code
        :arg string datefmt: Datetime format.
          Used for formatting ``(asctime)`` placeholder in ``prefix_fmt``.
        .. versionchanged:: 3.2
           Added ``fmt`` and ``datefmt`` arguments.
        """"""
        logging.Formatter.__init__(self, datefmt=datefmt)
        self._fmt = fmt

        self._colors = {}
        if color and _stderr_supports_color():
            # The curses module has some str/bytes confusion in
            # python3.  Until version 3.2.3, most methods return
            # bytes, but only accept strings.  In addition, we want to
            # output these strings with the logging module, which
            # works with unicode strings.  The explicit calls to
            # unicode() below are harmless in python2 but will do the
            # right conversion in python 3.
            fg_color = (curses.tigetstr(""setaf"") or
                        curses.tigetstr(""setf"") or """")
            if (3, 0) < sys.version_info < (3, 2, 3):
                fg_color = unicode_type(fg_color, ""ascii"")

            for levelno, code in six.iteritems(colors):
                self._colors[levelno] = unicode_type(curses.tparm(fg_color, code), ""ascii"")
            self._normal = unicode_type(curses.tigetstr(""sgr0""), ""ascii"")
        else:
            self._normal = ''",_9919.py,33,"for (levelno, code) in six.iteritems(colors):
    self._colors[levelno] = unicode_type(curses.tparm(fg_color, code), 'ascii')","self._colors = {levelno: unicode_type(curses.tparm(fg_color, code), 'ascii') for (levelno, code) in six.iteritems(colors)}",1,nan,nan
https://github.com/AxeldeRomblay/MLBox/tree/master/mlbox/encoding/categorical_encoder.py,"def fit(self, df_train, y_train):
        """"""Fit Categorical Encoder.

        Encode categorical variable of a dataframe
        following strategy parameters.

        Parameters
        ----------
        df_train : pandas.Dataframe of shape = (n_train, n_features).
            The training dataset with numerical and categorical features.
            NA values are allowed.
        y_train : pandas.Series of shape = (n_train, ).
            The target for classification or regression tasks.

        Returns
        -------
        object
            self

        """"""
        self.__Lcat = df_train.dtypes[df_train.dtypes == 'object'].index
        self.__Lnum = df_train.dtypes[df_train.dtypes != 'object'].index

        if (len(self.__Lcat) == 0):
            self.__fitOK = True

        else:

            #################################################
            #                Label Encoding
            #################################################

            if (self.strategy == 'label_encoding'):

                for col in self.__Lcat:

                    d = dict()
                    levels = list(df_train[col].unique())
                    nan = False

                    if np.NaN in levels:
                        nan = True
                        levels.remove(np.NaN)

                    for enc, level in enumerate([np.NaN]*nan + sorted(levels)):
                        d[level] = enc  # TODO: Optimize loop?

                    self.__Enc[col] = d

                self.__fitOK = True

            #################################################
            #                Dummification
            #################################################

            elif (self.strategy == 'dummification'):

                for col in self.__Lcat:
                    # TODO: Optimize?
                    self.__Enc[col] = list(df_train[col].dropna().unique())

                self.__fitOK = True

            #################################################
            #                Entity Embedding
            #################################################

            elif (self.strategy == 'entity_embedding'):

                # Parameters
                A = 10   # 15 : more complex
                B = 5    # 2 or 3 : more complex

                # computing interactions
                self.__K = {}
                for col in self.__Lcat:
                    exp_ = np.exp(-df_train[col].nunique() * 0.05)
                    self.__K[col] = np.int(5 * (1 - exp_) + 1)

                sum_ = sum([1. * np.log(k) for k in self.__K.values()])
                # TODO: Add reference for this formula?

                # Number of neurons for layer 1 and 2
                n_layer1 = min(1000,
                               int(A * (len(self.__K) ** 0.5) * sum_ + 1))
                n_layer2 = int(n_layer1 / B) + 2

                # Dropouts
                dropout1 = 0.1
                dropout2 = 0.1

                # Learning parameters
                epochs = 20  # 25 : more iterations
                batch_size = 128  # 256 : gradient more stable

                # Creating the neural network

                embeddings = []
                inputs = []

                for col in self.__Lcat:

                    d = dict()
                    levels = list(df_train[col].unique())
                    nan = False

                    if np.NaN in levels:
                        nan = True
                        levels.remove(np.NaN)

                    for enc, level in enumerate([np.NaN]*nan + sorted(levels)):
                        d[level] = enc  # TODO: Optimize loop?

                    self.__Enc[col] = d

                    var = Input(shape=(1,))
                    inputs.append(var)

                    emb = Embedding(input_dim=len(self.__Enc[col]),
                                    output_dim=self.__K[col],
                                    input_length=1)(var)
                    emb = Reshape(target_shape=(self.__K[col],))(emb)

                    embeddings.append(emb)

                if (len(self.__Lcat) > 1):
                    emb_layer = concatenate(embeddings)
                else:
                    emb_layer = embeddings[0]

                lay1 = Dense(n_layer1,
                             kernel_initializer='uniform',
                             activation='relu')(emb_layer)
                lay1 = Dropout(dropout1)(lay1)

                lay2 = Dense(n_layer2,
                             kernel_initializer='uniform',
                             activation='relu')(lay1)
                lay2 = Dropout(dropout2)(lay2)

                # Learning the weights

                if ((y_train.dtype == object) | (y_train.dtype == 'int')):

                    # Classification
                    if (y_train.nunique() == 2):

                        outputs = Dense(1,
                                        kernel_initializer='normal',
                                        activation='sigmoid')(lay2)

                        model = Model(inputs=inputs, outputs=outputs)
                        model.compile(loss='binary_crossentropy',
                                      optimizer='adam')
                        model.fit(
                            [df_train[col].apply(lambda x: self.__Enc[col][x]).values
                             for col in self.__Lcat],
                            pd.get_dummies(y_train,
                                           drop_first=True).astype(int).values,
                            epochs=epochs,
                            batch_size=batch_size,
                            verbose=int(self.verbose)
                        )

                    else:

                        outputs = Dense(y_train.nunique(),
                                        kernel_initializer='normal',
                                        activation='softmax')(lay2)

                        model = Model(inputs=inputs, outputs=outputs)
                        model.compile(loss='binary_crossentropy',
                                      optimizer='adam')
                        model.fit(
                            [df_train[col].apply(lambda x: self.__Enc[col][x]).values
                             for col in self.__Lcat],
                            pd.get_dummies(y_train,
                                           drop_first=False).astype(int).values,
                            epochs=epochs,
                            batch_size=batch_size,
                            verbose=int(self.verbose)
                        )

                else:

                    # Regression
                    outputs = Dense(1, kernel_initializer='normal')(lay2)
                    model = Model(inputs=inputs, outputs=outputs)
                    model.compile(loss='mean_squared_error', optimizer='adam')
                    model.fit(
                        [df_train[col].apply(lambda x: self.__Enc[col][x]).values
                         for col in self.__Lcat],
                        y_train.values,
                        epochs=epochs,
                        batch_size=batch_size,
                        verbose=int(self.verbose)
                    )

                self.__weights = model.get_weights()

                self.__fitOK = True

            #################################################
            #                Random Projection
            #################################################

            elif(self.strategy == 'random_projection'):

                for col in self.__Lcat:

                    exp_ = np.exp(-df_train[col].nunique() * 0.05)
                    # TODO: Add reference to formula used here below?
                    self.__K[col] = np.int(5 * (1 - exp_)) + 1

                    d = dict()
                    levels = list(df_train[col].unique())
                    nan = False

                    if np.NaN in levels:
                        nan = True
                        levels.remove(np.NaN)

                    for k in range(self.__K[col]):

                        if (k == 0):
                            levels = sorted(levels)

                        else:
                            np.random.seed(k)
                            np.random.shuffle(levels)

                        for enc, level in enumerate([np.NaN] * nan + levels):
                            if(k == 0):
                                d[level] = [enc]
                            else:
                                d[level] = d[level] + [enc]

                    self.__Enc[col] = d

                self.__fitOK = True

            else:
                raise ValueError(""Categorical encoding strategy is not valid"")

        return self",_9957.py,45,"for (enc, level) in enumerate([np.NaN] * nan + sorted(levels)):
    d[level] = enc","d = {level: enc for (enc, level) in enumerate([np.NaN] * nan + sorted(levels))}",1,nan,nan
https://github.com/AxeldeRomblay/MLBox/tree/master/mlbox/encoding/categorical_encoder.py,"def fit(self, df_train, y_train):
        """"""Fit Categorical Encoder.

        Encode categorical variable of a dataframe
        following strategy parameters.

        Parameters
        ----------
        df_train : pandas.Dataframe of shape = (n_train, n_features).
            The training dataset with numerical and categorical features.
            NA values are allowed.
        y_train : pandas.Series of shape = (n_train, ).
            The target for classification or regression tasks.

        Returns
        -------
        object
            self

        """"""
        self.__Lcat = df_train.dtypes[df_train.dtypes == 'object'].index
        self.__Lnum = df_train.dtypes[df_train.dtypes != 'object'].index

        if (len(self.__Lcat) == 0):
            self.__fitOK = True

        else:

            #################################################
            #                Label Encoding
            #################################################

            if (self.strategy == 'label_encoding'):

                for col in self.__Lcat:

                    d = dict()
                    levels = list(df_train[col].unique())
                    nan = False

                    if np.NaN in levels:
                        nan = True
                        levels.remove(np.NaN)

                    for enc, level in enumerate([np.NaN]*nan + sorted(levels)):
                        d[level] = enc  # TODO: Optimize loop?

                    self.__Enc[col] = d

                self.__fitOK = True

            #################################################
            #                Dummification
            #################################################

            elif (self.strategy == 'dummification'):

                for col in self.__Lcat:
                    # TODO: Optimize?
                    self.__Enc[col] = list(df_train[col].dropna().unique())

                self.__fitOK = True

            #################################################
            #                Entity Embedding
            #################################################

            elif (self.strategy == 'entity_embedding'):

                # Parameters
                A = 10   # 15 : more complex
                B = 5    # 2 or 3 : more complex

                # computing interactions
                self.__K = {}
                for col in self.__Lcat:
                    exp_ = np.exp(-df_train[col].nunique() * 0.05)
                    self.__K[col] = np.int(5 * (1 - exp_) + 1)

                sum_ = sum([1. * np.log(k) for k in self.__K.values()])
                # TODO: Add reference for this formula?

                # Number of neurons for layer 1 and 2
                n_layer1 = min(1000,
                               int(A * (len(self.__K) ** 0.5) * sum_ + 1))
                n_layer2 = int(n_layer1 / B) + 2

                # Dropouts
                dropout1 = 0.1
                dropout2 = 0.1

                # Learning parameters
                epochs = 20  # 25 : more iterations
                batch_size = 128  # 256 : gradient more stable

                # Creating the neural network

                embeddings = []
                inputs = []

                for col in self.__Lcat:

                    d = dict()
                    levels = list(df_train[col].unique())
                    nan = False

                    if np.NaN in levels:
                        nan = True
                        levels.remove(np.NaN)

                    for enc, level in enumerate([np.NaN]*nan + sorted(levels)):
                        d[level] = enc  # TODO: Optimize loop?

                    self.__Enc[col] = d

                    var = Input(shape=(1,))
                    inputs.append(var)

                    emb = Embedding(input_dim=len(self.__Enc[col]),
                                    output_dim=self.__K[col],
                                    input_length=1)(var)
                    emb = Reshape(target_shape=(self.__K[col],))(emb)

                    embeddings.append(emb)

                if (len(self.__Lcat) > 1):
                    emb_layer = concatenate(embeddings)
                else:
                    emb_layer = embeddings[0]

                lay1 = Dense(n_layer1,
                             kernel_initializer='uniform',
                             activation='relu')(emb_layer)
                lay1 = Dropout(dropout1)(lay1)

                lay2 = Dense(n_layer2,
                             kernel_initializer='uniform',
                             activation='relu')(lay1)
                lay2 = Dropout(dropout2)(lay2)

                # Learning the weights

                if ((y_train.dtype == object) | (y_train.dtype == 'int')):

                    # Classification
                    if (y_train.nunique() == 2):

                        outputs = Dense(1,
                                        kernel_initializer='normal',
                                        activation='sigmoid')(lay2)

                        model = Model(inputs=inputs, outputs=outputs)
                        model.compile(loss='binary_crossentropy',
                                      optimizer='adam')
                        model.fit(
                            [df_train[col].apply(lambda x: self.__Enc[col][x]).values
                             for col in self.__Lcat],
                            pd.get_dummies(y_train,
                                           drop_first=True).astype(int).values,
                            epochs=epochs,
                            batch_size=batch_size,
                            verbose=int(self.verbose)
                        )

                    else:

                        outputs = Dense(y_train.nunique(),
                                        kernel_initializer='normal',
                                        activation='softmax')(lay2)

                        model = Model(inputs=inputs, outputs=outputs)
                        model.compile(loss='binary_crossentropy',
                                      optimizer='adam')
                        model.fit(
                            [df_train[col].apply(lambda x: self.__Enc[col][x]).values
                             for col in self.__Lcat],
                            pd.get_dummies(y_train,
                                           drop_first=False).astype(int).values,
                            epochs=epochs,
                            batch_size=batch_size,
                            verbose=int(self.verbose)
                        )

                else:

                    # Regression
                    outputs = Dense(1, kernel_initializer='normal')(lay2)
                    model = Model(inputs=inputs, outputs=outputs)
                    model.compile(loss='mean_squared_error', optimizer='adam')
                    model.fit(
                        [df_train[col].apply(lambda x: self.__Enc[col][x]).values
                         for col in self.__Lcat],
                        y_train.values,
                        epochs=epochs,
                        batch_size=batch_size,
                        verbose=int(self.verbose)
                    )

                self.__weights = model.get_weights()

                self.__fitOK = True

            #################################################
            #                Random Projection
            #################################################

            elif(self.strategy == 'random_projection'):

                for col in self.__Lcat:

                    exp_ = np.exp(-df_train[col].nunique() * 0.05)
                    # TODO: Add reference to formula used here below?
                    self.__K[col] = np.int(5 * (1 - exp_)) + 1

                    d = dict()
                    levels = list(df_train[col].unique())
                    nan = False

                    if np.NaN in levels:
                        nan = True
                        levels.remove(np.NaN)

                    for k in range(self.__K[col]):

                        if (k == 0):
                            levels = sorted(levels)

                        else:
                            np.random.seed(k)
                            np.random.shuffle(levels)

                        for enc, level in enumerate([np.NaN] * nan + levels):
                            if(k == 0):
                                d[level] = [enc]
                            else:
                                d[level] = d[level] + [enc]

                    self.__Enc[col] = d

                self.__fitOK = True

            else:
                raise ValueError(""Categorical encoding strategy is not valid"")

        return self",_9957.py,111,"for (enc, level) in enumerate([np.NaN] * nan + sorted(levels)):
    d[level] = enc","d = {level: enc for (enc, level) in enumerate([np.NaN] * nan + sorted(levels))}",1,nan,nan
https://github.com/wting/autojump/tree/master/bin/autojump_argparse.py,"def _format_actions_usage(self, actions, groups):
        # find group indices and identify actions in groups
        group_actions = set()
        inserts = {}
        for group in groups:
            try:
                start = actions.index(group._group_actions[0])
            except ValueError:
                continue
            else:
                end = start + len(group._group_actions)
                if actions[start:end] == group._group_actions:
                    for action in group._group_actions:
                        group_actions.add(action)
                    if not group.required:
                        if start in inserts:
                            inserts[start] += ' ['
                        else:
                            inserts[start] = '['
                        inserts[end] = ']'
                    else:
                        if start in inserts:
                            inserts[start] += ' ('
                        else:
                            inserts[start] = '('
                        inserts[end] = ')'
                    for i in range(start + 1, end):
                        inserts[i] = '|'

        # collect all actions format strings
        parts = []
        for i, action in enumerate(actions):

            # suppressed arguments are marked with None
            # remove | separators for suppressed arguments
            if action.help is SUPPRESS:
                parts.append(None)
                if inserts.get(i) == '|':
                    inserts.pop(i)
                elif inserts.get(i + 1) == '|':
                    inserts.pop(i + 1)

            # produce all arg strings
            elif not action.option_strings:
                part = self._format_args(action, action.dest)

                # if it's in a group, strip the outer []
                if action in group_actions:
                    if part[0] == '[' and part[-1] == ']':
                        part = part[1:-1]

                # add the action string to the list
                parts.append(part)

            # produce the first way to invoke the option in brackets
            else:
                option_string = action.option_strings[0]

                # if the Optional doesn't take a value, format is:
                #    -s or --long
                if action.nargs == 0:
                    part = '%s' % option_string

                # if the Optional takes a value, format is:
                #    -s ARGS or --long ARGS
                else:
                    default = action.dest.upper()
                    args_string = self._format_args(action, default)
                    part = '%s %s' % (option_string, args_string)

                # make it look optional if it's not required or in a group
                if not action.required and action not in group_actions:
                    part = '[%s]' % part

                # add the action string to the list
                parts.append(part)

        # insert things at the necessary indices
        for i in sorted(inserts, reverse=True):
            parts[i:i] = [inserts[i]]

        # join all the action items with spaces
        text = ' '.join([item for item in parts if item is not None])

        # clean up separators for mutually exclusive groups
        open = r'[\[(]'
        close = r'[\])]'
        text = _re.sub(r'(%s) ' % open, r'\1', text)
        text = _re.sub(r' (%s)' % close, r'\1', text)
        text = _re.sub(r'%s *%s' % (open, close), r'', text)
        text = _re.sub(r'\(([^|]*)\)', r'\1', text)
        text = text.strip()

        # return the text
        return text",_10529.py,27,"for i in range(start + 1, end):
    inserts[i] = '|'","inserts.update({i: '|' for i in range(start + 1, end)})",1,nan,nan
https://github.com/saltstack/salt/tree/master/salt/pillar/pillar_ldap.py,"def _do_search(conf):
    """"""
    Builds connection and search arguments, performs the LDAP search and
    formats the results as a dictionary appropriate for pillar use.
    """"""
    # Build LDAP connection args
    connargs = {}
    for name in [""server"", ""port"", ""tls"", ""binddn"", ""bindpw"", ""anonymous""]:
        connargs[name] = _config(name, conf)
    if connargs[""binddn""] and connargs[""bindpw""]:
        connargs[""anonymous""] = False
    # Build search args
    try:
        _filter = conf[""filter""]
    except KeyError:
        raise SaltInvocationError(""missing filter"")
    _dn = _config(""dn"", conf)
    scope = _config(""scope"", conf)
    _lists = _config(""lists"", conf) or []
    _attrs = _config(""attrs"", conf) or []
    _dict_key_attr = _config(""dict_key_attr"", conf, ""dn"")
    attrs = _lists + _attrs + [_dict_key_attr]
    if not attrs:
        attrs = None
    # Perform the search
    try:
        result = __salt__[""ldap.search""](_filter, _dn, scope, attrs, **connargs)[
            ""results""
        ]
    except IndexError:  # we got no results for this search
        log.debug(""LDAP search returned no results for filter %s"", _filter)
        result = {}
    except Exception:  # pylint: disable=broad-except
        log.critical(""Failed to retrieve pillar data from LDAP:\n"", exc_info=True)
        return {}
    return result",_10816.py,8,"for name in ['server', 'port', 'tls', 'binddn', 'bindpw', 'anonymous']:
    connargs[name] = _config(name, conf)","connargs = {name: _config(name, conf) for name in ['server', 'port', 'tls', 'binddn', 'bindpw', 'anonymous']}",1,nan,nan
https://github.com/neuml/txtai/tree/master/src/python/txtai/api/application.py,"def apirouters():
    """"""
    Lists available APIRouters.

    Returns:
        {router name: router}
    """"""

    # Get handle to api module
    api = sys.modules[""."".join(__name__.split(""."")[:-1])]

    available = {}
    for name, rclass in inspect.getmembers(api, inspect.ismodule):
        if hasattr(rclass, ""router"") and isinstance(rclass.router, APIRouter):
            available[name.lower()] = rclass.router

    return available",_10820.py,13,"for (name, rclass) in inspect.getmembers(api, inspect.ismodule):
    if hasattr(rclass, 'router') and isinstance(rclass.router, APIRouter):
        available[name.lower()] = rclass.router","available = {name.lower(): rclass.router for (name, rclass) in inspect.getmembers(api, inspect.ismodule) if hasattr(rclass, 'router') and isinstance(rclass.router, APIRouter)}",1,nan,nan
https://github.com/osroom/osroom/tree/master/apps/modules/theme_setting/process/display_setting.py,"def get_display_settings():
    """"""
    :return:
    """"""

    keyword = request.argget.all(""keyword"")
    category_id = request.argget.all(""category_id"")
    ctype = request.argget.all(""ctype"")
    page = str_to_num(request.argget.all(""page"", 1))
    pre = str_to_num(request.argget.all(""pre"", 12))
    sort = json_to_pyseq(request.argget.all('sort'))
    theme_name = request.argget.all(""theme_name"")
    s, r = arg_verify([(gettext(""type""), ctype)],
                      only=get_config(""category"", ""CATEGORY_TYPE"").values())
    if not s:
        return r

    s, r = arg_verify([(gettext(""theme name""), theme_name)], required=True)
    if not s:
        return r

    data = {}
    if category_id:
        if category_id == ""default"":
            category_id = {""$in"": [None, """"]}
        query = {""category_id"": category_id, ""type"": ctype}
    else:
        query = {""type"": ctype}

    if keyword:
        k_rule = {""$regex"": keyword, ""$options"": ""$i""}
        query[""$or""] = [{""name"": k_rule}, {""title"": k_rule},
                        {""link"": k_rule}, {""text"": k_rule}]

    query[""theme_name""] = theme_name
    # sort
    if sort:
        for i, srt in enumerate(sort):
            sort[i] = (list(srt.keys())[0], list(srt.values())[0])
    else:
        sort = [(""time"", -1)]

    display_settings = mdbs[""sys""].db.theme_display_setting.find(query)
    data_cnt = display_settings.count(True)
    display_settings = list(display_settings.sort(
        sort).skip(pre * (page - 1)).limit(pre))

    r = list(mdbs[""web""].db.theme_category.find(
        {
            ""user_id"": 0,
            ""type"": ctype,
            ""theme_name"": theme_name}
    ))
    categories = {}
    for cate in r:
        categories[str(cate[""_id""])] = cate[""name""]
    for d in display_settings:
        d[""_id""] = str(d[""_id""])
        if ""url"" in d and d[""url""]:
            d[""url""] = get_file_url(d[""url""])
        if d[""category_id""] and str(d[""category_id""]) in categories:
            d[""category""] = categories[str(d[""category_id""])]

    data[""medias""] = datas_paging(
        pre=pre,
        page_num=page,
        data_cnt=data_cnt,
        datas=display_settings)
    data[""theme_name""] = theme_name
    return data",_11182.py,55,"for cate in r:
    categories[str(cate['_id'])] = cate['name']",categories = {str(cate['_id']): cate['name'] for cate in r},1,nan,nan
https://github.com/MrGiovanni/UNetPlusPlus/tree/master/pytorch/nnunet/evaluation/model_selection/rank_candidates_cascade.py,"if __name__ == ""__main__"":
    # run collect_all_fold0_results_and_summarize_in_one_csv.py first
    summary_files_dir = join(network_training_output_dir, ""summary_jsons_fold0_new"")
    output_file = join(network_training_output_dir, ""summary_cascade.csv"")

    folds = (0, )
    folds_str = """"
    for f in folds:
        folds_str += str(f)

    plans = ""nnUNetPlansv2.1""

    overwrite_plans = {
        'nnUNetTrainerCascadeFullRes': ['nnUNetPlans'],
    }

    trainers = [
        'nnUNetTrainerCascadeFullRes',
        'nnUNetTrainerV2CascadeFullRes_EducatedGuess',
        'nnUNetTrainerV2CascadeFullRes_EducatedGuess2',
        'nnUNetTrainerV2CascadeFullRes_EducatedGuess3',
        'nnUNetTrainerV2CascadeFullRes_lowerLR',
        'nnUNetTrainerV2CascadeFullRes',
        'nnUNetTrainerV2CascadeFullRes_noConnComp',
        'nnUNetTrainerV2CascadeFullRes_shorter_lowerLR',
        'nnUNetTrainerV2CascadeFullRes_shorter',
        'nnUNetTrainerV2CascadeFullRes_smallerBinStrel',
        #'',
        #'',
        #'',
        #'',
        #'',
        #'',
    ]

    datasets = \
        {
        ""Task003_Liver"": (""3d_cascade_fullres"", ),
        ""Task006_Lung"": (""3d_cascade_fullres"", ),
        ""Task007_Pancreas"": (""3d_cascade_fullres"", ),
        ""Task008_HepaticVessel"": (""3d_cascade_fullres"", ),
        ""Task009_Spleen"": (""3d_cascade_fullres"", ),
        ""Task010_Colon"": (""3d_cascade_fullres"", ),
        ""Task017_AbdominalOrganSegmentation"": (""3d_cascade_fullres"", ),
        #""Task029_LITS"": (""3d_cascade_fullres"", ),
        ""Task048_KiTS_clean"": (""3d_cascade_fullres"", ),
        ""Task055_SegTHOR"": (""3d_cascade_fullres"", ),
        ""Task056_VerSe"": (""3d_cascade_fullres"", ),
        #"""": (""3d_cascade_fullres"", ),
        }

    expected_validation_folder = ""validation_raw""
    alternative_validation_folder = ""validation""
    alternative_alternative_validation_folder = ""validation_tiledTrue_doMirror_True""

    interested_in = ""mean""

    result_per_dataset = {}
    for d in datasets:
        result_per_dataset[d] = {}
        for c in datasets[d]:
            result_per_dataset[d][c] = []

    valid_trainers = []
    all_trainers = []

    with open(output_file, 'w') as f:
        f.write(""trainer,"")
        for t in datasets.keys():
            s = t[4:7]
            for c in datasets[t]:
                s1 = s + ""_"" + c[3]
                f.write(""%s,"" % s1)
        f.write(""\n"")

        for trainer in trainers:
            trainer_plans = [plans]
            if trainer in overwrite_plans.keys():
                trainer_plans = overwrite_plans[trainer]

            result_per_dataset_here = {}
            for d in datasets:
                result_per_dataset_here[d] = {}

            for p in trainer_plans:
                name = ""%s__%s"" % (trainer, p)
                all_present = True
                all_trainers.append(name)

                f.write(""%s,"" % name)
                for dataset in datasets.keys():
                    for configuration in datasets[dataset]:
                        summary_file = join(summary_files_dir, ""%s__%s__%s__%s__%s__%s.json"" % (dataset, configuration, trainer, p, expected_validation_folder, folds_str))
                        if not isfile(summary_file):
                            summary_file = join(summary_files_dir, ""%s__%s__%s__%s__%s__%s.json"" % (dataset, configuration, trainer, p, alternative_validation_folder, folds_str))
                            if not isfile(summary_file):
                                summary_file = join(summary_files_dir, ""%s__%s__%s__%s__%s__%s.json"" % (
                                dataset, configuration, trainer, p, alternative_alternative_validation_folder, folds_str))
                                if not isfile(summary_file):
                                    all_present = False
                                    print(name, dataset, configuration, ""has missing summary file"")
                        if isfile(summary_file):
                            result = load_json(summary_file)['results'][interested_in]['mean']['Dice']
                            result_per_dataset_here[dataset][configuration] = result
                            f.write(""%02.4f,"" % result)
                        else:
                            f.write(""NA,"")
                            result_per_dataset_here[dataset][configuration] = 0

                f.write(""\n"")

                if True:
                    valid_trainers.append(name)
                    for d in datasets:
                        for c in datasets[d]:
                            result_per_dataset[d][c].append(result_per_dataset_here[d][c])

    invalid_trainers = [i for i in all_trainers if i not in valid_trainers]

    num_valid = len(valid_trainers)
    num_datasets = len(datasets.keys())
    # create an array that is trainer x dataset. If more than one configuration is there then use the best metric across the two
    all_res = np.zeros((num_valid, num_datasets))
    for j, d in enumerate(datasets.keys()):
        ks = list(result_per_dataset[d].keys())
        tmp = result_per_dataset[d][ks[0]]
        for k in ks[1:]:
            for i in range(len(tmp)):
                tmp[i] = max(tmp[i], result_per_dataset[d][k][i])
        all_res[:, j] = tmp

    ranks_arr = np.zeros_like(all_res)
    for d in range(ranks_arr.shape[1]):
        temp = np.argsort(all_res[:, d])[::-1] # inverse because we want the highest dice to be rank0
        ranks = np.empty_like(temp)
        ranks[temp] = np.arange(len(temp))

        ranks_arr[:, d] = ranks

    mn = np.mean(ranks_arr, 1)
    for i in np.argsort(mn):
        print(mn[i], valid_trainers[i])

    print()
    print(valid_trainers[np.argmin(mn)])",_11494.py,61,"for c in datasets[d]:
    result_per_dataset[d][c] = []",result_per_dataset[d] = {c: [] for c in datasets[d]},1,nan,nan
https://github.com/MrGiovanni/UNetPlusPlus/tree/master/pytorch/nnunet/evaluation/model_selection/rank_candidates_cascade.py,"if __name__ == ""__main__"":
    # run collect_all_fold0_results_and_summarize_in_one_csv.py first
    summary_files_dir = join(network_training_output_dir, ""summary_jsons_fold0_new"")
    output_file = join(network_training_output_dir, ""summary_cascade.csv"")

    folds = (0, )
    folds_str = """"
    for f in folds:
        folds_str += str(f)

    plans = ""nnUNetPlansv2.1""

    overwrite_plans = {
        'nnUNetTrainerCascadeFullRes': ['nnUNetPlans'],
    }

    trainers = [
        'nnUNetTrainerCascadeFullRes',
        'nnUNetTrainerV2CascadeFullRes_EducatedGuess',
        'nnUNetTrainerV2CascadeFullRes_EducatedGuess2',
        'nnUNetTrainerV2CascadeFullRes_EducatedGuess3',
        'nnUNetTrainerV2CascadeFullRes_lowerLR',
        'nnUNetTrainerV2CascadeFullRes',
        'nnUNetTrainerV2CascadeFullRes_noConnComp',
        'nnUNetTrainerV2CascadeFullRes_shorter_lowerLR',
        'nnUNetTrainerV2CascadeFullRes_shorter',
        'nnUNetTrainerV2CascadeFullRes_smallerBinStrel',
        #'',
        #'',
        #'',
        #'',
        #'',
        #'',
    ]

    datasets = \
        {
        ""Task003_Liver"": (""3d_cascade_fullres"", ),
        ""Task006_Lung"": (""3d_cascade_fullres"", ),
        ""Task007_Pancreas"": (""3d_cascade_fullres"", ),
        ""Task008_HepaticVessel"": (""3d_cascade_fullres"", ),
        ""Task009_Spleen"": (""3d_cascade_fullres"", ),
        ""Task010_Colon"": (""3d_cascade_fullres"", ),
        ""Task017_AbdominalOrganSegmentation"": (""3d_cascade_fullres"", ),
        #""Task029_LITS"": (""3d_cascade_fullres"", ),
        ""Task048_KiTS_clean"": (""3d_cascade_fullres"", ),
        ""Task055_SegTHOR"": (""3d_cascade_fullres"", ),
        ""Task056_VerSe"": (""3d_cascade_fullres"", ),
        #"""": (""3d_cascade_fullres"", ),
        }

    expected_validation_folder = ""validation_raw""
    alternative_validation_folder = ""validation""
    alternative_alternative_validation_folder = ""validation_tiledTrue_doMirror_True""

    interested_in = ""mean""

    result_per_dataset = {}
    for d in datasets:
        result_per_dataset[d] = {}
        for c in datasets[d]:
            result_per_dataset[d][c] = []

    valid_trainers = []
    all_trainers = []

    with open(output_file, 'w') as f:
        f.write(""trainer,"")
        for t in datasets.keys():
            s = t[4:7]
            for c in datasets[t]:
                s1 = s + ""_"" + c[3]
                f.write(""%s,"" % s1)
        f.write(""\n"")

        for trainer in trainers:
            trainer_plans = [plans]
            if trainer in overwrite_plans.keys():
                trainer_plans = overwrite_plans[trainer]

            result_per_dataset_here = {}
            for d in datasets:
                result_per_dataset_here[d] = {}

            for p in trainer_plans:
                name = ""%s__%s"" % (trainer, p)
                all_present = True
                all_trainers.append(name)

                f.write(""%s,"" % name)
                for dataset in datasets.keys():
                    for configuration in datasets[dataset]:
                        summary_file = join(summary_files_dir, ""%s__%s__%s__%s__%s__%s.json"" % (dataset, configuration, trainer, p, expected_validation_folder, folds_str))
                        if not isfile(summary_file):
                            summary_file = join(summary_files_dir, ""%s__%s__%s__%s__%s__%s.json"" % (dataset, configuration, trainer, p, alternative_validation_folder, folds_str))
                            if not isfile(summary_file):
                                summary_file = join(summary_files_dir, ""%s__%s__%s__%s__%s__%s.json"" % (
                                dataset, configuration, trainer, p, alternative_alternative_validation_folder, folds_str))
                                if not isfile(summary_file):
                                    all_present = False
                                    print(name, dataset, configuration, ""has missing summary file"")
                        if isfile(summary_file):
                            result = load_json(summary_file)['results'][interested_in]['mean']['Dice']
                            result_per_dataset_here[dataset][configuration] = result
                            f.write(""%02.4f,"" % result)
                        else:
                            f.write(""NA,"")
                            result_per_dataset_here[dataset][configuration] = 0

                f.write(""\n"")

                if True:
                    valid_trainers.append(name)
                    for d in datasets:
                        for c in datasets[d]:
                            result_per_dataset[d][c].append(result_per_dataset_here[d][c])

    invalid_trainers = [i for i in all_trainers if i not in valid_trainers]

    num_valid = len(valid_trainers)
    num_datasets = len(datasets.keys())
    # create an array that is trainer x dataset. If more than one configuration is there then use the best metric across the two
    all_res = np.zeros((num_valid, num_datasets))
    for j, d in enumerate(datasets.keys()):
        ks = list(result_per_dataset[d].keys())
        tmp = result_per_dataset[d][ks[0]]
        for k in ks[1:]:
            for i in range(len(tmp)):
                tmp[i] = max(tmp[i], result_per_dataset[d][k][i])
        all_res[:, j] = tmp

    ranks_arr = np.zeros_like(all_res)
    for d in range(ranks_arr.shape[1]):
        temp = np.argsort(all_res[:, d])[::-1] # inverse because we want the highest dice to be rank0
        ranks = np.empty_like(temp)
        ranks[temp] = np.arange(len(temp))

        ranks_arr[:, d] = ranks

    mn = np.mean(ranks_arr, 1)
    for i in np.argsort(mn):
        print(mn[i], valid_trainers[i])

    print()
    print(valid_trainers[np.argmin(mn)])",_11494.py,82,"for d in datasets:
    result_per_dataset_here[d] = {}",result_per_dataset_here = {d: {} for d in datasets},1,nan,nan
https://github.com/open-mmlab/mmaction2/tree/master/mmaction/models/heads/base.py,"def loss(self, cls_score, labels, **kwargs):
        """"""Calculate the loss given output ``cls_score``, target ``labels``.

        Args:
            cls_score (torch.Tensor): The output of the model.
            labels (torch.Tensor): The target output of the model.

        Returns:
            dict: A dict containing field 'loss_cls'(mandatory)
            and 'topk_acc'(optional).
        """"""
        losses = dict()
        if labels.shape == torch.Size([]):
            labels = labels.unsqueeze(0)
        elif labels.dim() == 1 and labels.size()[0] == self.num_classes \
                and cls_score.size()[0] == 1:
            # Fix a bug when training with soft labels and batch size is 1.
            # When using soft labels, `labels` and `cls_socre` share the same
            # shape.
            labels = labels.unsqueeze(0)

        if not self.multi_class and cls_score.size() != labels.size():
            top_k_acc = top_k_accuracy(cls_score.detach().cpu().numpy(),
                                       labels.detach().cpu().numpy(),
                                       self.topk)
            for k, a in zip(self.topk, top_k_acc):
                losses[f'top{k}_acc'] = torch.tensor(
                    a, device=cls_score.device)

        elif self.multi_class and self.label_smooth_eps != 0:
            labels = ((1 - self.label_smooth_eps) * labels +
                      self.label_smooth_eps / self.num_classes)

        loss_cls = self.loss_cls(cls_score, labels, **kwargs)
        # loss_cls may be dictionary or single tensor
        if isinstance(loss_cls, dict):
            losses.update(loss_cls)
        else:
            losses['loss_cls'] = loss_cls

        return losses",_12381.py,26,"for (k, a) in zip(self.topk, top_k_acc):
    losses[f'top{k}_acc'] = torch.tensor(a, device=cls_score.device)","losses = {f'top{k}_acc': torch.tensor(a, device=cls_score.device) for (k, a) in zip(self.topk, top_k_acc)}",1,nan,nan
https://github.com/tensorlayer/tensorlayer/tree/master/tensorlayer/files/dataset_loaders/mpii_dataset.py,"def load_mpii_pose_dataset(path='data', is_16_pos_only=False):
    """"""Load MPII Human Pose Dataset.

    Parameters
    -----------
    path : str
        The path that the data is downloaded to.
    is_16_pos_only : boolean
        If True, only return the peoples contain 16 pose keypoints. (Usually be used for single person pose estimation)

    Returns
    ----------
    img_train_list : list of str
        The image directories of training data.
    ann_train_list : list of dict
        The annotations of training data.
    img_test_list : list of str
        The image directories of testing data.
    ann_test_list : list of dict
        The annotations of testing data.

    Examples
    --------
    >>> import pprint
    >>> import tensorlayer as tl
    >>> img_train_list, ann_train_list, img_test_list, ann_test_list = tl.files.load_mpii_pose_dataset()
    >>> image = tl.vis.read_image(img_train_list[0])
    >>> tl.vis.draw_mpii_pose_to_image(image, ann_train_list[0], 'image.png')
    >>> pprint.pprint(ann_train_list[0])

    References
    -----------
    - `MPII Human Pose Dataset. CVPR 14 <http://human-pose.mpi-inf.mpg.de>`__
    - `MPII Human Pose Models. CVPR 16 <http://pose.mpi-inf.mpg.de>`__
    - `MPII Human Shape, Poselet Conditioned Pictorial Structures and etc <http://pose.mpi-inf.mpg.de/#related>`__
    - `MPII Keyponts and ID <http://human-pose.mpi-inf.mpg.de/#download>`__
    """"""
    path = os.path.join(path, 'mpii_human_pose')
    logging.info(""Load or Download MPII Human Pose > {}"".format(path))

    # annotation
    url = ""http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/""
    tar_filename = ""mpii_human_pose_v1_u12_2.zip""
    extracted_filename = ""mpii_human_pose_v1_u12_2""
    if folder_exists(os.path.join(path, extracted_filename)) is False:
        logging.info(""[MPII] (annotation) {} is nonexistent in {}"".format(extracted_filename, path))
        maybe_download_and_extract(tar_filename, path, url, extract=True)
        del_file(os.path.join(path, tar_filename))

    # images
    url = ""http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/""
    tar_filename = ""mpii_human_pose_v1.tar.gz""
    extracted_filename2 = ""images""
    if folder_exists(os.path.join(path, extracted_filename2)) is False:
        logging.info(""[MPII] (images) {} is nonexistent in {}"".format(extracted_filename, path))
        maybe_download_and_extract(tar_filename, path, url, extract=True)
        del_file(os.path.join(path, tar_filename))

    # parse annotation, format see http://human-pose.mpi-inf.mpg.de/#download
    import scipy.io as sio
    logging.info(""reading annotations from mat file ..."")
    # mat = sio.loadmat(os.path.join(path, extracted_filename, ""mpii_human_pose_v1_u12_1.mat""))

    # def fix_wrong_joints(joint):    # https://github.com/mitmul/deeppose/blob/master/datasets/mpii_dataset.py
    #     if '12' in joint and '13' in joint and '2' in joint and '3' in joint:
    #         if ((joint['12'][0] < joint['13'][0]) and
    #                 (joint['3'][0] < joint['2'][0])):
    #             joint['2'], joint['3'] = joint['3'], joint['2']
    #         if ((joint['12'][0] > joint['13'][0]) and
    #                 (joint['3'][0] > joint['2'][0])):
    #             joint['2'], joint['3'] = joint['3'], joint['2']
    #     return joint

    ann_train_list = []
    ann_test_list = []
    img_train_list = []
    img_test_list = []

    def save_joints():
        # joint_data_fn = os.path.join(path, 'data.json')
        # fp = open(joint_data_fn, 'w')
        mat = sio.loadmat(os.path.join(path, extracted_filename, ""mpii_human_pose_v1_u12_1.mat""))

        for _, (anno, train_flag) in enumerate(  # all images
                zip(mat['RELEASE']['annolist'][0, 0][0], mat['RELEASE']['img_train'][0, 0][0])):

            img_fn = anno['image']['name'][0, 0][0]
            train_flag = int(train_flag)

            # print(i, img_fn, train_flag) # DEBUG print all images

            if train_flag:
                img_train_list.append(img_fn)
                ann_train_list.append([])
            else:
                img_test_list.append(img_fn)
                ann_test_list.append([])

            head_rect = []
            if 'x1' in str(anno['annorect'].dtype):
                head_rect = zip(
                    [x1[0, 0] for x1 in anno['annorect']['x1'][0]], [y1[0, 0] for y1 in anno['annorect']['y1'][0]],
                    [x2[0, 0] for x2 in anno['annorect']['x2'][0]], [y2[0, 0] for y2 in anno['annorect']['y2'][0]]
                )
            else:
                head_rect = []  # TODO

            if 'annopoints' in str(anno['annorect'].dtype):
                annopoints = anno['annorect']['annopoints'][0]
                head_x1s = anno['annorect']['x1'][0]
                head_y1s = anno['annorect']['y1'][0]
                head_x2s = anno['annorect']['x2'][0]
                head_y2s = anno['annorect']['y2'][0]

                for annopoint, head_x1, head_y1, head_x2, head_y2 in zip(annopoints, head_x1s, head_y1s, head_x2s,
                                                                         head_y2s):
                    # if annopoint != []:
                    # if len(annopoint) != 0:
                    if annopoint.size:
                        head_rect = [
                            float(head_x1[0, 0]),
                            float(head_y1[0, 0]),
                            float(head_x2[0, 0]),
                            float(head_y2[0, 0])
                        ]

                        # joint coordinates
                        annopoint = annopoint['point'][0, 0]
                        j_id = [str(j_i[0, 0]) for j_i in annopoint['id'][0]]
                        x = [x[0, 0] for x in annopoint['x'][0]]
                        y = [y[0, 0] for y in annopoint['y'][0]]
                        joint_pos = {}
                        for _j_id, (_x, _y) in zip(j_id, zip(x, y)):
                            joint_pos[int(_j_id)] = [float(_x), float(_y)]
                        # joint_pos = fix_wrong_joints(joint_pos)

                        # visibility list
                        if 'is_visible' in str(annopoint.dtype):
                            vis = [v[0] if v.size > 0 else [0] for v in annopoint['is_visible'][0]]
                            vis = dict([(k, int(v[0])) if len(v) > 0 else v for k, v in zip(j_id, vis)])
                        else:
                            vis = None

                        # if len(joint_pos) == 16:
                        if ((is_16_pos_only ==True) and (len(joint_pos) == 16)) or (is_16_pos_only == False):
                            # only use image with 16 key points / or use all
                            data = {
                                'filename': img_fn,
                                'train': train_flag,
                                'head_rect': head_rect,
                                'is_visible': vis,
                                'joint_pos': joint_pos
                            }
                            # print(json.dumps(data), file=fp)  # py3
                            if train_flag:
                                ann_train_list[-1].append(data)
                            else:
                                ann_test_list[-1].append(data)

    # def write_line(datum, fp):
    #     joints = sorted([[int(k), v] for k, v in datum['joint_pos'].items()])
    #     joints = np.array([j for i, j in joints]).flatten()
    #
    #     out = [datum['filename']]
    #     out.extend(joints)
    #     out = [str(o) for o in out]
    #     out = ','.join(out)
    #
    #     print(out, file=fp)

    # def split_train_test():
    #     # fp_test = open('data/mpii/test_joints.csv', 'w')
    #     fp_test = open(os.path.join(path, 'test_joints.csv'), 'w')
    #     # fp_train = open('data/mpii/train_joints.csv', 'w')
    #     fp_train = open(os.path.join(path, 'train_joints.csv'), 'w')
    #     # all_data = open('data/mpii/data.json').readlines()
    #     all_data = open(os.path.join(path, 'data.json')).readlines()
    #     N = len(all_data)
    #     N_test = int(N * 0.1)
    #     N_train = N - N_test
    #
    #     print('N:{}'.format(N))
    #     print('N_train:{}'.format(N_train))
    #     print('N_test:{}'.format(N_test))
    #
    #     np.random.seed(1701)
    #     perm = np.random.permutation(N)
    #     test_indices = perm[:N_test]
    #     train_indices = perm[N_test:]
    #
    #     print('train_indices:{}'.format(len(train_indices)))
    #     print('test_indices:{}'.format(len(test_indices)))
    #
    #     for i in train_indices:
    #         datum = json.loads(all_data[i].strip())
    #         write_line(datum, fp_train)
    #
    #     for i in test_indices:
    #         datum = json.loads(all_data[i].strip())
    #         write_line(datum, fp_test)

    save_joints()
    # split_train_test()  #

    ## read images dir
    logging.info(""reading images list ..."")
    img_dir = os.path.join(path, extracted_filename2)
    _img_list = load_file_list(path=os.path.join(path, extracted_filename2), regx='\\.jpg', printable=False)
    # ann_list = json.load(open(os.path.join(path, 'data.json')))
    for i, im in enumerate(img_train_list):
        if im not in _img_list:
            print('missing training image {} in {} (remove from img(ann)_train_list)'.format(im, img_dir))
            # img_train_list.remove(im)
            del img_train_list[i]
            del ann_train_list[i]
    for i, im in enumerate(img_test_list):
        if im not in _img_list:
            print('missing testing image {} in {} (remove from img(ann)_test_list)'.format(im, img_dir))
            # img_test_list.remove(im)
            del img_train_list[i]
            del ann_train_list[i]

    ## check annotation and images
    n_train_images = len(img_train_list)
    n_test_images = len(img_test_list)
    n_images = n_train_images + n_test_images
    logging.info(""n_images: {} n_train_images: {} n_test_images: {}"".format(n_images, n_train_images, n_test_images))
    n_train_ann = len(ann_train_list)
    n_test_ann = len(ann_test_list)
    n_ann = n_train_ann + n_test_ann
    logging.info(""n_ann: {} n_train_ann: {} n_test_ann: {}"".format(n_ann, n_train_ann, n_test_ann))
    n_train_people = len(sum(ann_train_list, []))
    n_test_people = len(sum(ann_test_list, []))
    n_people = n_train_people + n_test_people
    logging.info(""n_people: {} n_train_people: {} n_test_people: {}"".format(n_people, n_train_people, n_test_people))
    # add path to all image file name
    for i, value in enumerate(img_train_list):
        img_train_list[i] = os.path.join(img_dir, value)
    for i, value in enumerate(img_test_list):
        img_test_list[i] = os.path.join(img_dir, value)
    return img_train_list, ann_train_list, img_test_list, ann_test_list",_12547.py,133,"for (_j_id, (_x, _y)) in zip(j_id, zip(x, y)):
    joint_pos[int(_j_id)] = [float(_x), float(_y)]","joint_pos = {int(_j_id): [float(_x), float(_y)] for (_j_id, (_x, _y)) in zip(j_id, zip(x, y))}",1,nan,nan
https://github.com/opendevops-cn/opendevops/tree/master/scripts/tornado_source_code/tornado/escape.py,"def _build_unicode_map() -> Dict[str, str]:
    unicode_map = {}
    for name, value in html.entities.name2codepoint.items():
        unicode_map[name] = chr(value)
    return unicode_map",_12599.py,3,"for (name, value) in html.entities.name2codepoint.items():
    unicode_map[name] = chr(value)","unicode_map = {name: chr(value) for (name, value) in html.entities.name2codepoint.items()}",1,nan,nan
https://github.com/linkedin/luminol/tree/master/src/luminol/algorithms/anomaly_detector_algorithms/sign_test.py,"def _set_scores(self):
        """"""
        Compute anomaly scores for the time series
        anomaly regions are computed with sign test which also assigns a likelihood
        to the entire region
        """"""

        scores = np.zeros(len(self.time_series.values))

        anomalies = SignTest._rolling_sign_test(self.scale * np.array(self.time_series.values),
                                                self.scale * np.array(self.baseline_time_series.values),
                                                k=self.scan_window,
                                                conf=self.confidence,
                                                alpha=float(self.percent_threshold) / 100,
                                                offset=self.scale * self.offset)
        for (s, e), prob in anomalies:
            scores[s:e] = 100 * prob

        scores_dict = dict()
        for i, timestamp in enumerate(self.time_series.timestamps):
            scores_dict[timestamp] = scores[i]

        self.anom_scores = TimeSeries(scores_dict)",_12600.py,20,"for (i, timestamp) in enumerate(self.time_series.timestamps):
    scores_dict[timestamp] = scores[i]","scores_dict = {timestamp: scores[i] for (i, timestamp) in enumerate(self.time_series.timestamps)}",1,nan,nan
https://github.com/microsoft/DeepSpeed/tree/master/deepspeed/runtime/engine.py,"def _get_gradients_for_reduction(self):
        non_expert_grads = []
        expert_grads = {}
        if self.has_moe_layers:
            for key in self.expert_data_parallel_group.keys():
                expert_grads[key] = []

        for param_name, param in self.module.named_parameters():
            if param.grad is None:
                # In cases where there is an imbalance of empty grads across
                # ranks we must create empty grads, this will ensure that every
                # rank is reducing the same size. In some cases it may make
                # sense in the future to support the ability to average not
                # w.r.t. world size but with a different value.
                param.grad = torch.zeros(param.size(),
                                         dtype=param.dtype,
                                         device=param.device)

            grad_data = param.grad.data
            if param_name in self.sparse_tensor_module_names or grad_data.is_sparse:
                # Call param.grad without data to avoid problem with setting of updated grads
                grad_data = SparseTensor(param.grad)

            if is_moe_param(param):
                expert_grads[param.group_name].append(grad_data)
            else:
                non_expert_grads.append(grad_data)

        return non_expert_grads, expert_grads",_12886.py,5,"for key in self.expert_data_parallel_group.keys():
    expert_grads[key] = []",expert_grads = {key: [] for key in self.expert_data_parallel_group.keys()},1,nan,nan
https://github.com/home-assistant/core/tree/master/homeassistant/components/geonetnz_quakes/geo_location.py,"def extra_state_attributes(self):
        """"""Return the device state attributes.""""""
        attributes = {}
        for key, value in (
            (ATTR_EXTERNAL_ID, self._external_id),
            (ATTR_ATTRIBUTION, self._attribution),
            (ATTR_DEPTH, self._depth),
            (ATTR_LOCALITY, self._locality),
            (ATTR_MAGNITUDE, self._magnitude),
            (ATTR_MMI, self._mmi),
            (ATTR_QUALITY, self._quality),
            (ATTR_TIME, self._time),
        ):
            if value or isinstance(value, bool):
                attributes[key] = value
        return attributes",_12942.py,4,"for (key, value) in ((ATTR_EXTERNAL_ID, self._external_id), (ATTR_ATTRIBUTION, self._attribution), (ATTR_DEPTH, self._depth), (ATTR_LOCALITY, self._locality), (ATTR_MAGNITUDE, self._magnitude), (ATTR_MMI, self._mmi), (ATTR_QUALITY, self._quality), (ATTR_TIME, self._time)):
    if value or isinstance(value, bool):
        attributes[key] = value","attributes = {key: value for (key, value) in ((ATTR_EXTERNAL_ID, self._external_id), (ATTR_ATTRIBUTION, self._attribution), (ATTR_DEPTH, self._depth), (ATTR_LOCALITY, self._locality), (ATTR_MAGNITUDE, self._magnitude), (ATTR_MMI, self._mmi), (ATTR_QUALITY, self._quality), (ATTR_TIME, self._time)) if value or isinstance(value, bool)}",1,nan,nan
https://github.com/mikedh/trimesh/tree/master/trimesh/path/exchange/dxf.py,"def load_dxf(file_obj, **kwargs):
    """"""
    Load a DXF file to a dictionary containing vertices and
    entities.

    Parameters
    ----------
    file_obj: file or file- like object (has object.read method)

    Returns
    ----------
    result: dict, keys are  entities, vertices and metadata
    """"""

    # in a DXF file, lines come in pairs,
    # a group code then the next line is the value
    # we are removing all whitespace then splitting with the
    # splitlines function which uses the universal newline method
    raw = file_obj.read()
    # if we've been passed bytes
    if hasattr(raw, 'decode'):
        # search for the sentinel string indicating binary DXF
        # do it by encoding sentinel to bytes and subset searching
        if raw[:22].find(b'AutoCAD Binary DXF') != -1:
            # no converter to ASCII DXF available
            raise ValueError('binary DXF not supported!')
        else:
            # we've been passed bytes that don't have the
            # header for binary DXF so try decoding as UTF-8
            raw = raw.decode('utf-8', errors='ignore')

    # remove trailing whitespace
    raw = str(raw).strip()
    # without any spaces and in upper case
    cleaned = raw.replace(' ', '').strip().upper()

    # blob with spaces and original case
    blob_raw = np.array(str.splitlines(raw)).reshape((-1, 2))
    # if this reshape fails, it means the DXF is malformed
    blob = np.array(str.splitlines(cleaned)).reshape((-1, 2))

    # get the section which contains the header in the DXF file
    endsec = np.nonzero(blob[:, 1] == 'ENDSEC')[0]

    # store metadata
    metadata = {}

    # try reading the header, which may be malformed
    header_start = np.nonzero(blob[:, 1] == 'HEADER')[0]
    if len(header_start) > 0:
        header_end = endsec[np.searchsorted(endsec, header_start[0])]
        header_blob = blob[header_start[0]:header_end]

        # store some properties from the DXF header
        metadata['DXF_HEADER'] = {}
        for key, group in [('$ACADVER', '1'),
                           ('$DIMSCALE', '40'),
                           ('$DIMALT', '70'),
                           ('$DIMALTF', '40'),
                           ('$DIMUNIT', '70'),
                           ('$INSUNITS', '70'),
                           ('$LUNITS', '70')]:
            value = get_key(header_blob,
                            key,
                            group)
            if value is not None:
                metadata['DXF_HEADER'][key] = value

        # store unit data pulled from the header of the DXF
        # prefer LUNITS over INSUNITS
        # I couldn't find a table for LUNITS values but they
        # look like they are 0- indexed versions of
        # the INSUNITS keys, so for now offset the key value
        for offset, key in [(-1, '$LUNITS'),
                            (0, '$INSUNITS')]:
            # get the key from the header blob
            units = get_key(header_blob, key, '70')
            # if it exists add the offset
            if units is None:
                continue
            metadata[key] = units
            units += offset
            # if the key is in our list of units store it
            if units in _DXF_UNITS:
                metadata['units'] = _DXF_UNITS[units]
        # warn on drawings with no units
        if 'units' not in metadata:
            log.debug('DXF doesn\'t have units specified!')

    # get the section which contains entities in the DXF file
    entity_start = np.nonzero(blob[:, 1] == 'ENTITIES')[0][0]
    entity_end = endsec[np.searchsorted(endsec, entity_start)]

    blocks = None
    # only load blocks if an entity references them via an INSERT
    if (blob[entity_start:entity_end] == ['0', 'INSERT']).all(axis=1).any():
        try:
            # which part of the raw file contains blocks
            block_start = np.nonzero(blob[:, 1] == 'BLOCKS')[0][0]
            block_end = endsec[np.searchsorted(endsec, block_start)]

            blob_block = blob[block_start:block_end]
            blob_block_raw = blob_raw[block_start:block_end]
            block_infl = np.nonzero((blob_block == ['0', 'BLOCK']).all(axis=1))[0]

            # collect blocks by name
            blocks = {}
            for index in np.array_split(np.arange(len(blob_block)), block_infl):
                try:
                    v, e, name = convert_entities(
                        blob_block[index],
                        blob_block_raw[index],
                        return_name=True)
                    if len(e) > 0:
                        blocks[name] = (v, e)
                except BaseException:
                    pass
        except BaseException:
            pass

    # actually load referenced entities
    vertices, entities = convert_entities(
        blob[entity_start:entity_end],
        blob_raw[entity_start:entity_end],
        blocks=blocks)

    # return result as kwargs for trimesh.path.Path2D constructor
    result = {'vertices': vertices,
              'entities': entities,
              'metadata': metadata}

    return result",_13006.py,56,"for (key, group) in [('$ACADVER', '1'), ('$DIMSCALE', '40'), ('$DIMALT', '70'), ('$DIMALTF', '40'), ('$DIMUNIT', '70'), ('$INSUNITS', '70'), ('$LUNITS', '70')]:
    value = get_key(header_blob, key, group)
    if value is not None:
        metadata['DXF_HEADER'][key] = value","metadata['DXF_HEADER'] = {key: get_key(header_blob, key, group) for (key, group) in [('$ACADVER', '1'), ('$DIMSCALE', '40'), ('$DIMALT', '70'), ('$DIMALTF', '40'), ('$DIMUNIT', '70'), ('$INSUNITS', '70'), ('$LUNITS', '70')] if get_key(header_blob, key, group) is not None}",1,nan,nan
https://github.com/apache/tvm/tree/master/python/tvm/relay/backend/contrib/ethosu/tir/passes.py,"def _merge_constants(mod):
        nonlocal const_dict
        try:
            mod[""main""]
        except:
            raise tvm.TVMError(
                ""Expected a single primitive function called 'main'. ""
                ""Please run the MergeConstants pass in conjunction with the LowerToTIR() pass.""
            )

        new_const_dict = {}
        for param in const_dict.keys():
            new_const_dict[tvm.tir.IntImm(""int64"", param)] = tvm.nd.array(const_dict[param])
        mod[""main""] = mod[""main""].with_attr(""ethos-u.const_dict"", new_const_dict)

        mod = _ffi_api.MergeConstants()(mod)
        const_dict = mod[""main""].attrs[""ethos-u.const_dict""]
        mod = _ffi_api.RemoveConstDictAttribute()(mod)

        new_const_dict = {}
        for param in const_dict.keys():
            new_const_dict[int(param)] = const_dict[param].numpy()

        return mod, new_const_dict",_13035.py,12,"for param in const_dict.keys():
    new_const_dict[tvm.tir.IntImm('int64', param)] = tvm.nd.array(const_dict[param])","new_const_dict = {tvm.tir.IntImm('int64', param): tvm.nd.array(const_dict[param]) for param in const_dict.keys()}",1,nan,nan
https://github.com/apache/tvm/tree/master/python/tvm/relay/backend/contrib/ethosu/tir/passes.py,"def _merge_constants(mod):
        nonlocal const_dict
        try:
            mod[""main""]
        except:
            raise tvm.TVMError(
                ""Expected a single primitive function called 'main'. ""
                ""Please run the MergeConstants pass in conjunction with the LowerToTIR() pass.""
            )

        new_const_dict = {}
        for param in const_dict.keys():
            new_const_dict[tvm.tir.IntImm(""int64"", param)] = tvm.nd.array(const_dict[param])
        mod[""main""] = mod[""main""].with_attr(""ethos-u.const_dict"", new_const_dict)

        mod = _ffi_api.MergeConstants()(mod)
        const_dict = mod[""main""].attrs[""ethos-u.const_dict""]
        mod = _ffi_api.RemoveConstDictAttribute()(mod)

        new_const_dict = {}
        for param in const_dict.keys():
            new_const_dict[int(param)] = const_dict[param].numpy()

        return mod, new_const_dict",_13035.py,21,"for param in const_dict.keys():
    new_const_dict[int(param)] = const_dict[param].numpy()",new_const_dict = {int(param): const_dict[param].numpy() for param in const_dict.keys()},1,nan,nan
https://github.com/volatilityfoundation/volatility3/tree/master/development/banner_server.py,"def run(self):
        context = contexts.Context()
        json_output = {'version': 1}

        path = self._path
        filename = '*'

        for banner_cache in [linux.LinuxBannerCache, mac.MacBannerCache]:
            sub_path = banner_cache.os
            potentials = []
            for extension in constants.ISF_EXTENSIONS:
                # Hopefully these will not be large lists, otherwise this might be slow
                try:
                    for found in pathlib.Path(path).joinpath(sub_path).resolve().rglob(filename + extension):
                        potentials.append(found.as_uri())
                except FileNotFoundError:
                    # If there's no linux symbols, don't cry about it
                    pass

            new_banners = banner_cache.read_new_banners(context, 'BannerServer', potentials, banner_cache.symbol_name,
                                                        banner_cache.os, progress_callback = PrintedProgress())
            result_banners = {}
            for new_banner in new_banners:
                # Only accept file schemes
                value = [self.convert_url(url) for url in new_banners[new_banner] if
                         urllib.parse.urlparse(url).scheme == 'file']
                if value and new_banner:
                    # Convert files into URLs
                    result_banners[str(base64.b64encode(new_banner), 'latin-1')] = value

            json_output[banner_cache.os] = result_banners

        output_path = os.path.join(self._path, 'banners.json')
        with open(output_path, 'w') as fp:
            vollog.warning(f""Banners file written to {output_path}"")
            json.dump(json_output, fp)",_13516.py,23,"for new_banner in new_banners:
    value = [self.convert_url(url) for url in new_banners[new_banner] if urllib.parse.urlparse(url).scheme == 'file']
    if value and new_banner:
        result_banners[str(base64.b64encode(new_banner), 'latin-1')] = value","result_banners = {str(base64.b64encode(new_banner), 'latin-1'): [self.convert_url(url) for url in new_banners[new_banner] if urllib.parse.urlparse(url).scheme == 'file'] for new_banner in new_banners if [self.convert_url(url) for url in new_banners[new_banner] if urllib.parse.urlparse(url).scheme == 'file'] and new_banner}",1,nan,nan
https://github.com/googlefonts/fontmake/tree/master/Lib/fontmake/font_project.py,"def _designspace_locations(self, designspace):
        """"""Map font filenames to their locations in a designspace.""""""

        maps = []
        for elements in (designspace.sources, designspace.instances):
            location_map = {}
            for element in elements:
                path = _normpath(element.path)
                location_map[path] = element.location
            maps.append(location_map)
        return maps",_13722.py,7,"for element in elements:
    path = _normpath(element.path)
    location_map[path] = element.location",location_map = {_normpath(element.path): element.location for element in elements},1,nan,nan
https://github.com/aws/aws-parallelcluster/tree/master/cli/tests/pcluster/api/controllers/test_cluster_instances_controller.py,"def test_filters(self, mocker, client, cluster_name, node_type, queue_name):
        describe_instances_mock = mocker.patch(
            ""pcluster.aws.ec2.Ec2Client.describe_instances"",
            return_value=([cfn_describe_instances_mock_response()], """"),
        )
        response = self._send_test_request(
            client, cluster_name=cluster_name, node_type=node_type, queue_name=queue_name
        )
        with soft_assertions():
            assert_that(response.status_code).is_equal_to(200)
            actual_filters = describe_instances_mock.call_args_list[0][0][0]
            actual_filters_dict = {}
            for filter in actual_filters:
                actual_filters_dict[filter[""Name""]] = filter[""Values""][0]
            assert_that(actual_filters_dict.get(""tag:parallelcluster:cluster-name"")).is_equal_to(cluster_name)
            if node_type:
                if node_type == NodeType.HEADNODE:
                    expected_value = ""HeadNode""
                else:
                    expected_value = ""Compute""
            else:
                expected_value = None
            assert_that(actual_filters_dict.get(""tag:parallelcluster:node-type"")).is_equal_to(expected_value)
            assert_that(actual_filters_dict.get(""tag:parallelcluster:queue-name"")).is_equal_to(queue_name)",_14539.py,13,"for filter in actual_filters:
    actual_filters_dict[filter['Name']] = filter['Values'][0]",actual_filters_dict = {filter['Name']: filter['Values'][0] for filter in actual_filters},1,nan,nan
https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/tree/master/pytorch_object_detection/yolov3_spp/train_utils/coco_eval.py,"def __init__(self, coco_gt, iou_types):
        assert isinstance(iou_types, (list, tuple))
        coco_gt = copy.deepcopy(coco_gt)
        self.coco_gt = coco_gt

        self.iou_types = iou_types
        self.coco_eval = {}
        for iou_type in iou_types:
            self.coco_eval[iou_type] = COCOeval(coco_gt, iouType=iou_type)

        self.img_ids = []
        self.eval_imgs = {k: [] for k in iou_types}",_14709.py,8,"for iou_type in iou_types:
    self.coco_eval[iou_type] = COCOeval(coco_gt, iouType=iou_type)","self.coco_eval = {iou_type: COCOeval(coco_gt, iouType=iou_type) for iou_type in iou_types}",1,nan,nan
https://github.com/facebookresearch/demucs/tree/master/demucs/solver.py,"def _run_one_epoch(self, epoch, train=True):
        args = self.args
        data_loader = self.loaders['train'] if train else self.loaders['valid']
        # get a different order for distributed training, otherwise this will get ignored
        data_loader.sampler.epoch = epoch

        label = [""Valid"", ""Train""][train]
        name = label + f"" | Epoch {epoch + 1}""
        total = len(data_loader)
        if args.max_batches:
            total = min(total, args.max_batches)
        logprog = LogProgress(logger, data_loader, total=total,
                              updates=self.args.misc.num_prints, name=name)
        averager = EMA()

        for idx, sources in enumerate(logprog):
            sources = sources.to(self.device)
            if train:
                sources = self.augment(sources)
                mix = sources.sum(dim=1)
            else:
                mix = sources[:, 0]
                sources = sources[:, 1:]

            if not train and self.args.valid_apply:
                estimate = apply_model(self.model, mix, split=self.args.test.split, overlap=0)
            else:
                estimate = self.dmodel(mix)
            if train and hasattr(self.model, 'transform_target'):
                sources = self.model.transform_target(mix, sources)
            assert estimate.shape == sources.shape, (estimate.shape, sources.shape)
            dims = tuple(range(2, sources.dim()))

            if args.optim.loss == 'l1':
                loss = F.l1_loss(estimate, sources, reduction='none')
                loss = loss.mean(dims).mean(0)
                reco = loss
            elif args.optim.loss == 'mse':
                loss = F.mse_loss(estimate, sources, reduction='none')
                loss = loss.mean(dims)
                reco = loss**0.5
                reco = reco.mean(0)
            else:
                raise ValueError(f""Invalid loss {self.args.loss}"")
            weights = torch.tensor(args.weights).to(sources)
            loss = (loss * weights).sum() / weights.sum()

            ms = 0
            if self.quantizer is not None:
                ms = self.quantizer.model_size()
            if args.quant.diffq:
                loss += args.quant.diffq * ms

            losses = {}
            losses['reco'] = (reco * weights).sum() / weights.sum()
            losses['ms'] = ms

            if not train:
                nsdrs = new_sdr(sources, estimate.detach()).mean(0)
                total = 0
                for source, nsdr, w in zip(self.model.sources, nsdrs, weights):
                    losses[f'nsdr_{source}'] = nsdr
                    total += w * nsdr
                losses['nsdr'] = total / weights.sum()

            if train and args.svd.penalty > 0:
                kw = dict(args.svd)
                kw.pop('penalty')
                penalty = svd_penalty(self.model, **kw)
                losses['penalty'] = penalty
                loss += args.svd.penalty * penalty

            losses['loss'] = loss

            for k, source in enumerate(self.model.sources):
                losses[f'reco_{source}'] = reco[k]

            # optimize model in training mode
            if train:
                loss.backward()
                grad_norm = 0
                grads = []
                for p in self.model.parameters():
                    if p.grad is not None:
                        grad_norm += p.grad.data.norm()**2
                        grads.append(p.grad.data)
                losses['grad'] = grad_norm ** 0.5
                if args.optim.clip_grad:
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        args.optim.clip_grad)

                if self.args.flag == 'uns':
                    for n, p in self.model.named_parameters():
                        if p.grad is None:
                            print('no grad', n)
                self.optimizer.step()
                self.optimizer.zero_grad()
                for ema in self.emas['batch']:
                    ema.update()
            losses = averager(losses)
            logs = self._format_train(losses)
            logprog.update(**logs)
            # Just in case, clear some memory
            del loss, estimate, reco, ms
            if args.max_batches == idx:
                break
            if self.args.debug and train:
                break
            if self.args.flag == 'debug':
                break
        if train:
            for ema in self.emas['epoch']:
                ema.update()
        return distrib.average(losses, idx + 1)",_14803.py,75,"for (k, source) in enumerate(self.model.sources):
    losses[f'reco_{source}'] = reco[k]","losses.update({f'reco_{source}': reco[k] for (k, source) in enumerate(self.model.sources)})",1,nan,nan
https://github.com/awslabs/aws-data-wrangler/tree/master/awswrangler/opensearch/_read.py,"def _resolve_fields(row: Mapping[str, Any]) -> Mapping[str, Any]:
    fields = {}
    for field in row:
        if isinstance(row[field], dict):
            nested_fields = _resolve_fields(row[field])
            for n_field, val in nested_fields.items():
                fields[f""{field}.{n_field}""] = val
        else:
            fields[field] = row[field]
    return fields",_14933.py,6,"for (n_field, val) in nested_fields.items():
    fields[f'{field}.{n_field}'] = val","fields.update({f'{field}.{n_field}': val for (n_field, val) in nested_fields.items()})",1,,
https://github.com/PaddlePaddle/PaddleX/tree/master/paddlex_restful/restful/project/operate.py,"def _call_paddlex_predict(task_path,
                          predict_status_path,
                          params,
                          img_list,
                          img_data,
                          save_dir,
                          score_thresh,
                          epoch=None):
    total_num = open(
        osp.join(predict_status_path, 'total_num'), 'w', encoding='utf-8')

    def write_file_num(total_file_num):
        total_num.write(str(total_file_num))
        total_num.close()

    sys.stdout = open(
        osp.join(predict_status_path, 'out.log'), 'w', encoding='utf-8')
    sys.stderr = open(
        osp.join(predict_status_path, 'err.log'), 'w', encoding='utf-8')

    import paddlex as pdx
    pdx.log_level = 3
    task_type = params['task_type']
    dataset_path = params['dataset_path']
    if epoch is None:
        model_path = osp.join(task_path, 'output', 'best_model')
    else:
        model_path = osp.join(task_path, 'output', 'epoch_{}'.format(epoch))
    model = pdx.load_model(model_path)
    file_list = dict()
    predicted_num = 0
    if task_type == ""classification"":
        if img_data is None:
            if len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                with open(osp.join(dataset_path, ""test_list.txt"")) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = items[1]
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            for image, label_id in file_list.items():
                pred_result = {}
                if label_id is not None:
                    pred_result[""gt_label""] = model.labels[int(label_id)]
                results = model.predict(img_file=image)
                pred_result[""label""] = []
                pred_result[""score""] = []
                pred_result[""topk""] = len(results)
                for res in results:
                    pred_result[""label""].append(res['category'])
                    pred_result[""score""].append(res['score'])
                visualize_classified_result(save_dir, image, pred_result)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            pred_result = {}
            pred_result[""label""] = []
            pred_result[""score""] = []
            pred_result[""topk""] = len(results)
            for res in results:
                pred_result[""label""].append(res['category'])
                pred_result[""score""].append(res['score'])
            visualize_classified_result(save_dir, img, pred_result)
    elif task_type in [""detection"", ""instance_segmentation""]:
        if img_data is None:
            if task_type == ""detection"" and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                if len(img_list) == 0 and osp.exists(
                        osp.join(dataset_path, ""test_list.txt"")):
                    with open(
                            osp.join(dataset_path, ""test_list.txt""),
                            encoding=get_encoding(
                                osp.join(dataset_path, ""test_list.txt""))) as f:
                        for line in f:
                            items = line.strip().split()
                            file_list[osp.join(dataset_path, items[0])] = \
                                osp.join(dataset_path, items[1])
                else:
                    for image in img_list:
                        file_list[image] = None
                total_file_num = len(file_list)
                write_file_num(total_file_num)
                for image, anno in file_list.items():
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(
                        image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    image_gt = None
                    if anno is not None:
                        image_gt = plot_det_label(image, anno, model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            elif len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test.json"")):
                from pycocotools.coco import COCO
                anno_path = osp.join(dataset_path, ""test.json"")
                coco = COCO(anno_path)
                img_ids = coco.getImgIds()
                total_file_num = len(img_ids)
                write_file_num(total_file_num)
                for img_id in img_ids:
                    img_anno = coco.loadImgs(img_id)[0]
                    file_name = img_anno['file_name']
                    name = (osp.split(file_name)[-1]).split(""."")[0]
                    anno = osp.join(dataset_path, ""Annotations"", name + "".npy"")
                    img_file = osp.join(dataset_path, ""JPEGImages"", file_name)
                    results = model.predict(img_file=img_file)
                    image_pred = pdx.det.visualize(
                        img_file,
                        results,
                        threshold=score_thresh,
                        save_dir=None)
                    save_name = osp.join(save_dir, osp.split(img_file)[-1])
                    if task_type == ""detection"":
                        image_gt = plot_det_label(img_file, anno, model.labels)
                    else:
                        image_gt = plot_insseg_label(img_file, anno,
                                                     model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            else:
                total_file_num = len(img_list)
                write_file_num(total_file_num)
                for image in img_list:
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(
                        image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    visualize_detected_result(save_name, None, image_pred)
                    predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            image_pred = pdx.det.visualize(
                img, results, threshold=score_thresh, save_dir=None)
            image_gt = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_detected_result(save_name, image_gt, image_pred)

    elif task_type == ""segmentation"":
        if img_data is None:
            if len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                with open(
                        osp.join(dataset_path, ""test_list.txt""),
                        encoding=get_encoding(
                            osp.join(dataset_path, ""test_list.txt""))) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = \
                            osp.join(dataset_path, items[1])
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            for image, anno in file_list.items():
                results = model.predict(img_file=image)
                image_pred = pdx.seg.visualize(image, results, save_dir=None)
                pse_pred = pdx.seg.visualize(
                    image, results, weight=0, save_dir=None)
                image_ground = None
                pse_label = None
                if anno is not None:
                    label = np.asarray(Image.open(anno)).astype('uint8')
                    image_ground = pdx.seg.visualize(
                        image, {'label_map': label}, save_dir=None)
                    pse_label = pdx.seg.visualize(
                        image, {'label_map': label}, weight=0, save_dir=None)
                save_name = osp.join(save_dir, osp.split(image)[-1])
                visualize_segmented_result(save_name, image_ground, pse_label,
                                           image_pred, pse_pred, legend)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            results = model.predict(img)
            image_pred = pdx.seg.visualize(img, results, save_dir=None)
            pse_pred = pdx.seg.visualize(img, results, weight=0, save_dir=None)
            image_ground = None
            pse_label = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_segmented_result(save_name, image_ground, pse_label,
                                       image_pred, pse_pred, legend)
    set_folder_status(predict_status_path, PredictStatus.XPREDONE)",_15501.py,41,"for image in img_list:
    file_list[image] = None",file_list.update({image: None for image in img_list}),1,,
https://github.com/PaddlePaddle/PaddleX/tree/master/paddlex_restful/restful/project/operate.py,"def _call_paddlex_predict(task_path,
                          predict_status_path,
                          params,
                          img_list,
                          img_data,
                          save_dir,
                          score_thresh,
                          epoch=None):
    total_num = open(
        osp.join(predict_status_path, 'total_num'), 'w', encoding='utf-8')

    def write_file_num(total_file_num):
        total_num.write(str(total_file_num))
        total_num.close()

    sys.stdout = open(
        osp.join(predict_status_path, 'out.log'), 'w', encoding='utf-8')
    sys.stderr = open(
        osp.join(predict_status_path, 'err.log'), 'w', encoding='utf-8')

    import paddlex as pdx
    pdx.log_level = 3
    task_type = params['task_type']
    dataset_path = params['dataset_path']
    if epoch is None:
        model_path = osp.join(task_path, 'output', 'best_model')
    else:
        model_path = osp.join(task_path, 'output', 'epoch_{}'.format(epoch))
    model = pdx.load_model(model_path)
    file_list = dict()
    predicted_num = 0
    if task_type == ""classification"":
        if img_data is None:
            if len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                with open(osp.join(dataset_path, ""test_list.txt"")) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = items[1]
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            for image, label_id in file_list.items():
                pred_result = {}
                if label_id is not None:
                    pred_result[""gt_label""] = model.labels[int(label_id)]
                results = model.predict(img_file=image)
                pred_result[""label""] = []
                pred_result[""score""] = []
                pred_result[""topk""] = len(results)
                for res in results:
                    pred_result[""label""].append(res['category'])
                    pred_result[""score""].append(res['score'])
                visualize_classified_result(save_dir, image, pred_result)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            pred_result = {}
            pred_result[""label""] = []
            pred_result[""score""] = []
            pred_result[""topk""] = len(results)
            for res in results:
                pred_result[""label""].append(res['category'])
                pred_result[""score""].append(res['score'])
            visualize_classified_result(save_dir, img, pred_result)
    elif task_type in [""detection"", ""instance_segmentation""]:
        if img_data is None:
            if task_type == ""detection"" and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                if len(img_list) == 0 and osp.exists(
                        osp.join(dataset_path, ""test_list.txt"")):
                    with open(
                            osp.join(dataset_path, ""test_list.txt""),
                            encoding=get_encoding(
                                osp.join(dataset_path, ""test_list.txt""))) as f:
                        for line in f:
                            items = line.strip().split()
                            file_list[osp.join(dataset_path, items[0])] = \
                                osp.join(dataset_path, items[1])
                else:
                    for image in img_list:
                        file_list[image] = None
                total_file_num = len(file_list)
                write_file_num(total_file_num)
                for image, anno in file_list.items():
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(
                        image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    image_gt = None
                    if anno is not None:
                        image_gt = plot_det_label(image, anno, model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            elif len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test.json"")):
                from pycocotools.coco import COCO
                anno_path = osp.join(dataset_path, ""test.json"")
                coco = COCO(anno_path)
                img_ids = coco.getImgIds()
                total_file_num = len(img_ids)
                write_file_num(total_file_num)
                for img_id in img_ids:
                    img_anno = coco.loadImgs(img_id)[0]
                    file_name = img_anno['file_name']
                    name = (osp.split(file_name)[-1]).split(""."")[0]
                    anno = osp.join(dataset_path, ""Annotations"", name + "".npy"")
                    img_file = osp.join(dataset_path, ""JPEGImages"", file_name)
                    results = model.predict(img_file=img_file)
                    image_pred = pdx.det.visualize(
                        img_file,
                        results,
                        threshold=score_thresh,
                        save_dir=None)
                    save_name = osp.join(save_dir, osp.split(img_file)[-1])
                    if task_type == ""detection"":
                        image_gt = plot_det_label(img_file, anno, model.labels)
                    else:
                        image_gt = plot_insseg_label(img_file, anno,
                                                     model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            else:
                total_file_num = len(img_list)
                write_file_num(total_file_num)
                for image in img_list:
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(
                        image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    visualize_detected_result(save_name, None, image_pred)
                    predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            image_pred = pdx.det.visualize(
                img, results, threshold=score_thresh, save_dir=None)
            image_gt = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_detected_result(save_name, image_gt, image_pred)

    elif task_type == ""segmentation"":
        if img_data is None:
            if len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                with open(
                        osp.join(dataset_path, ""test_list.txt""),
                        encoding=get_encoding(
                            osp.join(dataset_path, ""test_list.txt""))) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = \
                            osp.join(dataset_path, items[1])
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            for image, anno in file_list.items():
                results = model.predict(img_file=image)
                image_pred = pdx.seg.visualize(image, results, save_dir=None)
                pse_pred = pdx.seg.visualize(
                    image, results, weight=0, save_dir=None)
                image_ground = None
                pse_label = None
                if anno is not None:
                    label = np.asarray(Image.open(anno)).astype('uint8')
                    image_ground = pdx.seg.visualize(
                        image, {'label_map': label}, save_dir=None)
                    pse_label = pdx.seg.visualize(
                        image, {'label_map': label}, weight=0, save_dir=None)
                save_name = osp.join(save_dir, osp.split(image)[-1])
                visualize_segmented_result(save_name, image_ground, pse_label,
                                           image_pred, pse_pred, legend)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            results = model.predict(img)
            image_pred = pdx.seg.visualize(img, results, save_dir=None)
            pse_pred = pdx.seg.visualize(img, results, weight=0, save_dir=None)
            image_ground = None
            pse_label = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_segmented_result(save_name, image_ground, pse_label,
                                       image_pred, pse_pred, legend)
    set_folder_status(predict_status_path, PredictStatus.XPREDONE)",_15501.py,37,"for line in f:
    items = line.strip().split()
    file_list[osp.join(dataset_path, items[0])] = items[1]","file_list = {osp.join(dataset_path, line.strip().split()[0]): line.strip().split()[1] for line in f}",1,,
https://github.com/PaddlePaddle/PaddleX/tree/master/paddlex_restful/restful/project/operate.py,"def _call_paddlex_predict(task_path,
                          predict_status_path,
                          params,
                          img_list,
                          img_data,
                          save_dir,
                          score_thresh,
                          epoch=None):
    total_num = open(
        osp.join(predict_status_path, 'total_num'), 'w', encoding='utf-8')

    def write_file_num(total_file_num):
        total_num.write(str(total_file_num))
        total_num.close()

    sys.stdout = open(
        osp.join(predict_status_path, 'out.log'), 'w', encoding='utf-8')
    sys.stderr = open(
        osp.join(predict_status_path, 'err.log'), 'w', encoding='utf-8')

    import paddlex as pdx
    pdx.log_level = 3
    task_type = params['task_type']
    dataset_path = params['dataset_path']
    if epoch is None:
        model_path = osp.join(task_path, 'output', 'best_model')
    else:
        model_path = osp.join(task_path, 'output', 'epoch_{}'.format(epoch))
    model = pdx.load_model(model_path)
    file_list = dict()
    predicted_num = 0
    if task_type == ""classification"":
        if img_data is None:
            if len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                with open(osp.join(dataset_path, ""test_list.txt"")) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = items[1]
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            for image, label_id in file_list.items():
                pred_result = {}
                if label_id is not None:
                    pred_result[""gt_label""] = model.labels[int(label_id)]
                results = model.predict(img_file=image)
                pred_result[""label""] = []
                pred_result[""score""] = []
                pred_result[""topk""] = len(results)
                for res in results:
                    pred_result[""label""].append(res['category'])
                    pred_result[""score""].append(res['score'])
                visualize_classified_result(save_dir, image, pred_result)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            pred_result = {}
            pred_result[""label""] = []
            pred_result[""score""] = []
            pred_result[""topk""] = len(results)
            for res in results:
                pred_result[""label""].append(res['category'])
                pred_result[""score""].append(res['score'])
            visualize_classified_result(save_dir, img, pred_result)
    elif task_type in [""detection"", ""instance_segmentation""]:
        if img_data is None:
            if task_type == ""detection"" and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                if len(img_list) == 0 and osp.exists(
                        osp.join(dataset_path, ""test_list.txt"")):
                    with open(
                            osp.join(dataset_path, ""test_list.txt""),
                            encoding=get_encoding(
                                osp.join(dataset_path, ""test_list.txt""))) as f:
                        for line in f:
                            items = line.strip().split()
                            file_list[osp.join(dataset_path, items[0])] = \
                                osp.join(dataset_path, items[1])
                else:
                    for image in img_list:
                        file_list[image] = None
                total_file_num = len(file_list)
                write_file_num(total_file_num)
                for image, anno in file_list.items():
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(
                        image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    image_gt = None
                    if anno is not None:
                        image_gt = plot_det_label(image, anno, model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            elif len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test.json"")):
                from pycocotools.coco import COCO
                anno_path = osp.join(dataset_path, ""test.json"")
                coco = COCO(anno_path)
                img_ids = coco.getImgIds()
                total_file_num = len(img_ids)
                write_file_num(total_file_num)
                for img_id in img_ids:
                    img_anno = coco.loadImgs(img_id)[0]
                    file_name = img_anno['file_name']
                    name = (osp.split(file_name)[-1]).split(""."")[0]
                    anno = osp.join(dataset_path, ""Annotations"", name + "".npy"")
                    img_file = osp.join(dataset_path, ""JPEGImages"", file_name)
                    results = model.predict(img_file=img_file)
                    image_pred = pdx.det.visualize(
                        img_file,
                        results,
                        threshold=score_thresh,
                        save_dir=None)
                    save_name = osp.join(save_dir, osp.split(img_file)[-1])
                    if task_type == ""detection"":
                        image_gt = plot_det_label(img_file, anno, model.labels)
                    else:
                        image_gt = plot_insseg_label(img_file, anno,
                                                     model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            else:
                total_file_num = len(img_list)
                write_file_num(total_file_num)
                for image in img_list:
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(
                        image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    visualize_detected_result(save_name, None, image_pred)
                    predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            image_pred = pdx.det.visualize(
                img, results, threshold=score_thresh, save_dir=None)
            image_gt = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_detected_result(save_name, image_gt, image_pred)

    elif task_type == ""segmentation"":
        if img_data is None:
            if len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                with open(
                        osp.join(dataset_path, ""test_list.txt""),
                        encoding=get_encoding(
                            osp.join(dataset_path, ""test_list.txt""))) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = \
                            osp.join(dataset_path, items[1])
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            for image, anno in file_list.items():
                results = model.predict(img_file=image)
                image_pred = pdx.seg.visualize(image, results, save_dir=None)
                pse_pred = pdx.seg.visualize(
                    image, results, weight=0, save_dir=None)
                image_ground = None
                pse_label = None
                if anno is not None:
                    label = np.asarray(Image.open(anno)).astype('uint8')
                    image_ground = pdx.seg.visualize(
                        image, {'label_map': label}, save_dir=None)
                    pse_label = pdx.seg.visualize(
                        image, {'label_map': label}, weight=0, save_dir=None)
                save_name = osp.join(save_dir, osp.split(image)[-1])
                visualize_segmented_result(save_name, image_ground, pse_label,
                                           image_pred, pse_pred, legend)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            results = model.predict(img)
            image_pred = pdx.seg.visualize(img, results, save_dir=None)
            pse_pred = pdx.seg.visualize(img, results, weight=0, save_dir=None)
            image_ground = None
            pse_label = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_segmented_result(save_name, image_ground, pse_label,
                                       image_pred, pse_pred, legend)
    set_folder_status(predict_status_path, PredictStatus.XPREDONE)",_15501.py,168,"for i in range(len(model.labels)):
    legend[model.labels[i]] = color_map[i]",legend = {model.labels[i]: color_map[i] for i in range(len(model.labels))},1,,
https://github.com/PaddlePaddle/PaddleX/tree/master/paddlex_restful/restful/project/operate.py,"def _call_paddlex_predict(task_path,
                          predict_status_path,
                          params,
                          img_list,
                          img_data,
                          save_dir,
                          score_thresh,
                          epoch=None):
    total_num = open(
        osp.join(predict_status_path, 'total_num'), 'w', encoding='utf-8')

    def write_file_num(total_file_num):
        total_num.write(str(total_file_num))
        total_num.close()

    sys.stdout = open(
        osp.join(predict_status_path, 'out.log'), 'w', encoding='utf-8')
    sys.stderr = open(
        osp.join(predict_status_path, 'err.log'), 'w', encoding='utf-8')

    import paddlex as pdx
    pdx.log_level = 3
    task_type = params['task_type']
    dataset_path = params['dataset_path']
    if epoch is None:
        model_path = osp.join(task_path, 'output', 'best_model')
    else:
        model_path = osp.join(task_path, 'output', 'epoch_{}'.format(epoch))
    model = pdx.load_model(model_path)
    file_list = dict()
    predicted_num = 0
    if task_type == ""classification"":
        if img_data is None:
            if len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                with open(osp.join(dataset_path, ""test_list.txt"")) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = items[1]
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            for image, label_id in file_list.items():
                pred_result = {}
                if label_id is not None:
                    pred_result[""gt_label""] = model.labels[int(label_id)]
                results = model.predict(img_file=image)
                pred_result[""label""] = []
                pred_result[""score""] = []
                pred_result[""topk""] = len(results)
                for res in results:
                    pred_result[""label""].append(res['category'])
                    pred_result[""score""].append(res['score'])
                visualize_classified_result(save_dir, image, pred_result)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            pred_result = {}
            pred_result[""label""] = []
            pred_result[""score""] = []
            pred_result[""topk""] = len(results)
            for res in results:
                pred_result[""label""].append(res['category'])
                pred_result[""score""].append(res['score'])
            visualize_classified_result(save_dir, img, pred_result)
    elif task_type in [""detection"", ""instance_segmentation""]:
        if img_data is None:
            if task_type == ""detection"" and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                if len(img_list) == 0 and osp.exists(
                        osp.join(dataset_path, ""test_list.txt"")):
                    with open(
                            osp.join(dataset_path, ""test_list.txt""),
                            encoding=get_encoding(
                                osp.join(dataset_path, ""test_list.txt""))) as f:
                        for line in f:
                            items = line.strip().split()
                            file_list[osp.join(dataset_path, items[0])] = \
                                osp.join(dataset_path, items[1])
                else:
                    for image in img_list:
                        file_list[image] = None
                total_file_num = len(file_list)
                write_file_num(total_file_num)
                for image, anno in file_list.items():
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(
                        image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    image_gt = None
                    if anno is not None:
                        image_gt = plot_det_label(image, anno, model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            elif len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test.json"")):
                from pycocotools.coco import COCO
                anno_path = osp.join(dataset_path, ""test.json"")
                coco = COCO(anno_path)
                img_ids = coco.getImgIds()
                total_file_num = len(img_ids)
                write_file_num(total_file_num)
                for img_id in img_ids:
                    img_anno = coco.loadImgs(img_id)[0]
                    file_name = img_anno['file_name']
                    name = (osp.split(file_name)[-1]).split(""."")[0]
                    anno = osp.join(dataset_path, ""Annotations"", name + "".npy"")
                    img_file = osp.join(dataset_path, ""JPEGImages"", file_name)
                    results = model.predict(img_file=img_file)
                    image_pred = pdx.det.visualize(
                        img_file,
                        results,
                        threshold=score_thresh,
                        save_dir=None)
                    save_name = osp.join(save_dir, osp.split(img_file)[-1])
                    if task_type == ""detection"":
                        image_gt = plot_det_label(img_file, anno, model.labels)
                    else:
                        image_gt = plot_insseg_label(img_file, anno,
                                                     model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            else:
                total_file_num = len(img_list)
                write_file_num(total_file_num)
                for image in img_list:
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(
                        image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    visualize_detected_result(save_name, None, image_pred)
                    predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            image_pred = pdx.det.visualize(
                img, results, threshold=score_thresh, save_dir=None)
            image_gt = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_detected_result(save_name, image_gt, image_pred)

    elif task_type == ""segmentation"":
        if img_data is None:
            if len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                with open(
                        osp.join(dataset_path, ""test_list.txt""),
                        encoding=get_encoding(
                            osp.join(dataset_path, ""test_list.txt""))) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = \
                            osp.join(dataset_path, items[1])
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            for image, anno in file_list.items():
                results = model.predict(img_file=image)
                image_pred = pdx.seg.visualize(image, results, save_dir=None)
                pse_pred = pdx.seg.visualize(
                    image, results, weight=0, save_dir=None)
                image_ground = None
                pse_label = None
                if anno is not None:
                    label = np.asarray(Image.open(anno)).astype('uint8')
                    image_ground = pdx.seg.visualize(
                        image, {'label_map': label}, save_dir=None)
                    pse_label = pdx.seg.visualize(
                        image, {'label_map': label}, weight=0, save_dir=None)
                save_name = osp.join(save_dir, osp.split(image)[-1])
                visualize_segmented_result(save_name, image_ground, pse_label,
                                           image_pred, pse_pred, legend)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            results = model.predict(img)
            image_pred = pdx.seg.visualize(img, results, save_dir=None)
            pse_pred = pdx.seg.visualize(img, results, weight=0, save_dir=None)
            image_ground = None
            pse_label = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_segmented_result(save_name, image_ground, pse_label,
                                       image_pred, pse_pred, legend)
    set_folder_status(predict_status_path, PredictStatus.XPREDONE)",_15501.py,193,"for i in range(len(model.labels)):
    legend[model.labels[i]] = color_map[i]",legend.update({model.labels[i]: color_map[i] for i in range(len(model.labels))}),1,,
https://github.com/PaddlePaddle/PaddleX/tree/master/paddlex_restful/restful/project/operate.py,"def _call_paddlex_predict(task_path,
                          predict_status_path,
                          params,
                          img_list,
                          img_data,
                          save_dir,
                          score_thresh,
                          epoch=None):
    total_num = open(
        osp.join(predict_status_path, 'total_num'), 'w', encoding='utf-8')

    def write_file_num(total_file_num):
        total_num.write(str(total_file_num))
        total_num.close()

    sys.stdout = open(
        osp.join(predict_status_path, 'out.log'), 'w', encoding='utf-8')
    sys.stderr = open(
        osp.join(predict_status_path, 'err.log'), 'w', encoding='utf-8')

    import paddlex as pdx
    pdx.log_level = 3
    task_type = params['task_type']
    dataset_path = params['dataset_path']
    if epoch is None:
        model_path = osp.join(task_path, 'output', 'best_model')
    else:
        model_path = osp.join(task_path, 'output', 'epoch_{}'.format(epoch))
    model = pdx.load_model(model_path)
    file_list = dict()
    predicted_num = 0
    if task_type == ""classification"":
        if img_data is None:
            if len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                with open(osp.join(dataset_path, ""test_list.txt"")) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = items[1]
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            for image, label_id in file_list.items():
                pred_result = {}
                if label_id is not None:
                    pred_result[""gt_label""] = model.labels[int(label_id)]
                results = model.predict(img_file=image)
                pred_result[""label""] = []
                pred_result[""score""] = []
                pred_result[""topk""] = len(results)
                for res in results:
                    pred_result[""label""].append(res['category'])
                    pred_result[""score""].append(res['score'])
                visualize_classified_result(save_dir, image, pred_result)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            pred_result = {}
            pred_result[""label""] = []
            pred_result[""score""] = []
            pred_result[""topk""] = len(results)
            for res in results:
                pred_result[""label""].append(res['category'])
                pred_result[""score""].append(res['score'])
            visualize_classified_result(save_dir, img, pred_result)
    elif task_type in [""detection"", ""instance_segmentation""]:
        if img_data is None:
            if task_type == ""detection"" and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                if len(img_list) == 0 and osp.exists(
                        osp.join(dataset_path, ""test_list.txt"")):
                    with open(
                            osp.join(dataset_path, ""test_list.txt""),
                            encoding=get_encoding(
                                osp.join(dataset_path, ""test_list.txt""))) as f:
                        for line in f:
                            items = line.strip().split()
                            file_list[osp.join(dataset_path, items[0])] = \
                                osp.join(dataset_path, items[1])
                else:
                    for image in img_list:
                        file_list[image] = None
                total_file_num = len(file_list)
                write_file_num(total_file_num)
                for image, anno in file_list.items():
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(
                        image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    image_gt = None
                    if anno is not None:
                        image_gt = plot_det_label(image, anno, model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            elif len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test.json"")):
                from pycocotools.coco import COCO
                anno_path = osp.join(dataset_path, ""test.json"")
                coco = COCO(anno_path)
                img_ids = coco.getImgIds()
                total_file_num = len(img_ids)
                write_file_num(total_file_num)
                for img_id in img_ids:
                    img_anno = coco.loadImgs(img_id)[0]
                    file_name = img_anno['file_name']
                    name = (osp.split(file_name)[-1]).split(""."")[0]
                    anno = osp.join(dataset_path, ""Annotations"", name + "".npy"")
                    img_file = osp.join(dataset_path, ""JPEGImages"", file_name)
                    results = model.predict(img_file=img_file)
                    image_pred = pdx.det.visualize(
                        img_file,
                        results,
                        threshold=score_thresh,
                        save_dir=None)
                    save_name = osp.join(save_dir, osp.split(img_file)[-1])
                    if task_type == ""detection"":
                        image_gt = plot_det_label(img_file, anno, model.labels)
                    else:
                        image_gt = plot_insseg_label(img_file, anno,
                                                     model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            else:
                total_file_num = len(img_list)
                write_file_num(total_file_num)
                for image in img_list:
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(
                        image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    visualize_detected_result(save_name, None, image_pred)
                    predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            image_pred = pdx.det.visualize(
                img, results, threshold=score_thresh, save_dir=None)
            image_gt = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_detected_result(save_name, image_gt, image_pred)

    elif task_type == ""segmentation"":
        if img_data is None:
            if len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                with open(
                        osp.join(dataset_path, ""test_list.txt""),
                        encoding=get_encoding(
                            osp.join(dataset_path, ""test_list.txt""))) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = \
                            osp.join(dataset_path, items[1])
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            for image, anno in file_list.items():
                results = model.predict(img_file=image)
                image_pred = pdx.seg.visualize(image, results, save_dir=None)
                pse_pred = pdx.seg.visualize(
                    image, results, weight=0, save_dir=None)
                image_ground = None
                pse_label = None
                if anno is not None:
                    label = np.asarray(Image.open(anno)).astype('uint8')
                    image_ground = pdx.seg.visualize(
                        image, {'label_map': label}, save_dir=None)
                    pse_label = pdx.seg.visualize(
                        image, {'label_map': label}, weight=0, save_dir=None)
                save_name = osp.join(save_dir, osp.split(image)[-1])
                visualize_segmented_result(save_name, image_ground, pse_label,
                                           image_pred, pse_pred, legend)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            results = model.predict(img)
            image_pred = pdx.seg.visualize(img, results, save_dir=None)
            pse_pred = pdx.seg.visualize(img, results, weight=0, save_dir=None)
            image_ground = None
            pse_label = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_segmented_result(save_name, image_ground, pse_label,
                                       image_pred, pse_pred, legend)
    set_folder_status(predict_status_path, PredictStatus.XPREDONE)",_15501.py,86,"for image in img_list:
    file_list[image] = None",file_list.update({image: None for image in img_list}),1,,
https://github.com/PaddlePaddle/PaddleX/tree/master/paddlex_restful/restful/project/operate.py,"def _call_paddlex_predict(task_path,
                          predict_status_path,
                          params,
                          img_list,
                          img_data,
                          save_dir,
                          score_thresh,
                          epoch=None):
    total_num = open(
        osp.join(predict_status_path, 'total_num'), 'w', encoding='utf-8')

    def write_file_num(total_file_num):
        total_num.write(str(total_file_num))
        total_num.close()

    sys.stdout = open(
        osp.join(predict_status_path, 'out.log'), 'w', encoding='utf-8')
    sys.stderr = open(
        osp.join(predict_status_path, 'err.log'), 'w', encoding='utf-8')

    import paddlex as pdx
    pdx.log_level = 3
    task_type = params['task_type']
    dataset_path = params['dataset_path']
    if epoch is None:
        model_path = osp.join(task_path, 'output', 'best_model')
    else:
        model_path = osp.join(task_path, 'output', 'epoch_{}'.format(epoch))
    model = pdx.load_model(model_path)
    file_list = dict()
    predicted_num = 0
    if task_type == ""classification"":
        if img_data is None:
            if len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                with open(osp.join(dataset_path, ""test_list.txt"")) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = items[1]
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            for image, label_id in file_list.items():
                pred_result = {}
                if label_id is not None:
                    pred_result[""gt_label""] = model.labels[int(label_id)]
                results = model.predict(img_file=image)
                pred_result[""label""] = []
                pred_result[""score""] = []
                pred_result[""topk""] = len(results)
                for res in results:
                    pred_result[""label""].append(res['category'])
                    pred_result[""score""].append(res['score'])
                visualize_classified_result(save_dir, image, pred_result)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            pred_result = {}
            pred_result[""label""] = []
            pred_result[""score""] = []
            pred_result[""topk""] = len(results)
            for res in results:
                pred_result[""label""].append(res['category'])
                pred_result[""score""].append(res['score'])
            visualize_classified_result(save_dir, img, pred_result)
    elif task_type in [""detection"", ""instance_segmentation""]:
        if img_data is None:
            if task_type == ""detection"" and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                if len(img_list) == 0 and osp.exists(
                        osp.join(dataset_path, ""test_list.txt"")):
                    with open(
                            osp.join(dataset_path, ""test_list.txt""),
                            encoding=get_encoding(
                                osp.join(dataset_path, ""test_list.txt""))) as f:
                        for line in f:
                            items = line.strip().split()
                            file_list[osp.join(dataset_path, items[0])] = \
                                osp.join(dataset_path, items[1])
                else:
                    for image in img_list:
                        file_list[image] = None
                total_file_num = len(file_list)
                write_file_num(total_file_num)
                for image, anno in file_list.items():
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(
                        image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    image_gt = None
                    if anno is not None:
                        image_gt = plot_det_label(image, anno, model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            elif len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test.json"")):
                from pycocotools.coco import COCO
                anno_path = osp.join(dataset_path, ""test.json"")
                coco = COCO(anno_path)
                img_ids = coco.getImgIds()
                total_file_num = len(img_ids)
                write_file_num(total_file_num)
                for img_id in img_ids:
                    img_anno = coco.loadImgs(img_id)[0]
                    file_name = img_anno['file_name']
                    name = (osp.split(file_name)[-1]).split(""."")[0]
                    anno = osp.join(dataset_path, ""Annotations"", name + "".npy"")
                    img_file = osp.join(dataset_path, ""JPEGImages"", file_name)
                    results = model.predict(img_file=img_file)
                    image_pred = pdx.det.visualize(
                        img_file,
                        results,
                        threshold=score_thresh,
                        save_dir=None)
                    save_name = osp.join(save_dir, osp.split(img_file)[-1])
                    if task_type == ""detection"":
                        image_gt = plot_det_label(img_file, anno, model.labels)
                    else:
                        image_gt = plot_insseg_label(img_file, anno,
                                                     model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            else:
                total_file_num = len(img_list)
                write_file_num(total_file_num)
                for image in img_list:
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(
                        image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    visualize_detected_result(save_name, None, image_pred)
                    predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            image_pred = pdx.det.visualize(
                img, results, threshold=score_thresh, save_dir=None)
            image_gt = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_detected_result(save_name, image_gt, image_pred)

    elif task_type == ""segmentation"":
        if img_data is None:
            if len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                with open(
                        osp.join(dataset_path, ""test_list.txt""),
                        encoding=get_encoding(
                            osp.join(dataset_path, ""test_list.txt""))) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = \
                            osp.join(dataset_path, items[1])
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            for image, anno in file_list.items():
                results = model.predict(img_file=image)
                image_pred = pdx.seg.visualize(image, results, save_dir=None)
                pse_pred = pdx.seg.visualize(
                    image, results, weight=0, save_dir=None)
                image_ground = None
                pse_label = None
                if anno is not None:
                    label = np.asarray(Image.open(anno)).astype('uint8')
                    image_ground = pdx.seg.visualize(
                        image, {'label_map': label}, save_dir=None)
                    pse_label = pdx.seg.visualize(
                        image, {'label_map': label}, weight=0, save_dir=None)
                save_name = osp.join(save_dir, osp.split(image)[-1])
                visualize_segmented_result(save_name, image_ground, pse_label,
                                           image_pred, pse_pred, legend)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            results = model.predict(img)
            image_pred = pdx.seg.visualize(img, results, save_dir=None)
            pse_pred = pdx.seg.visualize(img, results, weight=0, save_dir=None)
            image_ground = None
            pse_label = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_segmented_result(save_name, image_ground, pse_label,
                                       image_pred, pse_pred, legend)
    set_folder_status(predict_status_path, PredictStatus.XPREDONE)",_15501.py,162,"for image in img_list:
    file_list[image] = None",file_list.update({image: None for image in img_list}),1,,
https://github.com/PaddlePaddle/PaddleX/tree/master/paddlex_restful/restful/project/operate.py,"def _call_paddlex_predict(task_path,
                          predict_status_path,
                          params,
                          img_list,
                          img_data,
                          save_dir,
                          score_thresh,
                          epoch=None):
    total_num = open(
        osp.join(predict_status_path, 'total_num'), 'w', encoding='utf-8')

    def write_file_num(total_file_num):
        total_num.write(str(total_file_num))
        total_num.close()

    sys.stdout = open(
        osp.join(predict_status_path, 'out.log'), 'w', encoding='utf-8')
    sys.stderr = open(
        osp.join(predict_status_path, 'err.log'), 'w', encoding='utf-8')

    import paddlex as pdx
    pdx.log_level = 3
    task_type = params['task_type']
    dataset_path = params['dataset_path']
    if epoch is None:
        model_path = osp.join(task_path, 'output', 'best_model')
    else:
        model_path = osp.join(task_path, 'output', 'epoch_{}'.format(epoch))
    model = pdx.load_model(model_path)
    file_list = dict()
    predicted_num = 0
    if task_type == ""classification"":
        if img_data is None:
            if len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                with open(osp.join(dataset_path, ""test_list.txt"")) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = items[1]
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            for image, label_id in file_list.items():
                pred_result = {}
                if label_id is not None:
                    pred_result[""gt_label""] = model.labels[int(label_id)]
                results = model.predict(img_file=image)
                pred_result[""label""] = []
                pred_result[""score""] = []
                pred_result[""topk""] = len(results)
                for res in results:
                    pred_result[""label""].append(res['category'])
                    pred_result[""score""].append(res['score'])
                visualize_classified_result(save_dir, image, pred_result)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            pred_result = {}
            pred_result[""label""] = []
            pred_result[""score""] = []
            pred_result[""topk""] = len(results)
            for res in results:
                pred_result[""label""].append(res['category'])
                pred_result[""score""].append(res['score'])
            visualize_classified_result(save_dir, img, pred_result)
    elif task_type in [""detection"", ""instance_segmentation""]:
        if img_data is None:
            if task_type == ""detection"" and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                if len(img_list) == 0 and osp.exists(
                        osp.join(dataset_path, ""test_list.txt"")):
                    with open(
                            osp.join(dataset_path, ""test_list.txt""),
                            encoding=get_encoding(
                                osp.join(dataset_path, ""test_list.txt""))) as f:
                        for line in f:
                            items = line.strip().split()
                            file_list[osp.join(dataset_path, items[0])] = \
                                osp.join(dataset_path, items[1])
                else:
                    for image in img_list:
                        file_list[image] = None
                total_file_num = len(file_list)
                write_file_num(total_file_num)
                for image, anno in file_list.items():
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(
                        image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    image_gt = None
                    if anno is not None:
                        image_gt = plot_det_label(image, anno, model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            elif len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test.json"")):
                from pycocotools.coco import COCO
                anno_path = osp.join(dataset_path, ""test.json"")
                coco = COCO(anno_path)
                img_ids = coco.getImgIds()
                total_file_num = len(img_ids)
                write_file_num(total_file_num)
                for img_id in img_ids:
                    img_anno = coco.loadImgs(img_id)[0]
                    file_name = img_anno['file_name']
                    name = (osp.split(file_name)[-1]).split(""."")[0]
                    anno = osp.join(dataset_path, ""Annotations"", name + "".npy"")
                    img_file = osp.join(dataset_path, ""JPEGImages"", file_name)
                    results = model.predict(img_file=img_file)
                    image_pred = pdx.det.visualize(
                        img_file,
                        results,
                        threshold=score_thresh,
                        save_dir=None)
                    save_name = osp.join(save_dir, osp.split(img_file)[-1])
                    if task_type == ""detection"":
                        image_gt = plot_det_label(img_file, anno, model.labels)
                    else:
                        image_gt = plot_insseg_label(img_file, anno,
                                                     model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            else:
                total_file_num = len(img_list)
                write_file_num(total_file_num)
                for image in img_list:
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(
                        image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    visualize_detected_result(save_name, None, image_pred)
                    predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            image_pred = pdx.det.visualize(
                img, results, threshold=score_thresh, save_dir=None)
            image_gt = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_detected_result(save_name, image_gt, image_pred)

    elif task_type == ""segmentation"":
        if img_data is None:
            if len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                with open(
                        osp.join(dataset_path, ""test_list.txt""),
                        encoding=get_encoding(
                            osp.join(dataset_path, ""test_list.txt""))) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = \
                            osp.join(dataset_path, items[1])
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            for image, anno in file_list.items():
                results = model.predict(img_file=image)
                image_pred = pdx.seg.visualize(image, results, save_dir=None)
                pse_pred = pdx.seg.visualize(
                    image, results, weight=0, save_dir=None)
                image_ground = None
                pse_label = None
                if anno is not None:
                    label = np.asarray(Image.open(anno)).astype('uint8')
                    image_ground = pdx.seg.visualize(
                        image, {'label_map': label}, save_dir=None)
                    pse_label = pdx.seg.visualize(
                        image, {'label_map': label}, weight=0, save_dir=None)
                save_name = osp.join(save_dir, osp.split(image)[-1])
                visualize_segmented_result(save_name, image_ground, pse_label,
                                           image_pred, pse_pred, legend)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            results = model.predict(img)
            image_pred = pdx.seg.visualize(img, results, save_dir=None)
            pse_pred = pdx.seg.visualize(img, results, weight=0, save_dir=None)
            image_ground = None
            pse_label = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_segmented_result(save_name, image_ground, pse_label,
                                       image_pred, pse_pred, legend)
    set_folder_status(predict_status_path, PredictStatus.XPREDONE)",_15501.py,81,"for line in f:
    items = line.strip().split()
    file_list[osp.join(dataset_path, items[0])] = osp.join(dataset_path, items[1])","file_list.update({osp.join(dataset_path, line.strip().split()[0]): osp.join(dataset_path, line.strip().split()[1]) for line in f})",1,,
https://github.com/PaddlePaddle/PaddleX/tree/master/paddlex_restful/restful/project/operate.py,"def _call_paddlex_predict(task_path,
                          predict_status_path,
                          params,
                          img_list,
                          img_data,
                          save_dir,
                          score_thresh,
                          epoch=None):
    total_num = open(
        osp.join(predict_status_path, 'total_num'), 'w', encoding='utf-8')

    def write_file_num(total_file_num):
        total_num.write(str(total_file_num))
        total_num.close()

    sys.stdout = open(
        osp.join(predict_status_path, 'out.log'), 'w', encoding='utf-8')
    sys.stderr = open(
        osp.join(predict_status_path, 'err.log'), 'w', encoding='utf-8')

    import paddlex as pdx
    pdx.log_level = 3
    task_type = params['task_type']
    dataset_path = params['dataset_path']
    if epoch is None:
        model_path = osp.join(task_path, 'output', 'best_model')
    else:
        model_path = osp.join(task_path, 'output', 'epoch_{}'.format(epoch))
    model = pdx.load_model(model_path)
    file_list = dict()
    predicted_num = 0
    if task_type == ""classification"":
        if img_data is None:
            if len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                with open(osp.join(dataset_path, ""test_list.txt"")) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = items[1]
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            for image, label_id in file_list.items():
                pred_result = {}
                if label_id is not None:
                    pred_result[""gt_label""] = model.labels[int(label_id)]
                results = model.predict(img_file=image)
                pred_result[""label""] = []
                pred_result[""score""] = []
                pred_result[""topk""] = len(results)
                for res in results:
                    pred_result[""label""].append(res['category'])
                    pred_result[""score""].append(res['score'])
                visualize_classified_result(save_dir, image, pred_result)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            pred_result = {}
            pred_result[""label""] = []
            pred_result[""score""] = []
            pred_result[""topk""] = len(results)
            for res in results:
                pred_result[""label""].append(res['category'])
                pred_result[""score""].append(res['score'])
            visualize_classified_result(save_dir, img, pred_result)
    elif task_type in [""detection"", ""instance_segmentation""]:
        if img_data is None:
            if task_type == ""detection"" and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                if len(img_list) == 0 and osp.exists(
                        osp.join(dataset_path, ""test_list.txt"")):
                    with open(
                            osp.join(dataset_path, ""test_list.txt""),
                            encoding=get_encoding(
                                osp.join(dataset_path, ""test_list.txt""))) as f:
                        for line in f:
                            items = line.strip().split()
                            file_list[osp.join(dataset_path, items[0])] = \
                                osp.join(dataset_path, items[1])
                else:
                    for image in img_list:
                        file_list[image] = None
                total_file_num = len(file_list)
                write_file_num(total_file_num)
                for image, anno in file_list.items():
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(
                        image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    image_gt = None
                    if anno is not None:
                        image_gt = plot_det_label(image, anno, model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            elif len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test.json"")):
                from pycocotools.coco import COCO
                anno_path = osp.join(dataset_path, ""test.json"")
                coco = COCO(anno_path)
                img_ids = coco.getImgIds()
                total_file_num = len(img_ids)
                write_file_num(total_file_num)
                for img_id in img_ids:
                    img_anno = coco.loadImgs(img_id)[0]
                    file_name = img_anno['file_name']
                    name = (osp.split(file_name)[-1]).split(""."")[0]
                    anno = osp.join(dataset_path, ""Annotations"", name + "".npy"")
                    img_file = osp.join(dataset_path, ""JPEGImages"", file_name)
                    results = model.predict(img_file=img_file)
                    image_pred = pdx.det.visualize(
                        img_file,
                        results,
                        threshold=score_thresh,
                        save_dir=None)
                    save_name = osp.join(save_dir, osp.split(img_file)[-1])
                    if task_type == ""detection"":
                        image_gt = plot_det_label(img_file, anno, model.labels)
                    else:
                        image_gt = plot_insseg_label(img_file, anno,
                                                     model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            else:
                total_file_num = len(img_list)
                write_file_num(total_file_num)
                for image in img_list:
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(
                        image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    visualize_detected_result(save_name, None, image_pred)
                    predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            image_pred = pdx.det.visualize(
                img, results, threshold=score_thresh, save_dir=None)
            image_gt = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_detected_result(save_name, image_gt, image_pred)

    elif task_type == ""segmentation"":
        if img_data is None:
            if len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                with open(
                        osp.join(dataset_path, ""test_list.txt""),
                        encoding=get_encoding(
                            osp.join(dataset_path, ""test_list.txt""))) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = \
                            osp.join(dataset_path, items[1])
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            for image, anno in file_list.items():
                results = model.predict(img_file=image)
                image_pred = pdx.seg.visualize(image, results, save_dir=None)
                pse_pred = pdx.seg.visualize(
                    image, results, weight=0, save_dir=None)
                image_ground = None
                pse_label = None
                if anno is not None:
                    label = np.asarray(Image.open(anno)).astype('uint8')
                    image_ground = pdx.seg.visualize(
                        image, {'label_map': label}, save_dir=None)
                    pse_label = pdx.seg.visualize(
                        image, {'label_map': label}, weight=0, save_dir=None)
                save_name = osp.join(save_dir, osp.split(image)[-1])
                visualize_segmented_result(save_name, image_ground, pse_label,
                                           image_pred, pse_pred, legend)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            results = model.predict(img)
            image_pred = pdx.seg.visualize(img, results, save_dir=None)
            pse_pred = pdx.seg.visualize(img, results, weight=0, save_dir=None)
            image_ground = None
            pse_label = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_segmented_result(save_name, image_ground, pse_label,
                                       image_pred, pse_pred, legend)
    set_folder_status(predict_status_path, PredictStatus.XPREDONE)",_15501.py,157,"for line in f:
    items = line.strip().split()
    file_list[osp.join(dataset_path, items[0])] = osp.join(dataset_path, items[1])","file_list.update({osp.join(dataset_path, line.strip().split()[0]): osp.join(dataset_path, line.strip().split()[1]) for line in f})",1,,
https://github.com/blakeblackshear/frigate/tree/master/frigate/edgetpu.py,"def run_detector(
    name: str,
    detection_queue: mp.Queue,
    out_events: Dict[str, mp.Event],
    avg_speed,
    start,
    model_path,
    model_shape,
    tf_device,
    num_threads,
):
    threading.current_thread().name = f""detector:{name}""
    logger = logging.getLogger(f""detector.{name}"")
    logger.info(f""Starting detection process: {os.getpid()}"")
    setproctitle(f""frigate.detector.{name}"")
    listen()

    stop_event = mp.Event()

    def receiveSignal(signalNumber, frame):
        stop_event.set()

    signal.signal(signal.SIGTERM, receiveSignal)
    signal.signal(signal.SIGINT, receiveSignal)

    frame_manager = SharedMemoryFrameManager()
    object_detector = LocalObjectDetector(
        tf_device=tf_device, model_path=model_path, num_threads=num_threads
    )

    outputs = {}
    for name in out_events.keys():
        out_shm = mp.shared_memory.SharedMemory(name=f""out-{name}"", create=False)
        out_np = np.ndarray((20, 6), dtype=np.float32, buffer=out_shm.buf)
        outputs[name] = {""shm"": out_shm, ""np"": out_np}

    while not stop_event.is_set():
        try:
            connection_id = detection_queue.get(timeout=5)
        except queue.Empty:
            continue
        input_frame = frame_manager.get(
            connection_id, (1, model_shape[0], model_shape[1], 3)
        )

        if input_frame is None:
            continue

        # detect and send the output
        start.value = datetime.datetime.now().timestamp()
        detections = object_detector.detect_raw(input_frame)
        duration = datetime.datetime.now().timestamp() - start.value
        outputs[connection_id][""np""][:] = detections[:]
        out_events[connection_id].set()
        start.value = 0.0

        avg_speed.value = (avg_speed.value * 9 + duration) / 10",_15544.py,32,"for name in out_events.keys():
    out_shm = mp.shared_memory.SharedMemory(name=f'out-{name}', create=False)
    out_np = np.ndarray((20, 6), dtype=np.float32, buffer=out_shm.buf)
    outputs[name] = {'shm': out_shm, 'np': out_np}","outputs = {name: {'shm': mp.shared_memory.SharedMemory(name=f'out-{name}', create=False), 'np': np.ndarray((20, 6), dtype=np.float32, buffer=mp.shared_memory.SharedMemory(name=f'out-{name}', create=False).buf)} for name in out_events.keys()}",1,,
https://github.com/huggingface/transformers/tree/master/examples/research_projects/bert-loses-patience/run_glue_with_pabee.py,"def train(args, train_dataset, model, tokenizer):
    """"""Train the model""""""
    if args.local_rank in [-1, 0]:
        tb_writer = SummaryWriter()

    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)
    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)
    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)

    if args.max_steps > 0:
        t_total = args.max_steps
        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1
    else:
        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs

    # Prepare optimizer and schedule (linear warmup and decay)
    no_decay = [""bias"", ""LayerNorm.weight""]
    optimizer_grouped_parameters = [
        {
            ""params"": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
            ""weight_decay"": args.weight_decay,
        },
        {""params"": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], ""weight_decay"": 0.0},
    ]

    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)
    scheduler = get_linear_schedule_with_warmup(
        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total
    )

    # Check if saved optimizer or scheduler states exist
    if os.path.isfile(os.path.join(args.model_name_or_path, ""optimizer.pt"")) and os.path.isfile(
        os.path.join(args.model_name_or_path, ""scheduler.pt"")
    ):
        # Load in optimizer and scheduler states
        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, ""optimizer.pt"")))
        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, ""scheduler.pt"")))

    if args.fp16:
        try:
            from apex import amp
        except ImportError:
            raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to use fp16 training."")
        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)

    # multi-gpu training (should be after apex fp16 initialization)
    if args.n_gpu > 1:
        model = nn.DataParallel(model)

    # Distributed training (should be after apex fp16 initialization)
    if args.local_rank != -1:
        model = nn.parallel.DistributedDataParallel(
            model,
            device_ids=[args.local_rank],
            output_device=args.local_rank,
            find_unused_parameters=True,
        )

    # Train!
    logger.info(""***** Running training *****"")
    logger.info(""  Num examples = %d"", len(train_dataset))
    logger.info(""  Num Epochs = %d"", args.num_train_epochs)
    logger.info(""  Instantaneous batch size per GPU = %d"", args.per_gpu_train_batch_size)
    logger.info(
        ""  Total train batch size (w. parallel, distributed & accumulation) = %d"",
        args.train_batch_size
        * args.gradient_accumulation_steps
        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),
    )
    logger.info(""  Gradient Accumulation steps = %d"", args.gradient_accumulation_steps)
    logger.info(""  Total optimization steps = %d"", t_total)

    global_step = 0
    epochs_trained = 0
    steps_trained_in_current_epoch = 0
    # Check if continuing training from a checkpoint
    if os.path.exists(args.model_name_or_path):
        # set global_step to gobal_step of last saved checkpoint from model path
        global_step = int(args.model_name_or_path.split(""-"")[-1].split(""/"")[0])
        epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)
        steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)

        logger.info(""  Continuing training from checkpoint, will skip to saved global_step"")
        logger.info(""  Continuing training from epoch %d"", epochs_trained)
        logger.info(""  Continuing training from global step %d"", global_step)
        logger.info(
            ""  Will skip the first %d steps in the first epoch"",
            steps_trained_in_current_epoch,
        )

    tr_loss, logging_loss = 0.0, 0.0
    model.zero_grad()
    train_iterator = trange(
        epochs_trained,
        int(args.num_train_epochs),
        desc=""Epoch"",
        disable=args.local_rank not in [-1, 0],
    )
    set_seed(args)  # Added here for reproductibility
    for _ in train_iterator:
        epoch_iterator = tqdm(train_dataloader, desc=""Iteration"", disable=args.local_rank not in [-1, 0])
        for step, batch in enumerate(epoch_iterator):

            # Skip past any already trained steps if resuming training
            if steps_trained_in_current_epoch > 0:
                steps_trained_in_current_epoch -= 1
                continue

            model.train()
            batch = tuple(t.to(args.device) for t in batch)
            inputs = {
                ""input_ids"": batch[0],
                ""attention_mask"": batch[1],
                ""labels"": batch[3],
            }
            inputs[""token_type_ids""] = batch[2]
            outputs = model(**inputs)
            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)

            if args.n_gpu > 1:
                loss = loss.mean()  # mean() to average on multi-gpu parallel training
            if args.gradient_accumulation_steps > 1:
                loss = loss / args.gradient_accumulation_steps

            if args.fp16:
                with amp.scale_loss(loss, optimizer) as scaled_loss:
                    scaled_loss.backward()
            else:
                loss.backward()

            tr_loss += loss.item()
            if (step + 1) % args.gradient_accumulation_steps == 0:
                if args.fp16:
                    nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)
                else:
                    nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                optimizer.step()
                scheduler.step()  # Update learning rate schedule
                model.zero_grad()
                global_step += 1

                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:
                    logs = {}
                    if (
                        args.local_rank == -1 and args.evaluate_during_training
                    ):  # Only evaluate when single GPU otherwise metrics may not average well
                        results = evaluate(args, model, tokenizer)
                        for key, value in results.items():
                            eval_key = ""eval_{}"".format(key)
                            logs[eval_key] = value

                    loss_scalar = (tr_loss - logging_loss) / args.logging_steps
                    learning_rate_scalar = scheduler.get_lr()[0]
                    logs[""learning_rate""] = learning_rate_scalar
                    logs[""loss""] = loss_scalar
                    logging_loss = tr_loss

                    for key, value in logs.items():
                        tb_writer.add_scalar(key, value, global_step)
                    print(json.dumps({**logs, **{""step"": global_step}}))

                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:
                    # Save model checkpoint
                    output_dir = os.path.join(args.output_dir, ""checkpoint-{}"".format(global_step))
                    model_to_save = (
                        model.module if hasattr(model, ""module"") else model
                    )  # Take care of distributed/parallel training
                    model_to_save.save_pretrained(output_dir)
                    tokenizer.save_pretrained(output_dir)

                    torch.save(args, os.path.join(output_dir, ""training_args.bin""))
                    logger.info(""Saving model checkpoint to %s"", output_dir)

                    torch.save(optimizer.state_dict(), os.path.join(output_dir, ""optimizer.pt""))
                    torch.save(scheduler.state_dict(), os.path.join(output_dir, ""scheduler.pt""))
                    logger.info(""Saving optimizer and scheduler states to %s"", output_dir)

            if args.max_steps > 0 and global_step > args.max_steps:
                epoch_iterator.close()
                break
        if args.max_steps > 0 and global_step > args.max_steps:
            train_iterator.close()
            break

    if args.local_rank in [-1, 0]:
        tb_writer.close()

    return global_step, tr_loss / global_step",_15617.py,149,"for (key, value) in results.items():
    eval_key = 'eval_{}'.format(key)
    logs[eval_key] = value","logs = {'eval_{}'.format(key): value for (key, value) in results.items()}",1,,
https://github.com/gboeing/osmnx/tree/master/osmnx/geometries.py,"def _parse_way_to_linestring_or_polygon(element, coords, polygon_features=_polygon_features):
    """"""
    Parse open LineString, closed LineString or Polygon from OSM 'way'.

    Please see https://wiki.openstreetmap.org/wiki/Overpass_turbo/Polygon_Features
    for more information on which tags should be parsed to polygons

    Parameters
    ----------
    element : dict
        element type ""way"" from overpass response JSON
    coords : dict
        dict of node IDs and their latitude/longitude coordinates
    polygon_features : dict
        dict for determining whether closed ways are LineStrings or Polygons

    Returns
    -------
    linestring_or_polygon : dict
        dict of OSM ID, OSM element type, nodes, tags and geometry
    """"""
    nodes = element[""nodes""]

    linestring_or_polygon = dict()
    linestring_or_polygon[""osmid""] = element[""id""]
    linestring_or_polygon[""element_type""] = ""way""
    linestring_or_polygon[""nodes""] = nodes

    # un-nest individual tags
    if ""tags"" in element:
        for tag in element[""tags""]:
            linestring_or_polygon[tag] = element[""tags""][tag]

    # if the OSM element is an open way (i.e. first and last nodes are not the
    # same) the geometry should be a Shapely LineString
    if element[""nodes""][0] != element[""nodes""][-1]:
        try:
            geometry = LineString([(coords[node][""lon""], coords[node][""lat""]) for node in nodes])
        except KeyError as e:  # pragma: no cover
            # XMLs may include geometries that are incomplete, in which case
            # return an empty geometry
            utils.log(
                f""node/{e} was not found in `coords`.\n""
                f""https://www.openstreetmap.org/{element['type']}/{element['id']} was not created.""
            )
            geometry = LineString()

    # if the OSM element is a closed way (i.e. first and last nodes are the
    # same) depending upon the tags the geometry could be a Shapely LineString
    # or Polygon
    elif element[""nodes""][0] == element[""nodes""][-1]:

        # determine if closed way represents LineString or Polygon
        if _is_closed_way_a_polygon(element):
            # if it is a Polygon
            try:
                geometry = Polygon([(coords[node][""lon""], coords[node][""lat""]) for node in nodes])
            except ValueError as e:
                # XMLs may include geometries that are incomplete, in which
                # case return an empty geometry
                utils.log(
                    f""{e} . The geometry for ""
                    f""https://www.openstreetmap.org/{element['type']}/{element['id']} was not created.""
                )
                geometry = Polygon()
        else:
            # if it is a LineString
            try:
                geometry = LineString(
                    [(coords[node][""lon""], coords[node][""lat""]) for node in nodes]
                )
            except ValueError as e:
                # XMLs may include geometries that are incomplete, in which
                # case return an empty geometry
                utils.log(
                    f""{e} . The geometry for ""
                    f""https://www.openstreetmap.org/{element['type']}/{element['id']} was not created.""
                )
                geometry = LineString()

    linestring_or_polygon[""geometry""] = geometry
    return linestring_or_polygon",_15784.py,31,"for tag in element['tags']:
    linestring_or_polygon[tag] = element['tags'][tag]",linestring_or_polygon.update({tag: element['tags'][tag] for tag in element['tags']}),1,,
Check,,_15904.py,30,"for (pkg_name, pkg_nfo) in __salt__['lowpkg.info'](*names, failhard=failhard).items():
    t_nfo = dict()
    if pkg_nfo.get('status', 'ii')[1] != 'i':
        continue
    for (key, value) in pkg_nfo.items():
        if key == 'package':
            t_nfo['name'] = value
        elif key == 'origin':
            t_nfo['vendor'] = value
        elif key == 'section':
            t_nfo['group'] = value
        elif key == 'maintainer':
            t_nfo['packager'] = value
        elif key == 'homepage':
            t_nfo['url'] = value
        elif key == 'status':
            continue
        else:
            t_nfo[key] = value
    ret[pkg_name] = t_nfo","ret = {pkg_name: { 
""name"":value if key == ""package"" else ""vendor"": value if key == ""origin"" else ""group"":value if key == ""section"" else ""packager"": value if key == ""maintainer"" else ""url"": value if key == ""homepage""
 else key:value for key, value in pkg_nfo.items() if key != ""status""
}
for pkg_name, pkg_nfo in __salt__[""lowpkg.info""](*names, failhard=failhard).items() if pkg_nfo.get(""status"", ""ii"")[1] == ""i""
}
",,,
