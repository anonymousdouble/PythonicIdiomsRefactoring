file_html,method_content,file_name,lineno,old_code,new_code
https://github.com/openvinotoolkit/open_model_zoo/tree/master/tools/accuracy_checker/openvino/tools/accuracy_checker/annotation_converters/_nlp_common.py,"def __init__(self, tokenizer_model, lower_case=True, remove_space=True):
        if isinstance(spm, UnsupportedPackage):
            spm.raise_error(""Sentence piece tokenizer"")
        self.encoder = spm.SentencePieceProcessor()
        self.encoder.Load(str(tokenizer_model))
        self.lower_case = lower_case
        self.remove_space = remove_space",_100.py,6,"self.lower_case = lower_case
self.remove_space = remove_space","self.lower_case , self.remove_space  = lower_case, remove_space"
https://github.com/databricks/koalas/tree/master/databricks/koalas/indexes/multi.py,"def intersection(self, other) -> ""MultiIndex"":
        """"""
        Form the intersection of two Index objects.

        This returns a new Index with elements common to the index and `other`.

        Parameters
        ----------
        other : Index or array-like

        Returns
        -------
        intersection : MultiIndex

        Examples
        --------
        >>> midx1 = ks.MultiIndex.from_tuples([(""a"", ""x""), (""b"", ""y""), (""c"", ""z"")])
        >>> midx2 = ks.MultiIndex.from_tuples([(""c"", ""z""), (""d"", ""w"")])
        >>> midx1.intersection(midx2).sort_values()  # doctest: +SKIP
        MultiIndex([('c', 'z')],
                   )
        """"""
        if isinstance(other, Series) or not is_list_like(other):
            raise TypeError(""other must be a MultiIndex or a list of tuples"")
        elif isinstance(other, DataFrame):
            raise ValueError(""Index data must be 1-dimensional"")
        elif isinstance(other, MultiIndex):
            spark_frame_other = other.to_frame().to_spark()
            keep_name = self.names == other.names
        elif isinstance(other, Index):
            # Always returns an empty MultiIndex if `other` is Index.
            return self.to_frame().head(0).index  # type: ignore
        elif not all(isinstance(item, tuple) for item in other):
            raise TypeError(""other must be a MultiIndex or a list of tuples"")
        else:
            other = MultiIndex.from_tuples(list(other))
            spark_frame_other = other.to_frame().to_spark()
            keep_name = True

        default_name = [SPARK_INDEX_NAME_FORMAT(i) for i in range(self.nlevels)]
        spark_frame_self = self.to_frame(name=default_name).to_spark()
        spark_frame_intersected = spark_frame_self.intersect(spark_frame_other)
        if keep_name:
            index_names = self._internal.index_names
        else:
            index_names = None
        internal = InternalFrame(  # TODO: dtypes?
            spark_frame=spark_frame_intersected,
            index_spark_columns=[scol_for(spark_frame_intersected, col) for col in default_name],
            index_names=index_names,
        )
        return cast(MultiIndex, DataFrame(internal).index)",_101.py,28,"spark_frame_other = other.to_frame().to_spark()
keep_name = self.names == other.names","spark_frame_other , keep_name  = other.to_frame().to_spark(), self.names == other.names"
https://github.com/anyoptimization/pymoo/tree/master/pymoo/problems/single/mopta08.py,"def _evaluate(self, x, out, *args, **kwargs):
        inputs = os.linesep.join([str(e) for e in x])
        res = subprocess.run(self.exec, input=inputs, text=True, stdout=subprocess.PIPE)
        val = np.array([float(e) for e in res.stdout.split()])
        out[""F""] = val[0]
        out[""G""] = val[1:]",_106.py,5,"out['F'] = val[0]
out['G'] = val[1:]","out['F'] , out['G']  = val[0], val[1:]"
https://github.com/virt-manager/virt-manager/tree/master/virtManager/clone.py,"def _build_cloner(self):
        conn = self.conn.get_backend()
        orig_name = self.vm.get_name()
        new_name = self.widget(""clone-new-name"").get_text()
        cloner = Cloner(conn, src_name=orig_name)
        if new_name:
            cloner.set_clone_name(new_name)
        return cloner",_11.py,2,"conn = self.conn.get_backend()
orig_name = self.vm.get_name()","conn , orig_name  = self.conn.get_backend(), self.vm.get_name()"
https://github.com/virt-manager/virt-manager/tree/master/virtManager/clone.py,"def _build_cloner(self):
        conn = self.conn.get_backend()
        orig_name = self.vm.get_name()
        new_name = self.widget(""clone-new-name"").get_text()
        cloner = Cloner(conn, src_name=orig_name)
        if new_name:
            cloner.set_clone_name(new_name)
        return cloner",_11.py,4,"new_name = self.widget('clone-new-name').get_text()
cloner = Cloner(conn, src_name=orig_name)","new_name , cloner  = self.widget('clone-new-name').get_text(), Cloner(conn, src_name=orig_name)"
https://github.com/GoogleChrome/chromium-dashboard/tree/master/api/metricsdata_test.py,"def test_datapoints_to_json_dicts__nongoogler(self):
    testing_config.sign_in('test@example.com', 222)
    datapoints = [self.datapoint]
    actual = metricsdata._datapoints_to_json_dicts(datapoints)
    expected = [{
        'bucket_id': 1,
        'date': str(datetime.date.today()),
        'day_percentage': 0.01234568,  # rounded
        'property_name': 'prop',
        }]
    self.assertEqual(expected, actual)",_113.py,4,"actual = metricsdata._datapoints_to_json_dicts(datapoints)
expected = [{'bucket_id': 1, 'date': str(datetime.date.today()), 'day_percentage': 0.01234568, 'property_name': 'prop'}]","actual , expected  = metricsdata._datapoints_to_json_dicts(datapoints), [{'bucket_id': 1, 'date': str(datetime.date.today()), 'day_percentage': 0.01234568, 'property_name': 'prop'}]"
https://github.com/openstack/nova/tree/master/nova/tests/unit/network/test_neutron.py,"def _test_nw_info_build_network(self, vif_type, mock_get_client,
                                    mock_get_physnet):
        mocked_client = mock.create_autospec(client.Client)
        mock_get_client.return_value = mocked_client
        fake_port = {
            'fixed_ips': [{'ip_address': '1.1.1.1'}],
            'id': 'port-id',
            'network_id': 'net-id',
            'binding:vif_type': vif_type,
            }
        fake_subnets = [model.Subnet(cidr='1.0.0.0/8')]
        fake_nets = [{'id': 'net-id', 'name': 'foo', 'tenant_id': 'tenant',
                      'mtu': 9000}]

        net, iid = self.api._nw_info_build_network(self.context, fake_port,
                                                   fake_nets, fake_subnets)

        self.assertEqual(fake_subnets, net['subnets'])
        self.assertEqual('net-id', net['id'])
        self.assertEqual('foo', net['label'])
        self.assertEqual('tenant', net.get_meta('tenant_id'))
        self.assertEqual(9000, net.get_meta('mtu'))
        self.assertEqual(CONF.flat_injected, net.get_meta('injected'))

        mock_get_client.assert_called_once_with(mock.ANY, admin=True)
        mock_get_physnet.assert_called_once_with(self.context, mocked_client,
                                                 'net-id')

        return net, iid",_117.py,4,"mock_get_client.return_value = mocked_client
fake_port = {'fixed_ips': [{'ip_address': '1.1.1.1'}], 'id': 'port-id', 'network_id': 'net-id', 'binding:vif_type': vif_type}
fake_subnets = [model.Subnet(cidr='1.0.0.0/8')]
fake_nets = [{'id': 'net-id', 'name': 'foo', 'tenant_id': 'tenant', 'mtu': 9000}]","mock_get_client.return_value , fake_port , fake_subnets , fake_nets  = mocked_client, {'fixed_ips': [{'ip_address': '1.1.1.1'}], 'id': 'port-id', 'network_id': 'net-id', 'binding:vif_type': vif_type}, [model.Subnet(cidr='1.0.0.0/8')], [{'id': 'net-id', 'name': 'foo', 'tenant_id': 'tenant', 'mtu': 9000}]"
https://github.com/dask/dask/tree/master/dask/array/slicing.py,"def _slice_1d(dim_shape, lengths, index):
    """"""Returns a dict of {blocknum: slice}

    This function figures out where each slice should start in each
    block for a single dimension. If the slice won't return any elements
    in the block, that block will not be in the output.

    Parameters
    ----------

    dim_shape - the number of elements in this dimension.
      This should be a positive, non-zero integer
    blocksize - the number of elements per block in this dimension
      This should be a positive, non-zero integer
    index - a description of the elements in this dimension that we want
      This might be an integer, a slice(), or an Ellipsis

    Returns
    -------

    dictionary where the keys are the integer index of the blocks that
      should be sliced and the values are the slices

    Examples
    --------

    Trivial slicing

    >>> _slice_1d(100, [60, 40], slice(None, None, None))
    {0: slice(None, None, None), 1: slice(None, None, None)}

    100 length array cut into length 20 pieces, slice 0:35

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(0, 35))
    {0: slice(None, None, None), 1: slice(0, 15, 1)}

    Support irregular blocks and various slices

    >>> _slice_1d(100, [20, 10, 10, 10, 25, 25], slice(10, 35))
    {0: slice(10, 20, 1), 1: slice(None, None, None), 2: slice(0, 5, 1)}

    Support step sizes

    >>> _slice_1d(100, [15, 14, 13], slice(10, 41, 3))
    {0: slice(10, 15, 3), 1: slice(1, 14, 3), 2: slice(2, 12, 3)}

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(0, 100, 40))  # step > blocksize
    {0: slice(0, 20, 40), 2: slice(0, 20, 40), 4: slice(0, 20, 40)}

    Also support indexing single elements

    >>> _slice_1d(100, [20, 20, 20, 20, 20], 25)
    {1: 5}

    And negative slicing

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(100, 0, -3)) # doctest: +NORMALIZE_WHITESPACE
    {4: slice(-1, -21, -3),
     3: slice(-2, -21, -3),
     2: slice(-3, -21, -3),
     1: slice(-1, -21, -3),
     0: slice(-2, -20, -3)}

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(100, 12, -3)) # doctest: +NORMALIZE_WHITESPACE
    {4: slice(-1, -21, -3),
     3: slice(-2, -21, -3),
     2: slice(-3, -21, -3),
     1: slice(-1, -21, -3),
     0: slice(-2, -8, -3)}

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(100, -12, -3))
    {4: slice(-1, -12, -3)}
    """"""
    chunk_boundaries = cached_cumsum(lengths)

    if isinstance(index, Integral):
        # use right-side search to be consistent with previous result
        i = bisect.bisect_right(chunk_boundaries, index)
        if i > 0:
            # the very first chunk has no relative shift
            ind = index - chunk_boundaries[i - 1]
        else:
            ind = index
        return {int(i): int(ind)}

    assert isinstance(index, slice)

    if index == colon:
        return {k: colon for k in range(len(lengths))}

    step = index.step or 1
    if step > 0:
        start = index.start or 0
        stop = index.stop if index.stop is not None else dim_shape
    else:
        start = index.start if index.start is not None else dim_shape - 1
        start = dim_shape - 1 if start >= dim_shape else start
        stop = -(dim_shape + 1) if index.stop is None else index.stop

    # posify start and stop
    if start < 0:
        start += dim_shape
    if stop < 0:
        stop += dim_shape

    d = dict()
    if step > 0:
        istart = bisect.bisect_right(chunk_boundaries, start)
        istop = bisect.bisect_left(chunk_boundaries, stop)

        # the bound is not exactly tight; make it tighter?
        istop = min(istop + 1, len(lengths))

        # jump directly to istart
        if istart > 0:
            start = start - chunk_boundaries[istart - 1]
            stop = stop - chunk_boundaries[istart - 1]

        for i in range(istart, istop):
            length = lengths[i]
            if start < length and stop > 0:
                d[i] = slice(start, min(stop, length), step)
                start = (start - length) % step
            else:
                start = start - length
            stop -= length
    else:
        rstart = start  # running start

        istart = bisect.bisect_left(chunk_boundaries, start)
        istop = bisect.bisect_right(chunk_boundaries, stop)

        # the bound is not exactly tight; make it tighter?
        istart = min(istart + 1, len(chunk_boundaries) - 1)
        istop = max(istop - 1, -1)

        for i in range(istart, istop, -1):
            chunk_stop = chunk_boundaries[i]
            # create a chunk start and stop
            if i == 0:
                chunk_start = 0
            else:
                chunk_start = chunk_boundaries[i - 1]

            # if our slice is in this chunk
            if (chunk_start <= rstart < chunk_stop) and (rstart > stop):
                d[i] = slice(
                    rstart - chunk_stop,
                    max(chunk_start - chunk_stop - 1, stop - chunk_stop),
                    step,
                )

                # compute the next running start point,
                offset = (rstart - (chunk_start - 1)) % step
                rstart = chunk_start + offset - 1

    # replace 0:20:1 with : if appropriate
    for k, v in d.items():
        if v == slice(0, lengths[k], 1):
            d[k] = slice(None, None, None)

    if not d:  # special case x[:0]
        d[0] = slice(0, 0, 1)

    return d",_120.py,93,"start = index.start or 0
stop = index.stop if index.stop is not None else dim_shape","start , stop  = index.start or 0, index.stop if index.stop is not None else dim_shape"
https://github.com/dask/dask/tree/master/dask/array/slicing.py,"def _slice_1d(dim_shape, lengths, index):
    """"""Returns a dict of {blocknum: slice}

    This function figures out where each slice should start in each
    block for a single dimension. If the slice won't return any elements
    in the block, that block will not be in the output.

    Parameters
    ----------

    dim_shape - the number of elements in this dimension.
      This should be a positive, non-zero integer
    blocksize - the number of elements per block in this dimension
      This should be a positive, non-zero integer
    index - a description of the elements in this dimension that we want
      This might be an integer, a slice(), or an Ellipsis

    Returns
    -------

    dictionary where the keys are the integer index of the blocks that
      should be sliced and the values are the slices

    Examples
    --------

    Trivial slicing

    >>> _slice_1d(100, [60, 40], slice(None, None, None))
    {0: slice(None, None, None), 1: slice(None, None, None)}

    100 length array cut into length 20 pieces, slice 0:35

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(0, 35))
    {0: slice(None, None, None), 1: slice(0, 15, 1)}

    Support irregular blocks and various slices

    >>> _slice_1d(100, [20, 10, 10, 10, 25, 25], slice(10, 35))
    {0: slice(10, 20, 1), 1: slice(None, None, None), 2: slice(0, 5, 1)}

    Support step sizes

    >>> _slice_1d(100, [15, 14, 13], slice(10, 41, 3))
    {0: slice(10, 15, 3), 1: slice(1, 14, 3), 2: slice(2, 12, 3)}

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(0, 100, 40))  # step > blocksize
    {0: slice(0, 20, 40), 2: slice(0, 20, 40), 4: slice(0, 20, 40)}

    Also support indexing single elements

    >>> _slice_1d(100, [20, 20, 20, 20, 20], 25)
    {1: 5}

    And negative slicing

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(100, 0, -3)) # doctest: +NORMALIZE_WHITESPACE
    {4: slice(-1, -21, -3),
     3: slice(-2, -21, -3),
     2: slice(-3, -21, -3),
     1: slice(-1, -21, -3),
     0: slice(-2, -20, -3)}

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(100, 12, -3)) # doctest: +NORMALIZE_WHITESPACE
    {4: slice(-1, -21, -3),
     3: slice(-2, -21, -3),
     2: slice(-3, -21, -3),
     1: slice(-1, -21, -3),
     0: slice(-2, -8, -3)}

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(100, -12, -3))
    {4: slice(-1, -12, -3)}
    """"""
    chunk_boundaries = cached_cumsum(lengths)

    if isinstance(index, Integral):
        # use right-side search to be consistent with previous result
        i = bisect.bisect_right(chunk_boundaries, index)
        if i > 0:
            # the very first chunk has no relative shift
            ind = index - chunk_boundaries[i - 1]
        else:
            ind = index
        return {int(i): int(ind)}

    assert isinstance(index, slice)

    if index == colon:
        return {k: colon for k in range(len(lengths))}

    step = index.step or 1
    if step > 0:
        start = index.start or 0
        stop = index.stop if index.stop is not None else dim_shape
    else:
        start = index.start if index.start is not None else dim_shape - 1
        start = dim_shape - 1 if start >= dim_shape else start
        stop = -(dim_shape + 1) if index.stop is None else index.stop

    # posify start and stop
    if start < 0:
        start += dim_shape
    if stop < 0:
        stop += dim_shape

    d = dict()
    if step > 0:
        istart = bisect.bisect_right(chunk_boundaries, start)
        istop = bisect.bisect_left(chunk_boundaries, stop)

        # the bound is not exactly tight; make it tighter?
        istop = min(istop + 1, len(lengths))

        # jump directly to istart
        if istart > 0:
            start = start - chunk_boundaries[istart - 1]
            stop = stop - chunk_boundaries[istart - 1]

        for i in range(istart, istop):
            length = lengths[i]
            if start < length and stop > 0:
                d[i] = slice(start, min(stop, length), step)
                start = (start - length) % step
            else:
                start = start - length
            stop -= length
    else:
        rstart = start  # running start

        istart = bisect.bisect_left(chunk_boundaries, start)
        istop = bisect.bisect_right(chunk_boundaries, stop)

        # the bound is not exactly tight; make it tighter?
        istart = min(istart + 1, len(chunk_boundaries) - 1)
        istop = max(istop - 1, -1)

        for i in range(istart, istop, -1):
            chunk_stop = chunk_boundaries[i]
            # create a chunk start and stop
            if i == 0:
                chunk_start = 0
            else:
                chunk_start = chunk_boundaries[i - 1]

            # if our slice is in this chunk
            if (chunk_start <= rstart < chunk_stop) and (rstart > stop):
                d[i] = slice(
                    rstart - chunk_stop,
                    max(chunk_start - chunk_stop - 1, stop - chunk_stop),
                    step,
                )

                # compute the next running start point,
                offset = (rstart - (chunk_start - 1)) % step
                rstart = chunk_start + offset - 1

    # replace 0:20:1 with : if appropriate
    for k, v in d.items():
        if v == slice(0, lengths[k], 1):
            d[k] = slice(None, None, None)

    if not d:  # special case x[:0]
        d[0] = slice(0, 0, 1)

    return d",_120.py,108,"istart = bisect.bisect_right(chunk_boundaries, start)
istop = bisect.bisect_left(chunk_boundaries, stop)","istart , istop  = bisect.bisect_right(chunk_boundaries, start), bisect.bisect_left(chunk_boundaries, stop)"
https://github.com/dask/dask/tree/master/dask/array/slicing.py,"def _slice_1d(dim_shape, lengths, index):
    """"""Returns a dict of {blocknum: slice}

    This function figures out where each slice should start in each
    block for a single dimension. If the slice won't return any elements
    in the block, that block will not be in the output.

    Parameters
    ----------

    dim_shape - the number of elements in this dimension.
      This should be a positive, non-zero integer
    blocksize - the number of elements per block in this dimension
      This should be a positive, non-zero integer
    index - a description of the elements in this dimension that we want
      This might be an integer, a slice(), or an Ellipsis

    Returns
    -------

    dictionary where the keys are the integer index of the blocks that
      should be sliced and the values are the slices

    Examples
    --------

    Trivial slicing

    >>> _slice_1d(100, [60, 40], slice(None, None, None))
    {0: slice(None, None, None), 1: slice(None, None, None)}

    100 length array cut into length 20 pieces, slice 0:35

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(0, 35))
    {0: slice(None, None, None), 1: slice(0, 15, 1)}

    Support irregular blocks and various slices

    >>> _slice_1d(100, [20, 10, 10, 10, 25, 25], slice(10, 35))
    {0: slice(10, 20, 1), 1: slice(None, None, None), 2: slice(0, 5, 1)}

    Support step sizes

    >>> _slice_1d(100, [15, 14, 13], slice(10, 41, 3))
    {0: slice(10, 15, 3), 1: slice(1, 14, 3), 2: slice(2, 12, 3)}

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(0, 100, 40))  # step > blocksize
    {0: slice(0, 20, 40), 2: slice(0, 20, 40), 4: slice(0, 20, 40)}

    Also support indexing single elements

    >>> _slice_1d(100, [20, 20, 20, 20, 20], 25)
    {1: 5}

    And negative slicing

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(100, 0, -3)) # doctest: +NORMALIZE_WHITESPACE
    {4: slice(-1, -21, -3),
     3: slice(-2, -21, -3),
     2: slice(-3, -21, -3),
     1: slice(-1, -21, -3),
     0: slice(-2, -20, -3)}

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(100, 12, -3)) # doctest: +NORMALIZE_WHITESPACE
    {4: slice(-1, -21, -3),
     3: slice(-2, -21, -3),
     2: slice(-3, -21, -3),
     1: slice(-1, -21, -3),
     0: slice(-2, -8, -3)}

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(100, -12, -3))
    {4: slice(-1, -12, -3)}
    """"""
    chunk_boundaries = cached_cumsum(lengths)

    if isinstance(index, Integral):
        # use right-side search to be consistent with previous result
        i = bisect.bisect_right(chunk_boundaries, index)
        if i > 0:
            # the very first chunk has no relative shift
            ind = index - chunk_boundaries[i - 1]
        else:
            ind = index
        return {int(i): int(ind)}

    assert isinstance(index, slice)

    if index == colon:
        return {k: colon for k in range(len(lengths))}

    step = index.step or 1
    if step > 0:
        start = index.start or 0
        stop = index.stop if index.stop is not None else dim_shape
    else:
        start = index.start if index.start is not None else dim_shape - 1
        start = dim_shape - 1 if start >= dim_shape else start
        stop = -(dim_shape + 1) if index.stop is None else index.stop

    # posify start and stop
    if start < 0:
        start += dim_shape
    if stop < 0:
        stop += dim_shape

    d = dict()
    if step > 0:
        istart = bisect.bisect_right(chunk_boundaries, start)
        istop = bisect.bisect_left(chunk_boundaries, stop)

        # the bound is not exactly tight; make it tighter?
        istop = min(istop + 1, len(lengths))

        # jump directly to istart
        if istart > 0:
            start = start - chunk_boundaries[istart - 1]
            stop = stop - chunk_boundaries[istart - 1]

        for i in range(istart, istop):
            length = lengths[i]
            if start < length and stop > 0:
                d[i] = slice(start, min(stop, length), step)
                start = (start - length) % step
            else:
                start = start - length
            stop -= length
    else:
        rstart = start  # running start

        istart = bisect.bisect_left(chunk_boundaries, start)
        istop = bisect.bisect_right(chunk_boundaries, stop)

        # the bound is not exactly tight; make it tighter?
        istart = min(istart + 1, len(chunk_boundaries) - 1)
        istop = max(istop - 1, -1)

        for i in range(istart, istop, -1):
            chunk_stop = chunk_boundaries[i]
            # create a chunk start and stop
            if i == 0:
                chunk_start = 0
            else:
                chunk_start = chunk_boundaries[i - 1]

            # if our slice is in this chunk
            if (chunk_start <= rstart < chunk_stop) and (rstart > stop):
                d[i] = slice(
                    rstart - chunk_stop,
                    max(chunk_start - chunk_stop - 1, stop - chunk_stop),
                    step,
                )

                # compute the next running start point,
                offset = (rstart - (chunk_start - 1)) % step
                rstart = chunk_start + offset - 1

    # replace 0:20:1 with : if appropriate
    for k, v in d.items():
        if v == slice(0, lengths[k], 1):
            d[k] = slice(None, None, None)

    if not d:  # special case x[:0]
        d[0] = slice(0, 0, 1)

    return d",_120.py,116,"start = start - chunk_boundaries[istart - 1]
stop = stop - chunk_boundaries[istart - 1]","start , stop  = start - chunk_boundaries[istart - 1], stop - chunk_boundaries[istart - 1]"
https://github.com/dask/dask/tree/master/dask/array/slicing.py,"def _slice_1d(dim_shape, lengths, index):
    """"""Returns a dict of {blocknum: slice}

    This function figures out where each slice should start in each
    block for a single dimension. If the slice won't return any elements
    in the block, that block will not be in the output.

    Parameters
    ----------

    dim_shape - the number of elements in this dimension.
      This should be a positive, non-zero integer
    blocksize - the number of elements per block in this dimension
      This should be a positive, non-zero integer
    index - a description of the elements in this dimension that we want
      This might be an integer, a slice(), or an Ellipsis

    Returns
    -------

    dictionary where the keys are the integer index of the blocks that
      should be sliced and the values are the slices

    Examples
    --------

    Trivial slicing

    >>> _slice_1d(100, [60, 40], slice(None, None, None))
    {0: slice(None, None, None), 1: slice(None, None, None)}

    100 length array cut into length 20 pieces, slice 0:35

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(0, 35))
    {0: slice(None, None, None), 1: slice(0, 15, 1)}

    Support irregular blocks and various slices

    >>> _slice_1d(100, [20, 10, 10, 10, 25, 25], slice(10, 35))
    {0: slice(10, 20, 1), 1: slice(None, None, None), 2: slice(0, 5, 1)}

    Support step sizes

    >>> _slice_1d(100, [15, 14, 13], slice(10, 41, 3))
    {0: slice(10, 15, 3), 1: slice(1, 14, 3), 2: slice(2, 12, 3)}

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(0, 100, 40))  # step > blocksize
    {0: slice(0, 20, 40), 2: slice(0, 20, 40), 4: slice(0, 20, 40)}

    Also support indexing single elements

    >>> _slice_1d(100, [20, 20, 20, 20, 20], 25)
    {1: 5}

    And negative slicing

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(100, 0, -3)) # doctest: +NORMALIZE_WHITESPACE
    {4: slice(-1, -21, -3),
     3: slice(-2, -21, -3),
     2: slice(-3, -21, -3),
     1: slice(-1, -21, -3),
     0: slice(-2, -20, -3)}

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(100, 12, -3)) # doctest: +NORMALIZE_WHITESPACE
    {4: slice(-1, -21, -3),
     3: slice(-2, -21, -3),
     2: slice(-3, -21, -3),
     1: slice(-1, -21, -3),
     0: slice(-2, -8, -3)}

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(100, -12, -3))
    {4: slice(-1, -12, -3)}
    """"""
    chunk_boundaries = cached_cumsum(lengths)

    if isinstance(index, Integral):
        # use right-side search to be consistent with previous result
        i = bisect.bisect_right(chunk_boundaries, index)
        if i > 0:
            # the very first chunk has no relative shift
            ind = index - chunk_boundaries[i - 1]
        else:
            ind = index
        return {int(i): int(ind)}

    assert isinstance(index, slice)

    if index == colon:
        return {k: colon for k in range(len(lengths))}

    step = index.step or 1
    if step > 0:
        start = index.start or 0
        stop = index.stop if index.stop is not None else dim_shape
    else:
        start = index.start if index.start is not None else dim_shape - 1
        start = dim_shape - 1 if start >= dim_shape else start
        stop = -(dim_shape + 1) if index.stop is None else index.stop

    # posify start and stop
    if start < 0:
        start += dim_shape
    if stop < 0:
        stop += dim_shape

    d = dict()
    if step > 0:
        istart = bisect.bisect_right(chunk_boundaries, start)
        istop = bisect.bisect_left(chunk_boundaries, stop)

        # the bound is not exactly tight; make it tighter?
        istop = min(istop + 1, len(lengths))

        # jump directly to istart
        if istart > 0:
            start = start - chunk_boundaries[istart - 1]
            stop = stop - chunk_boundaries[istart - 1]

        for i in range(istart, istop):
            length = lengths[i]
            if start < length and stop > 0:
                d[i] = slice(start, min(stop, length), step)
                start = (start - length) % step
            else:
                start = start - length
            stop -= length
    else:
        rstart = start  # running start

        istart = bisect.bisect_left(chunk_boundaries, start)
        istop = bisect.bisect_right(chunk_boundaries, stop)

        # the bound is not exactly tight; make it tighter?
        istart = min(istart + 1, len(chunk_boundaries) - 1)
        istop = max(istop - 1, -1)

        for i in range(istart, istop, -1):
            chunk_stop = chunk_boundaries[i]
            # create a chunk start and stop
            if i == 0:
                chunk_start = 0
            else:
                chunk_start = chunk_boundaries[i - 1]

            # if our slice is in this chunk
            if (chunk_start <= rstart < chunk_stop) and (rstart > stop):
                d[i] = slice(
                    rstart - chunk_stop,
                    max(chunk_start - chunk_stop - 1, stop - chunk_stop),
                    step,
                )

                # compute the next running start point,
                offset = (rstart - (chunk_start - 1)) % step
                rstart = chunk_start + offset - 1

    # replace 0:20:1 with : if appropriate
    for k, v in d.items():
        if v == slice(0, lengths[k], 1):
            d[k] = slice(None, None, None)

    if not d:  # special case x[:0]
        d[0] = slice(0, 0, 1)

    return d",_120.py,122,"d[i] = slice(start, min(stop, length), step)
start = (start - length) % step","d[i] , start  = slice(start, min(stop, length), step), (start - length) % step"
https://github.com/dask/dask/tree/master/dask/array/slicing.py,"def _slice_1d(dim_shape, lengths, index):
    """"""Returns a dict of {blocknum: slice}

    This function figures out where each slice should start in each
    block for a single dimension. If the slice won't return any elements
    in the block, that block will not be in the output.

    Parameters
    ----------

    dim_shape - the number of elements in this dimension.
      This should be a positive, non-zero integer
    blocksize - the number of elements per block in this dimension
      This should be a positive, non-zero integer
    index - a description of the elements in this dimension that we want
      This might be an integer, a slice(), or an Ellipsis

    Returns
    -------

    dictionary where the keys are the integer index of the blocks that
      should be sliced and the values are the slices

    Examples
    --------

    Trivial slicing

    >>> _slice_1d(100, [60, 40], slice(None, None, None))
    {0: slice(None, None, None), 1: slice(None, None, None)}

    100 length array cut into length 20 pieces, slice 0:35

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(0, 35))
    {0: slice(None, None, None), 1: slice(0, 15, 1)}

    Support irregular blocks and various slices

    >>> _slice_1d(100, [20, 10, 10, 10, 25, 25], slice(10, 35))
    {0: slice(10, 20, 1), 1: slice(None, None, None), 2: slice(0, 5, 1)}

    Support step sizes

    >>> _slice_1d(100, [15, 14, 13], slice(10, 41, 3))
    {0: slice(10, 15, 3), 1: slice(1, 14, 3), 2: slice(2, 12, 3)}

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(0, 100, 40))  # step > blocksize
    {0: slice(0, 20, 40), 2: slice(0, 20, 40), 4: slice(0, 20, 40)}

    Also support indexing single elements

    >>> _slice_1d(100, [20, 20, 20, 20, 20], 25)
    {1: 5}

    And negative slicing

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(100, 0, -3)) # doctest: +NORMALIZE_WHITESPACE
    {4: slice(-1, -21, -3),
     3: slice(-2, -21, -3),
     2: slice(-3, -21, -3),
     1: slice(-1, -21, -3),
     0: slice(-2, -20, -3)}

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(100, 12, -3)) # doctest: +NORMALIZE_WHITESPACE
    {4: slice(-1, -21, -3),
     3: slice(-2, -21, -3),
     2: slice(-3, -21, -3),
     1: slice(-1, -21, -3),
     0: slice(-2, -8, -3)}

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(100, -12, -3))
    {4: slice(-1, -12, -3)}
    """"""
    chunk_boundaries = cached_cumsum(lengths)

    if isinstance(index, Integral):
        # use right-side search to be consistent with previous result
        i = bisect.bisect_right(chunk_boundaries, index)
        if i > 0:
            # the very first chunk has no relative shift
            ind = index - chunk_boundaries[i - 1]
        else:
            ind = index
        return {int(i): int(ind)}

    assert isinstance(index, slice)

    if index == colon:
        return {k: colon for k in range(len(lengths))}

    step = index.step or 1
    if step > 0:
        start = index.start or 0
        stop = index.stop if index.stop is not None else dim_shape
    else:
        start = index.start if index.start is not None else dim_shape - 1
        start = dim_shape - 1 if start >= dim_shape else start
        stop = -(dim_shape + 1) if index.stop is None else index.stop

    # posify start and stop
    if start < 0:
        start += dim_shape
    if stop < 0:
        stop += dim_shape

    d = dict()
    if step > 0:
        istart = bisect.bisect_right(chunk_boundaries, start)
        istop = bisect.bisect_left(chunk_boundaries, stop)

        # the bound is not exactly tight; make it tighter?
        istop = min(istop + 1, len(lengths))

        # jump directly to istart
        if istart > 0:
            start = start - chunk_boundaries[istart - 1]
            stop = stop - chunk_boundaries[istart - 1]

        for i in range(istart, istop):
            length = lengths[i]
            if start < length and stop > 0:
                d[i] = slice(start, min(stop, length), step)
                start = (start - length) % step
            else:
                start = start - length
            stop -= length
    else:
        rstart = start  # running start

        istart = bisect.bisect_left(chunk_boundaries, start)
        istop = bisect.bisect_right(chunk_boundaries, stop)

        # the bound is not exactly tight; make it tighter?
        istart = min(istart + 1, len(chunk_boundaries) - 1)
        istop = max(istop - 1, -1)

        for i in range(istart, istop, -1):
            chunk_stop = chunk_boundaries[i]
            # create a chunk start and stop
            if i == 0:
                chunk_start = 0
            else:
                chunk_start = chunk_boundaries[i - 1]

            # if our slice is in this chunk
            if (chunk_start <= rstart < chunk_stop) and (rstart > stop):
                d[i] = slice(
                    rstart - chunk_stop,
                    max(chunk_start - chunk_stop - 1, stop - chunk_stop),
                    step,
                )

                # compute the next running start point,
                offset = (rstart - (chunk_start - 1)) % step
                rstart = chunk_start + offset - 1

    # replace 0:20:1 with : if appropriate
    for k, v in d.items():
        if v == slice(0, lengths[k], 1):
            d[k] = slice(None, None, None)

    if not d:  # special case x[:0]
        d[0] = slice(0, 0, 1)

    return d",_120.py,147,"d[i] = slice(rstart - chunk_stop, max(chunk_start - chunk_stop - 1, stop - chunk_stop), step)
offset = (rstart - (chunk_start - 1)) % step","d[i] , offset  = slice(rstart - chunk_stop, max(chunk_start - chunk_stop - 1, stop - chunk_stop), step), (rstart - (chunk_start - 1)) % step"
https://github.com/mapio/GraphvizAnim/tree/master/gvanim/animation.py,"def __init__( self, step = None ):
		if step:
			self.V = step.V.copy()
			self.E = step.E.copy()
			self.lV = step.lV.copy()
			self.lE = step.lE.copy()
		else:
			self.V = set()
			self.E = set()
			self.lV = dict()
			self.lE = dict()
		self.hV = dict()
		self.hE = dict()",_121.py,12,"self.hV = dict()
self.hE = dict()","self.hV , self.hE  = dict(), dict()"
https://github.com/mapio/GraphvizAnim/tree/master/gvanim/animation.py,"def __init__( self, step = None ):
		if step:
			self.V = step.V.copy()
			self.E = step.E.copy()
			self.lV = step.lV.copy()
			self.lE = step.lE.copy()
		else:
			self.V = set()
			self.E = set()
			self.lV = dict()
			self.lE = dict()
		self.hV = dict()
		self.hE = dict()",_121.py,3,"self.V = step.V.copy()
self.E = step.E.copy()
self.lV = step.lV.copy()
self.lE = step.lE.copy()","self.V , self.E , self.lV , self.lE  = step.V.copy(), step.E.copy(), step.lV.copy(), step.lE.copy()"
https://github.com/pkkid/python-plexapi/tree/master/plexapi/client.py,"def connect(self, timeout=None):
        """""" Alias of reload as any subsequent requests to this client will be
            made directly to the device even if the object attributes were initially
            populated from a PlexServer.
        """"""
        if not self.key:
            raise Unsupported('Cannot reload an object not built from a URL.')
        self._initpath = self.key
        data = self.query(self.key, timeout=timeout)
        if not data:
            raise NotFound(""Client not found at %s"" % self._baseurl)
        if self._clientIdentifier:
            client = next(
                (
                    x
                    for x in data
                    if x.attrib.get(""machineIdentifier"") == self._clientIdentifier
                ),
                None,
            )
            if client is None:
                raise NotFound(
                    ""Client with identifier %s not found at %s""
                    % (self._clientIdentifier, self._baseurl)
                )
        else:
            client = data[0]
        self._loadData(client)
        return self",_125.py,8,"self._initpath = self.key
data = self.query(self.key, timeout=timeout)","self._initpath, data = self.key, self.query(self.key, timeout=timeout)"
https://github.com/kundajelab/deeplift/tree/master/tests/conversion/sequential/test_conv1d_model_valid_padding.py,"def setUp(self):
        self.inp = (np.random.randn(10*10*51)
                    .reshape(10,10,51)).transpose(0,2,1)
        self.keras_model = keras.models.Sequential()
        self.keras_model.add(keras.layers.InputLayer((51,10)))
        conv_layer = keras.layers.convolutional.Convolution1D(
                        nb_filter=2, filter_length=4, subsample_length=2,
                        activation=""relu"", input_shape=(51,10))
        self.keras_model.add(conv_layer)
        self.keras_model.add(keras.layers.pooling.MaxPooling1D(
                             pool_length=4, stride=2)) 
        self.keras_model.add(keras.layers.pooling.AveragePooling1D(
                             pool_length=4, stride=2)) 
        self.keras_model.add(keras.layers.Flatten())
        self.keras_model.add(keras.layers.Dense(output_dim=1))
        self.keras_model.add(keras.layers.core.Activation(""sigmoid""))
        self.keras_model.compile(loss=""mse"", optimizer=""sgd"")
        self.keras_output_fprop_func = compile_func(
                        [self.keras_model.layers[0].input,
                         K.learning_phase()],
                        self.keras_model.layers[-1].output)

        grad = tf.gradients(tf.reduce_sum(
            self.keras_model.layers[-2].output[:,0]),
            [self.keras_model.layers[0].input])[0]
        self.grad_func = compile_func(
            [self.keras_model.layers[0].input,
             K.learning_phase()], grad) 

        self.saved_file_path = ""conv1model_validpadding.h5""
        if (os.path.isfile(self.saved_file_path)):
            os.remove(self.saved_file_path)
        self.keras_model.save(self.saved_file_path)",_128.py,18,"self.keras_output_fprop_func = compile_func([self.keras_model.layers[0].input, K.learning_phase()], self.keras_model.layers[-1].output)
grad = tf.gradients(tf.reduce_sum(self.keras_model.layers[-2].output[:, 0]), [self.keras_model.layers[0].input])[0]
self.grad_func = compile_func([self.keras_model.layers[0].input, K.learning_phase()], grad)
self.saved_file_path = 'conv1model_validpadding.h5'","self.keras_output_fprop_func, grad, self.grad_func, self.saved_file_path = compile_func([self.keras_model.layers[0].input, K.learning_phase()], self.keras_model.layers[-1].output), tf.gradients(tf.reduce_sum(self.keras_model.layers[-2].output[:, 0]), [self.keras_model.layers[0].input])[0], compile_func([self.keras_model.layers[0].input, K.learning_phase()], grad), 'conv1model_validpadding.h5'"
https://github.com/kundajelab/deeplift/tree/master/tests/conversion/sequential/test_conv1d_model_valid_padding.py,"def setUp(self):
        self.inp = (np.random.randn(10*10*51)
                    .reshape(10,10,51)).transpose(0,2,1)
        self.keras_model = keras.models.Sequential()
        self.keras_model.add(keras.layers.InputLayer((51,10)))
        conv_layer = keras.layers.convolutional.Convolution1D(
                        nb_filter=2, filter_length=4, subsample_length=2,
                        activation=""relu"", input_shape=(51,10))
        self.keras_model.add(conv_layer)
        self.keras_model.add(keras.layers.pooling.MaxPooling1D(
                             pool_length=4, stride=2)) 
        self.keras_model.add(keras.layers.pooling.AveragePooling1D(
                             pool_length=4, stride=2)) 
        self.keras_model.add(keras.layers.Flatten())
        self.keras_model.add(keras.layers.Dense(output_dim=1))
        self.keras_model.add(keras.layers.core.Activation(""sigmoid""))
        self.keras_model.compile(loss=""mse"", optimizer=""sgd"")
        self.keras_output_fprop_func = compile_func(
                        [self.keras_model.layers[0].input,
                         K.learning_phase()],
                        self.keras_model.layers[-1].output)

        grad = tf.gradients(tf.reduce_sum(
            self.keras_model.layers[-2].output[:,0]),
            [self.keras_model.layers[0].input])[0]
        self.grad_func = compile_func(
            [self.keras_model.layers[0].input,
             K.learning_phase()], grad) 

        self.saved_file_path = ""conv1model_validpadding.h5""
        if (os.path.isfile(self.saved_file_path)):
            os.remove(self.saved_file_path)
        self.keras_model.save(self.saved_file_path)",_128.py,2,"self.inp = np.random.randn(10 * 10 * 51).reshape(10, 10, 51).transpose(0, 2, 1)
self.keras_model = keras.models.Sequential()","self.inp , self.keras_model  = np.random.randn(10 * 10 * 51).reshape(10, 10, 51).transpose(0, 2, 1), keras.models.Sequential()"
https://github.com/kundajelab/deeplift/tree/master/tests/conversion/sequential/test_conv1d_model_valid_padding.py,"def setUp(self):
        self.inp = (np.random.randn(10*10*51)
                    .reshape(10,10,51)).transpose(0,2,1)
        self.keras_model = keras.models.Sequential()
        self.keras_model.add(keras.layers.InputLayer((51,10)))
        conv_layer = keras.layers.convolutional.Convolution1D(
                        nb_filter=2, filter_length=4, subsample_length=2,
                        activation=""relu"", input_shape=(51,10))
        self.keras_model.add(conv_layer)
        self.keras_model.add(keras.layers.pooling.MaxPooling1D(
                             pool_length=4, stride=2)) 
        self.keras_model.add(keras.layers.pooling.AveragePooling1D(
                             pool_length=4, stride=2)) 
        self.keras_model.add(keras.layers.Flatten())
        self.keras_model.add(keras.layers.Dense(output_dim=1))
        self.keras_model.add(keras.layers.core.Activation(""sigmoid""))
        self.keras_model.compile(loss=""mse"", optimizer=""sgd"")
        self.keras_output_fprop_func = compile_func(
                        [self.keras_model.layers[0].input,
                         K.learning_phase()],
                        self.keras_model.layers[-1].output)

        grad = tf.gradients(tf.reduce_sum(
            self.keras_model.layers[-2].output[:,0]),
            [self.keras_model.layers[0].input])[0]
        self.grad_func = compile_func(
            [self.keras_model.layers[0].input,
             K.learning_phase()], grad) 

        self.saved_file_path = ""conv1model_validpadding.h5""
        if (os.path.isfile(self.saved_file_path)):
            os.remove(self.saved_file_path)
        self.keras_model.save(self.saved_file_path)",_128.py,26,"self.grad_func = compile_func([self.keras_model.layers[0].input, K.learning_phase()], grad)
self.saved_file_path = 'conv1model_validpadding.h5'","self.grad_func , self.saved_file_path  = compile_func([self.keras_model.layers[0].input, K.learning_phase()], grad), 'conv1model_validpadding.h5'"
https://github.com/sberbank-ai-lab/LightAutoML/tree/master/lightautoml/transformers/text.py,"def transform(self, dataset: NumpyOrPandas) -> PandasDataset:
        """"""Transform text dataset to one text column.

        Args:
            dataset: Pandas or Numpy dataset of text features.

        Returns:
            Pandas dataset with one text column.

        """"""
        # checks here
        super().transform(dataset)
        # convert to accepted dtype and get attributes
        dataset = dataset.to_pandas()
        df = dataset.data

        # transform
        roles = TextRole()
        new_df = pd.DataFrame(
            df[df.columns].fillna("""").astype(str).apply(f""{self.special_token}"".join, axis=1),
            columns=[self._fname_prefix + ""__"" + ""__"".join(df.columns)],
        )
        # create resulted
        output = dataset.empty().to_pandas()
        output.set_data(new_df, None, {feat: roles for feat in new_df.columns})
        return output",_13.py,18,"roles = TextRole()
new_df = pd.DataFrame(df[df.columns].fillna('').astype(str).apply(f'{self.special_token}'.join, axis=1), columns=[self._fname_prefix + '__' + '__'.join(df.columns)])
output = dataset.empty().to_pandas()","roles , new_df , output  = TextRole(), pd.DataFrame(df[df.columns].fillna('').astype(str).apply(f'{self.special_token}'.join, axis=1), columns=[self._fname_prefix + '__' + '__'.join(df.columns)]), dataset.empty().to_pandas()"
https://github.com/dingjiansw101/AerialDetection/tree/master/mmdet/models/detectors/base.py,"def show_result(self,
                    data,
                    result,
                    img_norm_cfg,
                    dataset=None,
                    score_thr=0.3):
        if isinstance(result, tuple):
            bbox_result, segm_result = result
        else:
            bbox_result, segm_result = result, None

        img_tensor = data['img'][0]
        img_metas = data['img_meta'][0].data[0]
        imgs = tensor2imgs(img_tensor, **img_norm_cfg)
        assert len(imgs) == len(img_metas)

        if dataset is None:
            class_names = self.CLASSES
        elif isinstance(dataset, str):
            class_names = get_classes(dataset)
        elif isinstance(dataset, (list, tuple)):
            class_names = dataset
        else:
            raise TypeError(
                'dataset must be a valid dataset name or a sequence'
                ' of class names, not {}'.format(type(dataset)))

        for img, img_meta in zip(imgs, img_metas):
            h, w, _ = img_meta['img_shape']
            img_show = img[:h, :w, :]

            bboxes = np.vstack(bbox_result)
            # draw segmentation masks
            if segm_result is not None:
                segms = mmcv.concat_list(segm_result)
                inds = np.where(bboxes[:, -1] > score_thr)[0]
                for i in inds:
                    color_mask = np.random.randint(
                        0, 256, (1, 3), dtype=np.uint8)
                    mask = maskUtils.decode(segms[i]).astype(np.bool)
                    img_show[mask] = img_show[mask] * 0.5 + color_mask * 0.5
            # draw bounding boxes
            labels = [
                np.full(bbox.shape[0], i, dtype=np.int32)
                for i, bbox in enumerate(bbox_result)
            ]
            labels = np.concatenate(labels)
            mmcv.imshow_det_bboxes(
                img_show,
                bboxes,
                labels,
                class_names=class_names,
                score_thr=score_thr)",_131.py,13,"img_metas = data['img_meta'][0].data[0]
imgs = tensor2imgs(img_tensor, **img_norm_cfg)","img_metas , imgs  = data['img_meta'][0].data[0], tensor2imgs(img_tensor, **img_norm_cfg)"
https://github.com/dingjiansw101/AerialDetection/tree/master/mmdet/models/detectors/base.py,"def show_result(self,
                    data,
                    result,
                    img_norm_cfg,
                    dataset=None,
                    score_thr=0.3):
        if isinstance(result, tuple):
            bbox_result, segm_result = result
        else:
            bbox_result, segm_result = result, None

        img_tensor = data['img'][0]
        img_metas = data['img_meta'][0].data[0]
        imgs = tensor2imgs(img_tensor, **img_norm_cfg)
        assert len(imgs) == len(img_metas)

        if dataset is None:
            class_names = self.CLASSES
        elif isinstance(dataset, str):
            class_names = get_classes(dataset)
        elif isinstance(dataset, (list, tuple)):
            class_names = dataset
        else:
            raise TypeError(
                'dataset must be a valid dataset name or a sequence'
                ' of class names, not {}'.format(type(dataset)))

        for img, img_meta in zip(imgs, img_metas):
            h, w, _ = img_meta['img_shape']
            img_show = img[:h, :w, :]

            bboxes = np.vstack(bbox_result)
            # draw segmentation masks
            if segm_result is not None:
                segms = mmcv.concat_list(segm_result)
                inds = np.where(bboxes[:, -1] > score_thr)[0]
                for i in inds:
                    color_mask = np.random.randint(
                        0, 256, (1, 3), dtype=np.uint8)
                    mask = maskUtils.decode(segms[i]).astype(np.bool)
                    img_show[mask] = img_show[mask] * 0.5 + color_mask * 0.5
            # draw bounding boxes
            labels = [
                np.full(bbox.shape[0], i, dtype=np.int32)
                for i, bbox in enumerate(bbox_result)
            ]
            labels = np.concatenate(labels)
            mmcv.imshow_det_bboxes(
                img_show,
                bboxes,
                labels,
                class_names=class_names,
                score_thr=score_thr)",_131.py,30,"img_show = img[:h, :w, :]
bboxes = np.vstack(bbox_result)","img_show , bboxes  = img[:h, :w, :], np.vstack(bbox_result)"
https://github.com/dingjiansw101/AerialDetection/tree/master/mmdet/models/detectors/base.py,"def show_result(self,
                    data,
                    result,
                    img_norm_cfg,
                    dataset=None,
                    score_thr=0.3):
        if isinstance(result, tuple):
            bbox_result, segm_result = result
        else:
            bbox_result, segm_result = result, None

        img_tensor = data['img'][0]
        img_metas = data['img_meta'][0].data[0]
        imgs = tensor2imgs(img_tensor, **img_norm_cfg)
        assert len(imgs) == len(img_metas)

        if dataset is None:
            class_names = self.CLASSES
        elif isinstance(dataset, str):
            class_names = get_classes(dataset)
        elif isinstance(dataset, (list, tuple)):
            class_names = dataset
        else:
            raise TypeError(
                'dataset must be a valid dataset name or a sequence'
                ' of class names, not {}'.format(type(dataset)))

        for img, img_meta in zip(imgs, img_metas):
            h, w, _ = img_meta['img_shape']
            img_show = img[:h, :w, :]

            bboxes = np.vstack(bbox_result)
            # draw segmentation masks
            if segm_result is not None:
                segms = mmcv.concat_list(segm_result)
                inds = np.where(bboxes[:, -1] > score_thr)[0]
                for i in inds:
                    color_mask = np.random.randint(
                        0, 256, (1, 3), dtype=np.uint8)
                    mask = maskUtils.decode(segms[i]).astype(np.bool)
                    img_show[mask] = img_show[mask] * 0.5 + color_mask * 0.5
            # draw bounding boxes
            labels = [
                np.full(bbox.shape[0], i, dtype=np.int32)
                for i, bbox in enumerate(bbox_result)
            ]
            labels = np.concatenate(labels)
            mmcv.imshow_det_bboxes(
                img_show,
                bboxes,
                labels,
                class_names=class_names,
                score_thr=score_thr)",_131.py,35,"segms = mmcv.concat_list(segm_result)
inds = np.where(bboxes[:, -1] > score_thr)[0]","segms , inds  = mmcv.concat_list(segm_result), np.where(bboxes[:, -1] > score_thr)[0]"
https://github.com/dingjiansw101/AerialDetection/tree/master/mmdet/models/detectors/base.py,"def show_result(self,
                    data,
                    result,
                    img_norm_cfg,
                    dataset=None,
                    score_thr=0.3):
        if isinstance(result, tuple):
            bbox_result, segm_result = result
        else:
            bbox_result, segm_result = result, None

        img_tensor = data['img'][0]
        img_metas = data['img_meta'][0].data[0]
        imgs = tensor2imgs(img_tensor, **img_norm_cfg)
        assert len(imgs) == len(img_metas)

        if dataset is None:
            class_names = self.CLASSES
        elif isinstance(dataset, str):
            class_names = get_classes(dataset)
        elif isinstance(dataset, (list, tuple)):
            class_names = dataset
        else:
            raise TypeError(
                'dataset must be a valid dataset name or a sequence'
                ' of class names, not {}'.format(type(dataset)))

        for img, img_meta in zip(imgs, img_metas):
            h, w, _ = img_meta['img_shape']
            img_show = img[:h, :w, :]

            bboxes = np.vstack(bbox_result)
            # draw segmentation masks
            if segm_result is not None:
                segms = mmcv.concat_list(segm_result)
                inds = np.where(bboxes[:, -1] > score_thr)[0]
                for i in inds:
                    color_mask = np.random.randint(
                        0, 256, (1, 3), dtype=np.uint8)
                    mask = maskUtils.decode(segms[i]).astype(np.bool)
                    img_show[mask] = img_show[mask] * 0.5 + color_mask * 0.5
            # draw bounding boxes
            labels = [
                np.full(bbox.shape[0], i, dtype=np.int32)
                for i, bbox in enumerate(bbox_result)
            ]
            labels = np.concatenate(labels)
            mmcv.imshow_det_bboxes(
                img_show,
                bboxes,
                labels,
                class_names=class_names,
                score_thr=score_thr)",_131.py,38,"color_mask = np.random.randint(0, 256, (1, 3), dtype=np.uint8)
mask = maskUtils.decode(segms[i]).astype(np.bool)","color_mask , mask  = np.random.randint(0, 256, (1, 3), dtype=np.uint8), maskUtils.decode(segms[i]).astype(np.bool)"
https://github.com/dandelin/ViLT/tree/master/vilt/utils/write_conceptual_caption.py,"def path2rest(path, iid2captions):
    split, _, name = path.split(""/"")[-3:]
    split = split.split(""_"")[-1]
    iid = name

    with open(path, ""rb"") as fp:
        binary = fp.read()

    captions = iid2captions[iid]

    return [
        binary,
        captions,
        iid,
        split,
    ]",_134.py,3,"split = split.split('_')[-1]
iid = name","split , iid  = split.split('_')[-1], name"
https://github.com/JosephKJ/OWOD/tree/master/projects/DensePose/tests/test_frame_selector.py,"def test_frame_selector_random_k_2(self):
        _SEED = 43
        _K = 10
        random.seed(_SEED)
        selector = RandomKFramesSelector(_K)
        frame_tss = list(range(0, 6, 2))
        _SELECTED_GT = [0, 2, 4]
        selected = selector(frame_tss)
        self.assertEqual(_SELECTED_GT, selected)",_139.py,2,"_SEED = 43
_K = 10","_SEED , _K  = 43, 10"
https://github.com/JosephKJ/OWOD/tree/master/projects/DensePose/tests/test_frame_selector.py,"def test_frame_selector_random_k_2(self):
        _SEED = 43
        _K = 10
        random.seed(_SEED)
        selector = RandomKFramesSelector(_K)
        frame_tss = list(range(0, 6, 2))
        _SELECTED_GT = [0, 2, 4]
        selected = selector(frame_tss)
        self.assertEqual(_SELECTED_GT, selected)",_139.py,5,"selector = RandomKFramesSelector(_K)
frame_tss = list(range(0, 6, 2))","selector , frame_tss  = RandomKFramesSelector(_K), list(range(0, 6, 2))"
https://github.com/JosephKJ/OWOD/tree/master/projects/DensePose/tests/test_frame_selector.py,"def test_frame_selector_random_k_2(self):
        _SEED = 43
        _K = 10
        random.seed(_SEED)
        selector = RandomKFramesSelector(_K)
        frame_tss = list(range(0, 6, 2))
        _SELECTED_GT = [0, 2, 4]
        selected = selector(frame_tss)
        self.assertEqual(_SELECTED_GT, selected)",_139.py,7,"_SELECTED_GT = [0, 2, 4]
selected = selector(frame_tss)","_SELECTED_GT , selected  = [0, 2, 4], selector(frame_tss)"
https://github.com/mozilla-services/socorro/tree/master/webapp-django/crashstats/crashstats/templatetags/jinja_helpers.py,"def url(viewname, *args, **kwargs):
    """"""Makes it possible to construct URLs from templates.

    Because this function is used by taking user input, (e.g. query
    string values), we have to sanitize the values.
    """"""

    def clean_argument(s):
        if isinstance(s, str):
            # First remove all proper control characters like '\n',
            # '\r' or '\t'.
            s = """".join(c for c in s if ord(c) >= 32)
            # Then, if any '\' left (it might have started as '\\nn')
            # remove those too.
            while ""\\"" in s:
                s = s.replace(""\\"", """")
            return s
        return s

    args = [clean_argument(x) for x in args]
    kwargs = {x: clean_argument(y) for x, y in kwargs.items()}

    return reverse(viewname, args=args, kwargs=kwargs)",_141.py,20,"args = [clean_argument(x) for x in args]
kwargs = {x: clean_argument(y) for (x, y) in kwargs.items()}","args , kwargs  = [clean_argument(x) for x in args], {x: clean_argument(y) for (x, y) in kwargs.items()}"
https://github.com/google/capirca/tree/master/capirca/lib/junipermsmpc.py,"def __str__(self):
    # Verify platform specific terms. Skip whole term if platform does not
    # match.
    if self.term.platform:
      if self._PLATFORM not in self.term.platform:
        return ''
    if self.term.platform_exclude:
      if self._PLATFORM in self.term.platform_exclude:
        return ''

    if self.enable_dsmo:
      raise NotImplementedError('enable_dsmo not implemented for msmpc')

    ret_str = juniper.Config(indent=self._DEFAULT_INDENT)

    # COMMENTS
    # this deals just fine with multi line comments, but we could probably
    # output them a little cleaner; do things like make sure the
    # len(output) < 80, etc. Note, if 'noverbose' is set for the filter, skip
    # all comment processing.
    if not self.noverbose:
      if self.term.owner:
        self.term.comment.append('Owner: %s' % self.term.owner)
      if self.term.comment:
        ret_str.Append('/*')
        for comment in self.term.comment:
          for line in comment.split('\n'):
            ret_str.Append('** ' + line)
        ret_str.Append('*/')

    # Term verbatim output - this will skip over normal term creation
    # code.  Warning generated from policy.py if appropriate.
    if self.term.verbatim:
      for next_term in self.term.verbatim:
        if next_term[0] == self._PLATFORM:
          ret_str.Append(str(next_term[1]), verbatim=True)
      return str(ret_str)

    # Determine whether there are any match conditions for the term.
    has_match_criteria = (
        self.term.address or self.term.dscp_except or self.term.dscp_match or
        self.term.destination_address or self.term.destination_port or
        self.term.destination_prefix or self.term.destination_prefix_except or
        self.term.encapsulate or self.term.ether_type or
        self.term.flexible_match_range or self.term.forwarding_class or
        self.term.forwarding_class_except or self.term.fragment_offset or
        self.term.hop_limit or self.term.next_ip or self.term.port or
        self.term.precedence or self.term.protocol or
        self.term.protocol_except or self.term.source_address or
        self.term.source_port or self.term.source_prefix or
        self.term.source_prefix_except or self.term.traffic_type or
        self.term.ttl)

    suffixes = []
    duplicate_term = False
    has_icmp = 'icmp' in self.term.protocol
    has_icmpv6 = 'icmpv6' in self.term.protocol
    has_v4_ip = self.term.GetAddressOfVersion(
        'source_address',
        self.AF_MAP.get('inet')) or self.term.GetAddressOfVersion(
            'source_address_exclude',
            self.AF_MAP.get('inet')) or self.term.GetAddressOfVersion(
                'destination_address',
                self.AF_MAP.get('inet')) or self.term.GetAddressOfVersion(
                    'destination_address_exclude', self.AF_MAP.get('inet'))
    has_v6_ip = self.term.GetAddressOfVersion(
        'source_address',
        self.AF_MAP.get('inet6')) or self.term.GetAddressOfVersion(
            'source_address_exclude',
            self.AF_MAP.get('inet6')) or self.term.GetAddressOfVersion(
                'destination_address',
                self.AF_MAP.get('inet6')) or self.term.GetAddressOfVersion(
                    'destination_address_exclude', self.AF_MAP.get('inet6'))

    if self.term_type == 'mixed':
      if not (has_v4_ip or has_v6_ip):
        suffixes = ['inet']
      elif not has_v6_ip:
        suffixes = ['inet']
      elif not has_v4_ip:
        suffixes = ['inet6']
      else:
        suffixes = ['inet', 'inet6']
        duplicate_term = True
    if not suffixes and self.term_type in ['inet', 'inet6']:
      suffixes = [self.term_type]

    for suffix in suffixes:
      if self.term_type == 'mixed' and (not (has_icmp and has_icmpv6)) and (
          has_v4_ip and has_v6_ip):
        if (has_icmp and suffix != 'inet') or (has_icmpv6 and
                                               suffix != 'inet6'):
          continue
      source_address = self.term.GetAddressOfVersion('source_address',
                                                     self.AF_MAP.get(suffix))
      source_address_exclude = self.term.GetAddressOfVersion(
          'source_address_exclude', self.AF_MAP.get(suffix))
      source_address, source_address_exclude = self._MinimizePrefixes(
          source_address, source_address_exclude)
      destination_address = self.term.GetAddressOfVersion(
          'destination_address', self.AF_MAP.get(suffix))
      destination_address_exclude = self.term.GetAddressOfVersion(
          'destination_address_exclude', self.AF_MAP.get(suffix))
      destination_address, destination_address_exclude = self._MinimizePrefixes(
          destination_address, destination_address_exclude)
      if ((not source_address) and self.term.GetAddressOfVersion(
          'source_address', self.AF_MAP.get('mixed')) and
          not source_address_exclude) or (
              (not destination_address) and self.term.GetAddressOfVersion(
                  'destination_address', self.AF_MAP.get('mixed')) and
              not destination_address_exclude):
        continue
      if ((has_icmpv6 and not has_icmp and suffix == 'inet') or
          (has_icmp and not has_icmpv6 and
           suffix == 'inet6')) and self.term_type != 'mixed':
        logging.debug(
            self.NO_AF_LOG_PROTO.substitute(
                term=self.term.name,
                proto=', '.join(self.term.protocol),
                af=suffix))
        return ''

      # NAME
      # if the term is inactive we have to set the prefix
      if self.term.inactive:
        term_prefix = 'inactive:'
      else:
        term_prefix = ''

      ret_str.Append(
          '%s term %s%s {' %
          (term_prefix, self.term.name, '-' + suffix if duplicate_term else ''))

      # We only need a ""from {"" clause if there are any conditions to match.
      if has_match_criteria:
        ret_str.Append('from {')
        # SOURCE ADDRESS
        if source_address or source_address_exclude:
          ret_str.Append('source-address {')
          if source_address:
            for saddr in source_address:
              for comment in self._Comment(saddr):
                ret_str.Append('%s' % comment)
              if saddr.version == 6 and 0 < saddr.prefixlen < 16:
                for saddr2 in saddr.subnets(new_prefix=16):
                  ret_str.Append('%s;' % saddr2)
              else:
                if saddr == nacaddr.IPv6('0::0/0'):
                  saddr = 'any-ipv6'
                elif saddr == nacaddr.IPv4('0.0.0.0/0'):
                  saddr = 'any-ipv4'
                ret_str.Append('%s;' % saddr)

          # SOURCE ADDRESS EXCLUDE
          if source_address_exclude:
            for ex in source_address_exclude:
              for comment in self._Comment(ex):
                ret_str.Append('%s' % comment)
              if ex.version == 6 and 0 < ex.prefixlen < 16:
                for ex2 in ex.subnets(new_prefix=16):
                  ret_str.Append('%s except;' % ex2)
              else:
                if ex == nacaddr.IPv6('0::0/0'):
                  ex = 'any-ipv6'
                elif ex == nacaddr.IPv4('0.0.0.0/0'):
                  ex = 'any-ipv4'
                ret_str.Append('%s except;' % ex)
          ret_str.Append('}')  # source-address {...}

        # DESTINATION ADDRESS
        if destination_address or destination_address_exclude:
          ret_str.Append('destination-address {')
          if destination_address:
            for daddr in destination_address:
              for comment in self._Comment(daddr):
                ret_str.Append('%s' % comment)
              if daddr.version == 6 and 0 < daddr.prefixlen < 16:
                for daddr2 in daddr.subnets(new_prefix=16):
                  ret_str.Append('%s;' % daddr2)
              else:
                if daddr == nacaddr.IPv6('0::0/0'):
                  daddr = 'any-ipv6'
                elif daddr == nacaddr.IPv4('0.0.0.0/0'):
                  daddr = 'any-ipv4'
                ret_str.Append('%s;' % daddr)

          # DESTINATION ADDRESS EXCLUDE
          if destination_address_exclude:
            for ex in destination_address_exclude:
              for comment in self._Comment(ex):
                ret_str.Append('%s' % comment)
              if ex.version == 6 and 0 < ex.prefixlen < 16:
                for ex2 in ex.subnets(new_prefix=16):
                  ret_str.Append('%s except;' % ex2)
              else:
                if ex == nacaddr.IPv6('0::0/0'):
                  ex = 'any-ipv6'
                elif ex == nacaddr.IPv4('0.0.0.0/0'):
                  ex = 'any-ipv4'
                ret_str.Append('%s except;' % ex)
          ret_str.Append('}')  # destination-address {...}

        # source prefix <except> list
        if self.term.source_prefix or self.term.source_prefix_except:
          for pfx in self.term.source_prefix:
            ret_str.Append('source-prefix-list ' + pfx + ';')
          for epfx in self.term.source_prefix_except:
            ret_str.Append('source-prefix-list ' + epfx + ' except;')

        # destination prefix <except> list
        if self.term.destination_prefix or self.term.destination_prefix_except:
          for pfx in self.term.destination_prefix:
            ret_str.Append('destination-prefix-list ' + pfx + ';')
          for epfx in self.term.destination_prefix_except:
            ret_str.Append('destination-prefix-list ' + epfx + ' except;')

        # APPLICATION
        if (self.term.source_port or self.term.destination_port or
            self.term.icmp_type or self.term.protocol):
          if hasattr(self.term, 'replacement_application_name'):
            ret_str.Append('application-sets ' +
                           self.term.replacement_application_name + '-app;')
          else:
            ret_str.Append('application-sets ' +
                           self.filter_name[:((MAX_IDENTIFIER_LEN) // 2)] +
                           self.term.name[-((MAX_IDENTIFIER_LEN) // 2):] +
                           '-app;')
        ret_str.Append('}')  # from {...}

      ret_str.Append('then {')
      # ACTION
      for action in self.term.action:
        ret_str.Append(self._ACTIONS.get(str(action)) + ';')
      if self.term.logging and 'disable' not in [
          x.value for x in self.term.logging
      ]:
        ret_str.Append('syslog;')
      ret_str.Append('}')  # then {...}
      ret_str.Append('}')  # term {...}
    return str(ret_str)",_147.py,40,"has_match_criteria = self.term.address or self.term.dscp_except or self.term.dscp_match or self.term.destination_address or self.term.destination_port or self.term.destination_prefix or self.term.destination_prefix_except or self.term.encapsulate or self.term.ether_type or self.term.flexible_match_range or self.term.forwarding_class or self.term.forwarding_class_except or self.term.fragment_offset or self.term.hop_limit or self.term.next_ip or self.term.port or self.term.precedence or self.term.protocol or self.term.protocol_except or self.term.source_address or self.term.source_port or self.term.source_prefix or self.term.source_prefix_except or self.term.traffic_type or self.term.ttl
suffixes = []
duplicate_term = False
has_icmp = 'icmp' in self.term.protocol
has_icmpv6 = 'icmpv6' in self.term.protocol
has_v4_ip = self.term.GetAddressOfVersion('source_address', self.AF_MAP.get('inet')) or self.term.GetAddressOfVersion('source_address_exclude', self.AF_MAP.get('inet')) or self.term.GetAddressOfVersion('destination_address', self.AF_MAP.get('inet')) or self.term.GetAddressOfVersion('destination_address_exclude', self.AF_MAP.get('inet'))
has_v6_ip = self.term.GetAddressOfVersion('source_address', self.AF_MAP.get('inet6')) or self.term.GetAddressOfVersion('source_address_exclude', self.AF_MAP.get('inet6')) or self.term.GetAddressOfVersion('destination_address', self.AF_MAP.get('inet6')) or self.term.GetAddressOfVersion('destination_address_exclude', self.AF_MAP.get('inet6'))","has_match_criteria , suffixes , duplicate_term , has_icmp , has_icmpv6 , has_v4_ip , has_v6_ip  = self.term.address or self.term.dscp_except or self.term.dscp_match or self.term.destination_address or self.term.destination_port or self.term.destination_prefix or self.term.destination_prefix_except or self.term.encapsulate or self.term.ether_type or self.term.flexible_match_range or self.term.forwarding_class or self.term.forwarding_class_except or self.term.fragment_offset or self.term.hop_limit or self.term.next_ip or self.term.port or self.term.precedence or self.term.protocol or self.term.protocol_except or self.term.source_address or self.term.source_port or self.term.source_prefix or self.term.source_prefix_except or self.term.traffic_type or self.term.ttl, [], False, 'icmp' in self.term.protocol, 'icmpv6' in self.term.protocol, self.term.GetAddressOfVersion('source_address', self.AF_MAP.get('inet')) or self.term.GetAddressOfVersion('source_address_exclude', self.AF_MAP.get('inet')) or self.term.GetAddressOfVersion('destination_address', self.AF_MAP.get('inet')) or self.term.GetAddressOfVersion('destination_address_exclude', self.AF_MAP.get('inet')), self.term.GetAddressOfVersion('source_address', self.AF_MAP.get('inet6')) or self.term.GetAddressOfVersion('source_address_exclude', self.AF_MAP.get('inet6')) or self.term.GetAddressOfVersion('destination_address', self.AF_MAP.get('inet6')) or self.term.GetAddressOfVersion('destination_address_exclude', self.AF_MAP.get('inet6'))"
https://github.com/google/capirca/tree/master/capirca/lib/junipermsmpc.py,"def __str__(self):
    # Verify platform specific terms. Skip whole term if platform does not
    # match.
    if self.term.platform:
      if self._PLATFORM not in self.term.platform:
        return ''
    if self.term.platform_exclude:
      if self._PLATFORM in self.term.platform_exclude:
        return ''

    if self.enable_dsmo:
      raise NotImplementedError('enable_dsmo not implemented for msmpc')

    ret_str = juniper.Config(indent=self._DEFAULT_INDENT)

    # COMMENTS
    # this deals just fine with multi line comments, but we could probably
    # output them a little cleaner; do things like make sure the
    # len(output) < 80, etc. Note, if 'noverbose' is set for the filter, skip
    # all comment processing.
    if not self.noverbose:
      if self.term.owner:
        self.term.comment.append('Owner: %s' % self.term.owner)
      if self.term.comment:
        ret_str.Append('/*')
        for comment in self.term.comment:
          for line in comment.split('\n'):
            ret_str.Append('** ' + line)
        ret_str.Append('*/')

    # Term verbatim output - this will skip over normal term creation
    # code.  Warning generated from policy.py if appropriate.
    if self.term.verbatim:
      for next_term in self.term.verbatim:
        if next_term[0] == self._PLATFORM:
          ret_str.Append(str(next_term[1]), verbatim=True)
      return str(ret_str)

    # Determine whether there are any match conditions for the term.
    has_match_criteria = (
        self.term.address or self.term.dscp_except or self.term.dscp_match or
        self.term.destination_address or self.term.destination_port or
        self.term.destination_prefix or self.term.destination_prefix_except or
        self.term.encapsulate or self.term.ether_type or
        self.term.flexible_match_range or self.term.forwarding_class or
        self.term.forwarding_class_except or self.term.fragment_offset or
        self.term.hop_limit or self.term.next_ip or self.term.port or
        self.term.precedence or self.term.protocol or
        self.term.protocol_except or self.term.source_address or
        self.term.source_port or self.term.source_prefix or
        self.term.source_prefix_except or self.term.traffic_type or
        self.term.ttl)

    suffixes = []
    duplicate_term = False
    has_icmp = 'icmp' in self.term.protocol
    has_icmpv6 = 'icmpv6' in self.term.protocol
    has_v4_ip = self.term.GetAddressOfVersion(
        'source_address',
        self.AF_MAP.get('inet')) or self.term.GetAddressOfVersion(
            'source_address_exclude',
            self.AF_MAP.get('inet')) or self.term.GetAddressOfVersion(
                'destination_address',
                self.AF_MAP.get('inet')) or self.term.GetAddressOfVersion(
                    'destination_address_exclude', self.AF_MAP.get('inet'))
    has_v6_ip = self.term.GetAddressOfVersion(
        'source_address',
        self.AF_MAP.get('inet6')) or self.term.GetAddressOfVersion(
            'source_address_exclude',
            self.AF_MAP.get('inet6')) or self.term.GetAddressOfVersion(
                'destination_address',
                self.AF_MAP.get('inet6')) or self.term.GetAddressOfVersion(
                    'destination_address_exclude', self.AF_MAP.get('inet6'))

    if self.term_type == 'mixed':
      if not (has_v4_ip or has_v6_ip):
        suffixes = ['inet']
      elif not has_v6_ip:
        suffixes = ['inet']
      elif not has_v4_ip:
        suffixes = ['inet6']
      else:
        suffixes = ['inet', 'inet6']
        duplicate_term = True
    if not suffixes and self.term_type in ['inet', 'inet6']:
      suffixes = [self.term_type]

    for suffix in suffixes:
      if self.term_type == 'mixed' and (not (has_icmp and has_icmpv6)) and (
          has_v4_ip and has_v6_ip):
        if (has_icmp and suffix != 'inet') or (has_icmpv6 and
                                               suffix != 'inet6'):
          continue
      source_address = self.term.GetAddressOfVersion('source_address',
                                                     self.AF_MAP.get(suffix))
      source_address_exclude = self.term.GetAddressOfVersion(
          'source_address_exclude', self.AF_MAP.get(suffix))
      source_address, source_address_exclude = self._MinimizePrefixes(
          source_address, source_address_exclude)
      destination_address = self.term.GetAddressOfVersion(
          'destination_address', self.AF_MAP.get(suffix))
      destination_address_exclude = self.term.GetAddressOfVersion(
          'destination_address_exclude', self.AF_MAP.get(suffix))
      destination_address, destination_address_exclude = self._MinimizePrefixes(
          destination_address, destination_address_exclude)
      if ((not source_address) and self.term.GetAddressOfVersion(
          'source_address', self.AF_MAP.get('mixed')) and
          not source_address_exclude) or (
              (not destination_address) and self.term.GetAddressOfVersion(
                  'destination_address', self.AF_MAP.get('mixed')) and
              not destination_address_exclude):
        continue
      if ((has_icmpv6 and not has_icmp and suffix == 'inet') or
          (has_icmp and not has_icmpv6 and
           suffix == 'inet6')) and self.term_type != 'mixed':
        logging.debug(
            self.NO_AF_LOG_PROTO.substitute(
                term=self.term.name,
                proto=', '.join(self.term.protocol),
                af=suffix))
        return ''

      # NAME
      # if the term is inactive we have to set the prefix
      if self.term.inactive:
        term_prefix = 'inactive:'
      else:
        term_prefix = ''

      ret_str.Append(
          '%s term %s%s {' %
          (term_prefix, self.term.name, '-' + suffix if duplicate_term else ''))

      # We only need a ""from {"" clause if there are any conditions to match.
      if has_match_criteria:
        ret_str.Append('from {')
        # SOURCE ADDRESS
        if source_address or source_address_exclude:
          ret_str.Append('source-address {')
          if source_address:
            for saddr in source_address:
              for comment in self._Comment(saddr):
                ret_str.Append('%s' % comment)
              if saddr.version == 6 and 0 < saddr.prefixlen < 16:
                for saddr2 in saddr.subnets(new_prefix=16):
                  ret_str.Append('%s;' % saddr2)
              else:
                if saddr == nacaddr.IPv6('0::0/0'):
                  saddr = 'any-ipv6'
                elif saddr == nacaddr.IPv4('0.0.0.0/0'):
                  saddr = 'any-ipv4'
                ret_str.Append('%s;' % saddr)

          # SOURCE ADDRESS EXCLUDE
          if source_address_exclude:
            for ex in source_address_exclude:
              for comment in self._Comment(ex):
                ret_str.Append('%s' % comment)
              if ex.version == 6 and 0 < ex.prefixlen < 16:
                for ex2 in ex.subnets(new_prefix=16):
                  ret_str.Append('%s except;' % ex2)
              else:
                if ex == nacaddr.IPv6('0::0/0'):
                  ex = 'any-ipv6'
                elif ex == nacaddr.IPv4('0.0.0.0/0'):
                  ex = 'any-ipv4'
                ret_str.Append('%s except;' % ex)
          ret_str.Append('}')  # source-address {...}

        # DESTINATION ADDRESS
        if destination_address or destination_address_exclude:
          ret_str.Append('destination-address {')
          if destination_address:
            for daddr in destination_address:
              for comment in self._Comment(daddr):
                ret_str.Append('%s' % comment)
              if daddr.version == 6 and 0 < daddr.prefixlen < 16:
                for daddr2 in daddr.subnets(new_prefix=16):
                  ret_str.Append('%s;' % daddr2)
              else:
                if daddr == nacaddr.IPv6('0::0/0'):
                  daddr = 'any-ipv6'
                elif daddr == nacaddr.IPv4('0.0.0.0/0'):
                  daddr = 'any-ipv4'
                ret_str.Append('%s;' % daddr)

          # DESTINATION ADDRESS EXCLUDE
          if destination_address_exclude:
            for ex in destination_address_exclude:
              for comment in self._Comment(ex):
                ret_str.Append('%s' % comment)
              if ex.version == 6 and 0 < ex.prefixlen < 16:
                for ex2 in ex.subnets(new_prefix=16):
                  ret_str.Append('%s except;' % ex2)
              else:
                if ex == nacaddr.IPv6('0::0/0'):
                  ex = 'any-ipv6'
                elif ex == nacaddr.IPv4('0.0.0.0/0'):
                  ex = 'any-ipv4'
                ret_str.Append('%s except;' % ex)
          ret_str.Append('}')  # destination-address {...}

        # source prefix <except> list
        if self.term.source_prefix or self.term.source_prefix_except:
          for pfx in self.term.source_prefix:
            ret_str.Append('source-prefix-list ' + pfx + ';')
          for epfx in self.term.source_prefix_except:
            ret_str.Append('source-prefix-list ' + epfx + ' except;')

        # destination prefix <except> list
        if self.term.destination_prefix or self.term.destination_prefix_except:
          for pfx in self.term.destination_prefix:
            ret_str.Append('destination-prefix-list ' + pfx + ';')
          for epfx in self.term.destination_prefix_except:
            ret_str.Append('destination-prefix-list ' + epfx + ' except;')

        # APPLICATION
        if (self.term.source_port or self.term.destination_port or
            self.term.icmp_type or self.term.protocol):
          if hasattr(self.term, 'replacement_application_name'):
            ret_str.Append('application-sets ' +
                           self.term.replacement_application_name + '-app;')
          else:
            ret_str.Append('application-sets ' +
                           self.filter_name[:((MAX_IDENTIFIER_LEN) // 2)] +
                           self.term.name[-((MAX_IDENTIFIER_LEN) // 2):] +
                           '-app;')
        ret_str.Append('}')  # from {...}

      ret_str.Append('then {')
      # ACTION
      for action in self.term.action:
        ret_str.Append(self._ACTIONS.get(str(action)) + ';')
      if self.term.logging and 'disable' not in [
          x.value for x in self.term.logging
      ]:
        ret_str.Append('syslog;')
      ret_str.Append('}')  # then {...}
      ret_str.Append('}')  # term {...}
    return str(ret_str)",_147.py,94,"source_address = self.term.GetAddressOfVersion('source_address', self.AF_MAP.get(suffix))
source_address_exclude = self.term.GetAddressOfVersion('source_address_exclude', self.AF_MAP.get(suffix))","source_address , source_address_exclude  = self.term.GetAddressOfVersion('source_address', self.AF_MAP.get(suffix)), self.term.GetAddressOfVersion('source_address_exclude', self.AF_MAP.get(suffix))"
https://github.com/google/capirca/tree/master/capirca/lib/junipermsmpc.py,"def __str__(self):
    # Verify platform specific terms. Skip whole term if platform does not
    # match.
    if self.term.platform:
      if self._PLATFORM not in self.term.platform:
        return ''
    if self.term.platform_exclude:
      if self._PLATFORM in self.term.platform_exclude:
        return ''

    if self.enable_dsmo:
      raise NotImplementedError('enable_dsmo not implemented for msmpc')

    ret_str = juniper.Config(indent=self._DEFAULT_INDENT)

    # COMMENTS
    # this deals just fine with multi line comments, but we could probably
    # output them a little cleaner; do things like make sure the
    # len(output) < 80, etc. Note, if 'noverbose' is set for the filter, skip
    # all comment processing.
    if not self.noverbose:
      if self.term.owner:
        self.term.comment.append('Owner: %s' % self.term.owner)
      if self.term.comment:
        ret_str.Append('/*')
        for comment in self.term.comment:
          for line in comment.split('\n'):
            ret_str.Append('** ' + line)
        ret_str.Append('*/')

    # Term verbatim output - this will skip over normal term creation
    # code.  Warning generated from policy.py if appropriate.
    if self.term.verbatim:
      for next_term in self.term.verbatim:
        if next_term[0] == self._PLATFORM:
          ret_str.Append(str(next_term[1]), verbatim=True)
      return str(ret_str)

    # Determine whether there are any match conditions for the term.
    has_match_criteria = (
        self.term.address or self.term.dscp_except or self.term.dscp_match or
        self.term.destination_address or self.term.destination_port or
        self.term.destination_prefix or self.term.destination_prefix_except or
        self.term.encapsulate or self.term.ether_type or
        self.term.flexible_match_range or self.term.forwarding_class or
        self.term.forwarding_class_except or self.term.fragment_offset or
        self.term.hop_limit or self.term.next_ip or self.term.port or
        self.term.precedence or self.term.protocol or
        self.term.protocol_except or self.term.source_address or
        self.term.source_port or self.term.source_prefix or
        self.term.source_prefix_except or self.term.traffic_type or
        self.term.ttl)

    suffixes = []
    duplicate_term = False
    has_icmp = 'icmp' in self.term.protocol
    has_icmpv6 = 'icmpv6' in self.term.protocol
    has_v4_ip = self.term.GetAddressOfVersion(
        'source_address',
        self.AF_MAP.get('inet')) or self.term.GetAddressOfVersion(
            'source_address_exclude',
            self.AF_MAP.get('inet')) or self.term.GetAddressOfVersion(
                'destination_address',
                self.AF_MAP.get('inet')) or self.term.GetAddressOfVersion(
                    'destination_address_exclude', self.AF_MAP.get('inet'))
    has_v6_ip = self.term.GetAddressOfVersion(
        'source_address',
        self.AF_MAP.get('inet6')) or self.term.GetAddressOfVersion(
            'source_address_exclude',
            self.AF_MAP.get('inet6')) or self.term.GetAddressOfVersion(
                'destination_address',
                self.AF_MAP.get('inet6')) or self.term.GetAddressOfVersion(
                    'destination_address_exclude', self.AF_MAP.get('inet6'))

    if self.term_type == 'mixed':
      if not (has_v4_ip or has_v6_ip):
        suffixes = ['inet']
      elif not has_v6_ip:
        suffixes = ['inet']
      elif not has_v4_ip:
        suffixes = ['inet6']
      else:
        suffixes = ['inet', 'inet6']
        duplicate_term = True
    if not suffixes and self.term_type in ['inet', 'inet6']:
      suffixes = [self.term_type]

    for suffix in suffixes:
      if self.term_type == 'mixed' and (not (has_icmp and has_icmpv6)) and (
          has_v4_ip and has_v6_ip):
        if (has_icmp and suffix != 'inet') or (has_icmpv6 and
                                               suffix != 'inet6'):
          continue
      source_address = self.term.GetAddressOfVersion('source_address',
                                                     self.AF_MAP.get(suffix))
      source_address_exclude = self.term.GetAddressOfVersion(
          'source_address_exclude', self.AF_MAP.get(suffix))
      source_address, source_address_exclude = self._MinimizePrefixes(
          source_address, source_address_exclude)
      destination_address = self.term.GetAddressOfVersion(
          'destination_address', self.AF_MAP.get(suffix))
      destination_address_exclude = self.term.GetAddressOfVersion(
          'destination_address_exclude', self.AF_MAP.get(suffix))
      destination_address, destination_address_exclude = self._MinimizePrefixes(
          destination_address, destination_address_exclude)
      if ((not source_address) and self.term.GetAddressOfVersion(
          'source_address', self.AF_MAP.get('mixed')) and
          not source_address_exclude) or (
              (not destination_address) and self.term.GetAddressOfVersion(
                  'destination_address', self.AF_MAP.get('mixed')) and
              not destination_address_exclude):
        continue
      if ((has_icmpv6 and not has_icmp and suffix == 'inet') or
          (has_icmp and not has_icmpv6 and
           suffix == 'inet6')) and self.term_type != 'mixed':
        logging.debug(
            self.NO_AF_LOG_PROTO.substitute(
                term=self.term.name,
                proto=', '.join(self.term.protocol),
                af=suffix))
        return ''

      # NAME
      # if the term is inactive we have to set the prefix
      if self.term.inactive:
        term_prefix = 'inactive:'
      else:
        term_prefix = ''

      ret_str.Append(
          '%s term %s%s {' %
          (term_prefix, self.term.name, '-' + suffix if duplicate_term else ''))

      # We only need a ""from {"" clause if there are any conditions to match.
      if has_match_criteria:
        ret_str.Append('from {')
        # SOURCE ADDRESS
        if source_address or source_address_exclude:
          ret_str.Append('source-address {')
          if source_address:
            for saddr in source_address:
              for comment in self._Comment(saddr):
                ret_str.Append('%s' % comment)
              if saddr.version == 6 and 0 < saddr.prefixlen < 16:
                for saddr2 in saddr.subnets(new_prefix=16):
                  ret_str.Append('%s;' % saddr2)
              else:
                if saddr == nacaddr.IPv6('0::0/0'):
                  saddr = 'any-ipv6'
                elif saddr == nacaddr.IPv4('0.0.0.0/0'):
                  saddr = 'any-ipv4'
                ret_str.Append('%s;' % saddr)

          # SOURCE ADDRESS EXCLUDE
          if source_address_exclude:
            for ex in source_address_exclude:
              for comment in self._Comment(ex):
                ret_str.Append('%s' % comment)
              if ex.version == 6 and 0 < ex.prefixlen < 16:
                for ex2 in ex.subnets(new_prefix=16):
                  ret_str.Append('%s except;' % ex2)
              else:
                if ex == nacaddr.IPv6('0::0/0'):
                  ex = 'any-ipv6'
                elif ex == nacaddr.IPv4('0.0.0.0/0'):
                  ex = 'any-ipv4'
                ret_str.Append('%s except;' % ex)
          ret_str.Append('}')  # source-address {...}

        # DESTINATION ADDRESS
        if destination_address or destination_address_exclude:
          ret_str.Append('destination-address {')
          if destination_address:
            for daddr in destination_address:
              for comment in self._Comment(daddr):
                ret_str.Append('%s' % comment)
              if daddr.version == 6 and 0 < daddr.prefixlen < 16:
                for daddr2 in daddr.subnets(new_prefix=16):
                  ret_str.Append('%s;' % daddr2)
              else:
                if daddr == nacaddr.IPv6('0::0/0'):
                  daddr = 'any-ipv6'
                elif daddr == nacaddr.IPv4('0.0.0.0/0'):
                  daddr = 'any-ipv4'
                ret_str.Append('%s;' % daddr)

          # DESTINATION ADDRESS EXCLUDE
          if destination_address_exclude:
            for ex in destination_address_exclude:
              for comment in self._Comment(ex):
                ret_str.Append('%s' % comment)
              if ex.version == 6 and 0 < ex.prefixlen < 16:
                for ex2 in ex.subnets(new_prefix=16):
                  ret_str.Append('%s except;' % ex2)
              else:
                if ex == nacaddr.IPv6('0::0/0'):
                  ex = 'any-ipv6'
                elif ex == nacaddr.IPv4('0.0.0.0/0'):
                  ex = 'any-ipv4'
                ret_str.Append('%s except;' % ex)
          ret_str.Append('}')  # destination-address {...}

        # source prefix <except> list
        if self.term.source_prefix or self.term.source_prefix_except:
          for pfx in self.term.source_prefix:
            ret_str.Append('source-prefix-list ' + pfx + ';')
          for epfx in self.term.source_prefix_except:
            ret_str.Append('source-prefix-list ' + epfx + ' except;')

        # destination prefix <except> list
        if self.term.destination_prefix or self.term.destination_prefix_except:
          for pfx in self.term.destination_prefix:
            ret_str.Append('destination-prefix-list ' + pfx + ';')
          for epfx in self.term.destination_prefix_except:
            ret_str.Append('destination-prefix-list ' + epfx + ' except;')

        # APPLICATION
        if (self.term.source_port or self.term.destination_port or
            self.term.icmp_type or self.term.protocol):
          if hasattr(self.term, 'replacement_application_name'):
            ret_str.Append('application-sets ' +
                           self.term.replacement_application_name + '-app;')
          else:
            ret_str.Append('application-sets ' +
                           self.filter_name[:((MAX_IDENTIFIER_LEN) // 2)] +
                           self.term.name[-((MAX_IDENTIFIER_LEN) // 2):] +
                           '-app;')
        ret_str.Append('}')  # from {...}

      ret_str.Append('then {')
      # ACTION
      for action in self.term.action:
        ret_str.Append(self._ACTIONS.get(str(action)) + ';')
      if self.term.logging and 'disable' not in [
          x.value for x in self.term.logging
      ]:
        ret_str.Append('syslog;')
      ret_str.Append('}')  # then {...}
      ret_str.Append('}')  # term {...}
    return str(ret_str)",_147.py,100,"destination_address = self.term.GetAddressOfVersion('destination_address', self.AF_MAP.get(suffix))
destination_address_exclude = self.term.GetAddressOfVersion('destination_address_exclude', self.AF_MAP.get(suffix))","destination_address , destination_address_exclude  = self.term.GetAddressOfVersion('destination_address', self.AF_MAP.get(suffix)), self.term.GetAddressOfVersion('destination_address_exclude', self.AF_MAP.get(suffix))"
https://github.com/TDAmeritrade/stumpy/tree/master/tests/test_stumped.py,"def test_stumped_one_subsequence_inf_self_join(
    T_A, T_B, substitution_location_B, dask_cluster
):
    with Client(dask_cluster) as dask_client:
        m = 3

        T_B_sub = T_B.copy()
        T_B_sub[substitution_location_B] = np.inf

        zone = int(np.ceil(m / 4))
        ref_mp = naive.stump(T_B_sub, m, exclusion_zone=zone)
        comp_mp = stumped(dask_client, T_B_sub, m, ignore_trivial=True)
        naive.replace_inf(ref_mp)
        naive.replace_inf(comp_mp)
        npt.assert_almost_equal(ref_mp, comp_mp)",_15.py,5,"m = 3
T_B_sub = T_B.copy()","m , T_B_sub  = 3, T_B.copy()"
https://github.com/TDAmeritrade/stumpy/tree/master/tests/test_stumped.py,"def test_stumped_one_subsequence_inf_self_join(
    T_A, T_B, substitution_location_B, dask_cluster
):
    with Client(dask_cluster) as dask_client:
        m = 3

        T_B_sub = T_B.copy()
        T_B_sub[substitution_location_B] = np.inf

        zone = int(np.ceil(m / 4))
        ref_mp = naive.stump(T_B_sub, m, exclusion_zone=zone)
        comp_mp = stumped(dask_client, T_B_sub, m, ignore_trivial=True)
        naive.replace_inf(ref_mp)
        naive.replace_inf(comp_mp)
        npt.assert_almost_equal(ref_mp, comp_mp)",_15.py,8,"T_B_sub[substitution_location_B] = np.inf
zone = int(np.ceil(m / 4))","T_B_sub[substitution_location_B] , zone  = np.inf, int(np.ceil(m / 4))"
https://github.com/TDAmeritrade/stumpy/tree/master/tests/test_stumped.py,"def test_stumped_one_subsequence_inf_self_join(
    T_A, T_B, substitution_location_B, dask_cluster
):
    with Client(dask_cluster) as dask_client:
        m = 3

        T_B_sub = T_B.copy()
        T_B_sub[substitution_location_B] = np.inf

        zone = int(np.ceil(m / 4))
        ref_mp = naive.stump(T_B_sub, m, exclusion_zone=zone)
        comp_mp = stumped(dask_client, T_B_sub, m, ignore_trivial=True)
        naive.replace_inf(ref_mp)
        naive.replace_inf(comp_mp)
        npt.assert_almost_equal(ref_mp, comp_mp)",_15.py,11,"ref_mp = naive.stump(T_B_sub, m, exclusion_zone=zone)
comp_mp = stumped(dask_client, T_B_sub, m, ignore_trivial=True)","ref_mp , comp_mp  = naive.stump(T_B_sub, m, exclusion_zone=zone), stumped(dask_client, T_B_sub, m, ignore_trivial=True)"
https://github.com/google/TensorNetwork/tree/master/tensornetwork/quantum/quantum.py,"def eliminate_identities(nodes: Collection[AbstractNode]) -> Tuple[dict, dict]:
  """"""Eliminates any connected CopyNodes that are identity matrices.

  This will modify the network represented by `nodes`.
  Only identities that are connected to other nodes are eliminated.

  Args:
    nodes: Collection of nodes to search.
  Returns:
    nodes_dict: Dictionary mapping remaining Nodes to any replacements.
    dangling_edges_dict: Dictionary specifying all dangling-edge replacements.
  """"""
  nodes_dict = {}
  dangling_edges_dict = {}
  for n in nodes:
    if isinstance(
        n, CopyNode) and n.get_rank() == 2 and not (n[0].is_dangling() and
                                                    n[1].is_dangling()):
      old_edges = [n[0], n[1]]
      _, new_edges = remove_node(n)
      if 0 in new_edges and 1 in new_edges:
        e = connect(new_edges[0], new_edges[1])
      elif 0 in new_edges:  # 1 was dangling
        dangling_edges_dict[old_edges[1]] = new_edges[0]
      elif 1 in new_edges:  # 0 was dangling
        dangling_edges_dict[old_edges[0]] = new_edges[1]
      else:
        # Trace of identity, so replace with a scalar node!
        d = n.get_dimension(0)
        # NOTE: Assume CopyNodes have numpy dtypes.
        nodes_dict[n] = Node(np.array(d, dtype=n.dtype), backend=n.backend)
    else:
      for e in n.get_all_dangling():
        dangling_edges_dict[e] = e
      nodes_dict[n] = n

  return nodes_dict, dangling_edges_dict",_154.py,13,"nodes_dict = {}
dangling_edges_dict = {}","nodes_dict , dangling_edges_dict  = {}, {}"
https://github.com/p5py/p5/tree/master/p5/pmath/curves.py,"def quadratic_point(start, control, stop, parameter):
    """"""Return the coordinates of a point along a bezier curve.

    :param point_1: The start point of the curve.
    :type point_1: float or n-tuple.

    :param point_3: The control point of the curve.
    :type point_3: float or n-tuple.

    :param point_4: The end point of the curve.
    :type point_4: float or n-tuple.

    :param parameter: The parameter for the required point location
        along the curve. Should be in the range [0.0, 1.0] where 0
        indicates the start of the curve and 1 indicates the end of
        the curve.
    :type parameter: float

    :returns: The coordinate of the point at the required location
        along the curve.
    :rtype: float or n-tuple

    """"""
    is_iterable = isinstance(start, Iterable)
    if not is_iterable:
        start, control, stop = (start,), (control,), (stop,)

    t = parameter
    t_ = 1 - parameter
    P = [start, control, stop]
    coeffs = [t_ * t_, 2 * t * t_, t * t]
    ans = tuple(sum(pt[i] * c for pt, c in zip(P, coeffs)) for i in range(len(start)))
    # Unpack answer if input is not iterable
    if not is_iterable:
        ans = ans[0]
    return ans",_156.py,28,"t = parameter
t_ = 1 - parameter","t , t_  = parameter, 1 - parameter"
https://github.com/p5py/p5/tree/master/p5/pmath/curves.py,"def quadratic_point(start, control, stop, parameter):
    """"""Return the coordinates of a point along a bezier curve.

    :param point_1: The start point of the curve.
    :type point_1: float or n-tuple.

    :param point_3: The control point of the curve.
    :type point_3: float or n-tuple.

    :param point_4: The end point of the curve.
    :type point_4: float or n-tuple.

    :param parameter: The parameter for the required point location
        along the curve. Should be in the range [0.0, 1.0] where 0
        indicates the start of the curve and 1 indicates the end of
        the curve.
    :type parameter: float

    :returns: The coordinate of the point at the required location
        along the curve.
    :rtype: float or n-tuple

    """"""
    is_iterable = isinstance(start, Iterable)
    if not is_iterable:
        start, control, stop = (start,), (control,), (stop,)

    t = parameter
    t_ = 1 - parameter
    P = [start, control, stop]
    coeffs = [t_ * t_, 2 * t * t_, t * t]
    ans = tuple(sum(pt[i] * c for pt, c in zip(P, coeffs)) for i in range(len(start)))
    # Unpack answer if input is not iterable
    if not is_iterable:
        ans = ans[0]
    return ans",_156.py,30,"P = [start, control, stop]
coeffs = [t_ * t_, 2 * t * t_, t * t]","P , coeffs  = [start, control, stop], [t_ * t_, 2 * t * t_, t * t]"
https://github.com/SiCKRAGE/SiCKRAGE/tree/master/sickrage/notification_providers/tweet.py,"def notify_version_update(self, new_version=""??""):
        if sickrage.app.config.twitter.enable:
            update_text = self.notifyStrings[self.NOTIFY_GIT_UPDATE_TEXT]
            title = self.notifyStrings[self.NOTIFY_GIT_UPDATE]
            self._notifyTwitter(title + "" - "" + update_text + new_version)",_157.py,3,"update_text = self.notifyStrings[self.NOTIFY_GIT_UPDATE_TEXT]
title = self.notifyStrings[self.NOTIFY_GIT_UPDATE]","update_text , title  = self.notifyStrings[self.NOTIFY_GIT_UPDATE_TEXT], self.notifyStrings[self.NOTIFY_GIT_UPDATE]"
https://github.com/django-json-api/django-rest-framework-json-api/tree/master/tests/test_relations.py,"def test_serialize(
        self, format_type, pluralize_type, resource_type, foreign_key_target, settings
    ):
        settings.JSON_API_FORMAT_TYPES = format_type
        settings.JSON_API_PLURALIZE_TYPES = pluralize_type

        serializer = ForeignKeySourceSerializer(instance={""target"": foreign_key_target})
        expected = {
            ""type"": resource_type,
            ""id"": str(foreign_key_target.pk),
        }

        assert serializer.data[""target""] == expected",_159.py,4,"settings.JSON_API_FORMAT_TYPES = format_type
settings.JSON_API_PLURALIZE_TYPES = pluralize_type
serializer = ForeignKeySourceSerializer(instance={'target': foreign_key_target})
expected = {'type': resource_type, 'id': str(foreign_key_target.pk)}","settings.JSON_API_FORMAT_TYPES , settings.JSON_API_PLURALIZE_TYPES , serializer , expected  = format_type, pluralize_type, ForeignKeySourceSerializer(instance={'target': foreign_key_target}), {'type': resource_type, 'id': str(foreign_key_target.pk)}"
https://github.com/aws/aws-parallelcluster/tree/master/util/generate-ami-list.py,"def get_ami_list_from_file(regions, json_amis):
    """"""
    Read the AMI mappings from json_amis for the given regions.

     The format of json_amis is as follows:
    {
      ""AWSRegionOS2AMIx86"": {
        ""af-south-1"": {
          ""alinux2"": ""ami-xxx"",
          ""centos7"": ""UNSUPPORTED"",
          ""ubuntu1804"": ""ami-zzz"",
          ""ubuntu2004"": ""ami-www""
        },
        ""ap-east-1"": {
          ""alinux2"": ""ami-01905ce1b2d63e7e0"",
          ""centos7"": ""UNSUPPORTED"",
          ...
      ""AWSRegionOS2AMIarm64"": {
         ...
    """"""
    amis_json = get_initialized_mappings_dicts()
    json_data = read_json_file(json_amis)

    current_amis = {}
    for mapping_name in amis_json:
        current_amis[mapping_name] = json_data.get(mapping_name, {})
        for region_name in regions:
            if region_name in current_amis.get(mapping_name, []):
                # Ensure mapping for the region_name is sorted by OS name
                amis_json[mapping_name][region_name] = OrderedDict(
                    sorted(current_amis.get(mapping_name).get(region_name).items())
                )
            else:
                print(
                    ""Warning: there are no AMIs in the region ({region}) within the mapping ({mapping})"".format(
                        region=region_name, mapping=mapping_name
                    )
                )
    return amis_json",_163.py,21,"amis_json = get_initialized_mappings_dicts()
json_data = read_json_file(json_amis)
current_amis = {}","amis_json , json_data , current_amis  = get_initialized_mappings_dicts(), read_json_file(json_amis), {}"
https://github.com/buildbot/buildbot/tree/master/master/buildbot/test/unit/process/test_build.py,"def test_build_retry_when_worker_substantiate_returns_false_due_to_cancel(self):
        b = self.build

        step = FakeBuildStep()
        b.setStepFactories([FakeStepFactory(step)])

        d = defer.Deferred()
        self.workerforbuilder.substantiate_if_needed = lambda _: d
        b.startBuild(self.workerforbuilder)
        b.stopBuild('Cancel Build', RETRY)
        d.callback(False)
        self.assertEqual(b.results, RETRY)
        self.assertWorkerPreparationFailure('error while worker_prepare')",_164.py,2,"b = self.build
step = FakeBuildStep()","b , step  = self.build, FakeBuildStep()"
https://github.com/richardaecn/class-balanced-loss/tree/master/tpu/models/official/retinanet/anchors.py,"def decode_box_outputs(rel_codes, anchors):
  """"""Transforms relative regression coordinates to absolute positions.

  Network predictions are normalized and relative to a given anchor; this
  reverses the transformation and outputs absolute coordinates for the input
  image.

  Args:
    rel_codes: box regression targets.
    anchors: anchors on all feature levels.
  Returns:
    outputs: bounding boxes.

  """"""
  ycenter_a = (anchors[0] + anchors[2]) / 2
  xcenter_a = (anchors[1] + anchors[3]) / 2
  ha = anchors[2] - anchors[0]
  wa = anchors[3] - anchors[1]
  ty, tx, th, tw = rel_codes

  w = np.exp(tw) * wa
  h = np.exp(th) * ha
  ycenter = ty * ha + ycenter_a
  xcenter = tx * wa + xcenter_a
  ymin = ycenter - h / 2.
  xmin = xcenter - w / 2.
  ymax = ycenter + h / 2.
  xmax = xcenter + w / 2.
  return np.column_stack([ymin, xmin, ymax, xmax])",_17.py,15,"ycenter_a = (anchors[0] + anchors[2]) / 2
xcenter_a = (anchors[1] + anchors[3]) / 2
ha = anchors[2] - anchors[0]
wa = anchors[3] - anchors[1]","ycenter_a , xcenter_a , ha , wa  = (anchors[0] + anchors[2]) / 2, (anchors[1] + anchors[3]) / 2, anchors[2] - anchors[0], anchors[3] - anchors[1]"
https://github.com/richardaecn/class-balanced-loss/tree/master/tpu/models/official/retinanet/anchors.py,"def decode_box_outputs(rel_codes, anchors):
  """"""Transforms relative regression coordinates to absolute positions.

  Network predictions are normalized and relative to a given anchor; this
  reverses the transformation and outputs absolute coordinates for the input
  image.

  Args:
    rel_codes: box regression targets.
    anchors: anchors on all feature levels.
  Returns:
    outputs: bounding boxes.

  """"""
  ycenter_a = (anchors[0] + anchors[2]) / 2
  xcenter_a = (anchors[1] + anchors[3]) / 2
  ha = anchors[2] - anchors[0]
  wa = anchors[3] - anchors[1]
  ty, tx, th, tw = rel_codes

  w = np.exp(tw) * wa
  h = np.exp(th) * ha
  ycenter = ty * ha + ycenter_a
  xcenter = tx * wa + xcenter_a
  ymin = ycenter - h / 2.
  xmin = xcenter - w / 2.
  ymax = ycenter + h / 2.
  xmax = xcenter + w / 2.
  return np.column_stack([ymin, xmin, ymax, xmax])",_17.py,21,"w = np.exp(tw) * wa
h = np.exp(th) * ha
ycenter = ty * ha + ycenter_a
xcenter = tx * wa + xcenter_a
 

","w , h , ycenter, xcenter  = np.exp(tw) * wa, np.exp(th) * ha, ty * ha + ycenter_a, tx * wa + xcenter_a"
https://github.com/richardaecn/class-balanced-loss/tree/master/tpu/models/official/retinanet/anchors.py,"def decode_box_outputs(rel_codes, anchors):
  """"""Transforms relative regression coordinates to absolute positions.

  Network predictions are normalized and relative to a given anchor; this
  reverses the transformation and outputs absolute coordinates for the input
  image.

  Args:
    rel_codes: box regression targets.
    anchors: anchors on all feature levels.
  Returns:
    outputs: bounding boxes.

  """"""
  ycenter_a = (anchors[0] + anchors[2]) / 2
  xcenter_a = (anchors[1] + anchors[3]) / 2
  ha = anchors[2] - anchors[0]
  wa = anchors[3] - anchors[1]
  ty, tx, th, tw = rel_codes

  w = np.exp(tw) * wa
  h = np.exp(th) * ha
  ycenter = ty * ha + ycenter_a
  xcenter = tx * wa + xcenter_a
  ymin = ycenter - h / 2.
  xmin = xcenter - w / 2.
  ymax = ycenter + h / 2.
  xmax = xcenter + w / 2.
  return np.column_stack([ymin, xmin, ymax, xmax])",_17.py,25,"ymin = ycenter - h / 2.0
xmin = xcenter - w / 2.0
ymax = ycenter + h / 2.0
xmax = xcenter + w / 2.0","ymin , xmin , ymax , xmax  = ycenter - h / 2.0, xcenter - w / 2.0, ycenter + h / 2.0, xcenter + w / 2.0"
https://github.com/XuShaohua/bcloud/tree/master/bcloud/SigninDialog.py,"def __init__(self, parent, username, cookie, tokens, codeString, vcodetype):
        super().__init__(_('Verification..'), parent, Gtk.DialogFlags.MODAL)

        self.set_default_size(280, 130)
        self.set_border_width(10)
        self.username = username
        self.cookie = cookie
        self.tokens = tokens
        self.codeString = codeString
        self.vcodetype = vcodetype

        box = self.get_content_area()
        box.set_spacing(5)

        self.vcode_img = Gtk.Image()
        box.pack_start(self.vcode_img, True, True, 0)

        button_box = Gtk.Box(spacing=5)
        box.pack_start(button_box, True, True, 0)

        self.vcode_entry = Gtk.Entry()
        self.vcode_entry.connect('activate', self.check_entry)
        button_box.pack_start(self.vcode_entry, True, True, 0)

        if Config.GTK_GE_312:
            vcode_refresh = Widgets.IconButton('view-refresh-symbolic')
        else:
            vcode_refresh = Gtk.Button.new_from_stock(Gtk.STOCK_REFRESH)
        vcode_refresh.props.valign = Gtk.Align.CENTER
        vcode_refresh.connect('clicked', self.on_vcode_refresh_clicked)
        button_box.pack_start(vcode_refresh, False, False, 0)

        # show loading process
        self.loading_spin = Gtk.Spinner()
        self.loading_spin.props.valign = Gtk.Align.CENTER
        button_box.pack_start(self.loading_spin, False, False, 0)

        vcode_confirm = Gtk.Button.new_from_stock(Gtk.STOCK_OK)
        vcode_confirm.connect('clicked', self.on_vcode_confirm_clicked)
        vcode_confirm.props.valign = Gtk.Align.END
        box.pack_start(vcode_confirm, False, False, 10)

        box.show_all()
        self.loading_spin.hide()

        gutil.async_call(auth.get_signin_vcode, cookie, codeString,
                         callback=self.update_img)",_171.py,6,"self.username = username
self.cookie = cookie
self.tokens = tokens
self.codeString = codeString
self.vcodetype = vcodetype","self.username , self.cookie , self.tokens , self.codeString , self.vcodetype  = username, cookie, tokens, codeString, vcodetype"
https://github.com/microsoft/hummingbird/tree/master/hummingbird/ml/operator_converters/sklearn/decomposition.py,"def convert_sklearn_pca(operator, device, extra_config):
    """"""
    Converter for `sklearn.decomposition.PCA`

    Args:
        operator: An operator wrapping a `sklearn.decomposition.PCA` transformer
        device: String defining the type of device the converted operator should be run on
        extra_config: Extra configuration used to select the best conversion strategy

    Returns:
        A PyTorch model
    """"""
    assert operator is not None, ""Cannot convert None operator""

    transform_matrix = operator.raw_operator.components_.transpose()
    mean = operator.raw_operator.mean_.reshape(1, -1)
    if operator.raw_operator.whiten:
        transform_matrix = transform_matrix / np.sqrt(operator.raw_operator.explained_variance_)

    return Decomposition(operator, mean, transform_matrix.astype(""float32""), device)",_172.py,15,"transform_matrix = operator.raw_operator.components_.transpose()
mean = operator.raw_operator.mean_.reshape(1, -1)","transform_matrix , mean  = operator.raw_operator.components_.transpose(), operator.raw_operator.mean_.reshape(1, -1)"
https://github.com/IntelLabs/coach/tree/master/rl_coach/dashboard_components/signals_file_base.py,"def __init__(self, plot):
        self.plot = plot
        self.full_csv_path = """"
        self.dir = """"
        self.filename = """"
        self.signals_averaging_window = 1
        self.show_bollinger_bands = False
        self.csv = None
        self.bokeh_source = None
        self.bokeh_source_orig = None
        self.last_modified = None
        self.signals = {}
        self.separate_files = False
        self.last_reload_data_fix = False",_174.py,2,"self.plot = plot
self.full_csv_path = ''
self.dir = ''
self.filename = ''
self.signals_averaging_window = 1
self.show_bollinger_bands = False
self.csv = None
self.bokeh_source = None
self.bokeh_source_orig = None
self.last_modified = None
self.signals = {}
self.separate_files = False
self.last_reload_data_fix = False","self.plot , self.full_csv_path , self.dir , self.filename , self.signals_averaging_window , self.show_bollinger_bands , self.csv , self.bokeh_source , self.bokeh_source_orig , self.last_modified , self.signals , self.separate_files , self.last_reload_data_fix  = plot, '', '', '', 1, False, None, None, None, None, {}, False, False"
https://github.com/terraform-compliance/cli/tree/master/terraform_compliance/extensions/security_groups.py,"def must_have(self):
        self.exact_match = False
        self.negative_match = False
        self.include_match = True
        self.singular_check = False",_175.py,2,"self.exact_match = False
self.negative_match = False
self.include_match = True
self.singular_check = False","self.exact_match , self.negative_match , self.include_match , self.singular_check  = False, False, True, False"
https://github.com/geatpy-dev/geatpy/tree/master/geatpy/templates/soeas/GA/studGA/soea_studGA_templet.py,"def run(self, prophetPop=None):  # prophetPop
        # =====================================================
        population = self.population
        NIND = population.sizes
        self.initialization()  # 
        # =======================================================
        population.initChrom(NIND)  # 
        self.call_aimFunc(population)  # 
        # prophetPopprophetPopChromObjVPhen
        if prophetPop is not None:
            population = (prophetPop + population)[:NIND]  # 
        population.FitnV = ea.scaling(population.ObjV, population.CV, self.problem.maxormins)  # 
        # =======================================================
        while self.terminated(population) == False:
            bestIdx = np.argmax(population.FitnV, axis=0)  # , axis=0
            studPop = population[np.tile(bestIdx, (NIND // 2))]  # NIND//2
            restPop = population[np.where(np.arange(NIND) != bestIdx)[0]]  # 
            # 
            tempPop = restPop[ea.selecting(self.selFunc, restPop.FitnV, (NIND - studPop.sizes))]
            # 
            population = studPop + tempPop
            # 
            population.Chrom = self.recOper.do(population.Chrom)  # 
            population.Chrom = self.mutOper.do(population.Encoding, population.Chrom, population.Field)  # 
            self.call_aimFunc(population)
            population.FitnV = ea.scaling(population.ObjV, population.CV, self.problem.maxormins)  # 
        return self.finishing(population)",_183.py,16,"studPop = population[np.tile(bestIdx, NIND // 2)]
restPop = population[np.where(np.arange(NIND) != bestIdx)[0]]","studPop , restPop  = population[np.tile(bestIdx, NIND // 2)], population[np.where(np.arange(NIND) != bestIdx)[0]]"
https://github.com/apache/airflow/tree/master/airflow/models/taskinstance.py,"def command_as_list(
        self,
        mark_success=False,
        ignore_all_deps=False,
        ignore_task_deps=False,
        ignore_depends_on_past=False,
        ignore_ti_state=False,
        local=False,
        pickle_id=None,
        raw=False,
        job_id=None,
        pool=None,
        cfg_path=None,
    ):
        """"""
        Returns a command that can be executed anywhere where airflow is
        installed. This command is part of the message sent to executors by
        the orchestrator.
        """"""
        dag: Union[""DAG"", ""DagModel""]
        # Use the dag if we have it, else fallback to the ORM dag_model, which might not be loaded
        if hasattr(self, 'task') and hasattr(self.task, 'dag'):
            dag = self.task.dag
        else:
            dag = self.dag_model

        should_pass_filepath = not pickle_id and dag
        path = None
        if should_pass_filepath:
            if dag.is_subdag:
                path = dag.parent_dag.relative_fileloc
            else:
                path = dag.relative_fileloc

            if path:
                if not path.is_absolute():
                    path = 'DAGS_FOLDER' / path
                path = str(path)

        return TaskInstance.generate_command(
            self.dag_id,
            self.task_id,
            run_id=self.run_id,
            mark_success=mark_success,
            ignore_all_deps=ignore_all_deps,
            ignore_task_deps=ignore_task_deps,
            ignore_depends_on_past=ignore_depends_on_past,
            ignore_ti_state=ignore_ti_state,
            local=local,
            pickle_id=pickle_id,
            file_path=path,
            raw=raw,
            job_id=job_id,
            pool=pool,
            cfg_path=cfg_path,
        )",_184.py,27,"should_pass_filepath = not pickle_id and dag
path = None","should_pass_filepath , path  = not pickle_id and dag, None"
https://github.com/jdavisp3/twisted-intro/tree/master/twisted-client-4/get-poetry.py,"def poetry_main():
    addresses = parse_args()

    from twisted.internet import reactor

    poems = []
    errors = []

    def got_poem(poem):
        poems.append(poem)

    def poem_failed(err):
        print('Poem failed:', err, file=sys.stderr)
        errors.append(err)

    def poem_done(_):
        if len(poems) + len(errors) == len(addresses):
            reactor.stop()

    for address in addresses:
        host, port = address
        d = get_poetry(host, port)
        d.addCallbacks(got_poem, poem_failed)
        d.addBoth(poem_done)

    reactor.run()

    for poem in poems:
        print(poem)",_189.py,6,"poems = []
errors = []","poems , errors  = [], []"
https://github.com/google-research/deeplab2/tree/master/model/layers/axial_blocks.py,"def build(self, input_shape_list):
    input_tensor_shape = input_shape_list[0]
    self._shortcut = None
    if input_tensor_shape[3] != self._filters_list[-1]:
      self._shortcut = convolutions.Conv2DSame(
          self._filters_list[-1], 1, 'shortcut',
          strides=self._strides,
          use_bias=False,
          use_bn=True,
          bn_layer=self._bn_layer,
          activation='none',
          conv_kernel_weight_decay=self._conv_kernel_weight_decay)",_192.py,2,"input_tensor_shape = input_shape_list[0]
self._shortcut = None","input_tensor_shape , self._shortcut  = input_shape_list[0], None"
https://github.com/RangiLyu/nanodet/tree/master/tests/test_models/test_loss/test_iou_loss.py,"def test_iou_type_loss_zeros_weight(loss_class):
    pred = torch.rand((10, 4))
    target = torch.rand((10, 4))
    weight = torch.zeros(10)

    with pytest.raises(AssertionError):
        loss_class()(pred, target, reduction_override=""2333"")

    loss = loss_class()(pred, target, weight)
    assert loss == 0.0

    weight = torch.rand(10)
    loss = loss_class()(pred, target, weight)
    assert loss != 0.0",_20.py,2,"pred = torch.rand((10, 4))
target = torch.rand((10, 4))
weight = torch.zeros(10)","pred , target , weight  = torch.rand((10, 4)), torch.rand((10, 4)), torch.zeros(10)"
https://github.com/shuup/shuup/tree/master/shuup_tests/front/test_product_variations.py,"def test_variation_detail_view(client):
    shop = get_default_shop()

    parent = create_product(printable_gibberish(), shop)

    assert parent.mode == ProductMode.NORMAL

    volume = ProductVariationVariable.objects.create(
        identifier=""volume"",
        name=""volume"",
        product=parent,
        ordering=0,
    )
    for index, value in enumerate([""1ml"", ""2ml"", ""3ml""]):
        ProductVariationVariableValue.objects.create(
            identifier=value,
            value=value,
            variable=volume,
            ordering=index,
        )

    color = ProductVariationVariable.objects.create(
        identifier=""color"",
        name=""color"",
        product=parent,
        ordering=1,
    )
    for index, value in enumerate([""red"", ""green"", ""blue""]):
        ProductVariationVariableValue.objects.create(
            identifier=value,
            value=value,
            variable=color,
            ordering=index,
        )

    child = create_product(printable_gibberish(), shop)

    values = [volume.values.all()[1], color.values.all()[1]]
    assert child.link_to_parent(
        parent,
        variables={
            volume: values[0],
            color: values[1],
        },
    )

    assert parent.mode == ProductMode.VARIABLE_VARIATION_PARENT

    variation_url = ""{}?variation={}"".format(
        reverse(""shuup:product"", kwargs=dict(pk=parent.pk, slug=parent.slug)), child.sku
    )
    response = client.get(variation_url + ""BAD_VARIATION_SKU"")
    assert response.status_code == 404

    response = client.get(variation_url)
    assert response.status_code == 200

    assert response.context_data[""shop_product""]

    assert response.context_data[""selected_variation""] == child
    assert response.context_data[""selected_variation_values""] == [v.pk for v in values]",_204.py,36,"child = create_product(printable_gibberish(), shop)
values = [volume.values.all()[1], color.values.all()[1]]","child , values  = create_product(printable_gibberish(), shop), [volume.values.all()[1], color.values.all()[1]]"
https://github.com/pandas-dev/pandas/tree/master/pandas/io/formats/style.py,"def to_latex(
        self,
        buf: FilePath | WriteBuffer[str] | None = None,
        *,
        column_format: str | None = None,
        position: str | None = None,
        position_float: str | None = None,
        hrules: bool | None = None,
        clines: str | None = None,
        label: str | None = None,
        caption: str | tuple | None = None,
        sparse_index: bool | None = None,
        sparse_columns: bool | None = None,
        multirow_align: str | None = None,
        multicol_align: str | None = None,
        siunitx: bool = False,
        environment: str | None = None,
        encoding: str | None = None,
        convert_css: bool = False,
    ) -> str | None:
        r""""""
        Write Styler to a file, buffer or string in LaTeX format.

        .. versionadded:: 1.3.0

        Parameters
        ----------
        buf : str, path object, file-like object, or None, default None
            String, path object (implementing ``os.PathLike[str]``), or file-like
            object implementing a string ``write()`` function. If None, the result is
            returned as a string.
        column_format : str, optional
            The LaTeX column specification placed in location:

            \\begin{tabular}{<column_format>}

            Defaults to 'l' for index and
            non-numeric data columns, and, for numeric data columns,
            to 'r' by default, or 'S' if ``siunitx`` is ``True``.
        position : str, optional
            The LaTeX positional argument (e.g. 'h!') for tables, placed in location:

            ``\\begin{table}[<position>]``.
        position_float : {""centering"", ""raggedleft"", ""raggedright""}, optional
            The LaTeX float command placed in location:

            \\begin{table}[<position>]

            \\<position_float>

            Cannot be used if ``environment`` is ""longtable"".
        hrules : bool
            Set to `True` to add \\toprule, \\midrule and \\bottomrule from the
            {booktabs} LaTeX package.
            Defaults to ``pandas.options.styler.latex.hrules``, which is `False`.

            .. versionchanged:: 1.4.0
        clines : str, optional
            Use to control adding \\cline commands for the index labels separation.
            Possible values are:

              - `None`: no cline commands are added (default).
              - `""all;data""`: a cline is added for every index value extending the
                width of the table, including data entries.
              - `""all;index""`: as above with lines extending only the width of the
                index entries.
              - `""skip-last;data""`: a cline is added for each index value except the
                last level (which is never sparsified), extending the widtn of the
                table.
              - `""skip-last;index""`: as above with lines extending only the width of the
                index entries.

            .. versionadded:: 1.4.0
        label : str, optional
            The LaTeX label included as: \\label{<label>}.
            This is used with \\ref{<label>} in the main .tex file.
        caption : str, tuple, optional
            If string, the LaTeX table caption included as: \\caption{<caption>}.
            If tuple, i.e (""full caption"", ""short caption""), the caption included
            as: \\caption[<caption[1]>]{<caption[0]>}.
        sparse_index : bool, optional
            Whether to sparsify the display of a hierarchical index. Setting to False
            will display each explicit level element in a hierarchical key for each row.
            Defaults to ``pandas.options.styler.sparse.index``, which is `True`.
        sparse_columns : bool, optional
            Whether to sparsify the display of a hierarchical index. Setting to False
            will display each explicit level element in a hierarchical key for each
            column. Defaults to ``pandas.options.styler.sparse.columns``, which
            is `True`.
        multirow_align : {""c"", ""t"", ""b"", ""naive""}, optional
            If sparsifying hierarchical MultiIndexes whether to align text centrally,
            at the top or bottom using the multirow package. If not given defaults to
            ``pandas.options.styler.latex.multirow_align``, which is `""c""`.
            If ""naive"" is given renders without multirow.

            .. versionchanged:: 1.4.0
        multicol_align : {""r"", ""c"", ""l"", ""naive-l"", ""naive-r""}, optional
            If sparsifying hierarchical MultiIndex columns whether to align text at
            the left, centrally, or at the right. If not given defaults to
            ``pandas.options.styler.latex.multicol_align``, which is ""r"".
            If a naive option is given renders without multicol.
            Pipe decorators can also be added to non-naive values to draw vertical
            rules, e.g. ""\|r"" will draw a rule on the left side of right aligned merged
            cells.

            .. versionchanged:: 1.4.0
        siunitx : bool, default False
            Set to ``True`` to structure LaTeX compatible with the {siunitx} package.
        environment : str, optional
            If given, the environment that will replace 'table' in ``\\begin{table}``.
            If 'longtable' is specified then a more suitable template is
            rendered. If not given defaults to
            ``pandas.options.styler.latex.environment``, which is `None`.

            .. versionadded:: 1.4.0
        encoding : str, optional
            Character encoding setting. Defaults
            to ``pandas.options.styler.render.encoding``, which is ""utf-8"".
        convert_css : bool, default False
            Convert simple cell-styles from CSS to LaTeX format. Any CSS not found in
            conversion table is dropped. A style can be forced by adding option
            `--latex`. See notes.

        Returns
        -------
        str or None
            If `buf` is None, returns the result as a string. Otherwise returns `None`.

        See Also
        --------
        Styler.format: Format the text display value of cells.

        Notes
        -----
        **Latex Packages**

        For the following features we recommend the following LaTeX inclusions:

        ===================== ==========================================================
        Feature               Inclusion
        ===================== ==========================================================
        sparse columns        none: included within default {tabular} environment
        sparse rows           \\usepackage{multirow}
        hrules                \\usepackage{booktabs}
        colors                \\usepackage[table]{xcolor}
        siunitx               \\usepackage{siunitx}
        bold (with siunitx)   | \\usepackage{etoolbox}
                              | \\robustify\\bfseries
                              | \\sisetup{detect-all = true}  *(within {document})*
        italic (with siunitx) | \\usepackage{etoolbox}
                              | \\robustify\\itshape
                              | \\sisetup{detect-all = true}  *(within {document})*
        environment           \\usepackage{longtable} if arg is ""longtable""
                              | or any other relevant environment package
        hyperlinks            \\usepackage{hyperref}
        ===================== ==========================================================

        **Cell Styles**

        LaTeX styling can only be rendered if the accompanying styling functions have
        been constructed with appropriate LaTeX commands. All styling
        functionality is built around the concept of a CSS ``(<attribute>, <value>)``
        pair (see `Table Visualization <../../user_guide/style.ipynb>`_), and this
        should be replaced by a LaTeX
        ``(<command>, <options>)`` approach. Each cell will be styled individually
        using nested LaTeX commands with their accompanied options.

        For example the following code will highlight and bold a cell in HTML-CSS:

        >>> df = pd.DataFrame([[1,2], [3,4]])
        >>> s = df.style.highlight_max(axis=None,
        ...                            props='background-color:red; font-weight:bold;')
        >>> s.to_html()  # doctest: +SKIP

        The equivalent using LaTeX only commands is the following:

        >>> s = df.style.highlight_max(axis=None,
        ...                            props='cellcolor:{red}; bfseries: ;')
        >>> s.to_latex()  # doctest: +SKIP

        Internally these structured LaTeX ``(<command>, <options>)`` pairs
        are translated to the
        ``display_value`` with the default structure:
        ``\<command><options> <display_value>``.
        Where there are multiple commands the latter is nested recursively, so that
        the above example highlighted cell is rendered as
        ``\cellcolor{red} \bfseries 4``.

        Occasionally this format does not suit the applied command, or
        combination of LaTeX packages that is in use, so additional flags can be
        added to the ``<options>``, within the tuple, to result in different
        positions of required braces (the **default** being the same as ``--nowrap``):

        =================================== ============================================
        Tuple Format                           Output Structure
        =================================== ============================================
        (<command>,<options>)               \\<command><options> <display_value>
        (<command>,<options> ``--nowrap``)  \\<command><options> <display_value>
        (<command>,<options> ``--rwrap``)   \\<command><options>{<display_value>}
        (<command>,<options> ``--wrap``)    {\\<command><options> <display_value>}
        (<command>,<options> ``--lwrap``)   {\\<command><options>} <display_value>
        (<command>,<options> ``--dwrap``)   {\\<command><options>}{<display_value>}
        =================================== ============================================

        For example the `textbf` command for font-weight
        should always be used with `--rwrap` so ``('textbf', '--rwrap')`` will render a
        working cell, wrapped with braces, as ``\textbf{<display_value>}``.

        A more comprehensive example is as follows:

        >>> df = pd.DataFrame([[1, 2.2, ""dogs""], [3, 4.4, ""cats""], [2, 6.6, ""cows""]],
        ...                   index=[""ix1"", ""ix2"", ""ix3""],
        ...                   columns=[""Integers"", ""Floats"", ""Strings""])
        >>> s = df.style.highlight_max(
        ...     props='cellcolor:[HTML]{FFFF00}; color:{red};'
        ...           'textit:--rwrap; textbf:--rwrap;'
        ... )
        >>> s.to_latex()  # doctest: +SKIP

        .. figure:: ../../_static/style/latex_1.png

        **Table Styles**

        Internally Styler uses its ``table_styles`` object to parse the
        ``column_format``, ``position``, ``position_float``, and ``label``
        input arguments. These arguments are added to table styles in the format:

        .. code-block:: python

            set_table_styles([
                {""selector"": ""column_format"", ""props"": f"":{column_format};""},
                {""selector"": ""position"", ""props"": f"":{position};""},
                {""selector"": ""position_float"", ""props"": f"":{position_float};""},
                {""selector"": ""label"", ""props"": f"":{{{label.replace(':','')}}};""}
            ], overwrite=False)

        Exception is made for the ``hrules`` argument which, in fact, controls all three
        commands: ``toprule``, ``bottomrule`` and ``midrule`` simultaneously. Instead of
        setting ``hrules`` to ``True``, it is also possible to set each
        individual rule definition, by manually setting the ``table_styles``,
        for example below we set a regular ``toprule``, set an ``hline`` for
        ``bottomrule`` and exclude the ``midrule``:

        .. code-block:: python

            set_table_styles([
                {'selector': 'toprule', 'props': ':toprule;'},
                {'selector': 'bottomrule', 'props': ':hline;'},
            ], overwrite=False)

        If other ``commands`` are added to table styles they will be detected, and
        positioned immediately above the '\\begin{tabular}' command. For example to
        add odd and even row coloring, from the {colortbl} package, in format
        ``\rowcolors{1}{pink}{red}``, use:

        .. code-block:: python

            set_table_styles([
                {'selector': 'rowcolors', 'props': ':{1}{pink}{red};'}
            ], overwrite=False)

        A more comprehensive example using these arguments is as follows:

        >>> df.columns = pd.MultiIndex.from_tuples([
        ...     (""Numeric"", ""Integers""),
        ...     (""Numeric"", ""Floats""),
        ...     (""Non-Numeric"", ""Strings"")
        ... ])
        >>> df.index = pd.MultiIndex.from_tuples([
        ...     (""L0"", ""ix1""), (""L0"", ""ix2""), (""L1"", ""ix3"")
        ... ])
        >>> s = df.style.highlight_max(
        ...     props='cellcolor:[HTML]{FFFF00}; color:{red}; itshape:; bfseries:;'
        ... )
        >>> s.to_latex(
        ...     column_format=""rrrrr"", position=""h"", position_float=""centering"",
        ...     hrules=True, label=""table:5"", caption=""Styled LaTeX Table"",
        ...     multirow_align=""t"", multicol_align=""r""
        ... )  # doctest: +SKIP

        .. figure:: ../../_static/style/latex_2.png

        **Formatting**

        To format values :meth:`Styler.format` should be used prior to calling
        `Styler.to_latex`, as well as other methods such as :meth:`Styler.hide`
        for example:

        >>> s.clear()
        >>> s.table_styles = []
        >>> s.caption = None
        >>> s.format({
        ...    (""Numeric"", ""Integers""): '\${}',
        ...    (""Numeric"", ""Floats""): '{:.3f}',
        ...    (""Non-Numeric"", ""Strings""): str.upper
        ... })  # doctest: +SKIP
                        Numeric      Non-Numeric
                  Integers   Floats    Strings
        L0    ix1       $1   2.200      DOGS
              ix2       $3   4.400      CATS
        L1    ix3       $2   6.600      COWS

        >>> s.to_latex()  # doctest: +SKIP
        \begin{tabular}{llrrl}
        {} & {} & \multicolumn{2}{r}{Numeric} & {Non-Numeric} \\
        {} & {} & {Integers} & {Floats} & {Strings} \\
        \multirow[c]{2}{*}{L0} & ix1 & \\$1 & 2.200 & DOGS \\
         & ix2 & \$3 & 4.400 & CATS \\
        L1 & ix3 & \$2 & 6.600 & COWS \\
        \end{tabular}

        **CSS Conversion**

        This method can convert a Styler constructured with HTML-CSS to LaTeX using
        the following limited conversions.

        ================== ==================== ============= ==========================
        CSS Attribute      CSS value            LaTeX Command LaTeX Options
        ================== ==================== ============= ==========================
        font-weight        | bold               | bfseries
                           | bolder             | bfseries
        font-style         | italic             | itshape
                           | oblique            | slshape
        background-color   | red                cellcolor     | {red}--lwrap
                           | #fe01ea                          | [HTML]{FE01EA}--lwrap
                           | #f0e                             | [HTML]{FF00EE}--lwrap
                           | rgb(128,255,0)                   | [rgb]{0.5,1,0}--lwrap
                           | rgba(128,0,0,0.5)                | [rgb]{0.5,0,0}--lwrap
                           | rgb(25%,255,50%)                 | [rgb]{0.25,1,0.5}--lwrap
        color              | red                color         | {red}
                           | #fe01ea                          | [HTML]{FE01EA}
                           | #f0e                             | [HTML]{FF00EE}
                           | rgb(128,255,0)                   | [rgb]{0.5,1,0}
                           | rgba(128,0,0,0.5)                | [rgb]{0.5,0,0}
                           | rgb(25%,255,50%)                 | [rgb]{0.25,1,0.5}
        ================== ==================== ============= ==========================

        It is also possible to add user-defined LaTeX only styles to a HTML-CSS Styler
        using the ``--latex`` flag, and to add LaTeX parsing options that the
        converter will detect within a CSS-comment.

        >>> df = pd.DataFrame([[1]])
        >>> df.style.set_properties(
        ...     **{""font-weight"": ""bold /* --dwrap */"", ""Huge"": ""--latex--rwrap""}
        ... ).to_latex(convert_css=True)  # doctest: +SKIP
        \begin{tabular}{lr}
        {} & {0} \\
        0 & {\bfseries}{\Huge{1}} \\
        \end{tabular}

        Examples
        --------
        Below we give a complete step by step example adding some advanced features
        and noting some common gotchas.

        First we create the DataFrame and Styler as usual, including MultiIndex rows
        and columns, which allow for more advanced formatting options:

        >>> cidx = pd.MultiIndex.from_arrays([
        ...     [""Equity"", ""Equity"", ""Equity"", ""Equity"",
        ...      ""Stats"", ""Stats"", ""Stats"", ""Stats"", ""Rating""],
        ...     [""Energy"", ""Energy"", ""Consumer"", ""Consumer"", """", """", """", """", """"],
        ...     [""BP"", ""Shell"", ""H&M"", ""Unilever"",
        ...      ""Std Dev"", ""Variance"", ""52w High"", ""52w Low"", """"]
        ... ])
        >>> iidx = pd.MultiIndex.from_arrays([
        ...     [""Equity"", ""Equity"", ""Equity"", ""Equity""],
        ...     [""Energy"", ""Energy"", ""Consumer"", ""Consumer""],
        ...     [""BP"", ""Shell"", ""H&M"", ""Unilever""]
        ... ])
        >>> styler = pd.DataFrame([
        ...     [1, 0.8, 0.66, 0.72, 32.1678, 32.1678**2, 335.12, 240.89, ""Buy""],
        ...     [0.8, 1.0, 0.69, 0.79, 1.876, 1.876**2, 14.12, 19.78, ""Hold""],
        ...     [0.66, 0.69, 1.0, 0.86, 7, 7**2, 210.9, 140.6, ""Buy""],
        ...     [0.72, 0.79, 0.86, 1.0, 213.76, 213.76**2, 2807, 3678, ""Sell""],
        ... ], columns=cidx, index=iidx).style

        Second we will format the display and, since our table is quite wide, will
        hide the repeated level-0 of the index:

        >>> styler.format(subset=""Equity"", precision=2)
        ...       .format(subset=""Stats"", precision=1, thousands="","")
        ...       .format(subset=""Rating"", formatter=str.upper)
        ...       .format_index(escape=""latex"", axis=1)
        ...       .format_index(escape=""latex"", axis=0)
        ...       .hide(level=0, axis=0)  # doctest: +SKIP

        Note that one of the string entries of the index and column headers is ""H&M"".
        Without applying the `escape=""latex""` option to the `format_index` method the
        resultant LaTeX will fail to render, and the error returned is quite
        difficult to debug. Using the appropriate escape the ""&"" is converted to ""\\&"".

        Thirdly we will apply some (CSS-HTML) styles to our object. We will use a
        builtin method and also define our own method to highlight the stock
        recommendation:

        >>> def rating_color(v):
        ...     if v == ""Buy"": color = ""#33ff85""
        ...     elif v == ""Sell"": color = ""#ff5933""
        ...     else: color = ""#ffdd33""
        ...     return f""color: {color}; font-weight: bold;""
        >>> styler.background_gradient(cmap=""inferno"", subset=""Equity"", vmin=0, vmax=1)
        ...       .applymap(rating_color, subset=""Rating"")  # doctest: +SKIP

        All the above styles will work with HTML (see below) and LaTeX upon conversion:

        .. figure:: ../../_static/style/latex_stocks_html.png

        However, we finally want to add one LaTeX only style
        (from the {graphicx} package), that is not easy to convert from CSS and
        pandas does not support it. Notice the `--latex` flag used here,
        as well as `--rwrap` to ensure this is formatted correctly and
        not ignored upon conversion.

        >>> styler.applymap_index(
        ...     lambda v: ""rotatebox:{45}--rwrap--latex;"", level=2, axis=1
        ... )  # doctest: +SKIP

        Finally we render our LaTeX adding in other options as required:

        >>> styler.to_latex(
        ...     caption=""Selected stock correlation and simple statistics."",
        ...     clines=""skip-last;data"",
        ...     convert_css=True,
        ...     position_float=""centering"",
        ...     multicol_align=""|c|"",
        ...     hrules=True,
        ... )  # doctest: +SKIP
        \begin{table}
        \centering
        \caption{Selected stock correlation and simple statistics.}
        \begin{tabular}{llrrrrrrrrl}
        \toprule
         &  & \multicolumn{4}{|c|}{Equity} & \multicolumn{4}{|c|}{Stats} & Rating \\
         &  & \multicolumn{2}{|c|}{Energy} & \multicolumn{2}{|c|}{Consumer} &
        \multicolumn{4}{|c|}{} &  \\
         &  & \rotatebox{45}{BP} & \rotatebox{45}{Shell} & \rotatebox{45}{H\&M} &
        \rotatebox{45}{Unilever} & \rotatebox{45}{Std Dev} & \rotatebox{45}{Variance} &
        \rotatebox{45}{52w High} & \rotatebox{45}{52w Low} & \rotatebox{45}{} \\
        \midrule
        \multirow[c]{2}{*}{Energy} & BP & {\cellcolor[HTML]{FCFFA4}}
        \color[HTML]{000000} 1.00 & {\cellcolor[HTML]{FCA50A}} \color[HTML]{000000}
        0.80 & {\cellcolor[HTML]{EB6628}} \color[HTML]{F1F1F1} 0.66 &
        {\cellcolor[HTML]{F68013}} \color[HTML]{F1F1F1} 0.72 & 32.2 & 1,034.8 & 335.1
        & 240.9 & \color[HTML]{33FF85} \bfseries BUY \\
         & Shell & {\cellcolor[HTML]{FCA50A}} \color[HTML]{000000} 0.80 &
        {\cellcolor[HTML]{FCFFA4}} \color[HTML]{000000} 1.00 &
        {\cellcolor[HTML]{F1731D}} \color[HTML]{F1F1F1} 0.69 &
        {\cellcolor[HTML]{FCA108}} \color[HTML]{000000} 0.79 & 1.9 & 3.5 & 14.1 &
        19.8 & \color[HTML]{FFDD33} \bfseries HOLD \\
        \cline{1-11}
        \multirow[c]{2}{*}{Consumer} & H\&M & {\cellcolor[HTML]{EB6628}}
        \color[HTML]{F1F1F1} 0.66 & {\cellcolor[HTML]{F1731D}} \color[HTML]{F1F1F1}
        0.69 & {\cellcolor[HTML]{FCFFA4}} \color[HTML]{000000} 1.00 &
        {\cellcolor[HTML]{FAC42A}} \color[HTML]{000000} 0.86 & 7.0 & 49.0 & 210.9 &
        140.6 & \color[HTML]{33FF85} \bfseries BUY \\
         & Unilever & {\cellcolor[HTML]{F68013}} \color[HTML]{F1F1F1} 0.72 &
        {\cellcolor[HTML]{FCA108}} \color[HTML]{000000} 0.79 &
        {\cellcolor[HTML]{FAC42A}} \color[HTML]{000000} 0.86 &
        {\cellcolor[HTML]{FCFFA4}} \color[HTML]{000000} 1.00 & 213.8 & 45,693.3 &
        2,807.0 & 3,678.0 & \color[HTML]{FF5933} \bfseries SELL \\
        \cline{1-11}
        \bottomrule
        \end{tabular}
        \end{table}

        .. figure:: ../../_static/style/latex_stocks.png
        """"""
        obj = self._copy(deepcopy=True)  # manipulate table_styles on obj, not self

        table_selectors = (
            [style[""selector""] for style in self.table_styles]
            if self.table_styles is not None
            else []
        )

        if column_format is not None:
            # add more recent setting to table_styles
            obj.set_table_styles(
                [{""selector"": ""column_format"", ""props"": f"":{column_format}""}],
                overwrite=False,
            )
        elif ""column_format"" in table_selectors:
            pass  # adopt what has been previously set in table_styles
        else:
            # create a default: set float, complex, int cols to 'r' ('S'), index to 'l'
            _original_columns = self.data.columns
            self.data.columns = RangeIndex(stop=len(self.data.columns))
            numeric_cols = self.data._get_numeric_data().columns.to_list()
            self.data.columns = _original_columns
            column_format = """"
            for level in range(self.index.nlevels):
                column_format += """" if self.hide_index_[level] else ""l""
            for ci, _ in enumerate(self.data.columns):
                if ci not in self.hidden_columns:
                    column_format += (
                        (""r"" if not siunitx else ""S"") if ci in numeric_cols else ""l""
                    )
            obj.set_table_styles(
                [{""selector"": ""column_format"", ""props"": f"":{column_format}""}],
                overwrite=False,
            )

        if position:
            obj.set_table_styles(
                [{""selector"": ""position"", ""props"": f"":{position}""}],
                overwrite=False,
            )

        if position_float:
            if environment == ""longtable"":
                raise ValueError(
                    ""`position_float` cannot be used in 'longtable' `environment`""
                )
            if position_float not in [""raggedright"", ""raggedleft"", ""centering""]:
                raise ValueError(
                    f""`position_float` should be one of ""
                    f""'raggedright', 'raggedleft', 'centering', ""
                    f""got: '{position_float}'""
                )
            obj.set_table_styles(
                [{""selector"": ""position_float"", ""props"": f"":{position_float}""}],
                overwrite=False,
            )

        hrules = get_option(""styler.latex.hrules"") if hrules is None else hrules
        if hrules:
            obj.set_table_styles(
                [
                    {""selector"": ""toprule"", ""props"": "":toprule""},
                    {""selector"": ""midrule"", ""props"": "":midrule""},
                    {""selector"": ""bottomrule"", ""props"": "":bottomrule""},
                ],
                overwrite=False,
            )

        if label:
            obj.set_table_styles(
                [{""selector"": ""label"", ""props"": f"":{{{label.replace(':', '')}}}""}],
                overwrite=False,
            )

        if caption:
            obj.set_caption(caption)

        if sparse_index is None:
            sparse_index = get_option(""styler.sparse.index"")
        if sparse_columns is None:
            sparse_columns = get_option(""styler.sparse.columns"")
        environment = environment or get_option(""styler.latex.environment"")
        multicol_align = multicol_align or get_option(""styler.latex.multicol_align"")
        multirow_align = multirow_align or get_option(""styler.latex.multirow_align"")
        latex = obj._render_latex(
            sparse_index=sparse_index,
            sparse_columns=sparse_columns,
            multirow_align=multirow_align,
            multicol_align=multicol_align,
            environment=environment,
            convert_css=convert_css,
            siunitx=siunitx,
            clines=clines,
        )

        encoding = (
            (encoding or get_option(""styler.render.encoding""))
            if isinstance(buf, str)  # i.e. a filepath
            else encoding
        )
        return save_to_buffer(latex, buf=buf, encoding=encoding)",_208.py,469,"obj = self._copy(deepcopy=True)
table_selectors = [style['selector'] for style in self.table_styles] if self.table_styles is not None else []","obj , table_selectors  = self._copy(deepcopy=True), [style['selector'] for style in self.table_styles] if self.table_styles is not None else []"
https://github.com/pandas-dev/pandas/tree/master/pandas/io/formats/style.py,"def to_latex(
        self,
        buf: FilePath | WriteBuffer[str] | None = None,
        *,
        column_format: str | None = None,
        position: str | None = None,
        position_float: str | None = None,
        hrules: bool | None = None,
        clines: str | None = None,
        label: str | None = None,
        caption: str | tuple | None = None,
        sparse_index: bool | None = None,
        sparse_columns: bool | None = None,
        multirow_align: str | None = None,
        multicol_align: str | None = None,
        siunitx: bool = False,
        environment: str | None = None,
        encoding: str | None = None,
        convert_css: bool = False,
    ) -> str | None:
        r""""""
        Write Styler to a file, buffer or string in LaTeX format.

        .. versionadded:: 1.3.0

        Parameters
        ----------
        buf : str, path object, file-like object, or None, default None
            String, path object (implementing ``os.PathLike[str]``), or file-like
            object implementing a string ``write()`` function. If None, the result is
            returned as a string.
        column_format : str, optional
            The LaTeX column specification placed in location:

            \\begin{tabular}{<column_format>}

            Defaults to 'l' for index and
            non-numeric data columns, and, for numeric data columns,
            to 'r' by default, or 'S' if ``siunitx`` is ``True``.
        position : str, optional
            The LaTeX positional argument (e.g. 'h!') for tables, placed in location:

            ``\\begin{table}[<position>]``.
        position_float : {""centering"", ""raggedleft"", ""raggedright""}, optional
            The LaTeX float command placed in location:

            \\begin{table}[<position>]

            \\<position_float>

            Cannot be used if ``environment`` is ""longtable"".
        hrules : bool
            Set to `True` to add \\toprule, \\midrule and \\bottomrule from the
            {booktabs} LaTeX package.
            Defaults to ``pandas.options.styler.latex.hrules``, which is `False`.

            .. versionchanged:: 1.4.0
        clines : str, optional
            Use to control adding \\cline commands for the index labels separation.
            Possible values are:

              - `None`: no cline commands are added (default).
              - `""all;data""`: a cline is added for every index value extending the
                width of the table, including data entries.
              - `""all;index""`: as above with lines extending only the width of the
                index entries.
              - `""skip-last;data""`: a cline is added for each index value except the
                last level (which is never sparsified), extending the widtn of the
                table.
              - `""skip-last;index""`: as above with lines extending only the width of the
                index entries.

            .. versionadded:: 1.4.0
        label : str, optional
            The LaTeX label included as: \\label{<label>}.
            This is used with \\ref{<label>} in the main .tex file.
        caption : str, tuple, optional
            If string, the LaTeX table caption included as: \\caption{<caption>}.
            If tuple, i.e (""full caption"", ""short caption""), the caption included
            as: \\caption[<caption[1]>]{<caption[0]>}.
        sparse_index : bool, optional
            Whether to sparsify the display of a hierarchical index. Setting to False
            will display each explicit level element in a hierarchical key for each row.
            Defaults to ``pandas.options.styler.sparse.index``, which is `True`.
        sparse_columns : bool, optional
            Whether to sparsify the display of a hierarchical index. Setting to False
            will display each explicit level element in a hierarchical key for each
            column. Defaults to ``pandas.options.styler.sparse.columns``, which
            is `True`.
        multirow_align : {""c"", ""t"", ""b"", ""naive""}, optional
            If sparsifying hierarchical MultiIndexes whether to align text centrally,
            at the top or bottom using the multirow package. If not given defaults to
            ``pandas.options.styler.latex.multirow_align``, which is `""c""`.
            If ""naive"" is given renders without multirow.

            .. versionchanged:: 1.4.0
        multicol_align : {""r"", ""c"", ""l"", ""naive-l"", ""naive-r""}, optional
            If sparsifying hierarchical MultiIndex columns whether to align text at
            the left, centrally, or at the right. If not given defaults to
            ``pandas.options.styler.latex.multicol_align``, which is ""r"".
            If a naive option is given renders without multicol.
            Pipe decorators can also be added to non-naive values to draw vertical
            rules, e.g. ""\|r"" will draw a rule on the left side of right aligned merged
            cells.

            .. versionchanged:: 1.4.0
        siunitx : bool, default False
            Set to ``True`` to structure LaTeX compatible with the {siunitx} package.
        environment : str, optional
            If given, the environment that will replace 'table' in ``\\begin{table}``.
            If 'longtable' is specified then a more suitable template is
            rendered. If not given defaults to
            ``pandas.options.styler.latex.environment``, which is `None`.

            .. versionadded:: 1.4.0
        encoding : str, optional
            Character encoding setting. Defaults
            to ``pandas.options.styler.render.encoding``, which is ""utf-8"".
        convert_css : bool, default False
            Convert simple cell-styles from CSS to LaTeX format. Any CSS not found in
            conversion table is dropped. A style can be forced by adding option
            `--latex`. See notes.

        Returns
        -------
        str or None
            If `buf` is None, returns the result as a string. Otherwise returns `None`.

        See Also
        --------
        Styler.format: Format the text display value of cells.

        Notes
        -----
        **Latex Packages**

        For the following features we recommend the following LaTeX inclusions:

        ===================== ==========================================================
        Feature               Inclusion
        ===================== ==========================================================
        sparse columns        none: included within default {tabular} environment
        sparse rows           \\usepackage{multirow}
        hrules                \\usepackage{booktabs}
        colors                \\usepackage[table]{xcolor}
        siunitx               \\usepackage{siunitx}
        bold (with siunitx)   | \\usepackage{etoolbox}
                              | \\robustify\\bfseries
                              | \\sisetup{detect-all = true}  *(within {document})*
        italic (with siunitx) | \\usepackage{etoolbox}
                              | \\robustify\\itshape
                              | \\sisetup{detect-all = true}  *(within {document})*
        environment           \\usepackage{longtable} if arg is ""longtable""
                              | or any other relevant environment package
        hyperlinks            \\usepackage{hyperref}
        ===================== ==========================================================

        **Cell Styles**

        LaTeX styling can only be rendered if the accompanying styling functions have
        been constructed with appropriate LaTeX commands. All styling
        functionality is built around the concept of a CSS ``(<attribute>, <value>)``
        pair (see `Table Visualization <../../user_guide/style.ipynb>`_), and this
        should be replaced by a LaTeX
        ``(<command>, <options>)`` approach. Each cell will be styled individually
        using nested LaTeX commands with their accompanied options.

        For example the following code will highlight and bold a cell in HTML-CSS:

        >>> df = pd.DataFrame([[1,2], [3,4]])
        >>> s = df.style.highlight_max(axis=None,
        ...                            props='background-color:red; font-weight:bold;')
        >>> s.to_html()  # doctest: +SKIP

        The equivalent using LaTeX only commands is the following:

        >>> s = df.style.highlight_max(axis=None,
        ...                            props='cellcolor:{red}; bfseries: ;')
        >>> s.to_latex()  # doctest: +SKIP

        Internally these structured LaTeX ``(<command>, <options>)`` pairs
        are translated to the
        ``display_value`` with the default structure:
        ``\<command><options> <display_value>``.
        Where there are multiple commands the latter is nested recursively, so that
        the above example highlighted cell is rendered as
        ``\cellcolor{red} \bfseries 4``.

        Occasionally this format does not suit the applied command, or
        combination of LaTeX packages that is in use, so additional flags can be
        added to the ``<options>``, within the tuple, to result in different
        positions of required braces (the **default** being the same as ``--nowrap``):

        =================================== ============================================
        Tuple Format                           Output Structure
        =================================== ============================================
        (<command>,<options>)               \\<command><options> <display_value>
        (<command>,<options> ``--nowrap``)  \\<command><options> <display_value>
        (<command>,<options> ``--rwrap``)   \\<command><options>{<display_value>}
        (<command>,<options> ``--wrap``)    {\\<command><options> <display_value>}
        (<command>,<options> ``--lwrap``)   {\\<command><options>} <display_value>
        (<command>,<options> ``--dwrap``)   {\\<command><options>}{<display_value>}
        =================================== ============================================

        For example the `textbf` command for font-weight
        should always be used with `--rwrap` so ``('textbf', '--rwrap')`` will render a
        working cell, wrapped with braces, as ``\textbf{<display_value>}``.

        A more comprehensive example is as follows:

        >>> df = pd.DataFrame([[1, 2.2, ""dogs""], [3, 4.4, ""cats""], [2, 6.6, ""cows""]],
        ...                   index=[""ix1"", ""ix2"", ""ix3""],
        ...                   columns=[""Integers"", ""Floats"", ""Strings""])
        >>> s = df.style.highlight_max(
        ...     props='cellcolor:[HTML]{FFFF00}; color:{red};'
        ...           'textit:--rwrap; textbf:--rwrap;'
        ... )
        >>> s.to_latex()  # doctest: +SKIP

        .. figure:: ../../_static/style/latex_1.png

        **Table Styles**

        Internally Styler uses its ``table_styles`` object to parse the
        ``column_format``, ``position``, ``position_float``, and ``label``
        input arguments. These arguments are added to table styles in the format:

        .. code-block:: python

            set_table_styles([
                {""selector"": ""column_format"", ""props"": f"":{column_format};""},
                {""selector"": ""position"", ""props"": f"":{position};""},
                {""selector"": ""position_float"", ""props"": f"":{position_float};""},
                {""selector"": ""label"", ""props"": f"":{{{label.replace(':','')}}};""}
            ], overwrite=False)

        Exception is made for the ``hrules`` argument which, in fact, controls all three
        commands: ``toprule``, ``bottomrule`` and ``midrule`` simultaneously. Instead of
        setting ``hrules`` to ``True``, it is also possible to set each
        individual rule definition, by manually setting the ``table_styles``,
        for example below we set a regular ``toprule``, set an ``hline`` for
        ``bottomrule`` and exclude the ``midrule``:

        .. code-block:: python

            set_table_styles([
                {'selector': 'toprule', 'props': ':toprule;'},
                {'selector': 'bottomrule', 'props': ':hline;'},
            ], overwrite=False)

        If other ``commands`` are added to table styles they will be detected, and
        positioned immediately above the '\\begin{tabular}' command. For example to
        add odd and even row coloring, from the {colortbl} package, in format
        ``\rowcolors{1}{pink}{red}``, use:

        .. code-block:: python

            set_table_styles([
                {'selector': 'rowcolors', 'props': ':{1}{pink}{red};'}
            ], overwrite=False)

        A more comprehensive example using these arguments is as follows:

        >>> df.columns = pd.MultiIndex.from_tuples([
        ...     (""Numeric"", ""Integers""),
        ...     (""Numeric"", ""Floats""),
        ...     (""Non-Numeric"", ""Strings"")
        ... ])
        >>> df.index = pd.MultiIndex.from_tuples([
        ...     (""L0"", ""ix1""), (""L0"", ""ix2""), (""L1"", ""ix3"")
        ... ])
        >>> s = df.style.highlight_max(
        ...     props='cellcolor:[HTML]{FFFF00}; color:{red}; itshape:; bfseries:;'
        ... )
        >>> s.to_latex(
        ...     column_format=""rrrrr"", position=""h"", position_float=""centering"",
        ...     hrules=True, label=""table:5"", caption=""Styled LaTeX Table"",
        ...     multirow_align=""t"", multicol_align=""r""
        ... )  # doctest: +SKIP

        .. figure:: ../../_static/style/latex_2.png

        **Formatting**

        To format values :meth:`Styler.format` should be used prior to calling
        `Styler.to_latex`, as well as other methods such as :meth:`Styler.hide`
        for example:

        >>> s.clear()
        >>> s.table_styles = []
        >>> s.caption = None
        >>> s.format({
        ...    (""Numeric"", ""Integers""): '\${}',
        ...    (""Numeric"", ""Floats""): '{:.3f}',
        ...    (""Non-Numeric"", ""Strings""): str.upper
        ... })  # doctest: +SKIP
                        Numeric      Non-Numeric
                  Integers   Floats    Strings
        L0    ix1       $1   2.200      DOGS
              ix2       $3   4.400      CATS
        L1    ix3       $2   6.600      COWS

        >>> s.to_latex()  # doctest: +SKIP
        \begin{tabular}{llrrl}
        {} & {} & \multicolumn{2}{r}{Numeric} & {Non-Numeric} \\
        {} & {} & {Integers} & {Floats} & {Strings} \\
        \multirow[c]{2}{*}{L0} & ix1 & \\$1 & 2.200 & DOGS \\
         & ix2 & \$3 & 4.400 & CATS \\
        L1 & ix3 & \$2 & 6.600 & COWS \\
        \end{tabular}

        **CSS Conversion**

        This method can convert a Styler constructured with HTML-CSS to LaTeX using
        the following limited conversions.

        ================== ==================== ============= ==========================
        CSS Attribute      CSS value            LaTeX Command LaTeX Options
        ================== ==================== ============= ==========================
        font-weight        | bold               | bfseries
                           | bolder             | bfseries
        font-style         | italic             | itshape
                           | oblique            | slshape
        background-color   | red                cellcolor     | {red}--lwrap
                           | #fe01ea                          | [HTML]{FE01EA}--lwrap
                           | #f0e                             | [HTML]{FF00EE}--lwrap
                           | rgb(128,255,0)                   | [rgb]{0.5,1,0}--lwrap
                           | rgba(128,0,0,0.5)                | [rgb]{0.5,0,0}--lwrap
                           | rgb(25%,255,50%)                 | [rgb]{0.25,1,0.5}--lwrap
        color              | red                color         | {red}
                           | #fe01ea                          | [HTML]{FE01EA}
                           | #f0e                             | [HTML]{FF00EE}
                           | rgb(128,255,0)                   | [rgb]{0.5,1,0}
                           | rgba(128,0,0,0.5)                | [rgb]{0.5,0,0}
                           | rgb(25%,255,50%)                 | [rgb]{0.25,1,0.5}
        ================== ==================== ============= ==========================

        It is also possible to add user-defined LaTeX only styles to a HTML-CSS Styler
        using the ``--latex`` flag, and to add LaTeX parsing options that the
        converter will detect within a CSS-comment.

        >>> df = pd.DataFrame([[1]])
        >>> df.style.set_properties(
        ...     **{""font-weight"": ""bold /* --dwrap */"", ""Huge"": ""--latex--rwrap""}
        ... ).to_latex(convert_css=True)  # doctest: +SKIP
        \begin{tabular}{lr}
        {} & {0} \\
        0 & {\bfseries}{\Huge{1}} \\
        \end{tabular}

        Examples
        --------
        Below we give a complete step by step example adding some advanced features
        and noting some common gotchas.

        First we create the DataFrame and Styler as usual, including MultiIndex rows
        and columns, which allow for more advanced formatting options:

        >>> cidx = pd.MultiIndex.from_arrays([
        ...     [""Equity"", ""Equity"", ""Equity"", ""Equity"",
        ...      ""Stats"", ""Stats"", ""Stats"", ""Stats"", ""Rating""],
        ...     [""Energy"", ""Energy"", ""Consumer"", ""Consumer"", """", """", """", """", """"],
        ...     [""BP"", ""Shell"", ""H&M"", ""Unilever"",
        ...      ""Std Dev"", ""Variance"", ""52w High"", ""52w Low"", """"]
        ... ])
        >>> iidx = pd.MultiIndex.from_arrays([
        ...     [""Equity"", ""Equity"", ""Equity"", ""Equity""],
        ...     [""Energy"", ""Energy"", ""Consumer"", ""Consumer""],
        ...     [""BP"", ""Shell"", ""H&M"", ""Unilever""]
        ... ])
        >>> styler = pd.DataFrame([
        ...     [1, 0.8, 0.66, 0.72, 32.1678, 32.1678**2, 335.12, 240.89, ""Buy""],
        ...     [0.8, 1.0, 0.69, 0.79, 1.876, 1.876**2, 14.12, 19.78, ""Hold""],
        ...     [0.66, 0.69, 1.0, 0.86, 7, 7**2, 210.9, 140.6, ""Buy""],
        ...     [0.72, 0.79, 0.86, 1.0, 213.76, 213.76**2, 2807, 3678, ""Sell""],
        ... ], columns=cidx, index=iidx).style

        Second we will format the display and, since our table is quite wide, will
        hide the repeated level-0 of the index:

        >>> styler.format(subset=""Equity"", precision=2)
        ...       .format(subset=""Stats"", precision=1, thousands="","")
        ...       .format(subset=""Rating"", formatter=str.upper)
        ...       .format_index(escape=""latex"", axis=1)
        ...       .format_index(escape=""latex"", axis=0)
        ...       .hide(level=0, axis=0)  # doctest: +SKIP

        Note that one of the string entries of the index and column headers is ""H&M"".
        Without applying the `escape=""latex""` option to the `format_index` method the
        resultant LaTeX will fail to render, and the error returned is quite
        difficult to debug. Using the appropriate escape the ""&"" is converted to ""\\&"".

        Thirdly we will apply some (CSS-HTML) styles to our object. We will use a
        builtin method and also define our own method to highlight the stock
        recommendation:

        >>> def rating_color(v):
        ...     if v == ""Buy"": color = ""#33ff85""
        ...     elif v == ""Sell"": color = ""#ff5933""
        ...     else: color = ""#ffdd33""
        ...     return f""color: {color}; font-weight: bold;""
        >>> styler.background_gradient(cmap=""inferno"", subset=""Equity"", vmin=0, vmax=1)
        ...       .applymap(rating_color, subset=""Rating"")  # doctest: +SKIP

        All the above styles will work with HTML (see below) and LaTeX upon conversion:

        .. figure:: ../../_static/style/latex_stocks_html.png

        However, we finally want to add one LaTeX only style
        (from the {graphicx} package), that is not easy to convert from CSS and
        pandas does not support it. Notice the `--latex` flag used here,
        as well as `--rwrap` to ensure this is formatted correctly and
        not ignored upon conversion.

        >>> styler.applymap_index(
        ...     lambda v: ""rotatebox:{45}--rwrap--latex;"", level=2, axis=1
        ... )  # doctest: +SKIP

        Finally we render our LaTeX adding in other options as required:

        >>> styler.to_latex(
        ...     caption=""Selected stock correlation and simple statistics."",
        ...     clines=""skip-last;data"",
        ...     convert_css=True,
        ...     position_float=""centering"",
        ...     multicol_align=""|c|"",
        ...     hrules=True,
        ... )  # doctest: +SKIP
        \begin{table}
        \centering
        \caption{Selected stock correlation and simple statistics.}
        \begin{tabular}{llrrrrrrrrl}
        \toprule
         &  & \multicolumn{4}{|c|}{Equity} & \multicolumn{4}{|c|}{Stats} & Rating \\
         &  & \multicolumn{2}{|c|}{Energy} & \multicolumn{2}{|c|}{Consumer} &
        \multicolumn{4}{|c|}{} &  \\
         &  & \rotatebox{45}{BP} & \rotatebox{45}{Shell} & \rotatebox{45}{H\&M} &
        \rotatebox{45}{Unilever} & \rotatebox{45}{Std Dev} & \rotatebox{45}{Variance} &
        \rotatebox{45}{52w High} & \rotatebox{45}{52w Low} & \rotatebox{45}{} \\
        \midrule
        \multirow[c]{2}{*}{Energy} & BP & {\cellcolor[HTML]{FCFFA4}}
        \color[HTML]{000000} 1.00 & {\cellcolor[HTML]{FCA50A}} \color[HTML]{000000}
        0.80 & {\cellcolor[HTML]{EB6628}} \color[HTML]{F1F1F1} 0.66 &
        {\cellcolor[HTML]{F68013}} \color[HTML]{F1F1F1} 0.72 & 32.2 & 1,034.8 & 335.1
        & 240.9 & \color[HTML]{33FF85} \bfseries BUY \\
         & Shell & {\cellcolor[HTML]{FCA50A}} \color[HTML]{000000} 0.80 &
        {\cellcolor[HTML]{FCFFA4}} \color[HTML]{000000} 1.00 &
        {\cellcolor[HTML]{F1731D}} \color[HTML]{F1F1F1} 0.69 &
        {\cellcolor[HTML]{FCA108}} \color[HTML]{000000} 0.79 & 1.9 & 3.5 & 14.1 &
        19.8 & \color[HTML]{FFDD33} \bfseries HOLD \\
        \cline{1-11}
        \multirow[c]{2}{*}{Consumer} & H\&M & {\cellcolor[HTML]{EB6628}}
        \color[HTML]{F1F1F1} 0.66 & {\cellcolor[HTML]{F1731D}} \color[HTML]{F1F1F1}
        0.69 & {\cellcolor[HTML]{FCFFA4}} \color[HTML]{000000} 1.00 &
        {\cellcolor[HTML]{FAC42A}} \color[HTML]{000000} 0.86 & 7.0 & 49.0 & 210.9 &
        140.6 & \color[HTML]{33FF85} \bfseries BUY \\
         & Unilever & {\cellcolor[HTML]{F68013}} \color[HTML]{F1F1F1} 0.72 &
        {\cellcolor[HTML]{FCA108}} \color[HTML]{000000} 0.79 &
        {\cellcolor[HTML]{FAC42A}} \color[HTML]{000000} 0.86 &
        {\cellcolor[HTML]{FCFFA4}} \color[HTML]{000000} 1.00 & 213.8 & 45,693.3 &
        2,807.0 & 3,678.0 & \color[HTML]{FF5933} \bfseries SELL \\
        \cline{1-11}
        \bottomrule
        \end{tabular}
        \end{table}

        .. figure:: ../../_static/style/latex_stocks.png
        """"""
        obj = self._copy(deepcopy=True)  # manipulate table_styles on obj, not self

        table_selectors = (
            [style[""selector""] for style in self.table_styles]
            if self.table_styles is not None
            else []
        )

        if column_format is not None:
            # add more recent setting to table_styles
            obj.set_table_styles(
                [{""selector"": ""column_format"", ""props"": f"":{column_format}""}],
                overwrite=False,
            )
        elif ""column_format"" in table_selectors:
            pass  # adopt what has been previously set in table_styles
        else:
            # create a default: set float, complex, int cols to 'r' ('S'), index to 'l'
            _original_columns = self.data.columns
            self.data.columns = RangeIndex(stop=len(self.data.columns))
            numeric_cols = self.data._get_numeric_data().columns.to_list()
            self.data.columns = _original_columns
            column_format = """"
            for level in range(self.index.nlevels):
                column_format += """" if self.hide_index_[level] else ""l""
            for ci, _ in enumerate(self.data.columns):
                if ci not in self.hidden_columns:
                    column_format += (
                        (""r"" if not siunitx else ""S"") if ci in numeric_cols else ""l""
                    )
            obj.set_table_styles(
                [{""selector"": ""column_format"", ""props"": f"":{column_format}""}],
                overwrite=False,
            )

        if position:
            obj.set_table_styles(
                [{""selector"": ""position"", ""props"": f"":{position}""}],
                overwrite=False,
            )

        if position_float:
            if environment == ""longtable"":
                raise ValueError(
                    ""`position_float` cannot be used in 'longtable' `environment`""
                )
            if position_float not in [""raggedright"", ""raggedleft"", ""centering""]:
                raise ValueError(
                    f""`position_float` should be one of ""
                    f""'raggedright', 'raggedleft', 'centering', ""
                    f""got: '{position_float}'""
                )
            obj.set_table_styles(
                [{""selector"": ""position_float"", ""props"": f"":{position_float}""}],
                overwrite=False,
            )

        hrules = get_option(""styler.latex.hrules"") if hrules is None else hrules
        if hrules:
            obj.set_table_styles(
                [
                    {""selector"": ""toprule"", ""props"": "":toprule""},
                    {""selector"": ""midrule"", ""props"": "":midrule""},
                    {""selector"": ""bottomrule"", ""props"": "":bottomrule""},
                ],
                overwrite=False,
            )

        if label:
            obj.set_table_styles(
                [{""selector"": ""label"", ""props"": f"":{{{label.replace(':', '')}}}""}],
                overwrite=False,
            )

        if caption:
            obj.set_caption(caption)

        if sparse_index is None:
            sparse_index = get_option(""styler.sparse.index"")
        if sparse_columns is None:
            sparse_columns = get_option(""styler.sparse.columns"")
        environment = environment or get_option(""styler.latex.environment"")
        multicol_align = multicol_align or get_option(""styler.latex.multicol_align"")
        multirow_align = multirow_align or get_option(""styler.latex.multirow_align"")
        latex = obj._render_latex(
            sparse_index=sparse_index,
            sparse_columns=sparse_columns,
            multirow_align=multirow_align,
            multicol_align=multicol_align,
            environment=environment,
            convert_css=convert_css,
            siunitx=siunitx,
            clines=clines,
        )

        encoding = (
            (encoding or get_option(""styler.render.encoding""))
            if isinstance(buf, str)  # i.e. a filepath
            else encoding
        )
        return save_to_buffer(latex, buf=buf, encoding=encoding)",_208.py,550,"environment = environment or get_option('styler.latex.environment')
multicol_align = multicol_align or get_option('styler.latex.multicol_align')
multirow_align = multirow_align or get_option('styler.latex.multirow_align')","environment , multicol_align , multirow_align  = environment or get_option('styler.latex.environment'), multicol_align or get_option('styler.latex.multicol_align'), multirow_align or get_option('styler.latex.multirow_align')"
https://github.com/pandas-dev/pandas/tree/master/pandas/io/formats/style.py,"def to_latex(
        self,
        buf: FilePath | WriteBuffer[str] | None = None,
        *,
        column_format: str | None = None,
        position: str | None = None,
        position_float: str | None = None,
        hrules: bool | None = None,
        clines: str | None = None,
        label: str | None = None,
        caption: str | tuple | None = None,
        sparse_index: bool | None = None,
        sparse_columns: bool | None = None,
        multirow_align: str | None = None,
        multicol_align: str | None = None,
        siunitx: bool = False,
        environment: str | None = None,
        encoding: str | None = None,
        convert_css: bool = False,
    ) -> str | None:
        r""""""
        Write Styler to a file, buffer or string in LaTeX format.

        .. versionadded:: 1.3.0

        Parameters
        ----------
        buf : str, path object, file-like object, or None, default None
            String, path object (implementing ``os.PathLike[str]``), or file-like
            object implementing a string ``write()`` function. If None, the result is
            returned as a string.
        column_format : str, optional
            The LaTeX column specification placed in location:

            \\begin{tabular}{<column_format>}

            Defaults to 'l' for index and
            non-numeric data columns, and, for numeric data columns,
            to 'r' by default, or 'S' if ``siunitx`` is ``True``.
        position : str, optional
            The LaTeX positional argument (e.g. 'h!') for tables, placed in location:

            ``\\begin{table}[<position>]``.
        position_float : {""centering"", ""raggedleft"", ""raggedright""}, optional
            The LaTeX float command placed in location:

            \\begin{table}[<position>]

            \\<position_float>

            Cannot be used if ``environment`` is ""longtable"".
        hrules : bool
            Set to `True` to add \\toprule, \\midrule and \\bottomrule from the
            {booktabs} LaTeX package.
            Defaults to ``pandas.options.styler.latex.hrules``, which is `False`.

            .. versionchanged:: 1.4.0
        clines : str, optional
            Use to control adding \\cline commands for the index labels separation.
            Possible values are:

              - `None`: no cline commands are added (default).
              - `""all;data""`: a cline is added for every index value extending the
                width of the table, including data entries.
              - `""all;index""`: as above with lines extending only the width of the
                index entries.
              - `""skip-last;data""`: a cline is added for each index value except the
                last level (which is never sparsified), extending the widtn of the
                table.
              - `""skip-last;index""`: as above with lines extending only the width of the
                index entries.

            .. versionadded:: 1.4.0
        label : str, optional
            The LaTeX label included as: \\label{<label>}.
            This is used with \\ref{<label>} in the main .tex file.
        caption : str, tuple, optional
            If string, the LaTeX table caption included as: \\caption{<caption>}.
            If tuple, i.e (""full caption"", ""short caption""), the caption included
            as: \\caption[<caption[1]>]{<caption[0]>}.
        sparse_index : bool, optional
            Whether to sparsify the display of a hierarchical index. Setting to False
            will display each explicit level element in a hierarchical key for each row.
            Defaults to ``pandas.options.styler.sparse.index``, which is `True`.
        sparse_columns : bool, optional
            Whether to sparsify the display of a hierarchical index. Setting to False
            will display each explicit level element in a hierarchical key for each
            column. Defaults to ``pandas.options.styler.sparse.columns``, which
            is `True`.
        multirow_align : {""c"", ""t"", ""b"", ""naive""}, optional
            If sparsifying hierarchical MultiIndexes whether to align text centrally,
            at the top or bottom using the multirow package. If not given defaults to
            ``pandas.options.styler.latex.multirow_align``, which is `""c""`.
            If ""naive"" is given renders without multirow.

            .. versionchanged:: 1.4.0
        multicol_align : {""r"", ""c"", ""l"", ""naive-l"", ""naive-r""}, optional
            If sparsifying hierarchical MultiIndex columns whether to align text at
            the left, centrally, or at the right. If not given defaults to
            ``pandas.options.styler.latex.multicol_align``, which is ""r"".
            If a naive option is given renders without multicol.
            Pipe decorators can also be added to non-naive values to draw vertical
            rules, e.g. ""\|r"" will draw a rule on the left side of right aligned merged
            cells.

            .. versionchanged:: 1.4.0
        siunitx : bool, default False
            Set to ``True`` to structure LaTeX compatible with the {siunitx} package.
        environment : str, optional
            If given, the environment that will replace 'table' in ``\\begin{table}``.
            If 'longtable' is specified then a more suitable template is
            rendered. If not given defaults to
            ``pandas.options.styler.latex.environment``, which is `None`.

            .. versionadded:: 1.4.0
        encoding : str, optional
            Character encoding setting. Defaults
            to ``pandas.options.styler.render.encoding``, which is ""utf-8"".
        convert_css : bool, default False
            Convert simple cell-styles from CSS to LaTeX format. Any CSS not found in
            conversion table is dropped. A style can be forced by adding option
            `--latex`. See notes.

        Returns
        -------
        str or None
            If `buf` is None, returns the result as a string. Otherwise returns `None`.

        See Also
        --------
        Styler.format: Format the text display value of cells.

        Notes
        -----
        **Latex Packages**

        For the following features we recommend the following LaTeX inclusions:

        ===================== ==========================================================
        Feature               Inclusion
        ===================== ==========================================================
        sparse columns        none: included within default {tabular} environment
        sparse rows           \\usepackage{multirow}
        hrules                \\usepackage{booktabs}
        colors                \\usepackage[table]{xcolor}
        siunitx               \\usepackage{siunitx}
        bold (with siunitx)   | \\usepackage{etoolbox}
                              | \\robustify\\bfseries
                              | \\sisetup{detect-all = true}  *(within {document})*
        italic (with siunitx) | \\usepackage{etoolbox}
                              | \\robustify\\itshape
                              | \\sisetup{detect-all = true}  *(within {document})*
        environment           \\usepackage{longtable} if arg is ""longtable""
                              | or any other relevant environment package
        hyperlinks            \\usepackage{hyperref}
        ===================== ==========================================================

        **Cell Styles**

        LaTeX styling can only be rendered if the accompanying styling functions have
        been constructed with appropriate LaTeX commands. All styling
        functionality is built around the concept of a CSS ``(<attribute>, <value>)``
        pair (see `Table Visualization <../../user_guide/style.ipynb>`_), and this
        should be replaced by a LaTeX
        ``(<command>, <options>)`` approach. Each cell will be styled individually
        using nested LaTeX commands with their accompanied options.

        For example the following code will highlight and bold a cell in HTML-CSS:

        >>> df = pd.DataFrame([[1,2], [3,4]])
        >>> s = df.style.highlight_max(axis=None,
        ...                            props='background-color:red; font-weight:bold;')
        >>> s.to_html()  # doctest: +SKIP

        The equivalent using LaTeX only commands is the following:

        >>> s = df.style.highlight_max(axis=None,
        ...                            props='cellcolor:{red}; bfseries: ;')
        >>> s.to_latex()  # doctest: +SKIP

        Internally these structured LaTeX ``(<command>, <options>)`` pairs
        are translated to the
        ``display_value`` with the default structure:
        ``\<command><options> <display_value>``.
        Where there are multiple commands the latter is nested recursively, so that
        the above example highlighted cell is rendered as
        ``\cellcolor{red} \bfseries 4``.

        Occasionally this format does not suit the applied command, or
        combination of LaTeX packages that is in use, so additional flags can be
        added to the ``<options>``, within the tuple, to result in different
        positions of required braces (the **default** being the same as ``--nowrap``):

        =================================== ============================================
        Tuple Format                           Output Structure
        =================================== ============================================
        (<command>,<options>)               \\<command><options> <display_value>
        (<command>,<options> ``--nowrap``)  \\<command><options> <display_value>
        (<command>,<options> ``--rwrap``)   \\<command><options>{<display_value>}
        (<command>,<options> ``--wrap``)    {\\<command><options> <display_value>}
        (<command>,<options> ``--lwrap``)   {\\<command><options>} <display_value>
        (<command>,<options> ``--dwrap``)   {\\<command><options>}{<display_value>}
        =================================== ============================================

        For example the `textbf` command for font-weight
        should always be used with `--rwrap` so ``('textbf', '--rwrap')`` will render a
        working cell, wrapped with braces, as ``\textbf{<display_value>}``.

        A more comprehensive example is as follows:

        >>> df = pd.DataFrame([[1, 2.2, ""dogs""], [3, 4.4, ""cats""], [2, 6.6, ""cows""]],
        ...                   index=[""ix1"", ""ix2"", ""ix3""],
        ...                   columns=[""Integers"", ""Floats"", ""Strings""])
        >>> s = df.style.highlight_max(
        ...     props='cellcolor:[HTML]{FFFF00}; color:{red};'
        ...           'textit:--rwrap; textbf:--rwrap;'
        ... )
        >>> s.to_latex()  # doctest: +SKIP

        .. figure:: ../../_static/style/latex_1.png

        **Table Styles**

        Internally Styler uses its ``table_styles`` object to parse the
        ``column_format``, ``position``, ``position_float``, and ``label``
        input arguments. These arguments are added to table styles in the format:

        .. code-block:: python

            set_table_styles([
                {""selector"": ""column_format"", ""props"": f"":{column_format};""},
                {""selector"": ""position"", ""props"": f"":{position};""},
                {""selector"": ""position_float"", ""props"": f"":{position_float};""},
                {""selector"": ""label"", ""props"": f"":{{{label.replace(':','')}}};""}
            ], overwrite=False)

        Exception is made for the ``hrules`` argument which, in fact, controls all three
        commands: ``toprule``, ``bottomrule`` and ``midrule`` simultaneously. Instead of
        setting ``hrules`` to ``True``, it is also possible to set each
        individual rule definition, by manually setting the ``table_styles``,
        for example below we set a regular ``toprule``, set an ``hline`` for
        ``bottomrule`` and exclude the ``midrule``:

        .. code-block:: python

            set_table_styles([
                {'selector': 'toprule', 'props': ':toprule;'},
                {'selector': 'bottomrule', 'props': ':hline;'},
            ], overwrite=False)

        If other ``commands`` are added to table styles they will be detected, and
        positioned immediately above the '\\begin{tabular}' command. For example to
        add odd and even row coloring, from the {colortbl} package, in format
        ``\rowcolors{1}{pink}{red}``, use:

        .. code-block:: python

            set_table_styles([
                {'selector': 'rowcolors', 'props': ':{1}{pink}{red};'}
            ], overwrite=False)

        A more comprehensive example using these arguments is as follows:

        >>> df.columns = pd.MultiIndex.from_tuples([
        ...     (""Numeric"", ""Integers""),
        ...     (""Numeric"", ""Floats""),
        ...     (""Non-Numeric"", ""Strings"")
        ... ])
        >>> df.index = pd.MultiIndex.from_tuples([
        ...     (""L0"", ""ix1""), (""L0"", ""ix2""), (""L1"", ""ix3"")
        ... ])
        >>> s = df.style.highlight_max(
        ...     props='cellcolor:[HTML]{FFFF00}; color:{red}; itshape:; bfseries:;'
        ... )
        >>> s.to_latex(
        ...     column_format=""rrrrr"", position=""h"", position_float=""centering"",
        ...     hrules=True, label=""table:5"", caption=""Styled LaTeX Table"",
        ...     multirow_align=""t"", multicol_align=""r""
        ... )  # doctest: +SKIP

        .. figure:: ../../_static/style/latex_2.png

        **Formatting**

        To format values :meth:`Styler.format` should be used prior to calling
        `Styler.to_latex`, as well as other methods such as :meth:`Styler.hide`
        for example:

        >>> s.clear()
        >>> s.table_styles = []
        >>> s.caption = None
        >>> s.format({
        ...    (""Numeric"", ""Integers""): '\${}',
        ...    (""Numeric"", ""Floats""): '{:.3f}',
        ...    (""Non-Numeric"", ""Strings""): str.upper
        ... })  # doctest: +SKIP
                        Numeric      Non-Numeric
                  Integers   Floats    Strings
        L0    ix1       $1   2.200      DOGS
              ix2       $3   4.400      CATS
        L1    ix3       $2   6.600      COWS

        >>> s.to_latex()  # doctest: +SKIP
        \begin{tabular}{llrrl}
        {} & {} & \multicolumn{2}{r}{Numeric} & {Non-Numeric} \\
        {} & {} & {Integers} & {Floats} & {Strings} \\
        \multirow[c]{2}{*}{L0} & ix1 & \\$1 & 2.200 & DOGS \\
         & ix2 & \$3 & 4.400 & CATS \\
        L1 & ix3 & \$2 & 6.600 & COWS \\
        \end{tabular}

        **CSS Conversion**

        This method can convert a Styler constructured with HTML-CSS to LaTeX using
        the following limited conversions.

        ================== ==================== ============= ==========================
        CSS Attribute      CSS value            LaTeX Command LaTeX Options
        ================== ==================== ============= ==========================
        font-weight        | bold               | bfseries
                           | bolder             | bfseries
        font-style         | italic             | itshape
                           | oblique            | slshape
        background-color   | red                cellcolor     | {red}--lwrap
                           | #fe01ea                          | [HTML]{FE01EA}--lwrap
                           | #f0e                             | [HTML]{FF00EE}--lwrap
                           | rgb(128,255,0)                   | [rgb]{0.5,1,0}--lwrap
                           | rgba(128,0,0,0.5)                | [rgb]{0.5,0,0}--lwrap
                           | rgb(25%,255,50%)                 | [rgb]{0.25,1,0.5}--lwrap
        color              | red                color         | {red}
                           | #fe01ea                          | [HTML]{FE01EA}
                           | #f0e                             | [HTML]{FF00EE}
                           | rgb(128,255,0)                   | [rgb]{0.5,1,0}
                           | rgba(128,0,0,0.5)                | [rgb]{0.5,0,0}
                           | rgb(25%,255,50%)                 | [rgb]{0.25,1,0.5}
        ================== ==================== ============= ==========================

        It is also possible to add user-defined LaTeX only styles to a HTML-CSS Styler
        using the ``--latex`` flag, and to add LaTeX parsing options that the
        converter will detect within a CSS-comment.

        >>> df = pd.DataFrame([[1]])
        >>> df.style.set_properties(
        ...     **{""font-weight"": ""bold /* --dwrap */"", ""Huge"": ""--latex--rwrap""}
        ... ).to_latex(convert_css=True)  # doctest: +SKIP
        \begin{tabular}{lr}
        {} & {0} \\
        0 & {\bfseries}{\Huge{1}} \\
        \end{tabular}

        Examples
        --------
        Below we give a complete step by step example adding some advanced features
        and noting some common gotchas.

        First we create the DataFrame and Styler as usual, including MultiIndex rows
        and columns, which allow for more advanced formatting options:

        >>> cidx = pd.MultiIndex.from_arrays([
        ...     [""Equity"", ""Equity"", ""Equity"", ""Equity"",
        ...      ""Stats"", ""Stats"", ""Stats"", ""Stats"", ""Rating""],
        ...     [""Energy"", ""Energy"", ""Consumer"", ""Consumer"", """", """", """", """", """"],
        ...     [""BP"", ""Shell"", ""H&M"", ""Unilever"",
        ...      ""Std Dev"", ""Variance"", ""52w High"", ""52w Low"", """"]
        ... ])
        >>> iidx = pd.MultiIndex.from_arrays([
        ...     [""Equity"", ""Equity"", ""Equity"", ""Equity""],
        ...     [""Energy"", ""Energy"", ""Consumer"", ""Consumer""],
        ...     [""BP"", ""Shell"", ""H&M"", ""Unilever""]
        ... ])
        >>> styler = pd.DataFrame([
        ...     [1, 0.8, 0.66, 0.72, 32.1678, 32.1678**2, 335.12, 240.89, ""Buy""],
        ...     [0.8, 1.0, 0.69, 0.79, 1.876, 1.876**2, 14.12, 19.78, ""Hold""],
        ...     [0.66, 0.69, 1.0, 0.86, 7, 7**2, 210.9, 140.6, ""Buy""],
        ...     [0.72, 0.79, 0.86, 1.0, 213.76, 213.76**2, 2807, 3678, ""Sell""],
        ... ], columns=cidx, index=iidx).style

        Second we will format the display and, since our table is quite wide, will
        hide the repeated level-0 of the index:

        >>> styler.format(subset=""Equity"", precision=2)
        ...       .format(subset=""Stats"", precision=1, thousands="","")
        ...       .format(subset=""Rating"", formatter=str.upper)
        ...       .format_index(escape=""latex"", axis=1)
        ...       .format_index(escape=""latex"", axis=0)
        ...       .hide(level=0, axis=0)  # doctest: +SKIP

        Note that one of the string entries of the index and column headers is ""H&M"".
        Without applying the `escape=""latex""` option to the `format_index` method the
        resultant LaTeX will fail to render, and the error returned is quite
        difficult to debug. Using the appropriate escape the ""&"" is converted to ""\\&"".

        Thirdly we will apply some (CSS-HTML) styles to our object. We will use a
        builtin method and also define our own method to highlight the stock
        recommendation:

        >>> def rating_color(v):
        ...     if v == ""Buy"": color = ""#33ff85""
        ...     elif v == ""Sell"": color = ""#ff5933""
        ...     else: color = ""#ffdd33""
        ...     return f""color: {color}; font-weight: bold;""
        >>> styler.background_gradient(cmap=""inferno"", subset=""Equity"", vmin=0, vmax=1)
        ...       .applymap(rating_color, subset=""Rating"")  # doctest: +SKIP

        All the above styles will work with HTML (see below) and LaTeX upon conversion:

        .. figure:: ../../_static/style/latex_stocks_html.png

        However, we finally want to add one LaTeX only style
        (from the {graphicx} package), that is not easy to convert from CSS and
        pandas does not support it. Notice the `--latex` flag used here,
        as well as `--rwrap` to ensure this is formatted correctly and
        not ignored upon conversion.

        >>> styler.applymap_index(
        ...     lambda v: ""rotatebox:{45}--rwrap--latex;"", level=2, axis=1
        ... )  # doctest: +SKIP

        Finally we render our LaTeX adding in other options as required:

        >>> styler.to_latex(
        ...     caption=""Selected stock correlation and simple statistics."",
        ...     clines=""skip-last;data"",
        ...     convert_css=True,
        ...     position_float=""centering"",
        ...     multicol_align=""|c|"",
        ...     hrules=True,
        ... )  # doctest: +SKIP
        \begin{table}
        \centering
        \caption{Selected stock correlation and simple statistics.}
        \begin{tabular}{llrrrrrrrrl}
        \toprule
         &  & \multicolumn{4}{|c|}{Equity} & \multicolumn{4}{|c|}{Stats} & Rating \\
         &  & \multicolumn{2}{|c|}{Energy} & \multicolumn{2}{|c|}{Consumer} &
        \multicolumn{4}{|c|}{} &  \\
         &  & \rotatebox{45}{BP} & \rotatebox{45}{Shell} & \rotatebox{45}{H\&M} &
        \rotatebox{45}{Unilever} & \rotatebox{45}{Std Dev} & \rotatebox{45}{Variance} &
        \rotatebox{45}{52w High} & \rotatebox{45}{52w Low} & \rotatebox{45}{} \\
        \midrule
        \multirow[c]{2}{*}{Energy} & BP & {\cellcolor[HTML]{FCFFA4}}
        \color[HTML]{000000} 1.00 & {\cellcolor[HTML]{FCA50A}} \color[HTML]{000000}
        0.80 & {\cellcolor[HTML]{EB6628}} \color[HTML]{F1F1F1} 0.66 &
        {\cellcolor[HTML]{F68013}} \color[HTML]{F1F1F1} 0.72 & 32.2 & 1,034.8 & 335.1
        & 240.9 & \color[HTML]{33FF85} \bfseries BUY \\
         & Shell & {\cellcolor[HTML]{FCA50A}} \color[HTML]{000000} 0.80 &
        {\cellcolor[HTML]{FCFFA4}} \color[HTML]{000000} 1.00 &
        {\cellcolor[HTML]{F1731D}} \color[HTML]{F1F1F1} 0.69 &
        {\cellcolor[HTML]{FCA108}} \color[HTML]{000000} 0.79 & 1.9 & 3.5 & 14.1 &
        19.8 & \color[HTML]{FFDD33} \bfseries HOLD \\
        \cline{1-11}
        \multirow[c]{2}{*}{Consumer} & H\&M & {\cellcolor[HTML]{EB6628}}
        \color[HTML]{F1F1F1} 0.66 & {\cellcolor[HTML]{F1731D}} \color[HTML]{F1F1F1}
        0.69 & {\cellcolor[HTML]{FCFFA4}} \color[HTML]{000000} 1.00 &
        {\cellcolor[HTML]{FAC42A}} \color[HTML]{000000} 0.86 & 7.0 & 49.0 & 210.9 &
        140.6 & \color[HTML]{33FF85} \bfseries BUY \\
         & Unilever & {\cellcolor[HTML]{F68013}} \color[HTML]{F1F1F1} 0.72 &
        {\cellcolor[HTML]{FCA108}} \color[HTML]{000000} 0.79 &
        {\cellcolor[HTML]{FAC42A}} \color[HTML]{000000} 0.86 &
        {\cellcolor[HTML]{FCFFA4}} \color[HTML]{000000} 1.00 & 213.8 & 45,693.3 &
        2,807.0 & 3,678.0 & \color[HTML]{FF5933} \bfseries SELL \\
        \cline{1-11}
        \bottomrule
        \end{tabular}
        \end{table}

        .. figure:: ../../_static/style/latex_stocks.png
        """"""
        obj = self._copy(deepcopy=True)  # manipulate table_styles on obj, not self

        table_selectors = (
            [style[""selector""] for style in self.table_styles]
            if self.table_styles is not None
            else []
        )

        if column_format is not None:
            # add more recent setting to table_styles
            obj.set_table_styles(
                [{""selector"": ""column_format"", ""props"": f"":{column_format}""}],
                overwrite=False,
            )
        elif ""column_format"" in table_selectors:
            pass  # adopt what has been previously set in table_styles
        else:
            # create a default: set float, complex, int cols to 'r' ('S'), index to 'l'
            _original_columns = self.data.columns
            self.data.columns = RangeIndex(stop=len(self.data.columns))
            numeric_cols = self.data._get_numeric_data().columns.to_list()
            self.data.columns = _original_columns
            column_format = """"
            for level in range(self.index.nlevels):
                column_format += """" if self.hide_index_[level] else ""l""
            for ci, _ in enumerate(self.data.columns):
                if ci not in self.hidden_columns:
                    column_format += (
                        (""r"" if not siunitx else ""S"") if ci in numeric_cols else ""l""
                    )
            obj.set_table_styles(
                [{""selector"": ""column_format"", ""props"": f"":{column_format}""}],
                overwrite=False,
            )

        if position:
            obj.set_table_styles(
                [{""selector"": ""position"", ""props"": f"":{position}""}],
                overwrite=False,
            )

        if position_float:
            if environment == ""longtable"":
                raise ValueError(
                    ""`position_float` cannot be used in 'longtable' `environment`""
                )
            if position_float not in [""raggedright"", ""raggedleft"", ""centering""]:
                raise ValueError(
                    f""`position_float` should be one of ""
                    f""'raggedright', 'raggedleft', 'centering', ""
                    f""got: '{position_float}'""
                )
            obj.set_table_styles(
                [{""selector"": ""position_float"", ""props"": f"":{position_float}""}],
                overwrite=False,
            )

        hrules = get_option(""styler.latex.hrules"") if hrules is None else hrules
        if hrules:
            obj.set_table_styles(
                [
                    {""selector"": ""toprule"", ""props"": "":toprule""},
                    {""selector"": ""midrule"", ""props"": "":midrule""},
                    {""selector"": ""bottomrule"", ""props"": "":bottomrule""},
                ],
                overwrite=False,
            )

        if label:
            obj.set_table_styles(
                [{""selector"": ""label"", ""props"": f"":{{{label.replace(':', '')}}}""}],
                overwrite=False,
            )

        if caption:
            obj.set_caption(caption)

        if sparse_index is None:
            sparse_index = get_option(""styler.sparse.index"")
        if sparse_columns is None:
            sparse_columns = get_option(""styler.sparse.columns"")
        environment = environment or get_option(""styler.latex.environment"")
        multicol_align = multicol_align or get_option(""styler.latex.multicol_align"")
        multirow_align = multirow_align or get_option(""styler.latex.multirow_align"")
        latex = obj._render_latex(
            sparse_index=sparse_index,
            sparse_columns=sparse_columns,
            multirow_align=multirow_align,
            multicol_align=multicol_align,
            environment=environment,
            convert_css=convert_css,
            siunitx=siunitx,
            clines=clines,
        )

        encoding = (
            (encoding or get_option(""styler.render.encoding""))
            if isinstance(buf, str)  # i.e. a filepath
            else encoding
        )
        return save_to_buffer(latex, buf=buf, encoding=encoding)",_208.py,553,"latex = obj._render_latex(sparse_index=sparse_index, sparse_columns=sparse_columns, multirow_align=multirow_align, multicol_align=multicol_align, environment=environment, convert_css=convert_css, siunitx=siunitx, clines=clines)
encoding = encoding or get_option('styler.render.encoding') if isinstance(buf, str) else encoding","latex , encoding  = obj._render_latex(sparse_index=sparse_index, sparse_columns=sparse_columns, multirow_align=multirow_align, multicol_align=multicol_align, environment=environment, convert_css=convert_css, siunitx=siunitx, clines=clines), encoding or get_option('styler.render.encoding') if isinstance(buf, str) else encoding"
https://github.com/apache/tvm/tree/master/tests/python/relay/test_vm.py,"def test_get_output_multiple(target, dev):
    # Build a IRModule.
    x = relay.var(""x"", shape=(10,))
    f = relay.Function([x], relay.Tuple([x + x, x]))
    mod = IRModule.from_expr(f)

    # Compile to VMExecutable.
    vm_exec = vm.compile(mod, target=target)
    vm_factory = runtime.vm.VirtualMachine(vm_exec, dev)
    inp = np.ones(10, dtype=""float32"")
    vm_factory.invoke_stateful(""main"", inp)
    outputs = vm_factory.get_outputs()
    assert len(outputs) == 2
    np.testing.assert_allclose(outputs[0].numpy(), inp + inp)
    np.testing.assert_allclose(outputs[1].numpy(), inp)",_211.py,9,"vm_factory = runtime.vm.VirtualMachine(vm_exec, dev)
inp = np.ones(10, dtype='float32')","vm_factory , inp  = runtime.vm.VirtualMachine(vm_exec, dev), np.ones(10, dtype='float32')"
https://github.com/devsnd/cherrymusic/tree/master/cherrymusicserver/progress.py,"def spawnchild(self, name=None):
        '''Creates a child progress that will tick this progress on finish'''
        if name is None:
            name = self.name
        child = ProgressTree(name, parent=self)
        child.root = self.root
        child.level = self.level + 1
        self.extend()
        return child",_212.py,6,"child.root = self.root
child.level = self.level + 1","child.root , child.level  = self.root, self.level + 1"
https://github.com/osmr/imgclsmob/tree/master/gluon/datasets/librispeech_asr_dataset.py,"def update_from_dataset(self,
                            dataset):
        """"""
        Update dataset metainfo after a dataset class instance creation.

        Parameters:
        ----------
        args : obj
            A dataset class instance.
        """"""
        vocabulary = dataset._data.vocabulary
        self.num_classes = len(vocabulary) + 1
        self.val_metric_extra_kwargs[0][""vocabulary""] = vocabulary
        self.test_metric_extra_kwargs[0][""vocabulary""] = vocabulary",_215.py,12,"self.num_classes = len(vocabulary) + 1
self.val_metric_extra_kwargs[0]['vocabulary'] = vocabulary
self.test_metric_extra_kwargs[0]['vocabulary'] = vocabulary","self.num_classes , self.val_metric_extra_kwargs[0]['vocabulary'] , self.test_metric_extra_kwargs[0]['vocabulary']  = len(vocabulary) + 1, vocabulary, vocabulary"
https://github.com/matplotlib/matplotlib/tree/master/lib/mpl_toolkits/axes_grid1/axes_size.py,"def get_size(self, renderer):
        rel_size = 0.

        bb = self._func(renderer)
        dpi = renderer.points_to_pixels(72.)
        abs_size = bb/dpi

        return rel_size, abs_size",_218.py,2,"rel_size = 0.0
bb = self._func(renderer)
dpi = renderer.points_to_pixels(72.0)","rel_size , bb , dpi  = 0.0, self._func(renderer), renderer.points_to_pixels(72.0)"
https://github.com/ansible/galaxy/tree/master/lib/galaxy/datatypes/data.py,"def split(cls, input_datasets, subdir_generator_function, split_params):
        """"""
        Split the input files by line.
        """"""
        if split_params is None:
            return

        if len(input_datasets) > 1:
            raise Exception(""Text file splitting does not support multiple files"")
        input_files = [ds.file_name for ds in input_datasets]

        lines_per_file = None
        chunk_size = None
        if split_params['split_mode'] == 'number_of_parts':
            lines_per_file = []

            # Computing the length is expensive!
            def _file_len(fname):
                with open(fname) as f:
                    return sum(1 for _ in f)
            length = _file_len(input_files[0])
            parts = int(split_params['split_size'])
            if length < parts:
                parts = length
            len_each, remainder = divmod(length, parts)
            while length > 0:
                chunk = len_each
                if remainder > 0:
                    chunk += 1
                lines_per_file.append(chunk)
                remainder -= 1
                length -= chunk
        elif split_params['split_mode'] == 'to_size':
            chunk_size = int(split_params['split_size'])
        else:
            raise Exception(f""Unsupported split mode {split_params['split_mode']}"")

        f = open(input_files[0])
        try:
            chunk_idx = 0
            file_done = False
            part_file = None
            while not file_done:
                if lines_per_file is None:
                    this_chunk_size = chunk_size
                elif chunk_idx < len(lines_per_file):
                    this_chunk_size = lines_per_file[chunk_idx]
                    chunk_idx += 1
                lines_remaining = this_chunk_size
                part_file = None
                while lines_remaining > 0:
                    a_line = f.readline()
                    if a_line == '':
                        file_done = True
                        break
                    if part_file is None:
                        part_dir = subdir_generator_function()
                        part_path = os.path.join(part_dir, os.path.basename(input_files[0]))
                        part_file = open(part_path, 'w')
                    part_file.write(a_line)
                    lines_remaining -= 1
        except Exception as e:
            log.error('Unable to split files: %s', unicodify(e))
            raise
        finally:
            f.close()
            if part_file:
                part_file.close()",_22.py,10,"input_files = [ds.file_name for ds in input_datasets]
lines_per_file = None
chunk_size = None","input_files , lines_per_file , chunk_size  = [ds.file_name for ds in input_datasets], None, None"
https://github.com/ansible/galaxy/tree/master/lib/galaxy/datatypes/data.py,"def split(cls, input_datasets, subdir_generator_function, split_params):
        """"""
        Split the input files by line.
        """"""
        if split_params is None:
            return

        if len(input_datasets) > 1:
            raise Exception(""Text file splitting does not support multiple files"")
        input_files = [ds.file_name for ds in input_datasets]

        lines_per_file = None
        chunk_size = None
        if split_params['split_mode'] == 'number_of_parts':
            lines_per_file = []

            # Computing the length is expensive!
            def _file_len(fname):
                with open(fname) as f:
                    return sum(1 for _ in f)
            length = _file_len(input_files[0])
            parts = int(split_params['split_size'])
            if length < parts:
                parts = length
            len_each, remainder = divmod(length, parts)
            while length > 0:
                chunk = len_each
                if remainder > 0:
                    chunk += 1
                lines_per_file.append(chunk)
                remainder -= 1
                length -= chunk
        elif split_params['split_mode'] == 'to_size':
            chunk_size = int(split_params['split_size'])
        else:
            raise Exception(f""Unsupported split mode {split_params['split_mode']}"")

        f = open(input_files[0])
        try:
            chunk_idx = 0
            file_done = False
            part_file = None
            while not file_done:
                if lines_per_file is None:
                    this_chunk_size = chunk_size
                elif chunk_idx < len(lines_per_file):
                    this_chunk_size = lines_per_file[chunk_idx]
                    chunk_idx += 1
                lines_remaining = this_chunk_size
                part_file = None
                while lines_remaining > 0:
                    a_line = f.readline()
                    if a_line == '':
                        file_done = True
                        break
                    if part_file is None:
                        part_dir = subdir_generator_function()
                        part_path = os.path.join(part_dir, os.path.basename(input_files[0]))
                        part_file = open(part_path, 'w')
                    part_file.write(a_line)
                    lines_remaining -= 1
        except Exception as e:
            log.error('Unable to split files: %s', unicodify(e))
            raise
        finally:
            f.close()
            if part_file:
                part_file.close()",_22.py,21,"length = _file_len(input_files[0])
parts = int(split_params['split_size'])","length , parts  = _file_len(input_files[0]), int(split_params['split_size'])"
https://github.com/ansible/galaxy/tree/master/lib/galaxy/datatypes/data.py,"def split(cls, input_datasets, subdir_generator_function, split_params):
        """"""
        Split the input files by line.
        """"""
        if split_params is None:
            return

        if len(input_datasets) > 1:
            raise Exception(""Text file splitting does not support multiple files"")
        input_files = [ds.file_name for ds in input_datasets]

        lines_per_file = None
        chunk_size = None
        if split_params['split_mode'] == 'number_of_parts':
            lines_per_file = []

            # Computing the length is expensive!
            def _file_len(fname):
                with open(fname) as f:
                    return sum(1 for _ in f)
            length = _file_len(input_files[0])
            parts = int(split_params['split_size'])
            if length < parts:
                parts = length
            len_each, remainder = divmod(length, parts)
            while length > 0:
                chunk = len_each
                if remainder > 0:
                    chunk += 1
                lines_per_file.append(chunk)
                remainder -= 1
                length -= chunk
        elif split_params['split_mode'] == 'to_size':
            chunk_size = int(split_params['split_size'])
        else:
            raise Exception(f""Unsupported split mode {split_params['split_mode']}"")

        f = open(input_files[0])
        try:
            chunk_idx = 0
            file_done = False
            part_file = None
            while not file_done:
                if lines_per_file is None:
                    this_chunk_size = chunk_size
                elif chunk_idx < len(lines_per_file):
                    this_chunk_size = lines_per_file[chunk_idx]
                    chunk_idx += 1
                lines_remaining = this_chunk_size
                part_file = None
                while lines_remaining > 0:
                    a_line = f.readline()
                    if a_line == '':
                        file_done = True
                        break
                    if part_file is None:
                        part_dir = subdir_generator_function()
                        part_path = os.path.join(part_dir, os.path.basename(input_files[0]))
                        part_file = open(part_path, 'w')
                    part_file.write(a_line)
                    lines_remaining -= 1
        except Exception as e:
            log.error('Unable to split files: %s', unicodify(e))
            raise
        finally:
            f.close()
            if part_file:
                part_file.close()",_22.py,40,"chunk_idx = 0
file_done = False
part_file = None","chunk_idx , file_done , part_file  = 0, False, None"
https://github.com/ansible/galaxy/tree/master/lib/galaxy/datatypes/data.py,"def split(cls, input_datasets, subdir_generator_function, split_params):
        """"""
        Split the input files by line.
        """"""
        if split_params is None:
            return

        if len(input_datasets) > 1:
            raise Exception(""Text file splitting does not support multiple files"")
        input_files = [ds.file_name for ds in input_datasets]

        lines_per_file = None
        chunk_size = None
        if split_params['split_mode'] == 'number_of_parts':
            lines_per_file = []

            # Computing the length is expensive!
            def _file_len(fname):
                with open(fname) as f:
                    return sum(1 for _ in f)
            length = _file_len(input_files[0])
            parts = int(split_params['split_size'])
            if length < parts:
                parts = length
            len_each, remainder = divmod(length, parts)
            while length > 0:
                chunk = len_each
                if remainder > 0:
                    chunk += 1
                lines_per_file.append(chunk)
                remainder -= 1
                length -= chunk
        elif split_params['split_mode'] == 'to_size':
            chunk_size = int(split_params['split_size'])
        else:
            raise Exception(f""Unsupported split mode {split_params['split_mode']}"")

        f = open(input_files[0])
        try:
            chunk_idx = 0
            file_done = False
            part_file = None
            while not file_done:
                if lines_per_file is None:
                    this_chunk_size = chunk_size
                elif chunk_idx < len(lines_per_file):
                    this_chunk_size = lines_per_file[chunk_idx]
                    chunk_idx += 1
                lines_remaining = this_chunk_size
                part_file = None
                while lines_remaining > 0:
                    a_line = f.readline()
                    if a_line == '':
                        file_done = True
                        break
                    if part_file is None:
                        part_dir = subdir_generator_function()
                        part_path = os.path.join(part_dir, os.path.basename(input_files[0]))
                        part_file = open(part_path, 'w')
                    part_file.write(a_line)
                    lines_remaining -= 1
        except Exception as e:
            log.error('Unable to split files: %s', unicodify(e))
            raise
        finally:
            f.close()
            if part_file:
                part_file.close()",_22.py,49,"lines_remaining = this_chunk_size
part_file = None","lines_remaining , part_file  = this_chunk_size, None"
https://github.com/Arthur151/ROMP/tree/master/romp/lib/evaluation/pw3d_eval/evaluate.py,"def joint_angle_error(pred_mat, gt_mat):
    """"""
    Compute the geodesic distance between the two input matrices.
    :param pred_mat: predicted rotation matrices. Shape: ( Seq, 9g, 3, 3)
    :param gt_mat: ground truth rotation matrices. Shape: ( Seq, 24, 3, 3)
    :return: Mean geodesic distance between input matrices.
    """"""

    gt_mat = gt_mat[:, SMPL_OR_JOINTS, :, :]

    # Reshape the matrices into B x 3 x 3 arrays
    r1 = np.reshape(pred_mat, [-1, 3, 3])
    r2 = np.reshape(gt_mat, [-1, 3, 3])

    # Transpose gt matrices
    r2t = np.transpose(r2, [0, 2, 1])

    # compute R1 * R2.T, if prediction and target match, this will be the identity matrix
    r = np.matmul(r1, r2t)

    angles = []
    # Convert rotation matrix to axis angle representation and find the angle
    for i in range(r1.shape[0]):
        aa, _ = cv2.Rodrigues(r[i])
        angles.append(np.linalg.norm(aa))

    return np.mean(np.array(angles))",_221.py,12,"r1 = np.reshape(pred_mat, [-1, 3, 3])
r2 = np.reshape(gt_mat, [-1, 3, 3])","r1 , r2  = np.reshape(pred_mat, [-1, 3, 3]), np.reshape(gt_mat, [-1, 3, 3])"
https://github.com/Arthur151/ROMP/tree/master/romp/lib/evaluation/pw3d_eval/evaluate.py,"def joint_angle_error(pred_mat, gt_mat):
    """"""
    Compute the geodesic distance between the two input matrices.
    :param pred_mat: predicted rotation matrices. Shape: ( Seq, 9g, 3, 3)
    :param gt_mat: ground truth rotation matrices. Shape: ( Seq, 24, 3, 3)
    :return: Mean geodesic distance between input matrices.
    """"""

    gt_mat = gt_mat[:, SMPL_OR_JOINTS, :, :]

    # Reshape the matrices into B x 3 x 3 arrays
    r1 = np.reshape(pred_mat, [-1, 3, 3])
    r2 = np.reshape(gt_mat, [-1, 3, 3])

    # Transpose gt matrices
    r2t = np.transpose(r2, [0, 2, 1])

    # compute R1 * R2.T, if prediction and target match, this will be the identity matrix
    r = np.matmul(r1, r2t)

    angles = []
    # Convert rotation matrix to axis angle representation and find the angle
    for i in range(r1.shape[0]):
        aa, _ = cv2.Rodrigues(r[i])
        angles.append(np.linalg.norm(aa))

    return np.mean(np.array(angles))",_221.py,19,"r = np.matmul(r1, r2t)
angles = []","r , angles  = np.matmul(r1, r2t), []"
https://github.com/KurtBestor/Hitomi-Downloader/tree/master/src/extractor/danbooru_downloader.py,"def get_imgs(url, title=None, range_=None, cw=None):
    if 'donmai.us/artists' in url:
        raise NotImplementedError('Not Implemented')
    if 'donmai.us/posts/' in url:
        raise NotImplementedError('Not Implemented')

    print_ = get_print(cw)

    # Range
    max_pid = get_max_range(cw)

    if range_ is None:
        range_ = range(1, 1001)
    print(range_)
    imgs = []
    i = 0
    empty_count = 0
    empty_count_global = 0
    url_imgs = set()
    while i < len(range_):
        check_alive(cw)
        p = range_[i]
        url = setPage(url, p)
        print_(url)
        soup = read_soup(url, cw)
        articles = soup.findAll('article')
        if articles:
            empty_count_global = 0
        else:
            empty_count += 1
            if empty_count < 4:
                s = 'empty page; retry... {}'.format(p)
                print_(s)
                continue
            else:
                empty_count = 0
                empty_count_global += 1

        if empty_count_global >= 6:
            break

        for article in articles:
            id = article.attrs['data-id']

            #url_img = article.attrs['data-file-url'].strip()
            url_img = urljoin(url, article.find('a', class_='post-preview-link')['href']) #4160

            #print(url_img)
            if url_img not in url_imgs:
                url_imgs.add(url_img)
                img = Image(id, url_img, cw)
                imgs.append(img)

        if len(imgs) >= max_pid:
            break

        if cw is not None:
            cw.setTitle('{}  {} - {}'.format(tr_(' ...'), title, len(imgs)))
        i += 1

    return imgs[:max_pid]",_224.py,7,"print_ = get_print(cw)
max_pid = get_max_range(cw)","print_ , max_pid  = get_print(cw), get_max_range(cw)"
https://github.com/KurtBestor/Hitomi-Downloader/tree/master/src/extractor/danbooru_downloader.py,"def get_imgs(url, title=None, range_=None, cw=None):
    if 'donmai.us/artists' in url:
        raise NotImplementedError('Not Implemented')
    if 'donmai.us/posts/' in url:
        raise NotImplementedError('Not Implemented')

    print_ = get_print(cw)

    # Range
    max_pid = get_max_range(cw)

    if range_ is None:
        range_ = range(1, 1001)
    print(range_)
    imgs = []
    i = 0
    empty_count = 0
    empty_count_global = 0
    url_imgs = set()
    while i < len(range_):
        check_alive(cw)
        p = range_[i]
        url = setPage(url, p)
        print_(url)
        soup = read_soup(url, cw)
        articles = soup.findAll('article')
        if articles:
            empty_count_global = 0
        else:
            empty_count += 1
            if empty_count < 4:
                s = 'empty page; retry... {}'.format(p)
                print_(s)
                continue
            else:
                empty_count = 0
                empty_count_global += 1

        if empty_count_global >= 6:
            break

        for article in articles:
            id = article.attrs['data-id']

            #url_img = article.attrs['data-file-url'].strip()
            url_img = urljoin(url, article.find('a', class_='post-preview-link')['href']) #4160

            #print(url_img)
            if url_img not in url_imgs:
                url_imgs.add(url_img)
                img = Image(id, url_img, cw)
                imgs.append(img)

        if len(imgs) >= max_pid:
            break

        if cw is not None:
            cw.setTitle('{}  {} - {}'.format(tr_(' ...'), title, len(imgs)))
        i += 1

    return imgs[:max_pid]",_224.py,15,"imgs = []
i = 0
empty_count = 0
empty_count_global = 0
url_imgs = set()","imgs , i , empty_count , empty_count_global , url_imgs  = [], 0, 0, 0, set()"
https://github.com/KurtBestor/Hitomi-Downloader/tree/master/src/extractor/danbooru_downloader.py,"def get_imgs(url, title=None, range_=None, cw=None):
    if 'donmai.us/artists' in url:
        raise NotImplementedError('Not Implemented')
    if 'donmai.us/posts/' in url:
        raise NotImplementedError('Not Implemented')

    print_ = get_print(cw)

    # Range
    max_pid = get_max_range(cw)

    if range_ is None:
        range_ = range(1, 1001)
    print(range_)
    imgs = []
    i = 0
    empty_count = 0
    empty_count_global = 0
    url_imgs = set()
    while i < len(range_):
        check_alive(cw)
        p = range_[i]
        url = setPage(url, p)
        print_(url)
        soup = read_soup(url, cw)
        articles = soup.findAll('article')
        if articles:
            empty_count_global = 0
        else:
            empty_count += 1
            if empty_count < 4:
                s = 'empty page; retry... {}'.format(p)
                print_(s)
                continue
            else:
                empty_count = 0
                empty_count_global += 1

        if empty_count_global >= 6:
            break

        for article in articles:
            id = article.attrs['data-id']

            #url_img = article.attrs['data-file-url'].strip()
            url_img = urljoin(url, article.find('a', class_='post-preview-link')['href']) #4160

            #print(url_img)
            if url_img not in url_imgs:
                url_imgs.add(url_img)
                img = Image(id, url_img, cw)
                imgs.append(img)

        if len(imgs) >= max_pid:
            break

        if cw is not None:
            cw.setTitle('{}  {} - {}'.format(tr_(' ...'), title, len(imgs)))
        i += 1

    return imgs[:max_pid]",_224.py,43,"id = article.attrs['data-id']
url_img = urljoin(url, article.find('a', class_='post-preview-link')['href'])","id , url_img  = article.attrs['data-id'], urljoin(url, article.find('a', class_='post-preview-link')['href'])"
https://github.com/facebookresearch/ClassyVision/tree/master/test/generic_util_test.py,"def test_save_and_load_checkpoint(self):
        checkpoint_dict = {str(i): i * 2 for i in range(1000)}

        # save to the default checkpoint file
        save_checkpoint(self.base_dir, checkpoint_dict)

        # load the checkpoint by using the default file
        loaded_checkpoint = load_checkpoint(self.base_dir)
        self.assertDictEqual(checkpoint_dict, loaded_checkpoint)

        # load the checkpoint by passing the full path
        checkpoint_path = f""{self.base_dir}/{CHECKPOINT_FILE}""
        loaded_checkpoint = load_checkpoint(checkpoint_path)
        self.assertDictEqual(checkpoint_dict, loaded_checkpoint)

        # create a new checkpoint dict
        filename = ""my_checkpoint.torch""
        checkpoint_dict = {str(i): i * 3 for i in range(1000)}

        # save the checkpoint to a different file
        save_checkpoint(self.base_dir, checkpoint_dict, checkpoint_file=filename)

        # load the checkpoint by passing the full path
        checkpoint_path = f""{self.base_dir}/{filename}""
        loaded_checkpoint = load_checkpoint(checkpoint_path)
        self.assertDictEqual(checkpoint_dict, loaded_checkpoint)",_23.py,17,"filename = 'my_checkpoint.torch'
checkpoint_dict = {str(i): i * 3 for i in range(1000)}","filename , checkpoint_dict  = 'my_checkpoint.torch', {str(i): i * 3 for i in range(1000)}"
https://github.com/mars-project/mars/tree/master/mars/learn/tests/test_wrappers.py,"def test_parallel_post_fit_basic(setup):
    clf = ParallelPostFit(GradientBoostingClassifier())
    clf.fit(X, y)

    assert isinstance(clf.predict(X), mt.Tensor)
    assert isinstance(clf.predict_proba(X), mt.Tensor)

    result = clf.score(X, y)
    expected = clf.estimator.score(X, y)
    assert result.fetch() == expected

    clf = ParallelPostFit(LinearRegression())
    clf.fit(X, y)
    with pytest.raises(
        AttributeError, match=""The wrapped estimator (.|\n)* 'predict_proba' method.""
    ):
        clf.predict_proba(X)",_232.py,8,"result = clf.score(X, y)
expected = clf.estimator.score(X, y)","result , expected  = clf.score(X, y), clf.estimator.score(X, y)"
https://github.com/PaddlePaddle/PaddleClas/tree/master/ppcls/arch/backbone/legendary_models/inception_v3.py,"def __init__(self, num_channels, channels_7x7):
        super().__init__()
        self.branch1x1 = ConvBNLayer(
            num_channels=num_channels,
            num_filters=192,
            filter_size=1,
            act=""relu"")

        self.branch7x7_1 = ConvBNLayer(
            num_channels=num_channels,
            num_filters=channels_7x7,
            filter_size=1,
            stride=1,
            act=""relu"")
        self.branch7x7_2 = ConvBNLayer(
            num_channels=channels_7x7,
            num_filters=channels_7x7,
            filter_size=(1, 7),
            stride=1,
            padding=(0, 3),
            act=""relu"")
        self.branch7x7_3 = ConvBNLayer(
            num_channels=channels_7x7,
            num_filters=192,
            filter_size=(7, 1),
            stride=1,
            padding=(3, 0),
            act=""relu"")

        self.branch7x7dbl_1 = ConvBNLayer(
            num_channels=num_channels,
            num_filters=channels_7x7,
            filter_size=1,
            act=""relu"")
        self.branch7x7dbl_2 = ConvBNLayer(
            num_channels=channels_7x7,
            num_filters=channels_7x7,
            filter_size=(7, 1),
            padding=(3, 0),
            act=""relu"")
        self.branch7x7dbl_3 = ConvBNLayer(
            num_channels=channels_7x7,
            num_filters=channels_7x7,
            filter_size=(1, 7),
            padding=(0, 3),
            act=""relu"")
        self.branch7x7dbl_4 = ConvBNLayer(
            num_channels=channels_7x7,
            num_filters=channels_7x7,
            filter_size=(7, 1),
            padding=(3, 0),
            act=""relu"")
        self.branch7x7dbl_5 = ConvBNLayer(
            num_channels=channels_7x7,
            num_filters=192,
            filter_size=(1, 7),
            padding=(0, 3),
            act=""relu"")

        self.branch_pool = AvgPool2D(
            kernel_size=3, stride=1, padding=1, exclusive=False)
        self.branch_pool_conv = ConvBNLayer(
            num_channels=num_channels,
            num_filters=192,
            filter_size=1,
            act=""relu"")",_245.py,3,"self.branch1x1 = ConvBNLayer(num_channels=num_channels, num_filters=192, filter_size=1, act='relu')
self.branch7x7_1 = ConvBNLayer(num_channels=num_channels, num_filters=channels_7x7, filter_size=1, stride=1, act='relu')
self.branch7x7_2 = ConvBNLayer(num_channels=channels_7x7, num_filters=channels_7x7, filter_size=(1, 7), stride=1, padding=(0, 3), act='relu')
self.branch7x7_3 = ConvBNLayer(num_channels=channels_7x7, num_filters=192, filter_size=(7, 1), stride=1, padding=(3, 0), act='relu')
self.branch7x7dbl_1 = ConvBNLayer(num_channels=num_channels, num_filters=channels_7x7, filter_size=1, act='relu')
self.branch7x7dbl_2 = ConvBNLayer(num_channels=channels_7x7, num_filters=channels_7x7, filter_size=(7, 1), padding=(3, 0), act='relu')
self.branch7x7dbl_3 = ConvBNLayer(num_channels=channels_7x7, num_filters=channels_7x7, filter_size=(1, 7), padding=(0, 3), act='relu')
self.branch7x7dbl_4 = ConvBNLayer(num_channels=channels_7x7, num_filters=channels_7x7, filter_size=(7, 1), padding=(3, 0), act='relu')
self.branch7x7dbl_5 = ConvBNLayer(num_channels=channels_7x7, num_filters=192, filter_size=(1, 7), padding=(0, 3), act='relu')
self.branch_pool = AvgPool2D(kernel_size=3, stride=1, padding=1, exclusive=False)
self.branch_pool_conv = ConvBNLayer(num_channels=num_channels, num_filters=192, filter_size=1, act='relu')","self.branch1x1 , self.branch7x7_1 , self.branch7x7_2 , self.branch7x7_3 , self.branch7x7dbl_1 , self.branch7x7dbl_2 , self.branch7x7dbl_3 , self.branch7x7dbl_4 , self.branch7x7dbl_5 , self.branch_pool , self.branch_pool_conv  = ConvBNLayer(num_channels=num_channels, num_filters=192, filter_size=1, act='relu'), ConvBNLayer(num_channels=num_channels, num_filters=channels_7x7, filter_size=1, stride=1, act='relu'), ConvBNLayer(num_channels=channels_7x7, num_filters=channels_7x7, filter_size=(1, 7), stride=1, padding=(0, 3), act='relu'), ConvBNLayer(num_channels=channels_7x7, num_filters=192, filter_size=(7, 1), stride=1, padding=(3, 0), act='relu'), ConvBNLayer(num_channels=num_channels, num_filters=channels_7x7, filter_size=1, act='relu'), ConvBNLayer(num_channels=channels_7x7, num_filters=channels_7x7, filter_size=(7, 1), padding=(3, 0), act='relu'), ConvBNLayer(num_channels=channels_7x7, num_filters=channels_7x7, filter_size=(1, 7), padding=(0, 3), act='relu'), ConvBNLayer(num_channels=channels_7x7, num_filters=channels_7x7, filter_size=(7, 1), padding=(3, 0), act='relu'), ConvBNLayer(num_channels=channels_7x7, num_filters=192, filter_size=(1, 7), padding=(0, 3), act='relu'), AvgPool2D(kernel_size=3, stride=1, padding=1, exclusive=False), ConvBNLayer(num_channels=num_channels, num_filters=192, filter_size=1, act='relu')"
https://github.com/scikit-learn-contrib/metric-learn/tree/master/metric_learn/nca.py,"def __init__(self, init='auto', n_components=None,
               max_iter=100, tol=None, verbose=False, preprocessor=None,
               random_state=None):
    self.n_components = n_components
    self.init = init
    self.max_iter = max_iter
    self.tol = tol
    self.verbose = verbose
    self.random_state = random_state
    super(NCA, self).__init__(preprocessor)",_249.py,4,"self.n_components = n_components
self.init = init
self.max_iter = max_iter
self.tol = tol
self.verbose = verbose
self.random_state = random_state","self.n_components , self.init , self.max_iter , self.tol , self.verbose , self.random_state  = n_components, init, max_iter, tol, verbose, random_state"
https://github.com/facebookresearch/ReAgent/tree/master/reagent/gym/envs/pomdp/pocman.py,"def _smell_food(self):
        smell_range = self.board[""_smell_range""]
        agent_pos = self.internal_state.agent_pos

        for x in range(-smell_range, smell_range + 1):
            for y in range(-smell_range, smell_range + 1):
                smell_x = agent_pos.x + x
                smell_y = agent_pos.y + y
                if (
                    0 <= smell_x < self.maze.shape[0]
                    and 0 <= smell_y < self.maze.shape[1]
                    and self.maze[smell_x, smell_y] == Element.FOOD_PELLET
                ):
                    return True
        return False",_253.py,2,"smell_range = self.board['_smell_range']
agent_pos = self.internal_state.agent_pos","smell_range , agent_pos  = self.board['_smell_range'], self.internal_state.agent_pos"
https://github.com/facebookresearch/ReAgent/tree/master/reagent/gym/envs/pomdp/pocman.py,"def _smell_food(self):
        smell_range = self.board[""_smell_range""]
        agent_pos = self.internal_state.agent_pos

        for x in range(-smell_range, smell_range + 1):
            for y in range(-smell_range, smell_range + 1):
                smell_x = agent_pos.x + x
                smell_y = agent_pos.y + y
                if (
                    0 <= smell_x < self.maze.shape[0]
                    and 0 <= smell_y < self.maze.shape[1]
                    and self.maze[smell_x, smell_y] == Element.FOOD_PELLET
                ):
                    return True
        return False",_253.py,7,"smell_x = agent_pos.x + x
smell_y = agent_pos.y + y","smell_x , smell_y  = agent_pos.x + x, agent_pos.y + y"
https://github.com/HariSekhon/DevOps-Python-tools/tree/master//getent.py,"def mac_getent_group(self, args):
        arg = self.mac_get_arg(args)
        final_returncode = 0
        final_output = """"
        if arg:
            (final_output, final_returncode) = self.mac_getent_group_name(arg)
        else:
            groups = [group for group in
                      subprocess.Popen('dscl . -list /Groups'.split(),
                                       stdout=subprocess.PIPE).stdout.read().split('\n')
                      if group]
            log.info('found groups: %s', groups)
            for group in groups:
                (formatted_output, returncode) = self.mac_getent_group_name(group)
                if formatted_output:
                    final_output += formatted_output + '\n'
                if returncode > final_returncode:
                    final_returncode = returncode
            final_output = final_output.rstrip('\n')
            # reorder output by GID to be similar to what you'd see on Linux
            lines = final_output.split('\n')
            final_output = '\n'.join(sorted(lines, cmp=lambda x, y: cmp(int(x.split(':')[0]), int(y.split(':')[0]))))
        return (final_output, final_returncode)",_254.py,2,"arg = self.mac_get_arg(args)
final_returncode = 0
final_output = ''","arg , final_returncode , final_output  = self.mac_get_arg(args), 0, ''"
https://github.com/ansible/ansible/tree/master/lib/ansible/playbook/role/requirement.py,"def role_yaml_parse(role):

        if isinstance(role, string_types):
            name = None
            scm = None
            src = None
            version = None
            if ',' in role:
                if role.count(',') == 1:
                    (src, version) = role.strip().split(',', 1)
                elif role.count(',') == 2:
                    (src, version, name) = role.strip().split(',', 2)
                else:
                    raise AnsibleError(""Invalid role line (%s). Proper format is 'role_name[,version[,name]]'"" % role)
            else:
                src = role

            if name is None:
                name = RoleRequirement.repo_url_to_role_name(src)
            if '+' in src:
                (scm, src) = src.split('+', 1)

            return dict(name=name, src=src, scm=scm, version=version)

        if 'role' in role:
            name = role['role']
            if ',' in name:
                raise AnsibleError(""Invalid old style role requirement: %s"" % name)
            else:
                del role['role']
                role['name'] = name
        else:
            role = role.copy()

            if 'src' in role:
                # New style: { src: 'galaxy.role,version,name', other_vars: ""here"" }
                if 'github.com' in role[""src""] and 'http' in role[""src""] and '+' not in role[""src""] and not role[""src""].endswith('.tar.gz'):
                    role[""src""] = ""git+"" + role[""src""]

                if '+' in role[""src""]:
                    role[""scm""], dummy, role[""src""] = role[""src""].partition('+')

                if 'name' not in role:
                    role[""name""] = RoleRequirement.repo_url_to_role_name(role[""src""])

            if 'version' not in role:
                role['version'] = ''

            if 'scm' not in role:
                role['scm'] = None

        for key in list(role.keys()):
            if key not in VALID_SPEC_KEYS:
                role.pop(key)

        return role",_259.py,4,"name = None
scm = None
src = None
version = None","name , scm , src , version  = None, None, None, None"
https://github.com/Blazemeter/taurus/tree/master/bzt/modules/monitoring.py,"def get_data(self):
        now = time.time()

        if now > self._last_check + self.interval:
            self._cached_data = []
            self._last_check = now
            json_list = self._get_response()
            data_line = [now]

            for element in json_list:
                item = {
                    'ts': now,
                    'source': '%s' % self.host_label}

                for datapoint in reversed(element['datapoints']):
                    if datapoint[0] is not None:
                        item[element['target']] = datapoint[0]
                        data_line.append(datapoint[0])
                        break

                self._cached_data.append(item)

            if self.logs_file:
                with open(self.logs_file, ""a"", newline='') as g_logs:
                    logs_writer = csv.writer(g_logs, delimiter=',')
                    logs_writer.writerow(data_line)

        return self._cached_data",_262.py,5,"self._cached_data = []
self._last_check = now","self._cached_data , self._last_check  = [], now"
https://github.com/Blazemeter/taurus/tree/master/bzt/modules/monitoring.py,"def get_data(self):
        now = time.time()

        if now > self._last_check + self.interval:
            self._cached_data = []
            self._last_check = now
            json_list = self._get_response()
            data_line = [now]

            for element in json_list:
                item = {
                    'ts': now,
                    'source': '%s' % self.host_label}

                for datapoint in reversed(element['datapoints']):
                    if datapoint[0] is not None:
                        item[element['target']] = datapoint[0]
                        data_line.append(datapoint[0])
                        break

                self._cached_data.append(item)

            if self.logs_file:
                with open(self.logs_file, ""a"", newline='') as g_logs:
                    logs_writer = csv.writer(g_logs, delimiter=',')
                    logs_writer.writerow(data_line)

        return self._cached_data",_262.py,7,"json_list = self._get_response()
data_line = [now]","json_list , data_line  = self._get_response(), [now]"
https://github.com/pyca/cryptography/tree/master/tests/hazmat/primitives/test_x25519.py,"def test_rfc7748_1000_iteration(self, backend):
        old_private = private = public = binascii.unhexlify(
            b""090000000000000000000000000000000000000000000000000000000000""
            b""0000""
        )
        shared_key = binascii.unhexlify(
            b""684cf59ba83309552800ef566f2f4d3c1c3887c49360e3875f2eb94d9953""
            b""2c51""
        )
        private_key = X25519PrivateKey.from_private_bytes(private)
        public_key = X25519PublicKey.from_public_bytes(public)
        for _ in range(1000):
            computed_shared_key = private_key.exchange(public_key)
            private_key = X25519PrivateKey.from_private_bytes(
                computed_shared_key
            )
            public_key = X25519PublicKey.from_public_bytes(old_private)
            old_private = computed_shared_key

        assert computed_shared_key == shared_key",_263.py,6,"shared_key = binascii.unhexlify(b'684cf59ba83309552800ef566f2f4d3c1c3887c49360e3875f2eb94d99532c51')
private_key = X25519PrivateKey.from_private_bytes(private)
public_key = X25519PublicKey.from_public_bytes(public)","shared_key , private_key , public_key  = binascii.unhexlify(b'684cf59ba83309552800ef566f2f4d3c1c3887c49360e3875f2eb94d99532c51'), X25519PrivateKey.from_private_bytes(private), X25519PublicKey.from_public_bytes(public)"
https://github.com/pyca/cryptography/tree/master/tests/hazmat/primitives/test_x25519.py,"def test_rfc7748_1000_iteration(self, backend):
        old_private = private = public = binascii.unhexlify(
            b""090000000000000000000000000000000000000000000000000000000000""
            b""0000""
        )
        shared_key = binascii.unhexlify(
            b""684cf59ba83309552800ef566f2f4d3c1c3887c49360e3875f2eb94d9953""
            b""2c51""
        )
        private_key = X25519PrivateKey.from_private_bytes(private)
        public_key = X25519PublicKey.from_public_bytes(public)
        for _ in range(1000):
            computed_shared_key = private_key.exchange(public_key)
            private_key = X25519PrivateKey.from_private_bytes(
                computed_shared_key
            )
            public_key = X25519PublicKey.from_public_bytes(old_private)
            old_private = computed_shared_key

        assert computed_shared_key == shared_key",_263.py,14,"private_key = X25519PrivateKey.from_private_bytes(computed_shared_key)
public_key = X25519PublicKey.from_public_bytes(old_private)
old_private = computed_shared_key","private_key , public_key , old_private  = X25519PrivateKey.from_private_bytes(computed_shared_key), X25519PublicKey.from_public_bytes(old_private), computed_shared_key"
https://github.com/facebookresearch/dino/tree/master//eval_image_retrieval.py,"def __init__(self, dir_main, dataset, split, transform=None, imsize=None):
        if dataset not in ['roxford5k', 'rparis6k']:
            raise ValueError('Unknown dataset: {}!'.format(dataset))

        # loading imlist, qimlist, and gnd, in cfg as a dict
        gnd_fname = os.path.join(dir_main, dataset, 'gnd_{}.pkl'.format(dataset))
        with open(gnd_fname, 'rb') as f:
            cfg = pickle.load(f)
        cfg['gnd_fname'] = gnd_fname
        cfg['ext'] = '.jpg'
        cfg['qext'] = '.jpg'
        cfg['dir_data'] = os.path.join(dir_main, dataset)
        cfg['dir_images'] = os.path.join(cfg['dir_data'], 'jpg')
        cfg['n'] = len(cfg['imlist'])
        cfg['nq'] = len(cfg['qimlist'])
        cfg['im_fname'] = config_imname
        cfg['qim_fname'] = config_qimname
        cfg['dataset'] = dataset
        self.cfg = cfg

        self.samples = cfg[""qimlist""] if split == ""query"" else cfg[""imlist""]
        self.transform = transform
        self.imsize = imsize",_277.py,13,23,"def __init__(self, dir_main, dataset, split, transform=None, imsize=None):
    if dataset not in ['roxford5k', 'rparis6k']:
        raise ValueError('Unknown dataset: {}!'.format(dataset))
    gnd_fname = os.path.join(dir_main, dataset, 'gnd_{}.pkl'.format(dataset))
    with open(gnd_fname, 'rb') as f:
        cfg = pickle.load(f)
    cfg['gnd_fname'] = gnd_fname
    cfg['ext'] = '.jpg'
    cfg['qext'] = '.jpg'
    cfg['dir_data'] = os.path.join(dir_main, dataset)
    cfg['dir_images'] = os.path.join(cfg['dir_data'], 'jpg')
    cfg['n'] = len(cfg['imlist'])
    cfg['nq'] = len(cfg['qimlist'])
    cfg['im_fname'] = config_imname
    cfg['qim_fname'] = config_qimname
    cfg['dataset'] = dataset
    self.cfg = cfg
    self.samples = cfg['qimlist'] if split == 'query' else cfg['imlist']
    self.transform = transform
    self.imsize = imsize"
https://github.com/facebookresearch/dino/tree/master//eval_image_retrieval.py,"def __init__(self, dir_main, dataset, split, transform=None, imsize=None):
        if dataset not in ['roxford5k', 'rparis6k']:
            raise ValueError('Unknown dataset: {}!'.format(dataset))

        # loading imlist, qimlist, and gnd, in cfg as a dict
        gnd_fname = os.path.join(dir_main, dataset, 'gnd_{}.pkl'.format(dataset))
        with open(gnd_fname, 'rb') as f:
            cfg = pickle.load(f)
        cfg['gnd_fname'] = gnd_fname
        cfg['ext'] = '.jpg'
        cfg['qext'] = '.jpg'
        cfg['dir_data'] = os.path.join(dir_main, dataset)
        cfg['dir_images'] = os.path.join(cfg['dir_data'], 'jpg')
        cfg['n'] = len(cfg['imlist'])
        cfg['nq'] = len(cfg['qimlist'])
        cfg['im_fname'] = config_imname
        cfg['qim_fname'] = config_qimname
        cfg['dataset'] = dataset
        self.cfg = cfg

        self.samples = cfg[""qimlist""] if split == ""query"" else cfg[""imlist""]
        self.transform = transform
        self.imsize = imsize",_277.py,9,"cfg['gnd_fname'] = gnd_fname
cfg['ext'] = '.jpg'
cfg['qext'] = '.jpg'
cfg['dir_data'] = os.path.join(dir_main, dataset)","cfg['gnd_fname'] , cfg['ext'] , cfg['qext'] , cfg['dir_data']  = gnd_fname, '.jpg', '.jpg', os.path.join(dir_main, dataset)"
https://github.com/facebookresearch/dino/tree/master//eval_image_retrieval.py,"def __init__(self, dir_main, dataset, split, transform=None, imsize=None):
        if dataset not in ['roxford5k', 'rparis6k']:
            raise ValueError('Unknown dataset: {}!'.format(dataset))

        # loading imlist, qimlist, and gnd, in cfg as a dict
        gnd_fname = os.path.join(dir_main, dataset, 'gnd_{}.pkl'.format(dataset))
        with open(gnd_fname, 'rb') as f:
            cfg = pickle.load(f)
        cfg['gnd_fname'] = gnd_fname
        cfg['ext'] = '.jpg'
        cfg['qext'] = '.jpg'
        cfg['dir_data'] = os.path.join(dir_main, dataset)
        cfg['dir_images'] = os.path.join(cfg['dir_data'], 'jpg')
        cfg['n'] = len(cfg['imlist'])
        cfg['nq'] = len(cfg['qimlist'])
        cfg['im_fname'] = config_imname
        cfg['qim_fname'] = config_qimname
        cfg['dataset'] = dataset
        self.cfg = cfg

        self.samples = cfg[""qimlist""] if split == ""query"" else cfg[""imlist""]
        self.transform = transform
        self.imsize = imsize",_277.py,15,"cfg['nq'] = len(cfg['qimlist'])
cfg['im_fname'] = config_imname
cfg['qim_fname'] = config_qimname
cfg['dataset'] = dataset","cfg['nq'] , cfg['im_fname'] , cfg['qim_fname'] , cfg['dataset']  = len(cfg['qimlist']), config_imname, config_qimname, dataset"
https://github.com/facebookresearch/dino/tree/master//eval_image_retrieval.py,"def __init__(self, dir_main, dataset, split, transform=None, imsize=None):
        if dataset not in ['roxford5k', 'rparis6k']:
            raise ValueError('Unknown dataset: {}!'.format(dataset))

        # loading imlist, qimlist, and gnd, in cfg as a dict
        gnd_fname = os.path.join(dir_main, dataset, 'gnd_{}.pkl'.format(dataset))
        with open(gnd_fname, 'rb') as f:
            cfg = pickle.load(f)
        cfg['gnd_fname'] = gnd_fname
        cfg['ext'] = '.jpg'
        cfg['qext'] = '.jpg'
        cfg['dir_data'] = os.path.join(dir_main, dataset)
        cfg['dir_images'] = os.path.join(cfg['dir_data'], 'jpg')
        cfg['n'] = len(cfg['imlist'])
        cfg['nq'] = len(cfg['qimlist'])
        cfg['im_fname'] = config_imname
        cfg['qim_fname'] = config_qimname
        cfg['dataset'] = dataset
        self.cfg = cfg

        self.samples = cfg[""qimlist""] if split == ""query"" else cfg[""imlist""]
        self.transform = transform
        self.imsize = imsize",_277.py,19,"self.cfg = cfg
self.samples = cfg['qimlist'] if split == 'query' else cfg['imlist']
self.transform = transform
self.imsize = imsize","self.cfg , self.samples , self.transform , self.imsize  = cfg, cfg['qimlist'] if split == 'query' else cfg['imlist'], transform, imsize"
https://github.com/mahmoud/glom/tree/master/glom/core.py,"def glom(target, spec, **kwargs):
    """"""Access or construct a value from a given *target* based on the
    specification declared by *spec*.

    Accessing nested data, aka deep-get:

    >>> target = {'a': {'b': 'c'}}
    >>> glom(target, 'a.b')
    'c'

    Here the *spec* was just a string denoting a path,
    ``'a.b.``. As simple as it should be. The next example shows
    how to use nested data to access many fields at once, and make
    a new nested structure.

    Constructing, or restructuring more-complicated nested data:

    >>> target = {'a': {'b': 'c', 'd': 'e'}, 'f': 'g', 'h': [0, 1, 2]}
    >>> spec = {'a': 'a.b', 'd': 'a.d', 'h': ('h', [lambda x: x * 2])}
    >>> output = glom(target, spec)
    >>> pprint(output)
    {'a': 'c', 'd': 'e', 'h': [0, 2, 4]}

    ``glom`` also takes a keyword-argument, *default*. When set,
    if a ``glom`` operation fails with a :exc:`GlomError`, the
    *default* will be returned, very much like
    :meth:`dict.get()`:

    >>> glom(target, 'a.xx', default='nada')
    'nada'

    The *skip_exc* keyword argument controls which errors should
    be ignored.

    >>> glom({}, lambda x: 100.0 / len(x), default=0.0, skip_exc=ZeroDivisionError)
    0.0

    Args:
       target (object): the object on which the glom will operate.
       spec (object): Specification of the output object in the form
         of a dict, list, tuple, string, other glom construct, or
         any composition of these.
       default (object): An optional default to return in the case
         an exception, specified by *skip_exc*, is raised.
       skip_exc (Exception): An optional exception or tuple of
         exceptions to ignore and return *default* (None if
         omitted). If *skip_exc* and *default* are both not set,
         glom raises errors through.
       scope (dict): Additional data that can be accessed
         via S inside the glom-spec. Read more: :ref:`scope`.

    It's a small API with big functionality, and glom's power is
    only surpassed by its intuitiveness. Give it a whirl!

    """"""
    # TODO: check spec up front
    default = kwargs.pop('default', None if 'skip_exc' in kwargs else _MISSING)
    skip_exc = kwargs.pop('skip_exc', () if default is _MISSING else GlomError)
    glom_debug = kwargs.pop('glom_debug', GLOM_DEBUG)
    scope = _DEFAULT_SCOPE.new_child({
        Path: kwargs.pop('path', []),
        Inspect: kwargs.pop('inspector', None),
        MODE: AUTO,
        CHILD_ERRORS: [],
        'globals': ScopeVars({}, {}),
    })
    scope[UP] = scope
    scope[ROOT] = scope
    scope[T] = target
    scope.update(kwargs.pop('scope', {}))
    err = None
    if kwargs:
        raise TypeError('unexpected keyword args: %r' % sorted(kwargs.keys()))
    try:
        try:
            ret = _glom(target, spec, scope)
        except skip_exc:
            if default is _MISSING:
                raise
            ret = default
    except Exception as e:
        if glom_debug:
            raise
        if isinstance(e, GlomError):
            # need to change id or else py3 seems to not let us truncate the
            # stack trace with the explicit ""raise err"" below
            err = copy.copy(e)
            err._set_wrapped(e)
        else:
            err = GlomError.wrap(e)
        if isinstance(err, GlomError):
            err._finalize(scope[LAST_CHILD_SCOPE])
        else:  # wrapping failed, fall back to default behavior
            raise

    if err:
        raise err
    return ret",_28.py,58,"skip_exc = kwargs.pop('skip_exc', () if default is _MISSING else GlomError)
glom_debug = kwargs.pop('glom_debug', GLOM_DEBUG)
scope = _DEFAULT_SCOPE.new_child({Path: kwargs.pop('path', []), Inspect: kwargs.pop('inspector', None), MODE: AUTO, CHILD_ERRORS: [], 'globals': ScopeVars({}, {})})","skip_exc , glom_debug , scope  = kwargs.pop('skip_exc', () if default is _MISSING else GlomError), kwargs.pop('glom_debug', GLOM_DEBUG), _DEFAULT_SCOPE.new_child({Path: kwargs.pop('path', []), Inspect: kwargs.pop('inspector', None), MODE: AUTO, CHILD_ERRORS: [], 'globals': ScopeVars({}, {})})"
https://github.com/mahmoud/glom/tree/master/glom/core.py,"def glom(target, spec, **kwargs):
    """"""Access or construct a value from a given *target* based on the
    specification declared by *spec*.

    Accessing nested data, aka deep-get:

    >>> target = {'a': {'b': 'c'}}
    >>> glom(target, 'a.b')
    'c'

    Here the *spec* was just a string denoting a path,
    ``'a.b.``. As simple as it should be. The next example shows
    how to use nested data to access many fields at once, and make
    a new nested structure.

    Constructing, or restructuring more-complicated nested data:

    >>> target = {'a': {'b': 'c', 'd': 'e'}, 'f': 'g', 'h': [0, 1, 2]}
    >>> spec = {'a': 'a.b', 'd': 'a.d', 'h': ('h', [lambda x: x * 2])}
    >>> output = glom(target, spec)
    >>> pprint(output)
    {'a': 'c', 'd': 'e', 'h': [0, 2, 4]}

    ``glom`` also takes a keyword-argument, *default*. When set,
    if a ``glom`` operation fails with a :exc:`GlomError`, the
    *default* will be returned, very much like
    :meth:`dict.get()`:

    >>> glom(target, 'a.xx', default='nada')
    'nada'

    The *skip_exc* keyword argument controls which errors should
    be ignored.

    >>> glom({}, lambda x: 100.0 / len(x), default=0.0, skip_exc=ZeroDivisionError)
    0.0

    Args:
       target (object): the object on which the glom will operate.
       spec (object): Specification of the output object in the form
         of a dict, list, tuple, string, other glom construct, or
         any composition of these.
       default (object): An optional default to return in the case
         an exception, specified by *skip_exc*, is raised.
       skip_exc (Exception): An optional exception or tuple of
         exceptions to ignore and return *default* (None if
         omitted). If *skip_exc* and *default* are both not set,
         glom raises errors through.
       scope (dict): Additional data that can be accessed
         via S inside the glom-spec. Read more: :ref:`scope`.

    It's a small API with big functionality, and glom's power is
    only surpassed by its intuitiveness. Give it a whirl!

    """"""
    # TODO: check spec up front
    default = kwargs.pop('default', None if 'skip_exc' in kwargs else _MISSING)
    skip_exc = kwargs.pop('skip_exc', () if default is _MISSING else GlomError)
    glom_debug = kwargs.pop('glom_debug', GLOM_DEBUG)
    scope = _DEFAULT_SCOPE.new_child({
        Path: kwargs.pop('path', []),
        Inspect: kwargs.pop('inspector', None),
        MODE: AUTO,
        CHILD_ERRORS: [],
        'globals': ScopeVars({}, {}),
    })
    scope[UP] = scope
    scope[ROOT] = scope
    scope[T] = target
    scope.update(kwargs.pop('scope', {}))
    err = None
    if kwargs:
        raise TypeError('unexpected keyword args: %r' % sorted(kwargs.keys()))
    try:
        try:
            ret = _glom(target, spec, scope)
        except skip_exc:
            if default is _MISSING:
                raise
            ret = default
    except Exception as e:
        if glom_debug:
            raise
        if isinstance(e, GlomError):
            # need to change id or else py3 seems to not let us truncate the
            # stack trace with the explicit ""raise err"" below
            err = copy.copy(e)
            err._set_wrapped(e)
        else:
            err = GlomError.wrap(e)
        if isinstance(err, GlomError):
            err._finalize(scope[LAST_CHILD_SCOPE])
        else:  # wrapping failed, fall back to default behavior
            raise

    if err:
        raise err
    return ret",_28.py,68,"scope[ROOT] = scope
scope[T] = target","scope[ROOT] , scope[T]  = scope, target"
https://github.com/SystemErrorWang/White-box-Cartoonization/tree/master/train_code/utils.py,"def write_batch_image(image, save_dir, name, n):
    
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    
    fused_dir = os.path.join(save_dir, name)
    fused_image = [0] * n
    for i in range(n):
        fused_image[i] = []
        for j in range(n):
            k = i * n + j
            image[k] = (image[k]+1) * 127.5
            #image[k] = image[k] - np.min(image[k])
            #image[k] = image[k]/np.max(image[k])
            #image[k] = image[k] * 255.0
            fused_image[i].append(image[k])
        fused_image[i] = np.hstack(fused_image[i])
    fused_image = np.vstack(fused_image)
    cv2.imwrite(fused_dir, fused_image.astype(np.uint8))",_281.py,6,"fused_dir = os.path.join(save_dir, name)
fused_image = [0] * n","fused_dir , fused_image  = os.path.join(save_dir, name), [0] * n"
https://github.com/quantopian/trading_calendars/tree/master/trading_calendars/trading_calendar.py,"def _special_dates(self, calendars, ad_hoc_dates, start_date, end_date):
        """"""
        Compute a Series of times associated with special dates.

        Parameters
        ----------
        holiday_calendars : list[(datetime.time, HolidayCalendar)]
            Pairs of time and calendar describing when that time occurs. These
            are used to describe regularly-scheduled late opens or early
            closes.
        ad_hoc_dates : list[(datetime.time, list[pd.Timestamp])]
            Pairs of time and list of dates associated with the given times.
            These are used to describe late opens or early closes that occurred
            for unscheduled or otherwise irregular reasons.
        start_date : pd.Timestamp
            Start of the range for which we should calculate special dates.
        end_date : pd.Timestamp
            End of the range for which we should calculate special dates.

        Returns
        -------
        special_dates : pd.Series
            Series mapping trading sessions with special opens/closes to the
            special open/close for that session.
        """"""
        # List of Series for regularly-scheduled times.
        regular = [
            scheduled_special_times(
                calendar,
                start_date,
                end_date,
                time_,
                self.tz,
            )
            for time_, calendar in calendars
        ]

        # List of Series for ad-hoc times.
        ad_hoc = [
            pd.Series(
                index=pd.to_datetime(datetimes, utc=True),
                data=days_at_time(datetimes, time_, self.tz),
            )
            for time_, datetimes in ad_hoc_dates
        ]

        merged = regular + ad_hoc
        if not merged:
            # Concat barfs if the input has length 0.
            return pd.Series([], dtype=""datetime64[ns, UTC]"")

        result = pd.concat(merged).sort_index()
        return result.loc[(result >= start_date) & (result <= end_date)]",_283.py,27,"regular = [scheduled_special_times(calendar, start_date, end_date, time_, self.tz) for (time_, calendar) in calendars]
ad_hoc = [pd.Series(index=pd.to_datetime(datetimes, utc=True), data=days_at_time(datetimes, time_, self.tz)) for (time_, datetimes) in ad_hoc_dates]","regular , ad_hoc  = [scheduled_special_times(calendar, start_date, end_date, time_, self.tz) for (time_, calendar) in calendars], [pd.Series(index=pd.to_datetime(datetimes, utc=True), data=days_at_time(datetimes, time_, self.tz)) for (time_, datetimes) in ad_hoc_dates]"
https://github.com/poodarchu/Det3D/tree/master/det3d/models/losses/smooth_l1_loss.py,"def __init__(self, beta=1.0, reduction=""mean"", loss_weight=1.0):
        super(SmoothL1Loss, self).__init__()
        self.beta = beta
        self.reduction = reduction
        self.loss_weight = loss_weight",_285.py,3,"self.beta = beta
self.reduction = reduction
self.loss_weight = loss_weight","self.beta , self.reduction , self.loss_weight  = beta, reduction, loss_weight"
https://github.com/NicklasTegner/pypandoc/tree/master/pypandoc/pandoc_download.py,"def _get_pandoc_urls(version=""latest""):
    """"""Get the urls of pandoc's binaries
    Uses sys.platform keys, but removes the 2 from linux2
    Adding a new platform means implementing unpacking in ""DownloadPandocCommand""
    and adding the URL here

    :param str version: pandoc version.
        Valid values are either a valid pandoc version e.g. ""1.19.1"", or ""latest""
        Default: ""latest"".

    :return: str pandoc_urls: a dictionary with keys as system platform
        and values as the url pointing to respective binaries

    :return: str version: actual pandoc version. (e.g. ""lastest"" will be resolved to the actual one)
    """"""
    # url to pandoc download page
    url = ""https://github.com/jgm/pandoc/releases/"" + \
          (""tag/"" if version != ""latest"" else """") + version
    # try to open the url
    try:
        response = urlopen(url)
        version_url_frags = response.url.split(""/"")
        version = version_url_frags[-1]
    except urllib.error.HTTPError as e:
        raise RuntimeError(""Invalid pandoc version {}."".format(version))
        return
    # read the HTML content
    response = urlopen(f""https://github.com/jgm/pandoc/releases/expanded_assets/{version}"")
    content = response.read()
    # regex for the binaries
    uname = platform.uname()[4]
    processor_architecture = ""arm"" if uname.startswith(""arm"") or uname.startswith(""aarch"") else ""amd""
    regex = re.compile(r""/jgm/pandoc/releases/download/.*(?:""+processor_architecture+""|x86|mac).*\.(?:msi|deb|pkg)"")
    # a list of urls to the binaries
    pandoc_urls_list = regex.findall(content.decode(""utf-8""))
    # actual pandoc version
    version = pandoc_urls_list[0].split('/')[5]
    # dict that lookup the platform from binary extension
    ext2platform = {
        'msi': 'win32',
        'deb': 'linux',
        'pkg': 'darwin'
    }
    # parse pandoc_urls from list to dict
    # py26 don't like dict comprehension. Use this one instead when py26 support is dropped
    pandoc_urls = {ext2platform[url_frag[-3:]]: (f""https://github.com{url_frag}"") for url_frag in pandoc_urls_list}
    return pandoc_urls, version",_288.py,29,"content = response.read()
uname = platform.uname()[4]","content , uname  = response.read(), platform.uname()[4]"
https://github.com/NicklasTegner/pypandoc/tree/master/pypandoc/pandoc_download.py,"def _get_pandoc_urls(version=""latest""):
    """"""Get the urls of pandoc's binaries
    Uses sys.platform keys, but removes the 2 from linux2
    Adding a new platform means implementing unpacking in ""DownloadPandocCommand""
    and adding the URL here

    :param str version: pandoc version.
        Valid values are either a valid pandoc version e.g. ""1.19.1"", or ""latest""
        Default: ""latest"".

    :return: str pandoc_urls: a dictionary with keys as system platform
        and values as the url pointing to respective binaries

    :return: str version: actual pandoc version. (e.g. ""lastest"" will be resolved to the actual one)
    """"""
    # url to pandoc download page
    url = ""https://github.com/jgm/pandoc/releases/"" + \
          (""tag/"" if version != ""latest"" else """") + version
    # try to open the url
    try:
        response = urlopen(url)
        version_url_frags = response.url.split(""/"")
        version = version_url_frags[-1]
    except urllib.error.HTTPError as e:
        raise RuntimeError(""Invalid pandoc version {}."".format(version))
        return
    # read the HTML content
    response = urlopen(f""https://github.com/jgm/pandoc/releases/expanded_assets/{version}"")
    content = response.read()
    # regex for the binaries
    uname = platform.uname()[4]
    processor_architecture = ""arm"" if uname.startswith(""arm"") or uname.startswith(""aarch"") else ""amd""
    regex = re.compile(r""/jgm/pandoc/releases/download/.*(?:""+processor_architecture+""|x86|mac).*\.(?:msi|deb|pkg)"")
    # a list of urls to the binaries
    pandoc_urls_list = regex.findall(content.decode(""utf-8""))
    # actual pandoc version
    version = pandoc_urls_list[0].split('/')[5]
    # dict that lookup the platform from binary extension
    ext2platform = {
        'msi': 'win32',
        'deb': 'linux',
        'pkg': 'darwin'
    }
    # parse pandoc_urls from list to dict
    # py26 don't like dict comprehension. Use this one instead when py26 support is dropped
    pandoc_urls = {ext2platform[url_frag[-3:]]: (f""https://github.com{url_frag}"") for url_frag in pandoc_urls_list}
    return pandoc_urls, version",_288.py,37,"version = pandoc_urls_list[0].split('/')[5]
ext2platform = {'msi': 'win32', 'deb': 'linux', 'pkg': 'darwin'}","version , ext2platform  = pandoc_urls_list[0].split('/')[5], {'msi': 'win32', 'deb': 'linux', 'pkg': 'darwin'}"
https://github.com/piccolo-orm/piccolo/tree/master/tests/query/test_freeze.py,"def test_frozen_performance(self):
        """"""
        The frozen query performance should exceed the non-frozen. If not,
        there's a problem.

        Only test this on SQLite, as the latency from the database itself
        is more predictable than with Postgres, and the test runs quickly.

        """"""
        iterations = 50
        query = Band.select().where(Band.name == ""Pythonistas"").first()
        query_duration = timeit.repeat(
            lambda: query.run_sync(), repeat=iterations, number=1
        )

        frozen_query = query.freeze()
        frozen_query_duration = timeit.repeat(
            lambda: frozen_query.run_sync(), repeat=iterations, number=1
        )

        # Remove the outliers before comparing
        self.assertGreater(
            sum(sorted(query_duration)[5:-5]),
            sum(sorted(frozen_query_duration)[5:-5]),
        )",_29.py,10,"iterations = 50
query = Band.select().where(Band.name == 'Pythonistas').first()","iterations , query  = 50, Band.select().where(Band.name == 'Pythonistas').first()"
https://github.com/piccolo-orm/piccolo/tree/master/tests/query/test_freeze.py,"def test_frozen_performance(self):
        """"""
        The frozen query performance should exceed the non-frozen. If not,
        there's a problem.

        Only test this on SQLite, as the latency from the database itself
        is more predictable than with Postgres, and the test runs quickly.

        """"""
        iterations = 50
        query = Band.select().where(Band.name == ""Pythonistas"").first()
        query_duration = timeit.repeat(
            lambda: query.run_sync(), repeat=iterations, number=1
        )

        frozen_query = query.freeze()
        frozen_query_duration = timeit.repeat(
            lambda: frozen_query.run_sync(), repeat=iterations, number=1
        )

        # Remove the outliers before comparing
        self.assertGreater(
            sum(sorted(query_duration)[5:-5]),
            sum(sorted(frozen_query_duration)[5:-5]),
        )",_29.py,12,"query_duration = timeit.repeat(lambda : query.run_sync(), repeat=iterations, number=1)
frozen_query = query.freeze()","query_duration , frozen_query  = timeit.repeat(lambda : query.run_sync(), repeat=iterations, number=1), query.freeze()"
https://github.com/rockstor/rockstor-core/tree/master/src/rockstor/scripts/initrock.py,"def require_postgres(logging):
    rs_dest = ""/etc/systemd/system/rockstor-pre.service""
    rs_src = ""{}/conf/rockstor-pre.service"".format(BASE_DIR)
    logging.info(""updating rockstor-pre service.."")
    with open(rs_dest, ""w"") as dfo, open(rs_src) as sfo:
        for l in sfo.readlines():
            dfo.write(l)
            if re.match(""After=postgresql.service"", l) is not None:
                dfo.write(""Requires=postgresql.service\n"")
                logging.info(""rockstor-pre now requires postgresql"")
    run_command([SYSCTL, ""daemon-reload""])
    return logging.info(""systemd daemon reloaded"")",_297.py,2,"rs_dest = '/etc/systemd/system/rockstor-pre.service'
rs_src = '{}/conf/rockstor-pre.service'.format(BASE_DIR)","rs_dest , rs_src  = '/etc/systemd/system/rockstor-pre.service', '{}/conf/rockstor-pre.service'.format(BASE_DIR)"
https://github.com/apache/tvm/tree/master/tests/python/unittest/test_target_codegen_llvm.py,"def test_llvm_vadd_pipeline():
    def check_llvm(n, lanes):
        A = te.placeholder((n,), name=""A"", dtype=""float32x%d"" % lanes)
        B = te.compute((n,), lambda i: A[i], name=""B"")
        C = te.compute((n,), lambda i: B[i] + tvm.tir.const(1, A.dtype), name=""C"")
        s = te.create_schedule(C.op)
        xo, xi = s[C].split(C.op.axis[0], nparts=2)
        _, xi = s[C].split(xi, factor=2)
        s[C].parallel(xo)
        s[C].vectorize(xi)
        s[B].compute_at(s[C], xo)
        xo, xi = s[B].split(B.op.axis[0], factor=2)
        s[B].vectorize(xi)
        # build and invoke the kernel.
        f = tvm.build(s, [A, C], ""llvm"")
        dev = tvm.cpu(0)
        # launch the kernel.
        a = tvm.nd.empty((n,), A.dtype).copyfrom(np.random.uniform(size=(n, lanes)))
        c = tvm.nd.empty((n,), C.dtype, dev)
        f(a, c)
        tvm.testing.assert_allclose(c.numpy(), a.numpy() + 1)

    check_llvm(64, 2)
    check_llvm(512, 2)",_298.py,15,"f = tvm.build(s, [A, C], 'llvm')
dev = tvm.cpu(0)","f , dev  = tvm.build(s, [A, C], 'llvm'), tvm.cpu(0)"
https://github.com/apache/tvm/tree/master/tests/python/unittest/test_target_codegen_llvm.py,"def test_llvm_vadd_pipeline():
    def check_llvm(n, lanes):
        A = te.placeholder((n,), name=""A"", dtype=""float32x%d"" % lanes)
        B = te.compute((n,), lambda i: A[i], name=""B"")
        C = te.compute((n,), lambda i: B[i] + tvm.tir.const(1, A.dtype), name=""C"")
        s = te.create_schedule(C.op)
        xo, xi = s[C].split(C.op.axis[0], nparts=2)
        _, xi = s[C].split(xi, factor=2)
        s[C].parallel(xo)
        s[C].vectorize(xi)
        s[B].compute_at(s[C], xo)
        xo, xi = s[B].split(B.op.axis[0], factor=2)
        s[B].vectorize(xi)
        # build and invoke the kernel.
        f = tvm.build(s, [A, C], ""llvm"")
        dev = tvm.cpu(0)
        # launch the kernel.
        a = tvm.nd.empty((n,), A.dtype).copyfrom(np.random.uniform(size=(n, lanes)))
        c = tvm.nd.empty((n,), C.dtype, dev)
        f(a, c)
        tvm.testing.assert_allclose(c.numpy(), a.numpy() + 1)

    check_llvm(64, 2)
    check_llvm(512, 2)",_298.py,18,"a = tvm.nd.empty((n,), A.dtype).copyfrom(np.random.uniform(size=(n, lanes)))
c = tvm.nd.empty((n,), C.dtype, dev)","a , c  = tvm.nd.empty((n,), A.dtype).copyfrom(np.random.uniform(size=(n, lanes))), tvm.nd.empty((n,), C.dtype, dev)"
https://github.com/diffgram/diffgram/tree/master/default/tests/methods/event/test_user_visit_history.py,"def setUp(self):
        # TODO: this test is assuming the 'my-sandbox-project' exists and some object have been previously created.
        # For future tests a mechanism of setting up and tearing down the database should be created.
        super(TestUserVisitHistory, self).setUp()
        project_data = data_mocking.create_project_with_context(
            {
                'users': [
                    {'username': 'Test',
                     'email': 'test@test.com',
                     'password': 'diffgram123',
                     }
                ]
            },
            self.session
        )
        self.project = project_data['project']
        self.project_data = project_data
        self.auth_api = common_actions.create_project_auth(project = self.project, session = self.session)
        self.member = self.auth_api.member",_3.py,16,"self.project = project_data['project']
self.project_data = project_data","self.project , self.project_data  = project_data['project'], project_data"
https://github.com/quay/quay/tree/master/util/tufmetadata/test/test_tufmetadata.py,"def test_gun(tuf_prefix, server_hostname, namespace, repo, gun):
    app = Flask(__name__)
    app.config.from_object(testconfig.TestConfig())
    app.config[""TUF_GUN_PREFIX""] = tuf_prefix
    app.config[""SERVER_HOSTNAME""] = server_hostname
    tuf_api = api.TUFMetadataAPI(app, app.config)
    assert gun == tuf_api._gun(namespace, repo)",_300.py,4,"app.config['TUF_GUN_PREFIX'] = tuf_prefix
app.config['SERVER_HOSTNAME'] = server_hostname","app.config['TUF_GUN_PREFIX'] , app.config['SERVER_HOSTNAME']  = tuf_prefix, server_hostname"
https://github.com/lyft/l5kit/tree/master/l5kit/l5kit/tests/dataset/dataset_test.py,"def test_vector_ego(zarr_dataset: ChunkedDataset, dmg: LocalDataManager, cfg: dict, history_num_frames_ego: int,
                    history_num_frames_agents: int) -> None:
    cfg[""model_params""][""history_num_frames_ego""] = history_num_frames_ego
    cfg[""model_params""][""history_num_frames_agents""] = history_num_frames_agents

    vect = build_vectorizer(cfg, dmg)
    dataset = EgoDatasetVectorized(cfg, zarr_dataset, vect)
    indexes = [0, 1, 10, -1]
    for idx in indexes:
        dataset[idx]
    check_torch_loading(dataset)",_303.py,3,"cfg['model_params']['history_num_frames_ego'] = history_num_frames_ego
cfg['model_params']['history_num_frames_agents'] = history_num_frames_agents","cfg['model_params']['history_num_frames_ego'] , cfg['model_params']['history_num_frames_agents']  = history_num_frames_ego, history_num_frames_agents"
https://github.com/lyft/l5kit/tree/master/l5kit/l5kit/tests/dataset/dataset_test.py,"def test_vector_ego(zarr_dataset: ChunkedDataset, dmg: LocalDataManager, cfg: dict, history_num_frames_ego: int,
                    history_num_frames_agents: int) -> None:
    cfg[""model_params""][""history_num_frames_ego""] = history_num_frames_ego
    cfg[""model_params""][""history_num_frames_agents""] = history_num_frames_agents

    vect = build_vectorizer(cfg, dmg)
    dataset = EgoDatasetVectorized(cfg, zarr_dataset, vect)
    indexes = [0, 1, 10, -1]
    for idx in indexes:
        dataset[idx]
    check_torch_loading(dataset)",_303.py,7,"dataset = EgoDatasetVectorized(cfg, zarr_dataset, vect)
indexes = [0, 1, 10, -1]","dataset , indexes  = EgoDatasetVectorized(cfg, zarr_dataset, vect), [0, 1, 10, -1]"
https://github.com/LinOTP/LinOTP/tree/master/linotp/lib/reply.py,"def create_img(data, width=0, alt=None, img_id=""challenge_qrcode""):
    """"""
    _create_img - create the qr image data

    :param data: input data that will be munched into the qrcode
    :type  data: string
    :param width: image width in pixel
    :type  width: int

    :return: <img/> taged data
    :rtype:  string
    """"""
    width_str = """"
    alt_str = """"

    img_src = create_img_src(data)

    if width != 0:
        width_str = "" width=%d "" % (int(width))

    if alt is not None:
        val = urllib.parse.urlencode({""alt"": alt})
        alt_str = "" alt=%r "" % (val[len(""alt="") :])

    ret_img = '<img id=""%s"" %s  %s  src=""%s""/>' % (
        img_id,
        alt_str,
        width_str,
        img_src,
    )

    return ret_img",_307.py,13,"width_str = ''
alt_str = ''
img_src = create_img_src(data)","width_str , alt_str , img_src  = '', '', create_img_src(data)"
https://github.com/MultiAgentLearning/playground/tree/master/pommerman/characters.py,"def set_agent_id(self, agent_id):
        self.agent_id = agent_id
        if self._game_type == constants.GameType.FFA:
            self.teammate = constants.Item.AgentDummy
            self.enemies = [
                getattr(constants.Item, 'Agent%d' % id_)
                for id_ in range(4)
                if id_ != agent_id
            ]
        elif self._game_type == constants.GameType.OneVsOne:
            self.teammate = constants.Item.AgentDummy
            self.enemies = [
                getattr(constants.Item, 'Agent%d' % id_)
                for id_ in range(2)
                if id_ != agent_id
            ]
        else:
            teammate_id = (agent_id + 2) % 4
            self.teammate = getattr(constants.Item, 'Agent%d' % teammate_id)
            self.enemies = [
                getattr(constants.Item, 'Agent%d' % id_)
                for id_ in range(4)
                if id_ != agent_id and id_ != teammate_id
            ]
            self.enemies.append(constants.Item.AgentDummy)",_308.py,4,"self.teammate = constants.Item.AgentDummy
self.enemies = [getattr(constants.Item, 'Agent%d' % id_) for id_ in range(4) if id_ != agent_id]","self.teammate , self.enemies  = constants.Item.AgentDummy, [getattr(constants.Item, 'Agent%d' % id_) for id_ in range(4) if id_ != agent_id]"
https://github.com/MultiAgentLearning/playground/tree/master/pommerman/characters.py,"def set_agent_id(self, agent_id):
        self.agent_id = agent_id
        if self._game_type == constants.GameType.FFA:
            self.teammate = constants.Item.AgentDummy
            self.enemies = [
                getattr(constants.Item, 'Agent%d' % id_)
                for id_ in range(4)
                if id_ != agent_id
            ]
        elif self._game_type == constants.GameType.OneVsOne:
            self.teammate = constants.Item.AgentDummy
            self.enemies = [
                getattr(constants.Item, 'Agent%d' % id_)
                for id_ in range(2)
                if id_ != agent_id
            ]
        else:
            teammate_id = (agent_id + 2) % 4
            self.teammate = getattr(constants.Item, 'Agent%d' % teammate_id)
            self.enemies = [
                getattr(constants.Item, 'Agent%d' % id_)
                for id_ in range(4)
                if id_ != agent_id and id_ != teammate_id
            ]
            self.enemies.append(constants.Item.AgentDummy)",_308.py,11,"self.teammate = constants.Item.AgentDummy
self.enemies = [getattr(constants.Item, 'Agent%d' % id_) for id_ in range(2) if id_ != agent_id]","self.teammate , self.enemies  = constants.Item.AgentDummy, [getattr(constants.Item, 'Agent%d' % id_) for id_ in range(2) if id_ != agent_id]"
https://github.com/datafolklabs/cement/tree/master/cement/cli/contrib/jinja2/lexer.py,"def look(self):
        """"""Look at the next token.""""""
        old_token = next(self)
        result = self.current
        self.push(result)
        self.current = old_token
        return result",_321.py,3,"old_token = next(self)
result = self.current","old_token , result  = next(self), self.current"
https://github.com/pandas-dev/pandas/tree/master/pandas/tests/dtypes/cast/test_downcast.py,"def test_downcast_conversion_nan(float_numpy_dtype):
    dtype = float_numpy_dtype
    data = [1.0, 2.0, np.nan]

    expected = np.array(data, dtype=dtype)
    arr = np.array(data, dtype=dtype)

    result = maybe_downcast_to_dtype(arr, ""infer"")
    tm.assert_almost_equal(result, expected)",_329.py,2,"dtype = float_numpy_dtype
data = [1.0, 2.0, np.nan]","dtype , data  = float_numpy_dtype, [1.0, 2.0, np.nan]"
https://github.com/pandas-dev/pandas/tree/master/pandas/tests/dtypes/cast/test_downcast.py,"def test_downcast_conversion_nan(float_numpy_dtype):
    dtype = float_numpy_dtype
    data = [1.0, 2.0, np.nan]

    expected = np.array(data, dtype=dtype)
    arr = np.array(data, dtype=dtype)

    result = maybe_downcast_to_dtype(arr, ""infer"")
    tm.assert_almost_equal(result, expected)",_329.py,5,"expected = np.array(data, dtype=dtype)
arr = np.array(data, dtype=dtype)","expected , arr  = np.array(data, dtype=dtype), np.array(data, dtype=dtype)"
https://github.com/gnes-ai/gnes/tree/master/gnes/encoder/numeric/vlad.py,"def _copy_from(self, x: 'VladEncoder') -> None:
        self.num_clusters = x.num_clusters
        self.centroids = x.centroids
        self.centroids_l2 = x.centroids_l2
        self.centroids_trans = np.transpose(self.centroids)
        self.using_faiss_pred = x.using_faiss_pred
        if self.using_faiss_pred:
            self.faiss_index()",_333.py,2,"self.num_clusters = x.num_clusters
self.centroids = x.centroids
self.centroids_l2 = x.centroids_l2","self.num_clusters , self.centroids , self.centroids_l2  = x.num_clusters, x.centroids, x.centroids_l2"
https://github.com/gnes-ai/gnes/tree/master/gnes/encoder/numeric/vlad.py,"def _copy_from(self, x: 'VladEncoder') -> None:
        self.num_clusters = x.num_clusters
        self.centroids = x.centroids
        self.centroids_l2 = x.centroids_l2
        self.centroids_trans = np.transpose(self.centroids)
        self.using_faiss_pred = x.using_faiss_pred
        if self.using_faiss_pred:
            self.faiss_index()",_333.py,5,"self.centroids_trans = np.transpose(self.centroids)
self.using_faiss_pred = x.using_faiss_pred","self.centroids_trans , self.using_faiss_pred  = np.transpose(self.centroids), x.using_faiss_pred"
https://github.com/facebookresearch/Detectron/tree/master/tools/infer.py,"def get_rpn_box_proposals(im, args):
    cfg.immutable(False)
    merge_cfg_from_file(args.rpn_cfg)
    cfg.NUM_GPUS = 1
    cfg.MODEL.RPN_ONLY = True
    cfg.TEST.RPN_PRE_NMS_TOP_N = 10000
    cfg.TEST.RPN_POST_NMS_TOP_N = 2000
    assert_and_infer_cfg(cache_urls=False)

    model = model_engine.initialize_model_from_cfg(args.rpn_pkl)
    with c2_utils.NamedCudaScope(0):
        boxes, scores = rpn_engine.im_proposals(model, im)
    return boxes, scores",_334.py,4,"cfg.NUM_GPUS = 1
cfg.MODEL.RPN_ONLY = True
cfg.TEST.RPN_PRE_NMS_TOP_N = 10000
cfg.TEST.RPN_POST_NMS_TOP_N = 2000","cfg.NUM_GPUS , cfg.MODEL.RPN_ONLY , cfg.TEST.RPN_PRE_NMS_TOP_N , cfg.TEST.RPN_POST_NMS_TOP_N  = 1, True, 10000, 2000"
https://github.com/frappe/frappe/tree/master/frappe/desk/notifications.py,"def get_notifications_for_targets(config, notification_percent):
	""""""Notifications for doc targets""""""
	can_read = frappe.get_user().get_can_read()
	doc_target_percents = {}

	# doc_target_percents = {
	# 	""Company"": {
	# 		""Acme"": 87,
	# 		""RobotsRUs"": 50,
	# 	}, {}...
	# }

	for doctype in config.targets:
		if doctype in can_read:
			if doctype in notification_percent:
				doc_target_percents[doctype] = notification_percent[doctype]
			else:
				doc_target_percents[doctype] = {}
				d = config.targets[doctype]
				condition = d[""filters""]
				target_field = d[""target_field""]
				value_field = d[""value_field""]
				try:
					if isinstance(condition, dict):
						doc_list = frappe.get_list(
							doctype,
							fields=[""name"", target_field, value_field],
							filters=condition,
							limit_page_length=100,
							ignore_ifnull=True,
						)

				except frappe.PermissionError:
					frappe.clear_messages()
					pass
				except Exception as e:
					if e.args[0] not in (1412, 1684):
						raise

				else:
					for doc in doc_list:
						value = doc[value_field]
						target = doc[target_field]
						doc_target_percents[doctype][doc.name] = (value / target * 100) if value < target else 100

	return doc_target_percents",_335.py,3,"can_read = frappe.get_user().get_can_read()
doc_target_percents = {}","can_read , doc_target_percents  = frappe.get_user().get_can_read(), {}"
https://github.com/frappe/frappe/tree/master/frappe/desk/notifications.py,"def get_notifications_for_targets(config, notification_percent):
	""""""Notifications for doc targets""""""
	can_read = frappe.get_user().get_can_read()
	doc_target_percents = {}

	# doc_target_percents = {
	# 	""Company"": {
	# 		""Acme"": 87,
	# 		""RobotsRUs"": 50,
	# 	}, {}...
	# }

	for doctype in config.targets:
		if doctype in can_read:
			if doctype in notification_percent:
				doc_target_percents[doctype] = notification_percent[doctype]
			else:
				doc_target_percents[doctype] = {}
				d = config.targets[doctype]
				condition = d[""filters""]
				target_field = d[""target_field""]
				value_field = d[""value_field""]
				try:
					if isinstance(condition, dict):
						doc_list = frappe.get_list(
							doctype,
							fields=[""name"", target_field, value_field],
							filters=condition,
							limit_page_length=100,
							ignore_ifnull=True,
						)

				except frappe.PermissionError:
					frappe.clear_messages()
					pass
				except Exception as e:
					if e.args[0] not in (1412, 1684):
						raise

				else:
					for doc in doc_list:
						value = doc[value_field]
						target = doc[target_field]
						doc_target_percents[doctype][doc.name] = (value / target * 100) if value < target else 100

	return doc_target_percents",_335.py,42,"value = doc[value_field]
target = doc[target_field]","value , target  = doc[value_field], doc[target_field]"
https://github.com/rspivak/lsbasi/tree/master/part9/python/spi.py,"def __init__(self, type, value):
        self.type = type
        self.value = value",_338.py,2,"self.type = type
self.value = value","self.type , self.value  = type, value"
https://github.com/locuslab/qpth/tree/master/qpth/qp.py,"def forward(ctx, Q_, p_, G_, h_, A_, b_):
            """"""Solve a batch of QPs.

            This function solves a batch of QPs, each optimizing over
            `nz` variables and having `nineq` inequality constraints
            and `neq` equality constraints.
            The optimization problem for each instance in the batch
            (dropping indexing from the notation) is of the form

                \hat z =   argmin_z 1/2 z^T Q z + p^T z
                        subject to Gz <= h
                                    Az  = b

            where Q \in S^{nz,nz},
                S^{nz,nz} is the set of all positive semi-definite matrices,
                p \in R^{nz}
                G \in R^{nineq,nz}
                h \in R^{nineq}
                A \in R^{neq,nz}
                b \in R^{neq}

            These parameters should all be passed to this function as
            Variable- or Parameter-wrapped Tensors.
            (See torch.autograd.Variable and torch.nn.parameter.Parameter)

            If you want to solve a batch of QPs where `nz`, `nineq` and `neq`
            are the same, but some of the contents differ across the
            minibatch, you can pass in tensors in the standard way
            where the first dimension indicates the batch example.
            This can be done with some or all of the coefficients.

            You do not need to add an extra dimension to coefficients
            that will not change across all of the minibatch examples.
            This function is able to infer such cases.

            If you don't want to use any equality or inequality constraints,
            you can set the appropriate values to:

                e = Variable(torch.Tensor())

            Parameters:
            Q:  A (nBatch, nz, nz) or (nz, nz) Tensor.
            p:  A (nBatch, nz) or (nz) Tensor.
            G:  A (nBatch, nineq, nz) or (nineq, nz) Tensor.
            h:  A (nBatch, nineq) or (nineq) Tensor.
            A:  A (nBatch, neq, nz) or (neq, nz) Tensor.
            b:  A (nBatch, neq) or (neq) Tensor.

            Returns: \hat z: a (nBatch, nz) Tensor.
            """"""
            nBatch = extract_nBatch(Q_, p_, G_, h_, A_, b_)
            Q, _ = expandParam(Q_, nBatch, 3)
            p, _ = expandParam(p_, nBatch, 2)
            G, _ = expandParam(G_, nBatch, 3)
            h, _ = expandParam(h_, nBatch, 2)
            A, _ = expandParam(A_, nBatch, 3)
            b, _ = expandParam(b_, nBatch, 2)

            if check_Q_spd:
                for i in range(nBatch):
                    e, _ = torch.eig(Q[i])
                    if not torch.all(e[:,0] > 0):
                        raise RuntimeError('Q is not SPD.')

            _, nineq, nz = G.size()
            neq = A.size(1) if A.nelement() > 0 else 0
            assert(neq > 0 or nineq > 0)
            ctx.neq, ctx.nineq, ctx.nz = neq, nineq, nz

            if solver == QPSolvers.PDIPM_BATCHED:
                ctx.Q_LU, ctx.S_LU, ctx.R = pdipm_b.pre_factor_kkt(Q, G, A)
                zhats, ctx.nus, ctx.lams, ctx.slacks = pdipm_b.forward(
                    Q, p, G, h, A, b, ctx.Q_LU, ctx.S_LU, ctx.R,
                    eps, verbose, notImprovedLim, maxIter)
            elif solver == QPSolvers.CVXPY:
                vals = torch.Tensor(nBatch).type_as(Q)
                zhats = torch.Tensor(nBatch, ctx.nz).type_as(Q)
                lams = torch.Tensor(nBatch, ctx.nineq).type_as(Q)
                nus = torch.Tensor(nBatch, ctx.neq).type_as(Q) \
                    if ctx.neq > 0 else torch.Tensor()
                slacks = torch.Tensor(nBatch, ctx.nineq).type_as(Q)
                for i in range(nBatch):
                    Ai, bi = (A[i], b[i]) if neq > 0 else (None, None)
                    vals[i], zhati, nui, lami, si = solvers.cvxpy.forward_single_np(
                        *[x.cpu().numpy() if x is not None else None
                        for x in (Q[i], p[i], G[i], h[i], Ai, bi)])
                    # if zhati[0] is None:
                    #     import IPython, sys; IPython.embed(); sys.exit(-1)
                    zhats[i] = torch.Tensor(zhati)
                    lams[i] = torch.Tensor(lami)
                    slacks[i] = torch.Tensor(si)
                    if neq > 0:
                        nus[i] = torch.Tensor(nui)

                ctx.vals = vals
                ctx.lams = lams
                ctx.nus = nus
                ctx.slacks = slacks
            else:
                assert False

            ctx.save_for_backward(zhats, Q_, p_, G_, h_, A_, b_)
            return zhats",_339.py,76,"vals = torch.Tensor(nBatch).type_as(Q)
zhats = torch.Tensor(nBatch, ctx.nz).type_as(Q)
lams = torch.Tensor(nBatch, ctx.nineq).type_as(Q)
nus = torch.Tensor(nBatch, ctx.neq).type_as(Q) if ctx.neq > 0 else torch.Tensor()
slacks = torch.Tensor(nBatch, ctx.nineq).type_as(Q)","vals , zhats , lams , nus , slacks  = torch.Tensor(nBatch).type_as(Q), torch.Tensor(nBatch, ctx.nz).type_as(Q), torch.Tensor(nBatch, ctx.nineq).type_as(Q), torch.Tensor(nBatch, ctx.neq).type_as(Q) if ctx.neq > 0 else torch.Tensor(), torch.Tensor(nBatch, ctx.nineq).type_as(Q)"
https://github.com/locuslab/qpth/tree/master/qpth/qp.py,"def forward(ctx, Q_, p_, G_, h_, A_, b_):
            """"""Solve a batch of QPs.

            This function solves a batch of QPs, each optimizing over
            `nz` variables and having `nineq` inequality constraints
            and `neq` equality constraints.
            The optimization problem for each instance in the batch
            (dropping indexing from the notation) is of the form

                \hat z =   argmin_z 1/2 z^T Q z + p^T z
                        subject to Gz <= h
                                    Az  = b

            where Q \in S^{nz,nz},
                S^{nz,nz} is the set of all positive semi-definite matrices,
                p \in R^{nz}
                G \in R^{nineq,nz}
                h \in R^{nineq}
                A \in R^{neq,nz}
                b \in R^{neq}

            These parameters should all be passed to this function as
            Variable- or Parameter-wrapped Tensors.
            (See torch.autograd.Variable and torch.nn.parameter.Parameter)

            If you want to solve a batch of QPs where `nz`, `nineq` and `neq`
            are the same, but some of the contents differ across the
            minibatch, you can pass in tensors in the standard way
            where the first dimension indicates the batch example.
            This can be done with some or all of the coefficients.

            You do not need to add an extra dimension to coefficients
            that will not change across all of the minibatch examples.
            This function is able to infer such cases.

            If you don't want to use any equality or inequality constraints,
            you can set the appropriate values to:

                e = Variable(torch.Tensor())

            Parameters:
            Q:  A (nBatch, nz, nz) or (nz, nz) Tensor.
            p:  A (nBatch, nz) or (nz) Tensor.
            G:  A (nBatch, nineq, nz) or (nineq, nz) Tensor.
            h:  A (nBatch, nineq) or (nineq) Tensor.
            A:  A (nBatch, neq, nz) or (neq, nz) Tensor.
            b:  A (nBatch, neq) or (neq) Tensor.

            Returns: \hat z: a (nBatch, nz) Tensor.
            """"""
            nBatch = extract_nBatch(Q_, p_, G_, h_, A_, b_)
            Q, _ = expandParam(Q_, nBatch, 3)
            p, _ = expandParam(p_, nBatch, 2)
            G, _ = expandParam(G_, nBatch, 3)
            h, _ = expandParam(h_, nBatch, 2)
            A, _ = expandParam(A_, nBatch, 3)
            b, _ = expandParam(b_, nBatch, 2)

            if check_Q_spd:
                for i in range(nBatch):
                    e, _ = torch.eig(Q[i])
                    if not torch.all(e[:,0] > 0):
                        raise RuntimeError('Q is not SPD.')

            _, nineq, nz = G.size()
            neq = A.size(1) if A.nelement() > 0 else 0
            assert(neq > 0 or nineq > 0)
            ctx.neq, ctx.nineq, ctx.nz = neq, nineq, nz

            if solver == QPSolvers.PDIPM_BATCHED:
                ctx.Q_LU, ctx.S_LU, ctx.R = pdipm_b.pre_factor_kkt(Q, G, A)
                zhats, ctx.nus, ctx.lams, ctx.slacks = pdipm_b.forward(
                    Q, p, G, h, A, b, ctx.Q_LU, ctx.S_LU, ctx.R,
                    eps, verbose, notImprovedLim, maxIter)
            elif solver == QPSolvers.CVXPY:
                vals = torch.Tensor(nBatch).type_as(Q)
                zhats = torch.Tensor(nBatch, ctx.nz).type_as(Q)
                lams = torch.Tensor(nBatch, ctx.nineq).type_as(Q)
                nus = torch.Tensor(nBatch, ctx.neq).type_as(Q) \
                    if ctx.neq > 0 else torch.Tensor()
                slacks = torch.Tensor(nBatch, ctx.nineq).type_as(Q)
                for i in range(nBatch):
                    Ai, bi = (A[i], b[i]) if neq > 0 else (None, None)
                    vals[i], zhati, nui, lami, si = solvers.cvxpy.forward_single_np(
                        *[x.cpu().numpy() if x is not None else None
                        for x in (Q[i], p[i], G[i], h[i], Ai, bi)])
                    # if zhati[0] is None:
                    #     import IPython, sys; IPython.embed(); sys.exit(-1)
                    zhats[i] = torch.Tensor(zhati)
                    lams[i] = torch.Tensor(lami)
                    slacks[i] = torch.Tensor(si)
                    if neq > 0:
                        nus[i] = torch.Tensor(nui)

                ctx.vals = vals
                ctx.lams = lams
                ctx.nus = nus
                ctx.slacks = slacks
            else:
                assert False

            ctx.save_for_backward(zhats, Q_, p_, G_, h_, A_, b_)
            return zhats",_339.py,95,"ctx.vals = vals
ctx.lams = lams
ctx.nus = nus
ctx.slacks = slacks","ctx.vals , ctx.lams , ctx.nus , ctx.slacks  = vals, lams, nus, slacks"
https://github.com/locuslab/qpth/tree/master/qpth/qp.py,"def forward(ctx, Q_, p_, G_, h_, A_, b_):
            """"""Solve a batch of QPs.

            This function solves a batch of QPs, each optimizing over
            `nz` variables and having `nineq` inequality constraints
            and `neq` equality constraints.
            The optimization problem for each instance in the batch
            (dropping indexing from the notation) is of the form

                \hat z =   argmin_z 1/2 z^T Q z + p^T z
                        subject to Gz <= h
                                    Az  = b

            where Q \in S^{nz,nz},
                S^{nz,nz} is the set of all positive semi-definite matrices,
                p \in R^{nz}
                G \in R^{nineq,nz}
                h \in R^{nineq}
                A \in R^{neq,nz}
                b \in R^{neq}

            These parameters should all be passed to this function as
            Variable- or Parameter-wrapped Tensors.
            (See torch.autograd.Variable and torch.nn.parameter.Parameter)

            If you want to solve a batch of QPs where `nz`, `nineq` and `neq`
            are the same, but some of the contents differ across the
            minibatch, you can pass in tensors in the standard way
            where the first dimension indicates the batch example.
            This can be done with some or all of the coefficients.

            You do not need to add an extra dimension to coefficients
            that will not change across all of the minibatch examples.
            This function is able to infer such cases.

            If you don't want to use any equality or inequality constraints,
            you can set the appropriate values to:

                e = Variable(torch.Tensor())

            Parameters:
            Q:  A (nBatch, nz, nz) or (nz, nz) Tensor.
            p:  A (nBatch, nz) or (nz) Tensor.
            G:  A (nBatch, nineq, nz) or (nineq, nz) Tensor.
            h:  A (nBatch, nineq) or (nineq) Tensor.
            A:  A (nBatch, neq, nz) or (neq, nz) Tensor.
            b:  A (nBatch, neq) or (neq) Tensor.

            Returns: \hat z: a (nBatch, nz) Tensor.
            """"""
            nBatch = extract_nBatch(Q_, p_, G_, h_, A_, b_)
            Q, _ = expandParam(Q_, nBatch, 3)
            p, _ = expandParam(p_, nBatch, 2)
            G, _ = expandParam(G_, nBatch, 3)
            h, _ = expandParam(h_, nBatch, 2)
            A, _ = expandParam(A_, nBatch, 3)
            b, _ = expandParam(b_, nBatch, 2)

            if check_Q_spd:
                for i in range(nBatch):
                    e, _ = torch.eig(Q[i])
                    if not torch.all(e[:,0] > 0):
                        raise RuntimeError('Q is not SPD.')

            _, nineq, nz = G.size()
            neq = A.size(1) if A.nelement() > 0 else 0
            assert(neq > 0 or nineq > 0)
            ctx.neq, ctx.nineq, ctx.nz = neq, nineq, nz

            if solver == QPSolvers.PDIPM_BATCHED:
                ctx.Q_LU, ctx.S_LU, ctx.R = pdipm_b.pre_factor_kkt(Q, G, A)
                zhats, ctx.nus, ctx.lams, ctx.slacks = pdipm_b.forward(
                    Q, p, G, h, A, b, ctx.Q_LU, ctx.S_LU, ctx.R,
                    eps, verbose, notImprovedLim, maxIter)
            elif solver == QPSolvers.CVXPY:
                vals = torch.Tensor(nBatch).type_as(Q)
                zhats = torch.Tensor(nBatch, ctx.nz).type_as(Q)
                lams = torch.Tensor(nBatch, ctx.nineq).type_as(Q)
                nus = torch.Tensor(nBatch, ctx.neq).type_as(Q) \
                    if ctx.neq > 0 else torch.Tensor()
                slacks = torch.Tensor(nBatch, ctx.nineq).type_as(Q)
                for i in range(nBatch):
                    Ai, bi = (A[i], b[i]) if neq > 0 else (None, None)
                    vals[i], zhati, nui, lami, si = solvers.cvxpy.forward_single_np(
                        *[x.cpu().numpy() if x is not None else None
                        for x in (Q[i], p[i], G[i], h[i], Ai, bi)])
                    # if zhati[0] is None:
                    #     import IPython, sys; IPython.embed(); sys.exit(-1)
                    zhats[i] = torch.Tensor(zhati)
                    lams[i] = torch.Tensor(lami)
                    slacks[i] = torch.Tensor(si)
                    if neq > 0:
                        nus[i] = torch.Tensor(nui)

                ctx.vals = vals
                ctx.lams = lams
                ctx.nus = nus
                ctx.slacks = slacks
            else:
                assert False

            ctx.save_for_backward(zhats, Q_, p_, G_, h_, A_, b_)
            return zhats",_339.py,89,"zhats[i] = torch.Tensor(zhati)
lams[i] = torch.Tensor(lami)
slacks[i] = torch.Tensor(si)","zhats[i] , lams[i] , slacks[i]  = torch.Tensor(zhati), torch.Tensor(lami), torch.Tensor(si)"
https://github.com/django/django/tree/master/tests/admin_changelist/tests.py,"def test_get_list_editable_queryset(self):
        a = Swallow.objects.create(origin='Swallow A', load=4, speed=1)
        Swallow.objects.create(origin='Swallow B', load=2, speed=2)
        data = {
            'form-TOTAL_FORMS': '2',
            'form-INITIAL_FORMS': '2',
            'form-MIN_NUM_FORMS': '0',
            'form-MAX_NUM_FORMS': '1000',
            'form-0-uuid': str(a.pk),
            'form-0-load': '10',
            '_save': 'Save',
        }
        superuser = self._create_superuser('superuser')
        self.client.force_login(superuser)
        changelist_url = reverse('admin:admin_changelist_swallow_changelist')
        m = SwallowAdmin(Swallow, custom_site)
        request = self.factory.post(changelist_url, data=data)
        queryset = m._get_list_editable_queryset(request, prefix='form')
        self.assertEqual(queryset.count(), 1)
        data['form-0-uuid'] = 'INVALD_PRIMARY_KEY'
        # The unfiltered queryset is returned if there's invalid data.
        request = self.factory.post(changelist_url, data=data)
        queryset = m._get_list_editable_queryset(request, prefix='form')
        self.assertEqual(queryset.count(), 2)",_341.py,4,"data = {'form-TOTAL_FORMS': '2', 'form-INITIAL_FORMS': '2', 'form-MIN_NUM_FORMS': '0', 'form-MAX_NUM_FORMS': '1000', 'form-0-uuid': str(a.pk), 'form-0-load': '10', '_save': 'Save'}
superuser = self._create_superuser('superuser')","data , superuser  = {'form-TOTAL_FORMS': '2', 'form-INITIAL_FORMS': '2', 'form-MIN_NUM_FORMS': '0', 'form-MAX_NUM_FORMS': '1000', 'form-0-uuid': str(a.pk), 'form-0-load': '10', '_save': 'Save'}, self._create_superuser('superuser')"
https://github.com/django/django/tree/master/tests/admin_changelist/tests.py,"def test_get_list_editable_queryset(self):
        a = Swallow.objects.create(origin='Swallow A', load=4, speed=1)
        Swallow.objects.create(origin='Swallow B', load=2, speed=2)
        data = {
            'form-TOTAL_FORMS': '2',
            'form-INITIAL_FORMS': '2',
            'form-MIN_NUM_FORMS': '0',
            'form-MAX_NUM_FORMS': '1000',
            'form-0-uuid': str(a.pk),
            'form-0-load': '10',
            '_save': 'Save',
        }
        superuser = self._create_superuser('superuser')
        self.client.force_login(superuser)
        changelist_url = reverse('admin:admin_changelist_swallow_changelist')
        m = SwallowAdmin(Swallow, custom_site)
        request = self.factory.post(changelist_url, data=data)
        queryset = m._get_list_editable_queryset(request, prefix='form')
        self.assertEqual(queryset.count(), 1)
        data['form-0-uuid'] = 'INVALD_PRIMARY_KEY'
        # The unfiltered queryset is returned if there's invalid data.
        request = self.factory.post(changelist_url, data=data)
        queryset = m._get_list_editable_queryset(request, prefix='form')
        self.assertEqual(queryset.count(), 2)",_341.py,16,"m = SwallowAdmin(Swallow, custom_site)
request = self.factory.post(changelist_url, data=data)","m , request  = SwallowAdmin(Swallow, custom_site), self.factory.post(changelist_url, data=data)"
https://github.com/rholder/retrying/tree/master//test_retrying.py,"def test_random_sleep(self):
        r = Retrying(wait_random_min=1000, wait_random_max=2000)
        times = set()
        times.add(r.wait(1, 6546))
        times.add(r.wait(1, 6546))
        times.add(r.wait(1, 6546))
        times.add(r.wait(1, 6546))

        # this is kind of non-deterministic...
        self.assertTrue(len(times) > 1)
        for t in times:
            self.assertTrue(t >= 1000)
            self.assertTrue(t <= 2000)",_35.py,2,"r = Retrying(wait_random_min=1000, wait_random_max=2000)
times = set()","r , times  = Retrying(wait_random_min=1000, wait_random_max=2000), set()"
https://github.com/CSAILVision/gandissect/tree/master/netdissect/aceoptimize.py,"def summarize_scores(args, corpus, cachedir, layer, classname, variant, scores):
    target_filename = os.path.join(cachedir, 'summary.json')

    ranking_name = '%s-%s' % (classname, variant)
    # Now convert ace scores to rankings
    new_rankings = [dict(
        name=ranking_name,
        score=(-scores).flatten().tolist(),
        metric=variant)]
    result = dict(layers=[dict(layer=layer, rankings=new_rankings)])

    # Replace the old dissect.json in-place
    with open(target_filename, 'w') as f:
        json.dump(result, f, indent=1)",_354.py,2,"target_filename = os.path.join(cachedir, 'summary.json')
ranking_name = '%s-%s' % (classname, variant)","target_filename , ranking_name  = os.path.join(cachedir, 'summary.json'), '%s-%s' % (classname, variant)"
https://github.com/quantumlib/OpenFermion/tree/master/src/openfermion/ops/operators/symbolic_operator_test.py,"def test_init_long_str(self):
        qubit_op = DummyOperator2(
            '(-2.0+3.0j) [X0 Y1] +\n\n -1.0[ X2 Y3 ] - []', -1.)
        correct = \
            DummyOperator2('X0 Y1', complex(2., -3.)) + \
            DummyOperator2('X2 Y3', 1.) + \
            DummyOperator2('', 1.)
        self.assertEqual(len((qubit_op - correct).terms), 0)
        reparsed_op = DummyOperator2(str(qubit_op))
        self.assertEqual(len((qubit_op - reparsed_op).terms), 0)

        qubit_op = DummyOperator2('[X0 X1] + [Y0 Y1]')
        correct = DummyOperator2('X0 X1') + DummyOperator2('Y0 Y1')
        self.assertTrue(qubit_op == correct)
        self.assertTrue(qubit_op == DummyOperator2(str(qubit_op)))",_355.py,2,"qubit_op = DummyOperator2('(-2.0+3.0j) [X0 Y1] +\n\n -1.0[ X2 Y3 ] - []', -1.0)
correct = DummyOperator2('X0 Y1', complex(2.0, -3.0)) + DummyOperator2('X2 Y3', 1.0) + DummyOperator2('', 1.0)","qubit_op , correct  = DummyOperator2('(-2.0+3.0j) [X0 Y1] +\n\n -1.0[ X2 Y3 ] - []', -1.0), DummyOperator2('X0 Y1', complex(2.0, -3.0)) + DummyOperator2('X2 Y3', 1.0) + DummyOperator2('', 1.0)"
https://github.com/quantumlib/OpenFermion/tree/master/src/openfermion/ops/operators/symbolic_operator_test.py,"def test_init_long_str(self):
        qubit_op = DummyOperator2(
            '(-2.0+3.0j) [X0 Y1] +\n\n -1.0[ X2 Y3 ] - []', -1.)
        correct = \
            DummyOperator2('X0 Y1', complex(2., -3.)) + \
            DummyOperator2('X2 Y3', 1.) + \
            DummyOperator2('', 1.)
        self.assertEqual(len((qubit_op - correct).terms), 0)
        reparsed_op = DummyOperator2(str(qubit_op))
        self.assertEqual(len((qubit_op - reparsed_op).terms), 0)

        qubit_op = DummyOperator2('[X0 X1] + [Y0 Y1]')
        correct = DummyOperator2('X0 X1') + DummyOperator2('Y0 Y1')
        self.assertTrue(qubit_op == correct)
        self.assertTrue(qubit_op == DummyOperator2(str(qubit_op)))",_355.py,12,"qubit_op = DummyOperator2('[X0 X1] + [Y0 Y1]')
correct = DummyOperator2('X0 X1') + DummyOperator2('Y0 Y1')","qubit_op , correct  = DummyOperator2('[X0 X1] + [Y0 Y1]'), DummyOperator2('X0 X1') + DummyOperator2('Y0 Y1')"
https://github.com/saadq/Materialize/tree/master//Materialize.py,"def run(self, action):

        names = [
            ""Material Augmented Reaction"",
            ""Material Behave"",
            ""Material Brogrammer"",
            ""Material Cobalt"",
            ""Material Dracula"",
            ""Material Facebook"",
            ""Material Firewatch"",
            ""Material Flatland"",
            ""Material Glacier"",
            ""Material Monokai Soda"",
            ""Material Monokai"",
            ""Material Oceanic Next"",
            ""Material One Dark"",
            ""Material Primer Light"",
            ""Material Seti"",
            ""Material Solarized Dark"",
            ""Material Solarized Light"",
            ""Material Spaceblack Ocenic"",
            ""Material Spaceblack"",
            ""Material Spacegray Eighties"",
            ""Material Spacegray Light"",
            ""Material Spacegray Mocha"",
            ""Material Spacegray"",
            ""Material Stereokai"",
            ""Material Toy Chest"",
            ""Material Twilight"",
            ""Material Vim Blackboard"",
            ""Material Zenburn"",
        ]
        self.themes = [
            ""Material Augmented Reaction.sublime-theme"",
            ""Material Behave.sublime-theme"",
            ""Material Brogrammer.sublime-theme"",
            ""Material Cobalt.sublime-theme"",
            ""Material Dracula.sublime-theme"",
            ""Material Facebook.sublime-theme"",
            ""Material Firewatch.sublime-theme"",
            ""Material Flatland.sublime-theme"",
            ""Material Glacier.sublime-theme"",
            ""Material Monokai Soda.sublime-theme"",
            ""Material Monokai.sublime-theme"",
            ""Material Oceanic Next.sublime-theme"",
            ""Material One Dark.sublime-theme"",
            ""Material Primer Light.sublime-theme"",
            ""Material Seti.sublime-theme"",
            ""Material Solarized Dark.sublime-theme"",
            ""Material Solarized Light.sublime-theme"",
            ""Material Spaceblack.sublime-theme"",
            ""Material Spaceblack.sublime-theme"",
            ""Material Spacegray Eighties.sublime-theme"",
            ""Material Spacegray Light.sublime-theme"",
            ""Material Spacegray Mocha.sublime-theme"",
            ""Material Spacegray.sublime-theme"",
            ""Material Stereokai.sublime-theme"",
            ""Material Toy Chest.sublime-theme"",
            ""Material Twilight.sublime-theme"",
            ""Material Vim Blackboard.sublime-theme"",
            ""Material Zenburn.sublime-theme"",
        ]
        self.schemes = [
            ""Packages/Materialize/schemes/Material Augmented Reaction.tmTheme"",
            ""Packages/Materialize/schemes/Material Behave.tmTheme"",
            ""Packages/Materialize/schemes/Material Brogrammer.tmTheme"",
            ""Packages/Materialize/schemes/Material Cobalt.tmTheme"",
            ""Packages/Materialize/schemes/Material Dracula.tmTheme"",
            ""Packages/Materialize/schemes/Material Facebook.tmTheme"",
            ""Packages/Materialize/schemes/Material Firewatch.tmTheme"",
            ""Packages/Materialize/schemes/Material Flatland.tmTheme"",
            ""Packages/Materialize/schemes/Material Glacier.tmTheme"",
            ""Packages/Materialize/schemes/Material Monokai Soda.tmTheme"",
            ""Packages/Materialize/schemes/Material Monokai.tmTheme"",
            ""Packages/Materialize/schemes/Material Oceanic Next.tmTheme"",
            ""Packages/Materialize/schemes/Material One Dark.tmTheme"",
            ""Packages/Materialize/schemes/Material Primer Light.tmTheme"",
            ""Packages/Materialize/schemes/Material Seti.tmTheme"",
            ""Packages/Materialize/schemes/Material Solarized Dark.tmTheme"",
            ""Packages/Materialize/schemes/Material Solarized Light.tmTheme"",
            ""Packages/Materialize/schemes/Material Spaceblack Oceanic.tmTheme"",
            ""Packages/Materialize/schemes/Material Spaceblack.tmTheme"",
            ""Packages/Materialize/schemes/Material Spacegray Eighties.tmTheme"",
            ""Packages/Materialize/schemes/Material Spacegray Light.tmTheme"",
            ""Packages/Materialize/schemes/Material Spacegray Mocha.tmTheme"",
            ""Packages/Materialize/schemes/Material Spacegray.tmTheme"",
            ""Packages/Materialize/schemes/Material Stereokai.tmTheme"",
            ""Packages/Materialize/schemes/Material Toy Chest.tmTheme"",
            ""Packages/Materialize/schemes/Material Twilight.tmTheme"",
            ""Packages/Materialize/schemes/Material Vim Blackboard.tmTheme"",
            ""Packages/Materialize/schemes/Material Zenburn.tmTheme"",
        ]

        self.view.window().show_quick_panel(names, self.on_done, on_highlight=self.on_highlighted)",_356.py,3,"names = ['Material Augmented Reaction', 'Material Behave', 'Material Brogrammer', 'Material Cobalt', 'Material Dracula', 'Material Facebook', 'Material Firewatch', 'Material Flatland', 'Material Glacier', 'Material Monokai Soda', 'Material Monokai', 'Material Oceanic Next', 'Material One Dark', 'Material Primer Light', 'Material Seti', 'Material Solarized Dark', 'Material Solarized Light', 'Material Spaceblack Ocenic', 'Material Spaceblack', 'Material Spacegray Eighties', 'Material Spacegray Light', 'Material Spacegray Mocha', 'Material Spacegray', 'Material Stereokai', 'Material Toy Chest', 'Material Twilight', 'Material Vim Blackboard', 'Material Zenburn']
self.themes = ['Material Augmented Reaction.sublime-theme', 'Material Behave.sublime-theme', 'Material Brogrammer.sublime-theme', 'Material Cobalt.sublime-theme', 'Material Dracula.sublime-theme', 'Material Facebook.sublime-theme', 'Material Firewatch.sublime-theme', 'Material Flatland.sublime-theme', 'Material Glacier.sublime-theme', 'Material Monokai Soda.sublime-theme', 'Material Monokai.sublime-theme', 'Material Oceanic Next.sublime-theme', 'Material One Dark.sublime-theme', 'Material Primer Light.sublime-theme', 'Material Seti.sublime-theme', 'Material Solarized Dark.sublime-theme', 'Material Solarized Light.sublime-theme', 'Material Spaceblack.sublime-theme', 'Material Spaceblack.sublime-theme', 'Material Spacegray Eighties.sublime-theme', 'Material Spacegray Light.sublime-theme', 'Material Spacegray Mocha.sublime-theme', 'Material Spacegray.sublime-theme', 'Material Stereokai.sublime-theme', 'Material Toy Chest.sublime-theme', 'Material Twilight.sublime-theme', 'Material Vim Blackboard.sublime-theme', 'Material Zenburn.sublime-theme']
self.schemes = ['Packages/Materialize/schemes/Material Augmented Reaction.tmTheme', 'Packages/Materialize/schemes/Material Behave.tmTheme', 'Packages/Materialize/schemes/Material Brogrammer.tmTheme', 'Packages/Materialize/schemes/Material Cobalt.tmTheme', 'Packages/Materialize/schemes/Material Dracula.tmTheme', 'Packages/Materialize/schemes/Material Facebook.tmTheme', 'Packages/Materialize/schemes/Material Firewatch.tmTheme', 'Packages/Materialize/schemes/Material Flatland.tmTheme', 'Packages/Materialize/schemes/Material Glacier.tmTheme', 'Packages/Materialize/schemes/Material Monokai Soda.tmTheme', 'Packages/Materialize/schemes/Material Monokai.tmTheme', 'Packages/Materialize/schemes/Material Oceanic Next.tmTheme', 'Packages/Materialize/schemes/Material One Dark.tmTheme', 'Packages/Materialize/schemes/Material Primer Light.tmTheme', 'Packages/Materialize/schemes/Material Seti.tmTheme', 'Packages/Materialize/schemes/Material Solarized Dark.tmTheme', 'Packages/Materialize/schemes/Material Solarized Light.tmTheme', 'Packages/Materialize/schemes/Material Spaceblack Oceanic.tmTheme', 'Packages/Materialize/schemes/Material Spaceblack.tmTheme', 'Packages/Materialize/schemes/Material Spacegray Eighties.tmTheme', 'Packages/Materialize/schemes/Material Spacegray Light.tmTheme', 'Packages/Materialize/schemes/Material Spacegray Mocha.tmTheme', 'Packages/Materialize/schemes/Material Spacegray.tmTheme', 'Packages/Materialize/schemes/Material Stereokai.tmTheme', 'Packages/Materialize/schemes/Material Toy Chest.tmTheme', 'Packages/Materialize/schemes/Material Twilight.tmTheme', 'Packages/Materialize/schemes/Material Vim Blackboard.tmTheme', 'Packages/Materialize/schemes/Material Zenburn.tmTheme']","names , self.themes , self.schemes  = ['Material Augmented Reaction', 'Material Behave', 'Material Brogrammer', 'Material Cobalt', 'Material Dracula', 'Material Facebook', 'Material Firewatch', 'Material Flatland', 'Material Glacier', 'Material Monokai Soda', 'Material Monokai', 'Material Oceanic Next', 'Material One Dark', 'Material Primer Light', 'Material Seti', 'Material Solarized Dark', 'Material Solarized Light', 'Material Spaceblack Ocenic', 'Material Spaceblack', 'Material Spacegray Eighties', 'Material Spacegray Light', 'Material Spacegray Mocha', 'Material Spacegray', 'Material Stereokai', 'Material Toy Chest', 'Material Twilight', 'Material Vim Blackboard', 'Material Zenburn'], ['Material Augmented Reaction.sublime-theme', 'Material Behave.sublime-theme', 'Material Brogrammer.sublime-theme', 'Material Cobalt.sublime-theme', 'Material Dracula.sublime-theme', 'Material Facebook.sublime-theme', 'Material Firewatch.sublime-theme', 'Material Flatland.sublime-theme', 'Material Glacier.sublime-theme', 'Material Monokai Soda.sublime-theme', 'Material Monokai.sublime-theme', 'Material Oceanic Next.sublime-theme', 'Material One Dark.sublime-theme', 'Material Primer Light.sublime-theme', 'Material Seti.sublime-theme', 'Material Solarized Dark.sublime-theme', 'Material Solarized Light.sublime-theme', 'Material Spaceblack.sublime-theme', 'Material Spaceblack.sublime-theme', 'Material Spacegray Eighties.sublime-theme', 'Material Spacegray Light.sublime-theme', 'Material Spacegray Mocha.sublime-theme', 'Material Spacegray.sublime-theme', 'Material Stereokai.sublime-theme', 'Material Toy Chest.sublime-theme', 'Material Twilight.sublime-theme', 'Material Vim Blackboard.sublime-theme', 'Material Zenburn.sublime-theme'], ['Packages/Materialize/schemes/Material Augmented Reaction.tmTheme', 'Packages/Materialize/schemes/Material Behave.tmTheme', 'Packages/Materialize/schemes/Material Brogrammer.tmTheme', 'Packages/Materialize/schemes/Material Cobalt.tmTheme', 'Packages/Materialize/schemes/Material Dracula.tmTheme', 'Packages/Materialize/schemes/Material Facebook.tmTheme', 'Packages/Materialize/schemes/Material Firewatch.tmTheme', 'Packages/Materialize/schemes/Material Flatland.tmTheme', 'Packages/Materialize/schemes/Material Glacier.tmTheme', 'Packages/Materialize/schemes/Material Monokai Soda.tmTheme', 'Packages/Materialize/schemes/Material Monokai.tmTheme', 'Packages/Materialize/schemes/Material Oceanic Next.tmTheme', 'Packages/Materialize/schemes/Material One Dark.tmTheme', 'Packages/Materialize/schemes/Material Primer Light.tmTheme', 'Packages/Materialize/schemes/Material Seti.tmTheme', 'Packages/Materialize/schemes/Material Solarized Dark.tmTheme', 'Packages/Materialize/schemes/Material Solarized Light.tmTheme', 'Packages/Materialize/schemes/Material Spaceblack Oceanic.tmTheme', 'Packages/Materialize/schemes/Material Spaceblack.tmTheme', 'Packages/Materialize/schemes/Material Spacegray Eighties.tmTheme', 'Packages/Materialize/schemes/Material Spacegray Light.tmTheme', 'Packages/Materialize/schemes/Material Spacegray Mocha.tmTheme', 'Packages/Materialize/schemes/Material Spacegray.tmTheme', 'Packages/Materialize/schemes/Material Stereokai.tmTheme', 'Packages/Materialize/schemes/Material Toy Chest.tmTheme', 'Packages/Materialize/schemes/Material Twilight.tmTheme', 'Packages/Materialize/schemes/Material Vim Blackboard.tmTheme', 'Packages/Materialize/schemes/Material Zenburn.tmTheme']"
https://github.com/sfzhang15/ATSS/tree/master/atss_core/modeling/roi_heads/box_head/box_head.py,"def __init__(self, cfg, in_channels):
        super(ROIBoxHead, self).__init__()
        self.feature_extractor = make_roi_box_feature_extractor(cfg, in_channels)
        self.predictor = make_roi_box_predictor(
            cfg, self.feature_extractor.out_channels)
        self.post_processor = make_roi_box_post_processor(cfg)
        self.loss_evaluator = make_roi_box_loss_evaluator(cfg)",_358.py,4,"self.predictor = make_roi_box_predictor(cfg, self.feature_extractor.out_channels)
self.post_processor = make_roi_box_post_processor(cfg)
self.loss_evaluator = make_roi_box_loss_evaluator(cfg)","self.predictor , self.post_processor , self.loss_evaluator  = make_roi_box_predictor(cfg, self.feature_extractor.out_channels), make_roi_box_post_processor(cfg), make_roi_box_loss_evaluator(cfg)"
https://github.com/SublimeHaskell/SublimeHaskell/tree/master//sublime_haskell_common.py,"def find_file_in_parent_dir(subdirectory, filename_pattern):
    """"""Look for a file with the specified name in a parent directory of the
    specified directory. If found, return the file's full path. Otherwise,
    return None.""""""
    current_dir = subdirectory
    while True:
        # See if the current directory contains the desired file:
        for name in os.listdir(current_dir):
            full_path = os.path.join(current_dir, name)
            matches_pattern = fnmatch.fnmatch(name, filename_pattern)
            if matches_pattern and os.path.isfile(full_path):
                return full_path
        # Get the next directory up:
        last_dir = current_dir
        current_dir = os.path.dirname(current_dir)
        # Check to see if we have reached the root directory:
        if last_dir == current_dir:
            return None",_359.py,14,"last_dir = current_dir
current_dir = os.path.dirname(current_dir)","last_dir , current_dir  = current_dir, os.path.dirname(current_dir)"
https://github.com/SublimeHaskell/SublimeHaskell/tree/master//sublime_haskell_common.py,"def find_file_in_parent_dir(subdirectory, filename_pattern):
    """"""Look for a file with the specified name in a parent directory of the
    specified directory. If found, return the file's full path. Otherwise,
    return None.""""""
    current_dir = subdirectory
    while True:
        # See if the current directory contains the desired file:
        for name in os.listdir(current_dir):
            full_path = os.path.join(current_dir, name)
            matches_pattern = fnmatch.fnmatch(name, filename_pattern)
            if matches_pattern and os.path.isfile(full_path):
                return full_path
        # Get the next directory up:
        last_dir = current_dir
        current_dir = os.path.dirname(current_dir)
        # Check to see if we have reached the root directory:
        if last_dir == current_dir:
            return None",_359.py,9,"full_path = os.path.join(current_dir, name)
matches_pattern = fnmatch.fnmatch(name, filename_pattern)","full_path , matches_pattern  = os.path.join(current_dir, name), fnmatch.fnmatch(name, filename_pattern)"
https://github.com/quodlibet/quodlibet/tree/master/quodlibet/ext/songsmenu/cover_download.py,"def _sent(self, msg, result, data):
        headers = self.message.get_property('response-headers')
        self.size = int(headers.get('content-length'))
        self._content_type = headers.get('content-type')
        self._original = result
        try:
            loader = GdkPixbuf.PixbufLoader()
        except GLib.GError as e:
            print_w(""Couldn't create GdkPixbuf (%s)"" % e)
        else:
            loader.write(result)
            loader.close()
            self._pixbuf = loader.get_pixbuf()
            self.emit(""info-known"", self._content_type, self.size,
                      self._pixbuf.props)
            self.resize()
            self.queue_draw()",_39.py,3,"self.size = int(headers.get('content-length'))
self._content_type = headers.get('content-type')
self._original = result","self.size , self._content_type , self._original  = int(headers.get('content-length')), headers.get('content-type'), result"
https://github.com/spulec/moto/tree/master/moto/ecr/models.py,"def _get_repository(self, name, registry_id=None) -> Repository:
        repo = self.repositories.get(name)
        reg_id = registry_id or DEFAULT_REGISTRY_ID

        if not repo or repo.registry_id != reg_id:
            raise RepositoryNotFoundException(name, reg_id)
        return repo",_41.py,2,"repo = self.repositories.get(name)
reg_id = registry_id or DEFAULT_REGISTRY_ID","repo , reg_id  = self.repositories.get(name), registry_id or DEFAULT_REGISTRY_ID"
https://github.com/heartexlabs/label-studio/tree/master/label_studio/organizations/middleware.py,"def __call__(self, request):
        org = Organization.objects.first()
        user = request.user
        if user and user.is_authenticated and user.active_organization is None:
            user.active_organization = org
            user.save(update_fields=['active_organization'])
        if org is not None:
            request.session['organization_pk'] = org.id
        response = self.get_response(request)
        return response",_5.py,2,"org = Organization.objects.first()
user = request.user","org , user  = Organization.objects.first(), request.user"
https://github.com/biocore/scikit-bio/tree/master/skbio/alignment/_pairwise.py,"def local_pairwise_align_ssw(sequence1, sequence2, **kwargs):
    """"""Align query and target sequences with Striped Smith-Waterman.

    Parameters
    ----------
    sequence1 : DNA, RNA, or Protein
        The first unaligned sequence
    sequence2 : DNA, RNA, or Protein
        The second unaligned sequence

    Returns
    -------
    tuple
        ``TabularMSA`` object containing the aligned sequences, alignment score
        (float), and start/end positions of each input sequence (iterable
        of two-item tuples). Note that start/end positions are indexes into the
        unaligned sequences.

    Notes
    -----
    This is a wrapper for the SSW package [1]_.

    For a complete list of optional keyword-arguments that can be provided,
    see ``skbio.alignment.StripedSmithWaterman``.

    The following kwargs will not have any effect: `suppress_sequences`,
    `zero_index`, and `protein`

    If an alignment does not meet a provided filter, `None` will be returned.

    References
    ----------
    .. [1] Zhao, Mengyao, Wan-Ping Lee, Erik P. Garrison, & Gabor T.
       Marth. ""SSW Library: An SIMD Smith-Waterman C/C++ Library for
       Applications"". PLOS ONE (2013). Web. 11 July 2014.
       http://www.plosone.org/article/info:doi/10.1371/journal.pone.0082138

    See Also
    --------
    skbio.alignment.StripedSmithWaterman

    """"""
    for seq in sequence1, sequence2:
        if not isinstance(seq, (DNA, RNA, Protein)):
            raise TypeError(
                ""`sequence1` and `sequence2` must be DNA, RNA, or Protein, ""
                ""not type %r"" % type(seq).__name__)

    if type(sequence1) is not type(sequence2):
        raise TypeError(
            ""`sequence1` and `sequence2` must be the same type: %r != %r""
            % (type(sequence1).__name__, type(sequence2).__name__))

    # We need the sequences for `TabularMSA` to make sense, so don't let the
    # user suppress them.
    kwargs['suppress_sequences'] = False
    kwargs['zero_index'] = True

    kwargs['protein'] = False
    if isinstance(sequence1, Protein):
        kwargs['protein'] = True

    query = StripedSmithWaterman(str(sequence1), **kwargs)
    alignment = query(str(sequence2))

    # If there is no cigar, then it has failed a filter. Return None.
    if not alignment.cigar:
        return None

    start_end = None
    if alignment.query_begin != -1:
        start_end = [
            (alignment.query_begin, alignment.query_end),
            (alignment.target_begin, alignment.target_end_optimal)
        ]

    metadata1 = metadata2 = None
    if sequence1.has_metadata():
        metadata1 = sequence1.metadata
    if sequence2.has_metadata():
        metadata2 = sequence2.metadata

    constructor = type(sequence1)
    msa = TabularMSA([
        constructor(alignment.aligned_query_sequence, metadata=metadata1,
                    validate=False),
        constructor(alignment.aligned_target_sequence, metadata=metadata2,
                    validate=False)
    ])

    return msa, alignment.optimal_alignment_score, start_end",_57.py,56,"kwargs['suppress_sequences'] = False
kwargs['zero_index'] = True
kwargs['protein'] = False","kwargs['suppress_sequences'] , kwargs['zero_index'] , kwargs['protein']  = False, True, False"
https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/vm/tests/hybrid_2019_03_01/test_vm_defaults.py,"def _set_ns(self, rg, location=None):
        self.ns.resource_group_name = rg
        self.ns.location = location",_60.py,2,"self.ns.resource_group_name = rg
self.ns.location = location","self.ns.resource_group_name , self.ns.location  = rg, location"
https://github.com/thingsboard/thingsboard-gateway/tree/master/thingsboard_gateway/connectors/opcua/opcua_connector.py,"def datachange_notification(self, node, val, data):
        try:
            log.debug(""Python: New data change event on node %s, with val: %s and data %s"", node, val, str(data))
            subscription = self.connector.subscribed[node]
            converted_data = subscription[""converter""].convert((subscription[""config_path""], subscription[""path""]), val)
            self.connector.statistics['MessagesReceived'] = self.connector.statistics['MessagesReceived'] + 1
            self.connector.data_to_send.append(converted_data)
            self.connector.statistics['MessagesSent'] = self.connector.statistics['MessagesSent'] + 1
            log.debug(""[SUBSCRIPTION] Data to ThingsBoard: %s"", converted_data)
        except KeyError:
            self.connector.scan_nodes_from_config()
        except Exception as e:
            log.exception(e)",_61.py,5,"converted_data = subscription['converter'].convert((subscription['config_path'], subscription['path']), val)
self.connector.statistics['MessagesReceived'] = self.connector.statistics['MessagesReceived'] + 1","converted_data , self.connector.statistics['MessagesReceived']  = subscription['converter'].convert((subscription['config_path'], subscription['path']), val), self.connector.statistics['MessagesReceived'] + 1"
https://github.com/garnaat/kappa/tree/master/kappa/log.py,"def tail(self):
        LOG.debug('tailing log group: %s', self.log_group_name)
        if not self._check_for_log_group():
            LOG.info(
                'log group %s has not been created yet', self.log_group_name)
            return []
        latest = None
        streams = self.streams()
        for stream in streams:
            if not latest:
                latest = stream
            elif stream['lastEventTimestamp'] > latest['lastEventTimestamp']:
                latest = stream
        response = self._log_client.call(
            'get_log_events',
            logGroupName=self.log_group_name,
            logStreamName=latest['logStreamName'])
        LOG.debug(response)
        return response['events']",_64.py,7,"latest = None
streams = self.streams()","latest , streams  = None, self.streams()"
https://github.com/microsoft/nni/tree/master/nni/algorithms/hpo/ppo_tuner/model.py,"def __init__(self, *, policy, nbatch_act, nbatch_train,
                 nsteps, ent_coef, vf_coef, max_grad_norm, microbatch_size=None, np_mask=None):
        self.sess = sess = get_session()

        with tf.variable_scope('ppo2_model', reuse=tf.AUTO_REUSE):
            # CREATE OUR TWO MODELS
            # act_model that is used for sampling
            act_model = policy(nbatch_act, 1, sess, np_mask=np_mask, is_act_model=True)

            # Train model for training
            if microbatch_size is None:
                train_model = policy(nbatch_train, nsteps, sess, np_mask=np_mask, is_act_model=False)
            else:
                train_model = policy(microbatch_size, nsteps, sess, np_mask=np_mask, is_act_model=False)

        # CREATE THE PLACEHOLDERS
        self.A = A = train_model.pdtype.sample_placeholder([None])
        self.ADV = ADV = tf.placeholder(tf.float32, [None])
        self.R = R = tf.placeholder(tf.float32, [None])
        # Keep track of old actor
        self.OLDNEGLOGPAC = OLDNEGLOGPAC = tf.placeholder(tf.float32, [None])
        # Keep track of old critic
        self.OLDVPRED = OLDVPRED = tf.placeholder(tf.float32, [None])
        self.LR = LR = tf.placeholder(tf.float32, [])
        # Cliprange
        self.CLIPRANGE = CLIPRANGE = tf.placeholder(tf.float32, [])

        neglogpac = train_model.pd.neglogp(A)

        # Calculate the entropy
        # Entropy is used to improve exploration by limiting the premature convergence to suboptimal policy.
        entropy = tf.reduce_mean(train_model.pd.entropy())

        # CALCULATE THE LOSS
        # Total loss = Policy gradient loss - entropy * entropy coefficient + Value coefficient * value loss

        # Clip the value to reduce variability during Critic training
        # Get the predicted value
        vpred = train_model.vf
        vpredclipped = OLDVPRED + tf.clip_by_value(train_model.vf - OLDVPRED, - CLIPRANGE, CLIPRANGE)
        # Unclipped value
        vf_losses1 = tf.square(vpred - R)
        # Clipped value
        vf_losses2 = tf.square(vpredclipped - R)

        vf_loss = .5 * tf.reduce_mean(tf.maximum(vf_losses1, vf_losses2))

        # Calculate ratio (pi current policy / pi old policy)
        ratio = tf.exp(OLDNEGLOGPAC - neglogpac)

        # Defining Loss = - J is equivalent to max J
        pg_losses = -ADV * ratio

        pg_losses2 = -ADV * tf.clip_by_value(ratio, 1.0 - CLIPRANGE, 1.0 + CLIPRANGE)

        # Final PG loss
        pg_loss = tf.reduce_mean(tf.maximum(pg_losses, pg_losses2))
        approxkl = .5 * tf.reduce_mean(tf.square(neglogpac - OLDNEGLOGPAC))
        clipfrac = tf.reduce_mean(tf.to_float(tf.greater(tf.abs(ratio - 1.0), CLIPRANGE)))

        # Total loss
        loss = pg_loss - entropy * ent_coef + vf_loss * vf_coef

        # UPDATE THE PARAMETERS USING LOSS
        # 1. Get the model parameters
        params = tf.trainable_variables('ppo2_model')
        # 2. Build our trainer
        self.trainer = tf.train.AdamOptimizer(learning_rate=LR, epsilon=1e-5)
        # 3. Calculate the gradients
        grads_and_var = self.trainer.compute_gradients(loss, params)
        grads, var = zip(*grads_and_var)

        if max_grad_norm is not None:
            # Clip the gradients (normalize)
            grads, _grad_norm = tf.clip_by_global_norm(grads, max_grad_norm)
        grads_and_var = list(zip(grads, var))
        # zip aggregate each gradient with parameters associated
        # For instance zip(ABCD, xyza) => Ax, By, Cz, Da

        self.grads = grads
        self.var = var
        self._train_op = self.trainer.apply_gradients(grads_and_var)
        self.loss_names = ['policy_loss', 'value_loss', 'policy_entropy', 'approxkl', 'clipfrac']
        self.stats_list = [pg_loss, vf_loss, entropy, approxkl, clipfrac]


        self.train_model = train_model
        self.act_model = act_model
        self.step = act_model.step
        self.value = act_model.value
        self.initial_state = act_model.initial_state

        initialize()",_67.py,28,"neglogpac = train_model.pd.neglogp(A)
entropy = tf.reduce_mean(train_model.pd.entropy())
vpred = train_model.vf","neglogpac , entropy , vpred  = train_model.pd.neglogp(A), tf.reduce_mean(train_model.pd.entropy()), train_model.vf"
https://github.com/microsoft/nni/tree/master/nni/algorithms/hpo/ppo_tuner/model.py,"def __init__(self, *, policy, nbatch_act, nbatch_train,
                 nsteps, ent_coef, vf_coef, max_grad_norm, microbatch_size=None, np_mask=None):
        self.sess = sess = get_session()

        with tf.variable_scope('ppo2_model', reuse=tf.AUTO_REUSE):
            # CREATE OUR TWO MODELS
            # act_model that is used for sampling
            act_model = policy(nbatch_act, 1, sess, np_mask=np_mask, is_act_model=True)

            # Train model for training
            if microbatch_size is None:
                train_model = policy(nbatch_train, nsteps, sess, np_mask=np_mask, is_act_model=False)
            else:
                train_model = policy(microbatch_size, nsteps, sess, np_mask=np_mask, is_act_model=False)

        # CREATE THE PLACEHOLDERS
        self.A = A = train_model.pdtype.sample_placeholder([None])
        self.ADV = ADV = tf.placeholder(tf.float32, [None])
        self.R = R = tf.placeholder(tf.float32, [None])
        # Keep track of old actor
        self.OLDNEGLOGPAC = OLDNEGLOGPAC = tf.placeholder(tf.float32, [None])
        # Keep track of old critic
        self.OLDVPRED = OLDVPRED = tf.placeholder(tf.float32, [None])
        self.LR = LR = tf.placeholder(tf.float32, [])
        # Cliprange
        self.CLIPRANGE = CLIPRANGE = tf.placeholder(tf.float32, [])

        neglogpac = train_model.pd.neglogp(A)

        # Calculate the entropy
        # Entropy is used to improve exploration by limiting the premature convergence to suboptimal policy.
        entropy = tf.reduce_mean(train_model.pd.entropy())

        # CALCULATE THE LOSS
        # Total loss = Policy gradient loss - entropy * entropy coefficient + Value coefficient * value loss

        # Clip the value to reduce variability during Critic training
        # Get the predicted value
        vpred = train_model.vf
        vpredclipped = OLDVPRED + tf.clip_by_value(train_model.vf - OLDVPRED, - CLIPRANGE, CLIPRANGE)
        # Unclipped value
        vf_losses1 = tf.square(vpred - R)
        # Clipped value
        vf_losses2 = tf.square(vpredclipped - R)

        vf_loss = .5 * tf.reduce_mean(tf.maximum(vf_losses1, vf_losses2))

        # Calculate ratio (pi current policy / pi old policy)
        ratio = tf.exp(OLDNEGLOGPAC - neglogpac)

        # Defining Loss = - J is equivalent to max J
        pg_losses = -ADV * ratio

        pg_losses2 = -ADV * tf.clip_by_value(ratio, 1.0 - CLIPRANGE, 1.0 + CLIPRANGE)

        # Final PG loss
        pg_loss = tf.reduce_mean(tf.maximum(pg_losses, pg_losses2))
        approxkl = .5 * tf.reduce_mean(tf.square(neglogpac - OLDNEGLOGPAC))
        clipfrac = tf.reduce_mean(tf.to_float(tf.greater(tf.abs(ratio - 1.0), CLIPRANGE)))

        # Total loss
        loss = pg_loss - entropy * ent_coef + vf_loss * vf_coef

        # UPDATE THE PARAMETERS USING LOSS
        # 1. Get the model parameters
        params = tf.trainable_variables('ppo2_model')
        # 2. Build our trainer
        self.trainer = tf.train.AdamOptimizer(learning_rate=LR, epsilon=1e-5)
        # 3. Calculate the gradients
        grads_and_var = self.trainer.compute_gradients(loss, params)
        grads, var = zip(*grads_and_var)

        if max_grad_norm is not None:
            # Clip the gradients (normalize)
            grads, _grad_norm = tf.clip_by_global_norm(grads, max_grad_norm)
        grads_and_var = list(zip(grads, var))
        # zip aggregate each gradient with parameters associated
        # For instance zip(ABCD, xyza) => Ax, By, Cz, Da

        self.grads = grads
        self.var = var
        self._train_op = self.trainer.apply_gradients(grads_and_var)
        self.loss_names = ['policy_loss', 'value_loss', 'policy_entropy', 'approxkl', 'clipfrac']
        self.stats_list = [pg_loss, vf_loss, entropy, approxkl, clipfrac]


        self.train_model = train_model
        self.act_model = act_model
        self.step = act_model.step
        self.value = act_model.value
        self.initial_state = act_model.initial_state

        initialize()",_67.py,42,"vf_losses1 = tf.square(vpred - R)
vf_losses2 = tf.square(vpredclipped - R)","vf_losses1 , vf_losses2  = tf.square(vpred - R), tf.square(vpredclipped - R)"
https://github.com/microsoft/nni/tree/master/nni/algorithms/hpo/ppo_tuner/model.py,"def __init__(self, *, policy, nbatch_act, nbatch_train,
                 nsteps, ent_coef, vf_coef, max_grad_norm, microbatch_size=None, np_mask=None):
        self.sess = sess = get_session()

        with tf.variable_scope('ppo2_model', reuse=tf.AUTO_REUSE):
            # CREATE OUR TWO MODELS
            # act_model that is used for sampling
            act_model = policy(nbatch_act, 1, sess, np_mask=np_mask, is_act_model=True)

            # Train model for training
            if microbatch_size is None:
                train_model = policy(nbatch_train, nsteps, sess, np_mask=np_mask, is_act_model=False)
            else:
                train_model = policy(microbatch_size, nsteps, sess, np_mask=np_mask, is_act_model=False)

        # CREATE THE PLACEHOLDERS
        self.A = A = train_model.pdtype.sample_placeholder([None])
        self.ADV = ADV = tf.placeholder(tf.float32, [None])
        self.R = R = tf.placeholder(tf.float32, [None])
        # Keep track of old actor
        self.OLDNEGLOGPAC = OLDNEGLOGPAC = tf.placeholder(tf.float32, [None])
        # Keep track of old critic
        self.OLDVPRED = OLDVPRED = tf.placeholder(tf.float32, [None])
        self.LR = LR = tf.placeholder(tf.float32, [])
        # Cliprange
        self.CLIPRANGE = CLIPRANGE = tf.placeholder(tf.float32, [])

        neglogpac = train_model.pd.neglogp(A)

        # Calculate the entropy
        # Entropy is used to improve exploration by limiting the premature convergence to suboptimal policy.
        entropy = tf.reduce_mean(train_model.pd.entropy())

        # CALCULATE THE LOSS
        # Total loss = Policy gradient loss - entropy * entropy coefficient + Value coefficient * value loss

        # Clip the value to reduce variability during Critic training
        # Get the predicted value
        vpred = train_model.vf
        vpredclipped = OLDVPRED + tf.clip_by_value(train_model.vf - OLDVPRED, - CLIPRANGE, CLIPRANGE)
        # Unclipped value
        vf_losses1 = tf.square(vpred - R)
        # Clipped value
        vf_losses2 = tf.square(vpredclipped - R)

        vf_loss = .5 * tf.reduce_mean(tf.maximum(vf_losses1, vf_losses2))

        # Calculate ratio (pi current policy / pi old policy)
        ratio = tf.exp(OLDNEGLOGPAC - neglogpac)

        # Defining Loss = - J is equivalent to max J
        pg_losses = -ADV * ratio

        pg_losses2 = -ADV * tf.clip_by_value(ratio, 1.0 - CLIPRANGE, 1.0 + CLIPRANGE)

        # Final PG loss
        pg_loss = tf.reduce_mean(tf.maximum(pg_losses, pg_losses2))
        approxkl = .5 * tf.reduce_mean(tf.square(neglogpac - OLDNEGLOGPAC))
        clipfrac = tf.reduce_mean(tf.to_float(tf.greater(tf.abs(ratio - 1.0), CLIPRANGE)))

        # Total loss
        loss = pg_loss - entropy * ent_coef + vf_loss * vf_coef

        # UPDATE THE PARAMETERS USING LOSS
        # 1. Get the model parameters
        params = tf.trainable_variables('ppo2_model')
        # 2. Build our trainer
        self.trainer = tf.train.AdamOptimizer(learning_rate=LR, epsilon=1e-5)
        # 3. Calculate the gradients
        grads_and_var = self.trainer.compute_gradients(loss, params)
        grads, var = zip(*grads_and_var)

        if max_grad_norm is not None:
            # Clip the gradients (normalize)
            grads, _grad_norm = tf.clip_by_global_norm(grads, max_grad_norm)
        grads_and_var = list(zip(grads, var))
        # zip aggregate each gradient with parameters associated
        # For instance zip(ABCD, xyza) => Ax, By, Cz, Da

        self.grads = grads
        self.var = var
        self._train_op = self.trainer.apply_gradients(grads_and_var)
        self.loss_names = ['policy_loss', 'value_loss', 'policy_entropy', 'approxkl', 'clipfrac']
        self.stats_list = [pg_loss, vf_loss, entropy, approxkl, clipfrac]


        self.train_model = train_model
        self.act_model = act_model
        self.step = act_model.step
        self.value = act_model.value
        self.initial_state = act_model.initial_state

        initialize()",_67.py,46,"vf_loss = 0.5 * tf.reduce_mean(tf.maximum(vf_losses1, vf_losses2))
ratio = tf.exp(OLDNEGLOGPAC - neglogpac)","vf_loss , ratio  = 0.5 * tf.reduce_mean(tf.maximum(vf_losses1, vf_losses2)), tf.exp(OLDNEGLOGPAC - neglogpac)"
https://github.com/microsoft/nni/tree/master/nni/algorithms/hpo/ppo_tuner/model.py,"def __init__(self, *, policy, nbatch_act, nbatch_train,
                 nsteps, ent_coef, vf_coef, max_grad_norm, microbatch_size=None, np_mask=None):
        self.sess = sess = get_session()

        with tf.variable_scope('ppo2_model', reuse=tf.AUTO_REUSE):
            # CREATE OUR TWO MODELS
            # act_model that is used for sampling
            act_model = policy(nbatch_act, 1, sess, np_mask=np_mask, is_act_model=True)

            # Train model for training
            if microbatch_size is None:
                train_model = policy(nbatch_train, nsteps, sess, np_mask=np_mask, is_act_model=False)
            else:
                train_model = policy(microbatch_size, nsteps, sess, np_mask=np_mask, is_act_model=False)

        # CREATE THE PLACEHOLDERS
        self.A = A = train_model.pdtype.sample_placeholder([None])
        self.ADV = ADV = tf.placeholder(tf.float32, [None])
        self.R = R = tf.placeholder(tf.float32, [None])
        # Keep track of old actor
        self.OLDNEGLOGPAC = OLDNEGLOGPAC = tf.placeholder(tf.float32, [None])
        # Keep track of old critic
        self.OLDVPRED = OLDVPRED = tf.placeholder(tf.float32, [None])
        self.LR = LR = tf.placeholder(tf.float32, [])
        # Cliprange
        self.CLIPRANGE = CLIPRANGE = tf.placeholder(tf.float32, [])

        neglogpac = train_model.pd.neglogp(A)

        # Calculate the entropy
        # Entropy is used to improve exploration by limiting the premature convergence to suboptimal policy.
        entropy = tf.reduce_mean(train_model.pd.entropy())

        # CALCULATE THE LOSS
        # Total loss = Policy gradient loss - entropy * entropy coefficient + Value coefficient * value loss

        # Clip the value to reduce variability during Critic training
        # Get the predicted value
        vpred = train_model.vf
        vpredclipped = OLDVPRED + tf.clip_by_value(train_model.vf - OLDVPRED, - CLIPRANGE, CLIPRANGE)
        # Unclipped value
        vf_losses1 = tf.square(vpred - R)
        # Clipped value
        vf_losses2 = tf.square(vpredclipped - R)

        vf_loss = .5 * tf.reduce_mean(tf.maximum(vf_losses1, vf_losses2))

        # Calculate ratio (pi current policy / pi old policy)
        ratio = tf.exp(OLDNEGLOGPAC - neglogpac)

        # Defining Loss = - J is equivalent to max J
        pg_losses = -ADV * ratio

        pg_losses2 = -ADV * tf.clip_by_value(ratio, 1.0 - CLIPRANGE, 1.0 + CLIPRANGE)

        # Final PG loss
        pg_loss = tf.reduce_mean(tf.maximum(pg_losses, pg_losses2))
        approxkl = .5 * tf.reduce_mean(tf.square(neglogpac - OLDNEGLOGPAC))
        clipfrac = tf.reduce_mean(tf.to_float(tf.greater(tf.abs(ratio - 1.0), CLIPRANGE)))

        # Total loss
        loss = pg_loss - entropy * ent_coef + vf_loss * vf_coef

        # UPDATE THE PARAMETERS USING LOSS
        # 1. Get the model parameters
        params = tf.trainable_variables('ppo2_model')
        # 2. Build our trainer
        self.trainer = tf.train.AdamOptimizer(learning_rate=LR, epsilon=1e-5)
        # 3. Calculate the gradients
        grads_and_var = self.trainer.compute_gradients(loss, params)
        grads, var = zip(*grads_and_var)

        if max_grad_norm is not None:
            # Clip the gradients (normalize)
            grads, _grad_norm = tf.clip_by_global_norm(grads, max_grad_norm)
        grads_and_var = list(zip(grads, var))
        # zip aggregate each gradient with parameters associated
        # For instance zip(ABCD, xyza) => Ax, By, Cz, Da

        self.grads = grads
        self.var = var
        self._train_op = self.trainer.apply_gradients(grads_and_var)
        self.loss_names = ['policy_loss', 'value_loss', 'policy_entropy', 'approxkl', 'clipfrac']
        self.stats_list = [pg_loss, vf_loss, entropy, approxkl, clipfrac]


        self.train_model = train_model
        self.act_model = act_model
        self.step = act_model.step
        self.value = act_model.value
        self.initial_state = act_model.initial_state

        initialize()",_67.py,52,"pg_losses = -ADV * ratio
pg_losses2 = -ADV * tf.clip_by_value(ratio, 1.0 - CLIPRANGE, 1.0 + CLIPRANGE)","pg_losses , pg_losses2  = -ADV * ratio, -ADV * tf.clip_by_value(ratio, 1.0 - CLIPRANGE, 1.0 + CLIPRANGE)"
https://github.com/microsoft/nni/tree/master/nni/algorithms/hpo/ppo_tuner/model.py,"def __init__(self, *, policy, nbatch_act, nbatch_train,
                 nsteps, ent_coef, vf_coef, max_grad_norm, microbatch_size=None, np_mask=None):
        self.sess = sess = get_session()

        with tf.variable_scope('ppo2_model', reuse=tf.AUTO_REUSE):
            # CREATE OUR TWO MODELS
            # act_model that is used for sampling
            act_model = policy(nbatch_act, 1, sess, np_mask=np_mask, is_act_model=True)

            # Train model for training
            if microbatch_size is None:
                train_model = policy(nbatch_train, nsteps, sess, np_mask=np_mask, is_act_model=False)
            else:
                train_model = policy(microbatch_size, nsteps, sess, np_mask=np_mask, is_act_model=False)

        # CREATE THE PLACEHOLDERS
        self.A = A = train_model.pdtype.sample_placeholder([None])
        self.ADV = ADV = tf.placeholder(tf.float32, [None])
        self.R = R = tf.placeholder(tf.float32, [None])
        # Keep track of old actor
        self.OLDNEGLOGPAC = OLDNEGLOGPAC = tf.placeholder(tf.float32, [None])
        # Keep track of old critic
        self.OLDVPRED = OLDVPRED = tf.placeholder(tf.float32, [None])
        self.LR = LR = tf.placeholder(tf.float32, [])
        # Cliprange
        self.CLIPRANGE = CLIPRANGE = tf.placeholder(tf.float32, [])

        neglogpac = train_model.pd.neglogp(A)

        # Calculate the entropy
        # Entropy is used to improve exploration by limiting the premature convergence to suboptimal policy.
        entropy = tf.reduce_mean(train_model.pd.entropy())

        # CALCULATE THE LOSS
        # Total loss = Policy gradient loss - entropy * entropy coefficient + Value coefficient * value loss

        # Clip the value to reduce variability during Critic training
        # Get the predicted value
        vpred = train_model.vf
        vpredclipped = OLDVPRED + tf.clip_by_value(train_model.vf - OLDVPRED, - CLIPRANGE, CLIPRANGE)
        # Unclipped value
        vf_losses1 = tf.square(vpred - R)
        # Clipped value
        vf_losses2 = tf.square(vpredclipped - R)

        vf_loss = .5 * tf.reduce_mean(tf.maximum(vf_losses1, vf_losses2))

        # Calculate ratio (pi current policy / pi old policy)
        ratio = tf.exp(OLDNEGLOGPAC - neglogpac)

        # Defining Loss = - J is equivalent to max J
        pg_losses = -ADV * ratio

        pg_losses2 = -ADV * tf.clip_by_value(ratio, 1.0 - CLIPRANGE, 1.0 + CLIPRANGE)

        # Final PG loss
        pg_loss = tf.reduce_mean(tf.maximum(pg_losses, pg_losses2))
        approxkl = .5 * tf.reduce_mean(tf.square(neglogpac - OLDNEGLOGPAC))
        clipfrac = tf.reduce_mean(tf.to_float(tf.greater(tf.abs(ratio - 1.0), CLIPRANGE)))

        # Total loss
        loss = pg_loss - entropy * ent_coef + vf_loss * vf_coef

        # UPDATE THE PARAMETERS USING LOSS
        # 1. Get the model parameters
        params = tf.trainable_variables('ppo2_model')
        # 2. Build our trainer
        self.trainer = tf.train.AdamOptimizer(learning_rate=LR, epsilon=1e-5)
        # 3. Calculate the gradients
        grads_and_var = self.trainer.compute_gradients(loss, params)
        grads, var = zip(*grads_and_var)

        if max_grad_norm is not None:
            # Clip the gradients (normalize)
            grads, _grad_norm = tf.clip_by_global_norm(grads, max_grad_norm)
        grads_and_var = list(zip(grads, var))
        # zip aggregate each gradient with parameters associated
        # For instance zip(ABCD, xyza) => Ax, By, Cz, Da

        self.grads = grads
        self.var = var
        self._train_op = self.trainer.apply_gradients(grads_and_var)
        self.loss_names = ['policy_loss', 'value_loss', 'policy_entropy', 'approxkl', 'clipfrac']
        self.stats_list = [pg_loss, vf_loss, entropy, approxkl, clipfrac]


        self.train_model = train_model
        self.act_model = act_model
        self.step = act_model.step
        self.value = act_model.value
        self.initial_state = act_model.initial_state

        initialize()",_67.py,58,"approxkl = 0.5 * tf.reduce_mean(tf.square(neglogpac - OLDNEGLOGPAC))
clipfrac = tf.reduce_mean(tf.to_float(tf.greater(tf.abs(ratio - 1.0), CLIPRANGE)))
loss = pg_loss - entropy * ent_coef + vf_loss * vf_coef
params = tf.trainable_variables('ppo2_model')
self.trainer = tf.train.AdamOptimizer(learning_rate=LR, epsilon=1e-05)","approxkl , clipfrac , loss , params , self.trainer  = 0.5 * tf.reduce_mean(tf.square(neglogpac - OLDNEGLOGPAC)), tf.reduce_mean(tf.to_float(tf.greater(tf.abs(ratio - 1.0), CLIPRANGE))), pg_loss - entropy * ent_coef + vf_loss * vf_coef, tf.trainable_variables('ppo2_model'), tf.train.AdamOptimizer(learning_rate=LR, epsilon=1e-05)"
https://github.com/microsoft/nni/tree/master/nni/algorithms/hpo/ppo_tuner/model.py,"def __init__(self, *, policy, nbatch_act, nbatch_train,
                 nsteps, ent_coef, vf_coef, max_grad_norm, microbatch_size=None, np_mask=None):
        self.sess = sess = get_session()

        with tf.variable_scope('ppo2_model', reuse=tf.AUTO_REUSE):
            # CREATE OUR TWO MODELS
            # act_model that is used for sampling
            act_model = policy(nbatch_act, 1, sess, np_mask=np_mask, is_act_model=True)

            # Train model for training
            if microbatch_size is None:
                train_model = policy(nbatch_train, nsteps, sess, np_mask=np_mask, is_act_model=False)
            else:
                train_model = policy(microbatch_size, nsteps, sess, np_mask=np_mask, is_act_model=False)

        # CREATE THE PLACEHOLDERS
        self.A = A = train_model.pdtype.sample_placeholder([None])
        self.ADV = ADV = tf.placeholder(tf.float32, [None])
        self.R = R = tf.placeholder(tf.float32, [None])
        # Keep track of old actor
        self.OLDNEGLOGPAC = OLDNEGLOGPAC = tf.placeholder(tf.float32, [None])
        # Keep track of old critic
        self.OLDVPRED = OLDVPRED = tf.placeholder(tf.float32, [None])
        self.LR = LR = tf.placeholder(tf.float32, [])
        # Cliprange
        self.CLIPRANGE = CLIPRANGE = tf.placeholder(tf.float32, [])

        neglogpac = train_model.pd.neglogp(A)

        # Calculate the entropy
        # Entropy is used to improve exploration by limiting the premature convergence to suboptimal policy.
        entropy = tf.reduce_mean(train_model.pd.entropy())

        # CALCULATE THE LOSS
        # Total loss = Policy gradient loss - entropy * entropy coefficient + Value coefficient * value loss

        # Clip the value to reduce variability during Critic training
        # Get the predicted value
        vpred = train_model.vf
        vpredclipped = OLDVPRED + tf.clip_by_value(train_model.vf - OLDVPRED, - CLIPRANGE, CLIPRANGE)
        # Unclipped value
        vf_losses1 = tf.square(vpred - R)
        # Clipped value
        vf_losses2 = tf.square(vpredclipped - R)

        vf_loss = .5 * tf.reduce_mean(tf.maximum(vf_losses1, vf_losses2))

        # Calculate ratio (pi current policy / pi old policy)
        ratio = tf.exp(OLDNEGLOGPAC - neglogpac)

        # Defining Loss = - J is equivalent to max J
        pg_losses = -ADV * ratio

        pg_losses2 = -ADV * tf.clip_by_value(ratio, 1.0 - CLIPRANGE, 1.0 + CLIPRANGE)

        # Final PG loss
        pg_loss = tf.reduce_mean(tf.maximum(pg_losses, pg_losses2))
        approxkl = .5 * tf.reduce_mean(tf.square(neglogpac - OLDNEGLOGPAC))
        clipfrac = tf.reduce_mean(tf.to_float(tf.greater(tf.abs(ratio - 1.0), CLIPRANGE)))

        # Total loss
        loss = pg_loss - entropy * ent_coef + vf_loss * vf_coef

        # UPDATE THE PARAMETERS USING LOSS
        # 1. Get the model parameters
        params = tf.trainable_variables('ppo2_model')
        # 2. Build our trainer
        self.trainer = tf.train.AdamOptimizer(learning_rate=LR, epsilon=1e-5)
        # 3. Calculate the gradients
        grads_and_var = self.trainer.compute_gradients(loss, params)
        grads, var = zip(*grads_and_var)

        if max_grad_norm is not None:
            # Clip the gradients (normalize)
            grads, _grad_norm = tf.clip_by_global_norm(grads, max_grad_norm)
        grads_and_var = list(zip(grads, var))
        # zip aggregate each gradient with parameters associated
        # For instance zip(ABCD, xyza) => Ax, By, Cz, Da

        self.grads = grads
        self.var = var
        self._train_op = self.trainer.apply_gradients(grads_and_var)
        self.loss_names = ['policy_loss', 'value_loss', 'policy_entropy', 'approxkl', 'clipfrac']
        self.stats_list = [pg_loss, vf_loss, entropy, approxkl, clipfrac]


        self.train_model = train_model
        self.act_model = act_model
        self.step = act_model.step
        self.value = act_model.value
        self.initial_state = act_model.initial_state

        initialize()",_67.py,76,"grads_and_var = list(zip(grads, var))
self.grads = grads
self.var = var","grads_and_var , self.grads , self.var  = list(zip(grads, var)), grads, var"
https://github.com/microsoft/nni/tree/master/nni/algorithms/hpo/ppo_tuner/model.py,"def __init__(self, *, policy, nbatch_act, nbatch_train,
                 nsteps, ent_coef, vf_coef, max_grad_norm, microbatch_size=None, np_mask=None):
        self.sess = sess = get_session()

        with tf.variable_scope('ppo2_model', reuse=tf.AUTO_REUSE):
            # CREATE OUR TWO MODELS
            # act_model that is used for sampling
            act_model = policy(nbatch_act, 1, sess, np_mask=np_mask, is_act_model=True)

            # Train model for training
            if microbatch_size is None:
                train_model = policy(nbatch_train, nsteps, sess, np_mask=np_mask, is_act_model=False)
            else:
                train_model = policy(microbatch_size, nsteps, sess, np_mask=np_mask, is_act_model=False)

        # CREATE THE PLACEHOLDERS
        self.A = A = train_model.pdtype.sample_placeholder([None])
        self.ADV = ADV = tf.placeholder(tf.float32, [None])
        self.R = R = tf.placeholder(tf.float32, [None])
        # Keep track of old actor
        self.OLDNEGLOGPAC = OLDNEGLOGPAC = tf.placeholder(tf.float32, [None])
        # Keep track of old critic
        self.OLDVPRED = OLDVPRED = tf.placeholder(tf.float32, [None])
        self.LR = LR = tf.placeholder(tf.float32, [])
        # Cliprange
        self.CLIPRANGE = CLIPRANGE = tf.placeholder(tf.float32, [])

        neglogpac = train_model.pd.neglogp(A)

        # Calculate the entropy
        # Entropy is used to improve exploration by limiting the premature convergence to suboptimal policy.
        entropy = tf.reduce_mean(train_model.pd.entropy())

        # CALCULATE THE LOSS
        # Total loss = Policy gradient loss - entropy * entropy coefficient + Value coefficient * value loss

        # Clip the value to reduce variability during Critic training
        # Get the predicted value
        vpred = train_model.vf
        vpredclipped = OLDVPRED + tf.clip_by_value(train_model.vf - OLDVPRED, - CLIPRANGE, CLIPRANGE)
        # Unclipped value
        vf_losses1 = tf.square(vpred - R)
        # Clipped value
        vf_losses2 = tf.square(vpredclipped - R)

        vf_loss = .5 * tf.reduce_mean(tf.maximum(vf_losses1, vf_losses2))

        # Calculate ratio (pi current policy / pi old policy)
        ratio = tf.exp(OLDNEGLOGPAC - neglogpac)

        # Defining Loss = - J is equivalent to max J
        pg_losses = -ADV * ratio

        pg_losses2 = -ADV * tf.clip_by_value(ratio, 1.0 - CLIPRANGE, 1.0 + CLIPRANGE)

        # Final PG loss
        pg_loss = tf.reduce_mean(tf.maximum(pg_losses, pg_losses2))
        approxkl = .5 * tf.reduce_mean(tf.square(neglogpac - OLDNEGLOGPAC))
        clipfrac = tf.reduce_mean(tf.to_float(tf.greater(tf.abs(ratio - 1.0), CLIPRANGE)))

        # Total loss
        loss = pg_loss - entropy * ent_coef + vf_loss * vf_coef

        # UPDATE THE PARAMETERS USING LOSS
        # 1. Get the model parameters
        params = tf.trainable_variables('ppo2_model')
        # 2. Build our trainer
        self.trainer = tf.train.AdamOptimizer(learning_rate=LR, epsilon=1e-5)
        # 3. Calculate the gradients
        grads_and_var = self.trainer.compute_gradients(loss, params)
        grads, var = zip(*grads_and_var)

        if max_grad_norm is not None:
            # Clip the gradients (normalize)
            grads, _grad_norm = tf.clip_by_global_norm(grads, max_grad_norm)
        grads_and_var = list(zip(grads, var))
        # zip aggregate each gradient with parameters associated
        # For instance zip(ABCD, xyza) => Ax, By, Cz, Da

        self.grads = grads
        self.var = var
        self._train_op = self.trainer.apply_gradients(grads_and_var)
        self.loss_names = ['policy_loss', 'value_loss', 'policy_entropy', 'approxkl', 'clipfrac']
        self.stats_list = [pg_loss, vf_loss, entropy, approxkl, clipfrac]


        self.train_model = train_model
        self.act_model = act_model
        self.step = act_model.step
        self.value = act_model.value
        self.initial_state = act_model.initial_state

        initialize()",_67.py,82,"self._train_op = self.trainer.apply_gradients(grads_and_var)
self.loss_names = ['policy_loss', 'value_loss', 'policy_entropy', 'approxkl', 'clipfrac']
self.stats_list = [pg_loss, vf_loss, entropy, approxkl, clipfrac]
self.train_model = train_model
self.act_model = act_model
self.step = act_model.step
self.value = act_model.value
self.initial_state = act_model.initial_state","self._train_op , self.loss_names , self.stats_list , self.train_model , self.act_model , self.step , self.value , self.initial_state  = self.trainer.apply_gradients(grads_and_var), ['policy_loss', 'value_loss', 'policy_entropy', 'approxkl', 'clipfrac'], [pg_loss, vf_loss, entropy, approxkl, clipfrac], train_model, act_model, act_model.step, act_model.value, act_model.initial_state"
https://github.com/pyro-ppl/pyro/tree/master/tests/contrib/funsor/test_valid_models_enum.py,"def model():
        p = pyro.param(""p"", 0.25 * torch.ones(2, 2))
        q = pyro.param(""q"", 0.25 * torch.ones(2))
        x_prev = torch.tensor(0)
        x_curr = torch.tensor(0)
        for t in pyro.markov(range(10), history=history):
            probs = p[x_prev, x_curr]
            x_prev, x_curr = (
                x_curr,
                pyro.sample(""x_{}"".format(t), dist.Bernoulli(probs)).long(),
            )
            pyro.sample(
                ""y_{}"".format(t), dist.Bernoulli(q[x_curr]), obs=torch.tensor(0.0)
            )",_68.py,2,"p = pyro.param('p', 0.25 * torch.ones(2, 2))
q = pyro.param('q', 0.25 * torch.ones(2))
x_prev = torch.tensor(0)
x_curr = torch.tensor(0)","p , q , x_prev , x_curr  = pyro.param('p', 0.25 * torch.ones(2, 2)), pyro.param('q', 0.25 * torch.ones(2)), torch.tensor(0), torch.tensor(0)"
https://github.com/archerysec/archerysec/tree/master/staticscanners/views.py,"def get(self, request, uu_id=None):
        jira_url = None
        jira = jirasetting.objects.all()
        for d in jira:
            jira_url = d.jira_server

        all_notify = Notification.objects.unread()
        if uu_id == None:
            scan_id = request.GET[""scan_id""]
            scan_name = request.GET[""scan_name""]
            vuln_data = StaticScanResultsDb.objects.filter(scan_id=scan_id, title=scan_name)
        else:
            try:
                vuln_data = StaticScanResultsDb.objects.filter(scan_id=uu_id)
            except:
                return Response(
                    {""message"": ""Scan Id Doesn't Exist""}, status=status.HTTP_404_NOT_FOUND
                )
        if request.path[: 4] == '/api':
            serialized_data = StaticScanResultsDbSerializer(vuln_data, many=True)
            return Response(serialized_data.data)
        else:
            return render(
                request,
                ""staticscanners/scans/list_vuln_info.html"",
                {""vuln_data"": vuln_data, ""jira_url"": jira_url},
            )",_70.py,2,"jira_url = None
jira = jirasetting.objects.all()","jira_url , jira  = None, jirasetting.objects.all()"
https://github.com/archerysec/archerysec/tree/master/staticscanners/views.py,"def get(self, request, uu_id=None):
        jira_url = None
        jira = jirasetting.objects.all()
        for d in jira:
            jira_url = d.jira_server

        all_notify = Notification.objects.unread()
        if uu_id == None:
            scan_id = request.GET[""scan_id""]
            scan_name = request.GET[""scan_name""]
            vuln_data = StaticScanResultsDb.objects.filter(scan_id=scan_id, title=scan_name)
        else:
            try:
                vuln_data = StaticScanResultsDb.objects.filter(scan_id=uu_id)
            except:
                return Response(
                    {""message"": ""Scan Id Doesn't Exist""}, status=status.HTTP_404_NOT_FOUND
                )
        if request.path[: 4] == '/api':
            serialized_data = StaticScanResultsDbSerializer(vuln_data, many=True)
            return Response(serialized_data.data)
        else:
            return render(
                request,
                ""staticscanners/scans/list_vuln_info.html"",
                {""vuln_data"": vuln_data, ""jira_url"": jira_url},
            )",_70.py,9,"scan_id = request.GET['scan_id']
scan_name = request.GET['scan_name']","scan_id , scan_name  = request.GET['scan_id'], request.GET['scan_name']"
https://github.com/lux-org/lux/tree/master/tests/test_export.py,"def test_heatmap_code_export(global_var):
    df = pd.read_csv(""https://raw.githubusercontent.com/lux-org/lux-datasets/master/data/airbnb_nyc.csv"")
    lux.config._heatmap_start = 100

    vis = Vis([""price"", ""longitude""], df)
    PandasExecutor.execute([vis], df)
    code = vis.to_code(""python"")

    try:
        exec(code, globals())
        create_chart_data(df, vis)
    except:
        assert False

    lux.config._heatmap_start = 5000",_76.py,3,"lux.config._heatmap_start = 100
vis = Vis(['price', 'longitude'], df)","lux.config._heatmap_start , vis  = 100, Vis(['price', 'longitude'], df)"
https://github.com/microsoft/msticpy/tree/master/tests/data/uploaders/test_splunk_uploader.py,"def test_not_connected(sp_upload):
    """"""Test no connection is handled correctly.""""""
    sp_upload.connected = False
    data_file = Path(_TEST_DATA).joinpath(""syslog_data.csv"")
    with pytest.raises(MsticpyConnectionError):
        sp_upload.upload_file(
            data_file, index_name=""test_upload"", table_name=""test_upload""
        )",_81.py,3,"sp_upload.connected = False
data_file = Path(_TEST_DATA).joinpath('syslog_data.csv')","sp_upload.connected , data_file  = False, Path(_TEST_DATA).joinpath('syslog_data.csv')"
https://github.com/mozilla/addons-server/tree/master/src/olympia/activity/tests/test_views.py,"def test_developer_reply(self):
        self.user = user_factory()
        self.user.addonuser_set.create(addon=self.addon)
        self.client.login_api(self.user)
        assert not self.get_review_activity_queryset().exists()

        response = self._post_reply()
        assert response.status_code == 201
        logs = self.get_review_activity_queryset()
        assert logs.count() == 1

        reply = logs[0]
        rdata = response.data
        assert reply.pk == rdata['id']
        assert (
            str(reply.details['comments']) == rdata['comments'] == 'commnty McCmmnt'
        )
        assert reply.user == self.user
        assert reply.user.name == rdata['user']['name'] == self.user.name
        assert reply.action == amo.LOG.DEVELOPER_REPLY_VERSION.id
        assert not rdata['highlight']",_84.py,12,"reply = logs[0]
rdata = response.data","reply , rdata  = logs[0], response.data"
https://github.com/kalliope-project/kalliope/tree/master/kalliope/neurons/settings/tests/test_settings.py,"def test_is_parameters_ok(self):
        # TODO this code relies on the current settings.yml file, should create a full mock instead

        # tts
        self.neuron_settings.default_tts = ""pico2wave""
        self.assertTrue(self.neuron_settings._is_parameters_ok())

        self.neuron_settings.text_to_speech = [{""pico2wave"": {""language"": ""fr-FR""}}]
        self.assertTrue(self.neuron_settings._is_parameters_ok())

        # stt
        self.neuron_settings.default_stt = ""google""
        self.assertTrue(self.neuron_settings._is_parameters_ok())

        self.neuron_settings.speech_to_text = [{""google"": {""language"": ""fr-FR""}}]
        self.assertTrue(self.neuron_settings._is_parameters_ok())

        # player
        self.neuron_settings.default_player = ""mplayer""
        self.assertTrue(self.neuron_settings._is_parameters_ok())

        self.neuron_settings.players = [{""mplayer"": {}}]

        # trigger
        self.neuron_settings.default_trigger = ""snowboy""
        self.assertTrue(self.neuron_settings._is_parameters_ok())

        # hooks
        self.neuron_settings.hooks = {""blabla"": [""coucou"", ""test""]}
        self.assertTrue(self.neuron_settings._is_parameters_ok())
        self.neuron_settings.hooks = {""blabla"": ""string""}
        self.assertTrue(self.neuron_settings._is_parameters_ok())

        # variables
        tmpfile = tempfile.NamedTemporaryFile()
        self.neuron_settings.var_files = [tmpfile.name]
        self.assertTrue(self.neuron_settings._is_parameters_ok())

        # deaf
        self.neuron_settings.deaf = 60
        self.assertFalse(self.neuron_settings._is_parameters_ok())
        self.neuron_settings.deaf = ""randomString""
        self.assertFalse(self.neuron_settings._is_parameters_ok())
        self.neuron_settings.deaf = 0
        self.assertFalse(self.neuron_settings._is_parameters_ok())
        self.neuron_settings.deaf = True
        self.assertTrue(self.neuron_settings._is_parameters_ok())


        # mute
        self.neuron_settings.mute = 60
        self.assertFalse(self.neuron_settings._is_parameters_ok())
        self.neuron_settings.mute = ""randomString""
        self.assertFalse(self.neuron_settings._is_parameters_ok())
        self.neuron_settings.mute = 0
        self.assertFalse(self.neuron_settings._is_parameters_ok())
        self.neuron_settings.mute = True
        self.assertTrue(self.neuron_settings._is_parameters_ok())

        # recognizer_multiplier
        self.neuron_settings.recognizer_multiplier = ""randomString""
        self.assertFalse(self.neuron_settings._is_parameters_ok())
        self.neuron_settings.recognizer_multiplier = 60
        self.assertTrue(self.neuron_settings._is_parameters_ok())

        # recognizer_energy_ratio
        self.neuron_settings.recognizer_energy_ratio = ""randomString""
        self.assertFalse(self.neuron_settings._is_parameters_ok())
        self.neuron_settings.recognizer_energy_ratio = 60
        self.assertTrue(self.neuron_settings._is_parameters_ok())

        # recognizer_recording_timeout
        self.neuron_settings.recognizer_recording_timeout = ""randomString""
        self.assertFalse(self.neuron_settings._is_parameters_ok())
        self.neuron_settings.recognizer_recording_timeout = 60
        self.assertTrue(self.neuron_settings._is_parameters_ok())

        # recognizer_recording_timeout_with_silence
        self.neuron_settings.recognizer_recording_timeout_with_silence = ""randomString""
        self.assertFalse(self.neuron_settings._is_parameters_ok())
        self.neuron_settings.recognizer_recording_timeout_with_silence = 60
        self.assertTrue(self.neuron_settings._is_parameters_ok())",_9.py,22,"self.neuron_settings.players = [{'mplayer': {}}]
self.neuron_settings.default_trigger = 'snowboy'","self.neuron_settings.players , self.neuron_settings.default_trigger  = [{'mplayer': {}}], 'snowboy'"
https://github.com/JoinQuant/jqdatasdk/tree/master/jqdatasdk/alpha191.py,"def alpha_147(code, end_date=None, fq=""pre""):
    """"""
    :
       REGBETA(MEAN(CLOSE,12),SEQUENCE(12))
    Inputs:
        code: 
        end_date: 
    Outputs:
        
    """"""
    end_date = to_date_str(end_date)
    func_name = sys._getframe().f_code.co_name
    return JQDataClient.instance().get_alpha_191(**locals())",_90.py,11,"end_date = to_date_str(end_date)
func_name = sys._getframe().f_code.co_name","end_date , func_name  = to_date_str(end_date), sys._getframe().f_code.co_name"
https://github.com/dask/dask/tree/master/dask/tests/test_optimization.py,"def test_fuse_subgraphs_linear_chains_of_duplicate_deps(compare_subgraph_callables):
    dsk = {
        ""x-1"": 1,
        ""add-1"": (add, ""x-1"", ""x-1""),
        ""add-2"": (add, ""add-1"", ""add-1""),
        ""add-3"": (add, ""add-2"", ""add-2""),
        ""add-4"": (add, ""add-3"", ""add-3""),
        ""add-5"": (add, ""add-4"", ""add-4""),
    }

    res = fuse(dsk, ""add-5"", fuse_subgraphs=True)
    sol = with_deps(
        {
            ""add-x-1"": (
                SubgraphCallable(
                    {
                        ""x-1"": 1,
                        ""add-1"": (add, ""x-1"", ""x-1""),
                        ""add-2"": (add, ""add-1"", ""add-1""),
                        ""add-3"": (add, ""add-2"", ""add-2""),
                        ""add-4"": (add, ""add-3"", ""add-3""),
                        ""add-5"": (add, ""add-4"", ""add-4""),
                    },
                    ""add-5"",
                    (),
                ),
            ),
            ""add-5"": ""add-x-1"",
        }
    )
    assert res == sol",_91.py,11,"res = fuse(dsk, 'add-5', fuse_subgraphs=True)
sol = with_deps({'add-x-1': (SubgraphCallable({'x-1': 1, 'add-1': (add, 'x-1', 'x-1'), 'add-2': (add, 'add-1', 'add-1'), 'add-3': (add, 'add-2', 'add-2'), 'add-4': (add, 'add-3', 'add-3'), 'add-5': (add, 'add-4', 'add-4')}, 'add-5', ()),), 'add-5': 'add-x-1'})","res , sol  = fuse(dsk, 'add-5', fuse_subgraphs=True), with_deps({'add-x-1': (SubgraphCallable({'x-1': 1, 'add-1': (add, 'x-1', 'x-1'), 'add-2': (add, 'add-1', 'add-1'), 'add-3': (add, 'add-2', 'add-2'), 'add-4': (add, 'add-3', 'add-3'), 'add-5': (add, 'add-4', 'add-4')}, 'add-5', ()),), 'add-5': 'add-x-1'})"
https://github.com/beancount/beancount/tree/master/experiments/docs/framedocs.py,"def parse_htaccess(filename):
    documents = collections.defaultdict(list)
    redirects = []
    for line in open(filename):
        match = re.match(r'RedirectMatch /doc/(.+?)\$\s+(.+)$', line)
        if match:
            name, url = match.groups()
            url_match = re.match('https://docs.google.com/document/d/(.+)/$', url)
            if url_match:
                docid = url_match.group(1)
                documents[docid].insert(0, name)
            else:
                redirects.append((name, url))

    doc2id = {name[0]: docid for docid, name in documents.items()}
    for name, url in redirects:
        if not url.startswith('/beancount/doc/'):
            continue
        url = re.sub('^/beancount/doc/', '', url)
        try:
            docid = doc2id[url]
            documents[docid].append(name)
        except KeyError:
            pass

    return documents",_99.py,2,"documents = collections.defaultdict(list)
redirects = []","documents , redirects  = collections.defaultdict(list), []"
https://github.com/colour-science/colour/tree/master/colour/models/tests/test_cie_luv.py,"def test_n_dimensional_xy_to_Luv_uv(self):
 """"""
 Tests :func:`colour.models.cie_luv.xy_to_Luv_uv` definition
 n-dimensional arrays support.
 """"""
 xy = np.array([0.54369558, 0.32107944])
 uv = xy_to_Luv_uv(xy)
 xy = np.tile(xy, (6, 1))
 uv = np.tile(uv, (6, 1))
 np.testing.assert_almost_equal(xy_to_Luv_uv(xy), uv, decimal=7)
 xy = np.reshape(xy, (2, 3, 2))
 uv = np.reshape(uv, (2, 3, 2))
 np.testing.assert_almost_equal(xy_to_Luv_uv(xy), uv, decimal=7)",_360.py,10,"xy = np.tile(xy, (6, 1))
uv = np.tile(uv, (6, 1))","xy , uv = np.tile(xy, (6, 1)), np.tile(uv, (6, 1))"
https://github.com/colour-science/colour/tree/master/colour/models/tests/test_cie_luv.py,"def test_n_dimensional_xy_to_Luv_uv(self):
 """"""
 Tests :func:`colour.models.cie_luv.xy_to_Luv_uv` definition
 n-dimensional arrays support.
 """"""
 xy = np.array([0.54369558, 0.32107944])
 uv = xy_to_Luv_uv(xy)
 xy = np.tile(xy, (6, 1))
 uv = np.tile(uv, (6, 1))
 np.testing.assert_almost_equal(xy_to_Luv_uv(xy), uv, decimal=7)
 xy = np.reshape(xy, (2, 3, 2))
 uv = np.reshape(uv, (2, 3, 2))
 np.testing.assert_almost_equal(xy_to_Luv_uv(xy), uv, decimal=7)",_360.py,14,"xy = np.reshape(xy, (2, 3, 2))
uv = np.reshape(uv, (2, 3, 2))","xy , uv = np.reshape(xy, (2, 3, 2)), np.reshape(uv, (2, 3, 2))"
https://github.com/ring04h/weakfilescan/tree/master/libs/requests/packages/chardet/chardistribution.py,"def __init__(self):
        CharDistributionAnalysis.__init__(self)
        self._mCharToFreqOrder = EUCKRCharToFreqOrder
        self._mTableSize = EUCKR_TABLE_SIZE
        self._mTypicalDistributionRatio = EUCKR_TYPICAL_DISTRIBUTION_RATIO",_368.py,3,"self._mCharToFreqOrder = EUCKRCharToFreqOrder
self._mTableSize = EUCKR_TABLE_SIZE
self._mTypicalDistributionRatio = EUCKR_TYPICAL_DISTRIBUTION_RATIO","self._mCharToFreqOrder , self._mTableSize , self._mTypicalDistributionRatio = EUCKRCharToFreqOrder, EUCKR_TABLE_SIZE, EUCKR_TYPICAL_DISTRIBUTION_RATIO"
https://github.com/shenweichen/GraphEmbedding/tree/master/examples/struc2vec_flight.py,"def plot_embeddings(embeddings,):

    X, Y = read_node_label('../data/flight/labels-brazil-airports.txt',skip_head=True)



    emb_list = []

    for k in X:

        emb_list.append(embeddings[k])

    emb_list = np.array(emb_list)



    model = TSNE(n_components=2)

    node_pos = model.fit_transform(emb_list)



    color_idx = {}

    for i in range(len(X)):

        color_idx.setdefault(Y[i][0], [])

        color_idx[Y[i][0]].append(i)



    for c, idx in color_idx.items():

        plt.scatter(node_pos[idx, 0], node_pos[idx, 1], label=c)  # c=node_colors)

    plt.legend()

    plt.show()",_381.py,13,"emb_list = np.array(emb_list)
model = TSNE(n_components=2)","emb_list , model = np.array(emb_list), TSNE(n_components=2)"
https://github.com/shenweichen/GraphEmbedding/tree/master/examples/struc2vec_flight.py,"def plot_embeddings(embeddings,):

    X, Y = read_node_label('../data/flight/labels-brazil-airports.txt',skip_head=True)



    emb_list = []

    for k in X:

        emb_list.append(embeddings[k])

    emb_list = np.array(emb_list)



    model = TSNE(n_components=2)

    node_pos = model.fit_transform(emb_list)



    color_idx = {}

    for i in range(len(X)):

        color_idx.setdefault(Y[i][0], [])

        color_idx[Y[i][0]].append(i)



    for c, idx in color_idx.items():

        plt.scatter(node_pos[idx, 0], node_pos[idx, 1], label=c)  # c=node_colors)

    plt.legend()

    plt.show()",_381.py,19,"node_pos = model.fit_transform(emb_list)
color_idx = {}","node_pos , color_idx = model.fit_transform(emb_list), {}"
https://github.com/nvaccess/nvda/tree/master/source/oleacc.py,"def AccessibleObjectFromEvent(hwnd,objectID,childID):
	""""""
	Retreaves an  IAccessible object from the given window, with the given object ID and child ID.
	@param hwnd: the handle of the window to retreave the object from.
	@type hwnd: int
	@param objectID: one of the OBJID_* constants or a custom positive value representing the specific object you want to retreave.
	@type objectID: int
	@param childID: the ID of the child element you wish to retreave.
	@type childID: int
	@return: the retreaved object.
	@rtype: COMObject
	""""""
	p=POINTER(IAccessible)()
	varChild=VARIANT()
	oledll.oleacc.AccessibleObjectFromEvent(hwnd,objectID,childID,byref(p),byref(varChild))
	if varChild.vt==VT_I4:
		childID=varChild.value
	return (p,childID)",_387.py,13,"p = POINTER(IAccessible)()
varChild = VARIANT()","p , varChild = POINTER(IAccessible)(), VARIANT()"
