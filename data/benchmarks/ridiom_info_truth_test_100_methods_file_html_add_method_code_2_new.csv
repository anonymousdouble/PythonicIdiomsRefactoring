file_html,method_content,file_name,lineno,old_code,new_code
https://github.com/coqui-ai/TTS/tree/master/TTS/tts/utils/helpers.py,"def average_over_durations(values, durs):
    """"""Average values over durations.

    Shapes:
        - values: :math:`[B, 1, T_de]`
        - durs: :math:`[B, T_en]`
        - avg: :math:`[B, 1, T_en]`
    """"""
    durs_cums_ends = torch.cumsum(durs, dim=1).long()
    durs_cums_starts = torch.nn.functional.pad(durs_cums_ends[:, :-1], (1, 0))
    values_nonzero_cums = torch.nn.functional.pad(torch.cumsum(values != 0.0, dim=2), (1, 0))
    values_cums = torch.nn.functional.pad(torch.cumsum(values, dim=2), (1, 0))

    bs, l = durs_cums_ends.size()
    n_formants = values.size(1)
    dcs = durs_cums_starts[:, None, :].expand(bs, n_formants, l)
    dce = durs_cums_ends[:, None, :].expand(bs, n_formants, l)

    values_sums = (torch.gather(values_cums, 2, dce) - torch.gather(values_cums, 2, dcs)).float()
    values_nelems = (torch.gather(values_nonzero_cums, 2, dce) - torch.gather(values_nonzero_cums, 2, dcs)).float()

    avg = torch.where(values_nelems == 0.0, values_nelems, values_sums / values_nelems)
    return avg",_433.py,22,values_nelems == 0.0,not values_nelems
https://github.com/pandas-dev/pandas/tree/master/pandas/tests/series/test_ufunc.py,"def test_multiple_output_binary_ufuncs(ufunc, sparse, shuffle, arrays_for_binary_ufunc):
    # Test that
    #  the same conditions from binary_ufunc_scalar apply to
    #  ufuncs with multiple outputs.

    a1, a2 = arrays_for_binary_ufunc
    # work around https://github.com/pandas-dev/pandas/issues/26987
    a1[a1 == 0] = 1
    a2[a2 == 0] = 1

    if sparse:
        a1 = SparseArray(a1, dtype=pd.SparseDtype(""int64"", 0))
        a2 = SparseArray(a2, dtype=pd.SparseDtype(""int64"", 0))

    s1 = pd.Series(a1)
    s2 = pd.Series(a2)

    if shuffle:
        # ensure we align before applying the ufunc
        s2 = s2.sample(frac=1)

    expected = ufunc(a1, a2)
    assert isinstance(expected, tuple)

    result = ufunc(s1, s2)
    assert isinstance(result, tuple)
    tm.assert_series_equal(result[0], pd.Series(expected[0]))
    tm.assert_series_equal(result[1], pd.Series(expected[1]))",_992.py,8,a1 == 0,not a1
https://github.com/pandas-dev/pandas/tree/master/pandas/tests/series/test_ufunc.py,"def test_multiple_output_binary_ufuncs(ufunc, sparse, shuffle, arrays_for_binary_ufunc):
    # Test that
    #  the same conditions from binary_ufunc_scalar apply to
    #  ufuncs with multiple outputs.

    a1, a2 = arrays_for_binary_ufunc
    # work around https://github.com/pandas-dev/pandas/issues/26987
    a1[a1 == 0] = 1
    a2[a2 == 0] = 1

    if sparse:
        a1 = SparseArray(a1, dtype=pd.SparseDtype(""int64"", 0))
        a2 = SparseArray(a2, dtype=pd.SparseDtype(""int64"", 0))

    s1 = pd.Series(a1)
    s2 = pd.Series(a2)

    if shuffle:
        # ensure we align before applying the ufunc
        s2 = s2.sample(frac=1)

    expected = ufunc(a1, a2)
    assert isinstance(expected, tuple)

    result = ufunc(s1, s2)
    assert isinstance(result, tuple)
    tm.assert_series_equal(result[0], pd.Series(expected[0]))
    tm.assert_series_equal(result[1], pd.Series(expected[1]))",_992.py,9,a2 == 0,not a2
https://github.com/dmlc/gluon-cv/tree/master/gluoncv/torch/data/detection/detection_utils.py,"def transform_keypoint_annotations(keypoints, transforms, image_size, keypoint_hflip_indices=None):
    """"""
    Transform keypoint annotations of an image.
    If a keypoint is transformed out of image boundary, it will be marked ""unlabeled"" (visibility=0)

    Args:
        keypoints (list[float]): Nx3 float in Detectron2's Dataset format.
            Each point is represented by (x, y, visibility).
        transforms (TransformList):
        image_size (tuple): the height, width of the transformed image
        keypoint_hflip_indices (ndarray[int]): see `create_keypoint_hflip_indices`.
            When `transforms` includes horizontal flip, will use the index
            mapping to flip keypoints.
    """"""
    # (N*3,) -> (N, 3)
    keypoints = np.asarray(keypoints, dtype=""float64"").reshape(-1, 3)
    keypoints_xy = transforms.apply_coords(keypoints[:, :2])

    # Set all out-of-boundary points to ""unlabeled""
    inside = (keypoints_xy >= np.array([0, 0])) & (keypoints_xy <= np.array(image_size[::-1]))
    inside = inside.all(axis=1)
    keypoints[:, :2] = keypoints_xy
    keypoints[:, 2][~inside] = 0

    # This assumes that HorizFlipTransform is the only one that does flip
    do_hflip = sum(isinstance(t, T.HFlipTransform) for t in transforms.transforms) % 2 == 1

    # Alternative way: check if probe points was horizontally flipped.
    # probe = np.asarray([[0.0, 0.0], [image_width, 0.0]])
    # probe_aug = transforms.apply_coords(probe.copy())
    # do_hflip = np.sign(probe[1][0] - probe[0][0]) != np.sign(probe_aug[1][0] - probe_aug[0][0])  # noqa

    # If flipped, swap each keypoint with its opposite-handed equivalent
    if do_hflip:
        assert keypoint_hflip_indices is not None
        keypoints = keypoints[keypoint_hflip_indices, :]

    # Maintain COCO convention that if visibility == 0 (unlabeled), then x, y = 0
    keypoints[keypoints[:, 2] == 0] = 0
    return keypoints",_1417.py,39,"keypoints[:, 2] == 0","not keypoints[:, 2]"
https://github.com/open-mmlab/mmgeneration/tree/master/mmgen/core/hooks/ema_hook.py,"def every_n_iters(self, runner, n):
        if runner.iter < self.start_iter:
            return True
        return (runner.iter + 1 - self.start_iter) % n == 0 if n > 0 else False",_2161.py,4,(runner.iter + 1 - self.start_iter) % n == 0,not (runner.iter + 1 - self.start_iter) % n
https://github.com/scaelles/DEXTR-PyTorch/tree/master/dataloaders/helpers.py,"def overlay_masks(im, masks, alpha=0.5):
    colors = np.load(os.path.join(os.path.dirname(__file__), 'pascal_map.npy'))/255.
    
    if isinstance(masks, np.ndarray):
        masks = [masks]

    assert len(colors) >= len(masks), 'Not enough colors'

    ov = im.copy()
    im = im.astype(np.float32)
    total_ma = np.zeros([im.shape[0], im.shape[1]])
    i = 1
    for ma in masks:
        ma = ma.astype(np.bool)
        fg = im * alpha+np.ones(im.shape) * (1 - alpha) * colors[i, :3]   # np.array([0,0,255])/255.0
        i = i + 1
        ov[ma == 1] = fg[ma == 1]
        total_ma += ma

        # [-2:] is s trick to be compatible both with opencv 2 and 3
        contours = cv2.findContours(ma.copy().astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[-2:]
        cv2.drawContours(ov, contours[0], -1, (0.0, 0.0, 0.0), 1)
    ov[total_ma == 0] = im[total_ma == 0]

    return ov",_2217.py,23,total_ma == 0,not total_ma
https://github.com/scaelles/DEXTR-PyTorch/tree/master/dataloaders/helpers.py,"def overlay_masks(im, masks, alpha=0.5):
    colors = np.load(os.path.join(os.path.dirname(__file__), 'pascal_map.npy'))/255.
    
    if isinstance(masks, np.ndarray):
        masks = [masks]

    assert len(colors) >= len(masks), 'Not enough colors'

    ov = im.copy()
    im = im.astype(np.float32)
    total_ma = np.zeros([im.shape[0], im.shape[1]])
    i = 1
    for ma in masks:
        ma = ma.astype(np.bool)
        fg = im * alpha+np.ones(im.shape) * (1 - alpha) * colors[i, :3]   # np.array([0,0,255])/255.0
        i = i + 1
        ov[ma == 1] = fg[ma == 1]
        total_ma += ma

        # [-2:] is s trick to be compatible both with opencv 2 and 3
        contours = cv2.findContours(ma.copy().astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[-2:]
        cv2.drawContours(ov, contours[0], -1, (0.0, 0.0, 0.0), 1)
    ov[total_ma == 0] = im[total_ma == 0]

    return ov",_2217.py,23,total_ma == 0,not total_ma
https://github.com/Zulko/easyAI/tree/master/easyAI/games/Nim.py,"def win(self):
        return max(self.piles) == 0",_2706.py,2,max(self.piles) == 0,not max(self.piles)
https://github.com/CellProfiler/CellProfiler/tree/master/tests/modules/test_measureobjectintensity.py,"def test_image_masked(measurements, module, objects, image, workspace):
    data = numpy.zeros((11, 20))

    i, j = numpy.mgrid[-5:6, -5:15]
    data[i ** 2 + j ** 2 <= 25] = 0.5
    data[i ** 2 + j ** 2 <= 16] = 0.25
    data[i ** 2 + j ** 2 == 0] = 1

    i, j = numpy.mgrid[-5:6, -15:5]
    data[i ** 2 + j ** 2 <= 9] = 0.5
    data[i ** 2 + j ** 2 <= 4] = 0.75
    data[i ** 2 + j ** 2 == 0] = 1

    mask = numpy.ones_like(data)
    mask[:, 11:] = 0

    image.pixel_data = data
    image.dimensions = 2
    image.mask = mask > 0

    labels = skimage.measure.label(data > 0)

    objects.segmented = labels

    module.run(workspace)

    expected = {
        ""Intensity_IntegratedIntensity_MyImage"": [29.0, 0.0],
        ""Intensity_MeanIntensity_MyImage"": [0.35802469135802467, numpy.nan],
        ""Intensity_StdIntensity_MyImage"": [0.14130275484271107, numpy.nan],
        ""Intensity_MinIntensity_MyImage"": [0.25, 0.0],
        ""Intensity_MaxIntensity_MyImage"": [1.0, 0.0],
        ""Intensity_IntegratedIntensityEdge_MyImage"": [14.0, 0.0],
        ""Intensity_MeanIntensityEdge_MyImage"": [0.5, numpy.nan],
        ""Intensity_StdIntensityEdge_MyImage"": [0.0, numpy.nan],
        ""Intensity_MinIntensityEdge_MyImage"": [0.5, 0.0],
        ""Intensity_MaxIntensityEdge_MyImage"": [0.5, 0.0],
        ""Intensity_MassDisplacement_MyImage"": [0.0, numpy.nan],
        ""Intensity_LowerQuartileIntensity_MyImage"": [0.25, 0.0],
        ""Intensity_MedianIntensity_MyImage"": [0.25, 0.0],
        ""Intensity_MADIntensity_MyImage"": [0.0, 0.0],
        ""Intensity_UpperQuartileIntensity_MyImage"": [0.5, 0.0],
        ""Location_CenterMassIntensity_X_MyImage"": [5.0, numpy.nan],
        ""Location_CenterMassIntensity_Y_MyImage"": [5.0, numpy.nan],
        ""Location_CenterMassIntensity_Z_MyImage"": [0.0, numpy.nan],
        ""Location_MaxIntensity_X_MyImage"": [5.0, 5.0],
        ""Location_MaxIntensity_Y_MyImage"": [5.0, 0.0],
        ""Location_MaxIntensity_Z_MyImage"": [0.0, 0.0],
    }

    for feature, value in list(expected.items()):
        actual = measurements.get_measurement(OBJECT_NAME, feature)

        numpy.testing.assert_array_almost_equal(
            value,
            actual,
            err_msg=""{} expected {}, got {}"".format(feature, value, actual),
        )",_2868.py,7,i ** 2 + j ** 2 == 0,not i ** 2 + j ** 2
https://github.com/CellProfiler/CellProfiler/tree/master/tests/modules/test_measureobjectintensity.py,"def test_image_masked(measurements, module, objects, image, workspace):
    data = numpy.zeros((11, 20))

    i, j = numpy.mgrid[-5:6, -5:15]
    data[i ** 2 + j ** 2 <= 25] = 0.5
    data[i ** 2 + j ** 2 <= 16] = 0.25
    data[i ** 2 + j ** 2 == 0] = 1

    i, j = numpy.mgrid[-5:6, -15:5]
    data[i ** 2 + j ** 2 <= 9] = 0.5
    data[i ** 2 + j ** 2 <= 4] = 0.75
    data[i ** 2 + j ** 2 == 0] = 1

    mask = numpy.ones_like(data)
    mask[:, 11:] = 0

    image.pixel_data = data
    image.dimensions = 2
    image.mask = mask > 0

    labels = skimage.measure.label(data > 0)

    objects.segmented = labels

    module.run(workspace)

    expected = {
        ""Intensity_IntegratedIntensity_MyImage"": [29.0, 0.0],
        ""Intensity_MeanIntensity_MyImage"": [0.35802469135802467, numpy.nan],
        ""Intensity_StdIntensity_MyImage"": [0.14130275484271107, numpy.nan],
        ""Intensity_MinIntensity_MyImage"": [0.25, 0.0],
        ""Intensity_MaxIntensity_MyImage"": [1.0, 0.0],
        ""Intensity_IntegratedIntensityEdge_MyImage"": [14.0, 0.0],
        ""Intensity_MeanIntensityEdge_MyImage"": [0.5, numpy.nan],
        ""Intensity_StdIntensityEdge_MyImage"": [0.0, numpy.nan],
        ""Intensity_MinIntensityEdge_MyImage"": [0.5, 0.0],
        ""Intensity_MaxIntensityEdge_MyImage"": [0.5, 0.0],
        ""Intensity_MassDisplacement_MyImage"": [0.0, numpy.nan],
        ""Intensity_LowerQuartileIntensity_MyImage"": [0.25, 0.0],
        ""Intensity_MedianIntensity_MyImage"": [0.25, 0.0],
        ""Intensity_MADIntensity_MyImage"": [0.0, 0.0],
        ""Intensity_UpperQuartileIntensity_MyImage"": [0.5, 0.0],
        ""Location_CenterMassIntensity_X_MyImage"": [5.0, numpy.nan],
        ""Location_CenterMassIntensity_Y_MyImage"": [5.0, numpy.nan],
        ""Location_CenterMassIntensity_Z_MyImage"": [0.0, numpy.nan],
        ""Location_MaxIntensity_X_MyImage"": [5.0, 5.0],
        ""Location_MaxIntensity_Y_MyImage"": [5.0, 0.0],
        ""Location_MaxIntensity_Z_MyImage"": [0.0, 0.0],
    }

    for feature, value in list(expected.items()):
        actual = measurements.get_measurement(OBJECT_NAME, feature)

        numpy.testing.assert_array_almost_equal(
            value,
            actual,
            err_msg=""{} expected {}, got {}"".format(feature, value, actual),
        )",_2868.py,12,i ** 2 + j ** 2 == 0,not i ** 2 + j ** 2
https://github.com/napari/napari/tree/master/napari/components/dims.py,"def _focus_down(self):
        """"""Shift focused dimension slider to be the next slider bellow.""""""
        sliders = [d for d in self.not_displayed if self.nsteps[d] > 1]
        if len(sliders) == 0:
            return

        index = (sliders.index(self.last_used) - 1) % len(sliders)
        self.last_used = sliders[index]",_16.py,4,len(sliders) == 0,not len(sliders)
https://github.com/RangiLyu/nanodet/tree/master/tests/test_models/test_loss/test_iou_loss.py,"def test_iou_type_loss_zeros_weight(loss_class):
    pred = torch.rand((10, 4))
    target = torch.rand((10, 4))
    weight = torch.zeros(10)

    with pytest.raises(AssertionError):
        loss_class()(pred, target, reduction_override=""2333"")

    loss = loss_class()(pred, target, weight)
    assert loss == 0.0

    weight = torch.rand(10)
    loss = loss_class()(pred, target, weight)
    assert loss != 0.0",_20.py,10,loss == 0.0,not loss
https://github.com/RangiLyu/nanodet/tree/master/tests/test_models/test_loss/test_iou_loss.py,"def test_iou_type_loss_zeros_weight(loss_class):
    pred = torch.rand((10, 4))
    target = torch.rand((10, 4))
    weight = torch.zeros(10)

    with pytest.raises(AssertionError):
        loss_class()(pred, target, reduction_override=""2333"")

    loss = loss_class()(pred, target, weight)
    assert loss == 0.0

    weight = torch.rand(10)
    loss = loss_class()(pred, target, weight)
    assert loss != 0.0",_20.py,14,loss != 0.0,loss
https://github.com/ansible/galaxy/tree/master/lib/galaxy/datatypes/data.py,"def split(cls, input_datasets, subdir_generator_function, split_params):
        """"""
        Split the input files by line.
        """"""
        if split_params is None:
            return

        if len(input_datasets) > 1:
            raise Exception(""Text file splitting does not support multiple files"")
        input_files = [ds.file_name for ds in input_datasets]

        lines_per_file = None
        chunk_size = None
        if split_params['split_mode'] == 'number_of_parts':
            lines_per_file = []

            # Computing the length is expensive!
            def _file_len(fname):
                with open(fname) as f:
                    return sum(1 for _ in f)
            length = _file_len(input_files[0])
            parts = int(split_params['split_size'])
            if length < parts:
                parts = length
            len_each, remainder = divmod(length, parts)
            while length > 0:
                chunk = len_each
                if remainder > 0:
                    chunk += 1
                lines_per_file.append(chunk)
                remainder -= 1
                length -= chunk
        elif split_params['split_mode'] == 'to_size':
            chunk_size = int(split_params['split_size'])
        else:
            raise Exception(f""Unsupported split mode {split_params['split_mode']}"")

        f = open(input_files[0])
        try:
            chunk_idx = 0
            file_done = False
            part_file = None
            while not file_done:
                if lines_per_file is None:
                    this_chunk_size = chunk_size
                elif chunk_idx < len(lines_per_file):
                    this_chunk_size = lines_per_file[chunk_idx]
                    chunk_idx += 1
                lines_remaining = this_chunk_size
                part_file = None
                while lines_remaining > 0:
                    a_line = f.readline()
                    if a_line == '':
                        file_done = True
                        break
                    if part_file is None:
                        part_dir = subdir_generator_function()
                        part_path = os.path.join(part_dir, os.path.basename(input_files[0]))
                        part_file = open(part_path, 'w')
                    part_file.write(a_line)
                    lines_remaining -= 1
        except Exception as e:
            log.error('Unable to split files: %s', unicodify(e))
            raise
        finally:
            f.close()
            if part_file:
                part_file.close()",_22.py,53,a_line == '',not a_line
https://github.com/archerysec/archerysec/tree/master/staticscanners/views.py,"def get(self, request, uu_id=None):
        jira_url = None
        jira = jirasetting.objects.all()
        for d in jira:
            jira_url = d.jira_server

        all_notify = Notification.objects.unread()
        if uu_id == None:
            scan_id = request.GET[""scan_id""]
            scan_name = request.GET[""scan_name""]
            vuln_data = StaticScanResultsDb.objects.filter(scan_id=scan_id, title=scan_name)
        else:
            try:
                vuln_data = StaticScanResultsDb.objects.filter(scan_id=uu_id)
            except:
                return Response(
                    {""message"": ""Scan Id Doesn't Exist""}, status=status.HTTP_404_NOT_FOUND
                )
        if request.path[: 4] == '/api':
            serialized_data = StaticScanResultsDbSerializer(vuln_data, many=True)
            return Response(serialized_data.data)
        else:
            return render(
                request,
                ""staticscanners/scans/list_vuln_info.html"",
                {""vuln_data"": vuln_data, ""jira_url"": jira_url},
            )",_70.py,8,uu_id == None,not uu_id
https://github.com/dask/dask/tree/master/dask/array/slicing.py,"def _slice_1d(dim_shape, lengths, index):
    """"""Returns a dict of {blocknum: slice}

    This function figures out where each slice should start in each
    block for a single dimension. If the slice won't return any elements
    in the block, that block will not be in the output.

    Parameters
    ----------

    dim_shape - the number of elements in this dimension.
      This should be a positive, non-zero integer
    blocksize - the number of elements per block in this dimension
      This should be a positive, non-zero integer
    index - a description of the elements in this dimension that we want
      This might be an integer, a slice(), or an Ellipsis

    Returns
    -------

    dictionary where the keys are the integer index of the blocks that
      should be sliced and the values are the slices

    Examples
    --------

    Trivial slicing

    >>> _slice_1d(100, [60, 40], slice(None, None, None))
    {0: slice(None, None, None), 1: slice(None, None, None)}

    100 length array cut into length 20 pieces, slice 0:35

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(0, 35))
    {0: slice(None, None, None), 1: slice(0, 15, 1)}

    Support irregular blocks and various slices

    >>> _slice_1d(100, [20, 10, 10, 10, 25, 25], slice(10, 35))
    {0: slice(10, 20, 1), 1: slice(None, None, None), 2: slice(0, 5, 1)}

    Support step sizes

    >>> _slice_1d(100, [15, 14, 13], slice(10, 41, 3))
    {0: slice(10, 15, 3), 1: slice(1, 14, 3), 2: slice(2, 12, 3)}

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(0, 100, 40))  # step > blocksize
    {0: slice(0, 20, 40), 2: slice(0, 20, 40), 4: slice(0, 20, 40)}

    Also support indexing single elements

    >>> _slice_1d(100, [20, 20, 20, 20, 20], 25)
    {1: 5}

    And negative slicing

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(100, 0, -3)) # doctest: +NORMALIZE_WHITESPACE
    {4: slice(-1, -21, -3),
     3: slice(-2, -21, -3),
     2: slice(-3, -21, -3),
     1: slice(-1, -21, -3),
     0: slice(-2, -20, -3)}

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(100, 12, -3)) # doctest: +NORMALIZE_WHITESPACE
    {4: slice(-1, -21, -3),
     3: slice(-2, -21, -3),
     2: slice(-3, -21, -3),
     1: slice(-1, -21, -3),
     0: slice(-2, -8, -3)}

    >>> _slice_1d(100, [20, 20, 20, 20, 20], slice(100, -12, -3))
    {4: slice(-1, -12, -3)}
    """"""
    chunk_boundaries = cached_cumsum(lengths)

    if isinstance(index, Integral):
        # use right-side search to be consistent with previous result
        i = bisect.bisect_right(chunk_boundaries, index)
        if i > 0:
            # the very first chunk has no relative shift
            ind = index - chunk_boundaries[i - 1]
        else:
            ind = index
        return {int(i): int(ind)}

    assert isinstance(index, slice)

    if index == colon:
        return {k: colon for k in range(len(lengths))}

    step = index.step or 1
    if step > 0:
        start = index.start or 0
        stop = index.stop if index.stop is not None else dim_shape
    else:
        start = index.start if index.start is not None else dim_shape - 1
        start = dim_shape - 1 if start >= dim_shape else start
        stop = -(dim_shape + 1) if index.stop is None else index.stop

    # posify start and stop
    if start < 0:
        start += dim_shape
    if stop < 0:
        stop += dim_shape

    d = dict()
    if step > 0:
        istart = bisect.bisect_right(chunk_boundaries, start)
        istop = bisect.bisect_left(chunk_boundaries, stop)

        # the bound is not exactly tight; make it tighter?
        istop = min(istop + 1, len(lengths))

        # jump directly to istart
        if istart > 0:
            start = start - chunk_boundaries[istart - 1]
            stop = stop - chunk_boundaries[istart - 1]

        for i in range(istart, istop):
            length = lengths[i]
            if start < length and stop > 0:
                d[i] = slice(start, min(stop, length), step)
                start = (start - length) % step
            else:
                start = start - length
            stop -= length
    else:
        rstart = start  # running start

        istart = bisect.bisect_left(chunk_boundaries, start)
        istop = bisect.bisect_right(chunk_boundaries, stop)

        # the bound is not exactly tight; make it tighter?
        istart = min(istart + 1, len(chunk_boundaries) - 1)
        istop = max(istop - 1, -1)

        for i in range(istart, istop, -1):
            chunk_stop = chunk_boundaries[i]
            # create a chunk start and stop
            if i == 0:
                chunk_start = 0
            else:
                chunk_start = chunk_boundaries[i - 1]

            # if our slice is in this chunk
            if (chunk_start <= rstart < chunk_stop) and (rstart > stop):
                d[i] = slice(
                    rstart - chunk_stop,
                    max(chunk_start - chunk_stop - 1, stop - chunk_stop),
                    step,
                )

                # compute the next running start point,
                offset = (rstart - (chunk_start - 1)) % step
                rstart = chunk_start + offset - 1

    # replace 0:20:1 with : if appropriate
    for k, v in d.items():
        if v == slice(0, lengths[k], 1):
            d[k] = slice(None, None, None)

    if not d:  # special case x[:0]
        d[0] = slice(0, 0, 1)

    return d",_120.py,140,i == 0,not i
https://github.com/geatpy-dev/geatpy/tree/master/geatpy/templates/soeas/GA/studGA/soea_studGA_templet.py,"def run(self, prophetPop=None):  # prophetPop为先知种群（即包含先验知识的种群）
        # ==========================初始化配置===========================
        population = self.population
        NIND = population.sizes
        self.initialization()  # 初始化算法模板的一些动态参数
        # ===========================准备进化============================
        population.initChrom(NIND)  # 初始化种群染色体矩阵
        self.call_aimFunc(population)  # 计算种群的目标函数值
        # 插入先验知识（注意：这里不会对先知种群prophetPop的合法性进行检查，故应确保prophetPop是一个种群类且拥有合法的Chrom、ObjV、Phen等属性）
        if prophetPop is not None:
            population = (prophetPop + population)[:NIND]  # 插入先知种群
        population.FitnV = ea.scaling(population.ObjV, population.CV, self.problem.maxormins)  # 计算适应度
        # ===========================开始进化============================
        while self.terminated(population) == False:
            bestIdx = np.argmax(population.FitnV, axis=0)  # 得到当代的最优个体的索引, 设置axis=0可使得返回一个向量
            studPop = population[np.tile(bestIdx, (NIND // 2))]  # 复制最优个体NIND//2份，组成一个“种马种群”
            restPop = population[np.where(np.arange(NIND) != bestIdx)[0]]  # 得到除去精英个体外其它个体组成的种群
            # 选择个体，以便后面与种马种群进行交配
            tempPop = restPop[ea.selecting(self.selFunc, restPop.FitnV, (NIND - studPop.sizes))]
            # 将种马种群与选择出来的个体进行合并
            population = studPop + tempPop
            # 进行进化操作
            population.Chrom = self.recOper.do(population.Chrom)  # 重组
            population.Chrom = self.mutOper.do(population.Encoding, population.Chrom, population.Field)  # 变异
            self.call_aimFunc(population)
            population.FitnV = ea.scaling(population.ObjV, population.CV, self.problem.maxormins)  # 计算适应度
        return self.finishing(population)",_183.py,14,self.terminated(population) == False,not self.terminated(population)
https://github.com/saltstack/salt/tree/master/salt/master.py,"def _file_recv(self, load):
        """"""
        Allows minions to send files to the master, files are sent to the
        master file cache
        """"""
        if any(key not in load for key in (""id"", ""path"", ""loc"")):
            return False
        if not isinstance(load[""path""], list):
            return False
        if not self.opts[""file_recv""]:
            return False
        if not salt.utils.verify.valid_id(self.opts, load[""id""]):
            return False
        file_recv_max_size = 1024 * 1024 * self.opts[""file_recv_max_size""]

        if ""loc"" in load and load[""loc""] < 0:
            log.error(""Invalid file pointer: load[loc] < 0"")
            return False

        if len(load[""data""]) + load.get(""loc"", 0) > file_recv_max_size:
            log.error(
                ""file_recv_max_size limit of %d MB exceeded! %s will be ""
                ""truncated. To successfully push this file, adjust ""
                ""file_recv_max_size to an integer (in MB) large enough to ""
                ""accommodate it."",
                file_recv_max_size,
                load[""path""],
            )
            return False
        if ""tok"" not in load:
            log.error(
                ""Received incomplete call from %s for '%s', missing '%s'"",
                load[""id""],
                inspect_stack()[""co_name""],
                ""tok"",
            )
            return False
        if not self.__verify_minion(load[""id""], load[""tok""]):
            # The minion is not who it says it is!
            # We don't want to listen to it!
            log.warning(""Minion id %s is not who it says it is!"", load[""id""])
            return {}
        load.pop(""tok"")

        # Join path
        sep_path = os.sep.join(load[""path""])

        # Path normalization should have been done by the sending
        # minion but we can't guarantee it. Re-do it here.
        normpath = os.path.normpath(sep_path)

        # Ensure that this safety check is done after the path
        # have been normalized.
        if os.path.isabs(normpath) or ""../"" in load[""path""]:
            # Can overwrite master files!!
            return False

        cpath = os.path.join(
            self.opts[""cachedir""], ""minions"", load[""id""], ""files"", normpath
        )
        # One last safety check here
        if not os.path.normpath(cpath).startswith(self.opts[""cachedir""]):
            log.warning(
                ""Attempt to write received file outside of master cache ""
                ""directory! Requested path: %s. Access denied."",
                cpath,
            )
            return False
        cdir = os.path.dirname(cpath)
        if not os.path.isdir(cdir):
            try:
                os.makedirs(cdir)
            except os.error:
                pass
        if os.path.isfile(cpath) and load[""loc""] != 0:
            mode = ""ab""
        else:
            mode = ""wb""
        with salt.utils.files.fopen(cpath, mode) as fp_:
            if load[""loc""]:
                fp_.seek(load[""loc""])

            fp_.write(salt.utils.stringutils.to_bytes(load[""data""]))
        return True",_194.py,75,load['loc'] != 0,load['loc']
https://github.com/taigaio/taiga-back/tree/master/scripts/manage_translations.py,"def lang_stats(resources=None, languages=None):
    """"""
    Output language statistics of committed translation files for each catalog.
    If resources is provided, it should be a list of translation resource to
    limit the output (e.g. ['main', 'taiga']).
    """"""
    locale_dirs = _get_locale_dirs(resources)

    for name, dir_ in locale_dirs:
        print(""\nShowing translations stats for '{res}':"".format(res=name))

        langs = []
        for d in os.listdir(dir_):
            if not d.startswith('_') and os.path.isdir(os.path.join(dir_, d)):
                langs.append(d)
        langs = sorted(langs)

        for lang in langs:
            if languages and lang not in languages:
                continue

            # TODO: merge first with the latest en catalog
            p = Popen(""msgfmt -vc -o /dev/null {path}/{lang}/LC_MESSAGES/django.po"".format(path=dir_, lang=lang),
                      stdout=PIPE, stderr=PIPE, shell=True)
            output, errors = p.communicate()

            if p.returncode == 0:
                # msgfmt output stats on stderr
                print(""{0}: {1}"".format(lang, errors.strip().decode(""utf-8"")))
            else:
                print(""Errors happened when checking {0} translation for {1}:\n{2}"".format(lang, name, errors))",_278.py,27,p.returncode == 0,not p.returncode
https://github.com/LinOTP/LinOTP/tree/master/linotp/lib/reply.py,"def create_img(data, width=0, alt=None, img_id=""challenge_qrcode""):
    """"""
    _create_img - create the qr image data

    :param data: input data that will be munched into the qrcode
    :type  data: string
    :param width: image width in pixel
    :type  width: int

    :return: <img/> taged data
    :rtype:  string
    """"""
    width_str = """"
    alt_str = """"

    img_src = create_img_src(data)

    if width != 0:
        width_str = "" width=%d "" % (int(width))

    if alt is not None:
        val = urllib.parse.urlencode({""alt"": alt})
        alt_str = "" alt=%r "" % (val[len(""alt="") :])

    ret_img = '<img id=""%s"" %s  %s  src=""%s""/>' % (
        img_id,
        alt_str,
        width_str,
        img_src,
    )

    return ret_img",_307.py,18,width != 0,width
https://github.com/sympy/sympy/tree/master/sympy/integrals/rubi/tests/test_utility_function.py,"def test_IntLinearcQ():
    assert IntLinearcQ(1, 2, 3, 4, 5, 6, x) == True
    assert IntLinearcQ(S(1)/100, S(2)/100, S(3)/100, S(4)/100, S(5)/100, S(6)/100, x) == False",_416.py,2,"IntLinearcQ(1, 2, 3, 4, 5, 6, x) == True","IntLinearcQ(1, 2, 3, 4, 5, 6, x)"
https://github.com/sympy/sympy/tree/master/sympy/integrals/rubi/tests/test_utility_function.py,"def test_IntLinearcQ():
    assert IntLinearcQ(1, 2, 3, 4, 5, 6, x) == True
    assert IntLinearcQ(S(1)/100, S(2)/100, S(3)/100, S(4)/100, S(5)/100, S(6)/100, x) == False",_416.py,3,"IntLinearcQ(S(1) / 100, S(2) / 100, S(3) / 100, S(4) / 100, S(5) / 100, S(6) / 100, x) == False","not IntLinearcQ(S(1) / 100, S(2) / 100, S(3) / 100, S(4) / 100, S(5) / 100, S(6) / 100, x)"
https://github.com/facebookresearch/PyTorch-BigGraph/tree/master/torchbiggraph/bucket_scheduling.py,"def _is_initialized(self, bucket: Bucket) -> bool:
        if self.initialized_entities_partitions is None:
            # No initialization is needed
            return True
        if len(self.initialized_entities_partitions) == 0:
            # Initialization is needed but nothing has been initialized yet:
            # it's up to us to inizialize something, and we can choose anything.
            return True
        # Initialization is needed: each embedding table (i.e., an (entity type,
        # partition) pair) must either have already been initialized or be
        # connected to an already-initialized one by a relation type. This is to
        # ensure that the embedding spaces of different partitions are aligned.
        # As here we don't have access to relation types we use an approximation
        # which should work well in all but the most pathological scenarios.
        return all(
            (entity, bucket.lhs) in self.initialized_entities_partitions
            for entity in self.entities_lhs
        ) or all(
            (entity, bucket.rhs) in self.initialized_entities_partitions
            for entity in self.entities_rhs
        )",_474.py,5,len(self.initialized_entities_partitions) == 0,not len(self.initialized_entities_partitions)
https://github.com/pyvista/pyvista/tree/master/tests/test_common.py,"def test_point_data(grid):
    key = 'test_array_points'
    grid[key] = np.arange(grid.n_points)
    assert key in grid.point_data

    orig_value = grid.point_data[key][0]/1.0
    grid.point_data[key][0] += 1
    assert orig_value == grid.point_data[key][0] - 1

    del grid.point_data[key]
    assert key not in grid.point_data

    grid.point_data[key] = np.arange(grid.n_points)
    assert key in grid.point_data

    assert np.allclose(grid[key], np.arange(grid.n_points))

    grid.clear_point_data()
    assert len(grid.point_data.keys()) == 0

    grid.point_data['list'] = np.arange(grid.n_points).tolist()
    assert isinstance(grid.point_data['list'], np.ndarray)
    assert np.allclose(grid.point_data['list'], np.arange(grid.n_points))",_544.py,19,len(grid.point_data.keys()) == 0,not len(grid.point_data.keys())
https://github.com/napalm-automation/napalm/tree/master/napalm/eos/eos.py,"def _load_config(self, filename=None, config=None, replace=True):
        if self.config_session is None:
            self.config_session = ""napalm_{}"".format(datetime.now().microsecond)

        commands = []
        commands.append(""configure session {}"".format(self.config_session))
        if replace:
            commands.append(""rollback clean-config"")

        if filename is not None:
            with open(filename, ""r"") as f:
                lines = f.readlines()
        else:
            if isinstance(config, list):
                lines = config
            else:
                lines = config.splitlines()

        for line in lines:
            line = line.strip()
            if line == """":
                continue
            if line.startswith(""!"") and not line.startswith(""!!""):
                continue
            commands.append(line)

        for start, depth in [
            (s, d) for (s, d) in self.HEREDOC_COMMANDS if s in commands
        ]:
            commands = self._multiline_convert(commands, start=start, depth=depth)

        commands = self._mode_comment_convert(commands)

        try:
            if self.eos_autoComplete is not None:
                self.device.run_commands(
                    commands,
                    autoComplete=self.eos_autoComplete,
                    fn0039_transform=self.fn0039_config,
                )
            else:
                self.device.run_commands(commands, fn0039_transform=self.fn0039_config)
        except pyeapi.eapilib.CommandError as e:
            self.discard_config()
            msg = str(e)
            if replace:
                raise ReplaceConfigException(msg)
            else:
                raise MergeConfigException(msg)",_557.py,21,line == '',not line
https://github.com/geek-ai/MAgent/tree/master/examples/train_single.py,"if __name__ == ""__main__"":
    parser = argparse.ArgumentParser()
    parser.add_argument(""--save_every"", type=int, default=5)
    parser.add_argument(""--render_every"", type=int, default=10)
    parser.add_argument(""--n_round"", type=int, default=2000)
    parser.add_argument(""--render"", action=""store_true"")
    parser.add_argument(""--load_from"", type=int)
    parser.add_argument(""--train"", action=""store_true"")
    parser.add_argument(""--map_size"", type=int, default=125)
    parser.add_argument(""--greedy"", action=""store_true"")
    parser.add_argument(""--name"", type=str, default=""battle"")
    parser.add_argument(""--eval"", action=""store_true"")
    parser.add_argument('--alg', default='dqn', choices=['dqn', 'drqn'])
    args = parser.parse_args()

    # set logger
    log.basicConfig(level=log.INFO, filename=args.name + '.log')
    console = log.StreamHandler()
    console.setLevel(log.INFO)
    log.getLogger('').addHandler(console)

    # init the game
    env = magent.GridWorld(""battle"", map_size=args.map_size)
    env.set_render_dir(""build/render"")

    # two groups of agents
    handles = env.get_handles()
    
    # sample eval observation set
    eval_obs = None
    if args.eval:
        print(""sample eval set..."")
        env.reset()
        generate_map(env, args.map_size, handles)
        eval_obs = magent.utility.sample_observation(env, handles, 2048, 500)[0]

    # init models
    batch_size = 512
    unroll_step = 8
    target_update = 1200
    train_freq = 5

    models = []
    if args.alg == 'dqn':
        from magent.builtin.tf_model import DeepQNetwork
        models.append(DeepQNetwork(env, handles[0], args.name,
                                   batch_size=batch_size,
                                   learning_rate=3e-4,
                                   memory_size=2 ** 21, target_update=target_update,
                                   train_freq=train_freq, eval_obs=eval_obs))
    elif args.alg == 'drqn':
        from magent.builtin.tf_model import DeepRecurrentQNetwork
        models.append(DeepRecurrentQNetwork(env, handles[0], args.name,
                                   learning_rate=3e-4,
                                   batch_size=batch_size/unroll_step, unroll_step=unroll_step,
                                   memory_size=2 * 8 * 625, target_update=target_update,
                                   train_freq=train_freq, eval_obs=eval_obs))
    else:
        # see train_against.py to know how to use a2c
        raise NotImplementedError

    models.append(models[0])

    # load if
    savedir = 'save_model'
    if args.load_from is not None:
        start_from = args.load_from
        print(""load ... %d"" % start_from)
        for model in models:
            model.load(savedir, start_from)
    else:
        start_from = 0

    # print debug info
    print(args)
    print(""view_space"", env.get_view_space(handles[0]))
    print(""feature_space"", env.get_feature_space(handles[0]))

    # play
    start = time.time()
    for k in range(start_from, start_from + args.n_round):
        tic = time.time()
        eps = magent.utility.piecewise_decay(k, [0, 700, 1400], [1, 0.2, 0.05]) if not args.greedy else 0
        loss, num, reward, value = play_a_round(env, args.map_size, handles, models,
                                                train=args.train, print_every=50,
                                                render=args.render or (k+1) % args.render_every == 0,
                                                eps=eps)  # for e-greedy

        log.info(""round %d\t loss: %s\t num: %s\t reward: %s\t value: %s"" % (k, loss, num, reward, value))
        print(""round time %.2f  total time %.2f\n"" % (time.time() - tic, time.time() - start))

        # save models
        if (k + 1) % args.save_every == 0 and args.train:
            print(""save model... "")
            for model in models:
                model.save(savedir, k)",_688.py,93,(k + 1) % args.save_every == 0,not (k + 1) % args.save_every
https://github.com/geek-ai/MAgent/tree/master/examples/train_single.py,"if __name__ == ""__main__"":
    parser = argparse.ArgumentParser()
    parser.add_argument(""--save_every"", type=int, default=5)
    parser.add_argument(""--render_every"", type=int, default=10)
    parser.add_argument(""--n_round"", type=int, default=2000)
    parser.add_argument(""--render"", action=""store_true"")
    parser.add_argument(""--load_from"", type=int)
    parser.add_argument(""--train"", action=""store_true"")
    parser.add_argument(""--map_size"", type=int, default=125)
    parser.add_argument(""--greedy"", action=""store_true"")
    parser.add_argument(""--name"", type=str, default=""battle"")
    parser.add_argument(""--eval"", action=""store_true"")
    parser.add_argument('--alg', default='dqn', choices=['dqn', 'drqn'])
    args = parser.parse_args()

    # set logger
    log.basicConfig(level=log.INFO, filename=args.name + '.log')
    console = log.StreamHandler()
    console.setLevel(log.INFO)
    log.getLogger('').addHandler(console)

    # init the game
    env = magent.GridWorld(""battle"", map_size=args.map_size)
    env.set_render_dir(""build/render"")

    # two groups of agents
    handles = env.get_handles()
    
    # sample eval observation set
    eval_obs = None
    if args.eval:
        print(""sample eval set..."")
        env.reset()
        generate_map(env, args.map_size, handles)
        eval_obs = magent.utility.sample_observation(env, handles, 2048, 500)[0]

    # init models
    batch_size = 512
    unroll_step = 8
    target_update = 1200
    train_freq = 5

    models = []
    if args.alg == 'dqn':
        from magent.builtin.tf_model import DeepQNetwork
        models.append(DeepQNetwork(env, handles[0], args.name,
                                   batch_size=batch_size,
                                   learning_rate=3e-4,
                                   memory_size=2 ** 21, target_update=target_update,
                                   train_freq=train_freq, eval_obs=eval_obs))
    elif args.alg == 'drqn':
        from magent.builtin.tf_model import DeepRecurrentQNetwork
        models.append(DeepRecurrentQNetwork(env, handles[0], args.name,
                                   learning_rate=3e-4,
                                   batch_size=batch_size/unroll_step, unroll_step=unroll_step,
                                   memory_size=2 * 8 * 625, target_update=target_update,
                                   train_freq=train_freq, eval_obs=eval_obs))
    else:
        # see train_against.py to know how to use a2c
        raise NotImplementedError

    models.append(models[0])

    # load if
    savedir = 'save_model'
    if args.load_from is not None:
        start_from = args.load_from
        print(""load ... %d"" % start_from)
        for model in models:
            model.load(savedir, start_from)
    else:
        start_from = 0

    # print debug info
    print(args)
    print(""view_space"", env.get_view_space(handles[0]))
    print(""feature_space"", env.get_feature_space(handles[0]))

    # play
    start = time.time()
    for k in range(start_from, start_from + args.n_round):
        tic = time.time()
        eps = magent.utility.piecewise_decay(k, [0, 700, 1400], [1, 0.2, 0.05]) if not args.greedy else 0
        loss, num, reward, value = play_a_round(env, args.map_size, handles, models,
                                                train=args.train, print_every=50,
                                                render=args.render or (k+1) % args.render_every == 0,
                                                eps=eps)  # for e-greedy

        log.info(""round %d\t loss: %s\t num: %s\t reward: %s\t value: %s"" % (k, loss, num, reward, value))
        print(""round time %.2f  total time %.2f\n"" % (time.time() - tic, time.time() - start))

        # save models
        if (k + 1) % args.save_every == 0 and args.train:
            print(""save model... "")
            for model in models:
                model.save(savedir, k)",_688.py,86,(k + 1) % args.render_every == 0,not (k + 1) % args.render_every
https://github.com/rasterio/rasterio/tree/master/tests/test_rio_main.py,"def test_version(runner):
    result = runner.invoke(main_group, ['--version'])
    assert result.exit_code == 0
    assert parse(result.output.strip())",_697.py,3,result.exit_code == 0,not result.exit_code
https://github.com/Scifabric/pybossa/tree/master/test/test_repository/test_projectstats_repository.py,"def test_filter_by_no_matches(self):
        """"""Test filter_by returns an empty list if no stats match the query""""""
        ps = self.prepare_stats()
        retrieved_ps = self.projectstats_repo.filter_by(n_tasks=100)
        assert isinstance(retrieved_ps, list)
        assert len(retrieved_ps) == 0, retrieved_ps",_701.py,6,len(retrieved_ps) == 0,not len(retrieved_ps)
https://github.com/BachiLi/diffvg/tree/master/apps/style_transfer.py,"def main(args):
    pydiffvg.set_use_gpu(torch.cuda.is_available())

    canvas_width, canvas_height, shapes, shape_groups = pydiffvg.svg_to_scene(args.content_file)
    scene_args = pydiffvg.RenderFunction.serialize_scene(\
        canvas_width, canvas_height, shapes, shape_groups)
    render = pydiffvg.RenderFunction.apply
    img = render(canvas_width, # width
                 canvas_height, # height
                 2,   # num_samples_x
                 2,   # num_samples_y
                 0,   # seed
                 None,
                 *scene_args)
    # Transform to gamma space
    pydiffvg.imwrite(img.cpu(), 'results/style_transfer/init.png', gamma=1.0)
    # HWC -> NCHW
    img = img.unsqueeze(0)
    img = img.permute(0, 3, 1, 2) # NHWC -> NCHW

    loader = transforms.Compose([
        transforms.ToTensor()])  # transform it into a torch tensor

    def image_loader(image_name):
        image = Image.open(image_name)
        # fake batch dimension required to fit network's input dimensions
        image = loader(image).unsqueeze(0)
        return image.to(pydiffvg.get_device(), torch.float)

    style_img = image_loader(args.style_img)
    # alpha blend content with a gray background
    content_img = img[:, :3, :, :] * img[:, 3, :, :] + \
                  0.5 * torch.ones([1, 3, img.shape[2], img.shape[3]]) * \
                  (1 - img[:, 3, :, :])

    assert style_img.size() == content_img.size(), \
        ""we need to import style and content images of the same size""

    unloader = transforms.ToPILImage()  # reconvert into PIL image

    class ContentLoss(nn.Module):
        def __init__(self, target,):
            super(ContentLoss, self).__init__()
            # we 'detach' the target content from the tree used
            # to dynamically compute the gradient: this is a stated value,
            # not a variable. Otherwise the forward method of the criterion
            # will throw an error.
            self.target = target.detach()

        def forward(self, input):
            self.loss = F.mse_loss(input, self.target)
            return input

    def gram_matrix(input):
        a, b, c, d = input.size()  # a=batch size(=1)
        # b=number of feature maps
        # (c,d)=dimensions of a f. map (N=c*d)

        features = input.view(a * b, c * d)  # resise F_XL into \hat F_XL

        G = torch.mm(features, features.t())  # compute the gram product

        # we 'normalize' the values of the gram matrix
        # by dividing by the number of element in each feature maps.
        return G.div(a * b * c * d)

    class StyleLoss(nn.Module):

        def __init__(self, target_feature):
            super(StyleLoss, self).__init__()
            self.target = gram_matrix(target_feature).detach()

        def forward(self, input):
            G = gram_matrix(input)
            self.loss = F.mse_loss(G, self.target)
            return input

    device = pydiffvg.get_device()
    cnn = models.vgg19(pretrained=True).features.to(device).eval()

    cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)
    cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)

    # create a module to normalize input image so we can easily put it in a
    # nn.Sequential
    class Normalization(nn.Module):
        def __init__(self, mean, std):
            super(Normalization, self).__init__()
            # .view the mean and std to make them [C x 1 x 1] so that they can
            # directly work with image Tensor of shape [B x C x H x W].
            # B is batch size. C is number of channels. H is height and W is width.
            self.mean = mean.clone().view(-1, 1, 1)
            self.std = std.clone().view(-1, 1, 1)

        def forward(self, img):
            # normalize img
            return (img - self.mean) / self.std

    # desired depth layers to compute style/content losses :
    content_layers_default = ['conv_4']
    style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']

    def get_style_model_and_losses(cnn, normalization_mean, normalization_std,
                                   style_img, content_img,
                                   content_layers=content_layers_default,
                                   style_layers=style_layers_default):
        cnn = copy.deepcopy(cnn)

        # normalization module
        normalization = Normalization(normalization_mean, normalization_std).to(device)

        # just in order to have an iterable access to or list of content/syle
        # losses
        content_losses = []
        style_losses = []

        # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential
        # to put in modules that are supposed to be activated sequentially
        model = nn.Sequential(normalization)

        i = 0  # increment every time we see a conv
        for layer in cnn.children():
            if isinstance(layer, nn.Conv2d):
                i += 1
                name = 'conv_{}'.format(i)
            elif isinstance(layer, nn.ReLU):
                name = 'relu_{}'.format(i)
                # The in-place version doesn't play very nicely with the ContentLoss
                # and StyleLoss we insert below. So we replace with out-of-place
                # ones here.
                layer = nn.ReLU(inplace=False)
            elif isinstance(layer, nn.MaxPool2d):
                name = 'pool_{}'.format(i)
            elif isinstance(layer, nn.BatchNorm2d):
                name = 'bn_{}'.format(i)
            else:
                raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))

            model.add_module(name, layer)

            if name in content_layers:
                # add content loss:
                target = model(content_img).detach()
                content_loss = ContentLoss(target)
                model.add_module(""content_loss_{}"".format(i), content_loss)
                content_losses.append(content_loss)

            if name in style_layers:
                # add style loss:
                target_feature = model(style_img).detach()
                style_loss = StyleLoss(target_feature)
                model.add_module(""style_loss_{}"".format(i), style_loss)
                style_losses.append(style_loss)

        # now we trim off the layers after the last content and style losses
        for i in range(len(model) - 1, -1, -1):
            if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):
                break

        model = model[:(i + 1)]

        return model, style_losses, content_losses

    def run_style_transfer(cnn, normalization_mean, normalization_std,
                           content_img, style_img,
                           canvas_width, canvas_height,
                           shapes, shape_groups,
                           num_steps=500, style_weight=5000, content_weight=1):
        """"""Run the style transfer.""""""
        print('Building the style transfer model..')
        model, style_losses, content_losses = get_style_model_and_losses(cnn,
            normalization_mean, normalization_std, style_img, content_img)
        point_params = []
        color_params = []
        stroke_width_params = []
        for shape in shapes:
            if isinstance(shape, pydiffvg.Path):
                point_params.append(shape.points.requires_grad_())
                stroke_width_params.append(shape.stroke_width.requires_grad_())
        for shape_group in shape_groups:
            if isinstance(shape_group.fill_color, torch.Tensor):
                color_params.append(shape_group.fill_color.requires_grad_())
            elif isinstance(shape_group.fill_color, pydiffvg.LinearGradient):
                point_params.append(shape_group.fill_color.begin.requires_grad_())
                point_params.append(shape_group.fill_color.end.requires_grad_())
                color_params.append(shape_group.fill_color.stop_colors.requires_grad_())
            if isinstance(shape_group.stroke_color, torch.Tensor):
                color_params.append(shape_group.stroke_color.requires_grad_())
            elif isinstance(shape_group.stroke_color, pydiffvg.LinearGradient):
                point_params.append(shape_group.stroke_color.begin.requires_grad_())
                point_params.append(shape_group.stroke_color.end.requires_grad_())
                color_params.append(shape_group.stroke_color.stop_colors.requires_grad_())

        point_optimizer = optim.Adam(point_params, lr=1.0)
        color_optimizer = optim.Adam(color_params, lr=0.01)
        stroke_width_optimizers = optim.Adam(stroke_width_params, lr=0.1)
        print('Optimizing..')
        run = [0]
        while run[0] <= num_steps:
            point_optimizer.zero_grad()
            color_optimizer.zero_grad()
            stroke_width_optimizers.zero_grad()

            scene_args = pydiffvg.RenderFunction.serialize_scene(\
                canvas_width, canvas_height, shapes, shape_groups)
            render = pydiffvg.RenderFunction.apply
            img = render(canvas_width, # width
                         canvas_height, # height
                         2,   # num_samples_x
                         2,   # num_samples_y
                         0,   # seed
                         None,
                         *scene_args)
            # alpha blend img with a gray background
            img = img[:, :, :3] * img[:, :, 3:4] + \
                  0.5 * torch.ones([img.shape[0], img.shape[1], 3]) * \
                  (1 - img[:, :, 3:4])

            pydiffvg.imwrite(img.cpu(),
                             'results/style_transfer/step_{}.png'.format(run[0]),
                             gamma=1.0)

            # HWC to NCHW
            img = img.permute([2, 0, 1]).unsqueeze(0)
            model(img)
            style_score = 0
            content_score = 0

            for sl in style_losses:
                style_score += sl.loss
            for cl in content_losses:
                content_score += cl.loss

            style_score *= style_weight
            content_score *= content_weight

            loss = style_score + content_score
            loss.backward()

            run[0] += 1
            if run[0] % 1 == 0:
                print(""run {}:"".format(run))
                print('Style Loss : {:4f} Content Loss: {:4f}'.format(
                    style_score.item(), content_score.item()))
                print()

            point_optimizer.step()
            color_optimizer.step()
            stroke_width_optimizers.step()

            for color in color_params:
                color.data.clamp_(0, 1)
            for w in stroke_width_params:
                w.data.clamp_(0.5, 4.0)

        return shapes, shape_groups

    shapes, shape_groups = run_style_transfer(\
        cnn, cnn_normalization_mean, cnn_normalization_std,
        content_img, style_img,
        canvas_width, canvas_height, shapes, shape_groups)

    scene_args = pydiffvg.RenderFunction.serialize_scene(shapes, shape_groups)
    render = pydiffvg.RenderFunction.apply
    img = render(canvas_width, # width
                 canvas_height, # height
                 2,   # num_samples_x
                 2,   # num_samples_y
                 0,   # seed
                 None,
                 *scene_args)
    # Transform to gamma space
    pydiffvg.imwrite(img.cpu(), 'results/style_transfer/output.png', gamma=1.0)",_705.py,241,run[0] % 1 == 0,not run[0] % 1
https://github.com/BachiLi/diffvg/tree/master/apps/style_transfer.py,"def main(args):
    pydiffvg.set_use_gpu(torch.cuda.is_available())

    canvas_width, canvas_height, shapes, shape_groups = pydiffvg.svg_to_scene(args.content_file)
    scene_args = pydiffvg.RenderFunction.serialize_scene(\
        canvas_width, canvas_height, shapes, shape_groups)
    render = pydiffvg.RenderFunction.apply
    img = render(canvas_width, # width
                 canvas_height, # height
                 2,   # num_samples_x
                 2,   # num_samples_y
                 0,   # seed
                 None,
                 *scene_args)
    # Transform to gamma space
    pydiffvg.imwrite(img.cpu(), 'results/style_transfer/init.png', gamma=1.0)
    # HWC -> NCHW
    img = img.unsqueeze(0)
    img = img.permute(0, 3, 1, 2) # NHWC -> NCHW

    loader = transforms.Compose([
        transforms.ToTensor()])  # transform it into a torch tensor

    def image_loader(image_name):
        image = Image.open(image_name)
        # fake batch dimension required to fit network's input dimensions
        image = loader(image).unsqueeze(0)
        return image.to(pydiffvg.get_device(), torch.float)

    style_img = image_loader(args.style_img)
    # alpha blend content with a gray background
    content_img = img[:, :3, :, :] * img[:, 3, :, :] + \
                  0.5 * torch.ones([1, 3, img.shape[2], img.shape[3]]) * \
                  (1 - img[:, 3, :, :])

    assert style_img.size() == content_img.size(), \
        ""we need to import style and content images of the same size""

    unloader = transforms.ToPILImage()  # reconvert into PIL image

    class ContentLoss(nn.Module):
        def __init__(self, target,):
            super(ContentLoss, self).__init__()
            # we 'detach' the target content from the tree used
            # to dynamically compute the gradient: this is a stated value,
            # not a variable. Otherwise the forward method of the criterion
            # will throw an error.
            self.target = target.detach()

        def forward(self, input):
            self.loss = F.mse_loss(input, self.target)
            return input

    def gram_matrix(input):
        a, b, c, d = input.size()  # a=batch size(=1)
        # b=number of feature maps
        # (c,d)=dimensions of a f. map (N=c*d)

        features = input.view(a * b, c * d)  # resise F_XL into \hat F_XL

        G = torch.mm(features, features.t())  # compute the gram product

        # we 'normalize' the values of the gram matrix
        # by dividing by the number of element in each feature maps.
        return G.div(a * b * c * d)

    class StyleLoss(nn.Module):

        def __init__(self, target_feature):
            super(StyleLoss, self).__init__()
            self.target = gram_matrix(target_feature).detach()

        def forward(self, input):
            G = gram_matrix(input)
            self.loss = F.mse_loss(G, self.target)
            return input

    device = pydiffvg.get_device()
    cnn = models.vgg19(pretrained=True).features.to(device).eval()

    cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)
    cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)

    # create a module to normalize input image so we can easily put it in a
    # nn.Sequential
    class Normalization(nn.Module):
        def __init__(self, mean, std):
            super(Normalization, self).__init__()
            # .view the mean and std to make them [C x 1 x 1] so that they can
            # directly work with image Tensor of shape [B x C x H x W].
            # B is batch size. C is number of channels. H is height and W is width.
            self.mean = mean.clone().view(-1, 1, 1)
            self.std = std.clone().view(-1, 1, 1)

        def forward(self, img):
            # normalize img
            return (img - self.mean) / self.std

    # desired depth layers to compute style/content losses :
    content_layers_default = ['conv_4']
    style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']

    def get_style_model_and_losses(cnn, normalization_mean, normalization_std,
                                   style_img, content_img,
                                   content_layers=content_layers_default,
                                   style_layers=style_layers_default):
        cnn = copy.deepcopy(cnn)

        # normalization module
        normalization = Normalization(normalization_mean, normalization_std).to(device)

        # just in order to have an iterable access to or list of content/syle
        # losses
        content_losses = []
        style_losses = []

        # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential
        # to put in modules that are supposed to be activated sequentially
        model = nn.Sequential(normalization)

        i = 0  # increment every time we see a conv
        for layer in cnn.children():
            if isinstance(layer, nn.Conv2d):
                i += 1
                name = 'conv_{}'.format(i)
            elif isinstance(layer, nn.ReLU):
                name = 'relu_{}'.format(i)
                # The in-place version doesn't play very nicely with the ContentLoss
                # and StyleLoss we insert below. So we replace with out-of-place
                # ones here.
                layer = nn.ReLU(inplace=False)
            elif isinstance(layer, nn.MaxPool2d):
                name = 'pool_{}'.format(i)
            elif isinstance(layer, nn.BatchNorm2d):
                name = 'bn_{}'.format(i)
            else:
                raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))

            model.add_module(name, layer)

            if name in content_layers:
                # add content loss:
                target = model(content_img).detach()
                content_loss = ContentLoss(target)
                model.add_module(""content_loss_{}"".format(i), content_loss)
                content_losses.append(content_loss)

            if name in style_layers:
                # add style loss:
                target_feature = model(style_img).detach()
                style_loss = StyleLoss(target_feature)
                model.add_module(""style_loss_{}"".format(i), style_loss)
                style_losses.append(style_loss)

        # now we trim off the layers after the last content and style losses
        for i in range(len(model) - 1, -1, -1):
            if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):
                break

        model = model[:(i + 1)]

        return model, style_losses, content_losses

    def run_style_transfer(cnn, normalization_mean, normalization_std,
                           content_img, style_img,
                           canvas_width, canvas_height,
                           shapes, shape_groups,
                           num_steps=500, style_weight=5000, content_weight=1):
        """"""Run the style transfer.""""""
        print('Building the style transfer model..')
        model, style_losses, content_losses = get_style_model_and_losses(cnn,
            normalization_mean, normalization_std, style_img, content_img)
        point_params = []
        color_params = []
        stroke_width_params = []
        for shape in shapes:
            if isinstance(shape, pydiffvg.Path):
                point_params.append(shape.points.requires_grad_())
                stroke_width_params.append(shape.stroke_width.requires_grad_())
        for shape_group in shape_groups:
            if isinstance(shape_group.fill_color, torch.Tensor):
                color_params.append(shape_group.fill_color.requires_grad_())
            elif isinstance(shape_group.fill_color, pydiffvg.LinearGradient):
                point_params.append(shape_group.fill_color.begin.requires_grad_())
                point_params.append(shape_group.fill_color.end.requires_grad_())
                color_params.append(shape_group.fill_color.stop_colors.requires_grad_())
            if isinstance(shape_group.stroke_color, torch.Tensor):
                color_params.append(shape_group.stroke_color.requires_grad_())
            elif isinstance(shape_group.stroke_color, pydiffvg.LinearGradient):
                point_params.append(shape_group.stroke_color.begin.requires_grad_())
                point_params.append(shape_group.stroke_color.end.requires_grad_())
                color_params.append(shape_group.stroke_color.stop_colors.requires_grad_())

        point_optimizer = optim.Adam(point_params, lr=1.0)
        color_optimizer = optim.Adam(color_params, lr=0.01)
        stroke_width_optimizers = optim.Adam(stroke_width_params, lr=0.1)
        print('Optimizing..')
        run = [0]
        while run[0] <= num_steps:
            point_optimizer.zero_grad()
            color_optimizer.zero_grad()
            stroke_width_optimizers.zero_grad()

            scene_args = pydiffvg.RenderFunction.serialize_scene(\
                canvas_width, canvas_height, shapes, shape_groups)
            render = pydiffvg.RenderFunction.apply
            img = render(canvas_width, # width
                         canvas_height, # height
                         2,   # num_samples_x
                         2,   # num_samples_y
                         0,   # seed
                         None,
                         *scene_args)
            # alpha blend img with a gray background
            img = img[:, :, :3] * img[:, :, 3:4] + \
                  0.5 * torch.ones([img.shape[0], img.shape[1], 3]) * \
                  (1 - img[:, :, 3:4])

            pydiffvg.imwrite(img.cpu(),
                             'results/style_transfer/step_{}.png'.format(run[0]),
                             gamma=1.0)

            # HWC to NCHW
            img = img.permute([2, 0, 1]).unsqueeze(0)
            model(img)
            style_score = 0
            content_score = 0

            for sl in style_losses:
                style_score += sl.loss
            for cl in content_losses:
                content_score += cl.loss

            style_score *= style_weight
            content_score *= content_weight

            loss = style_score + content_score
            loss.backward()

            run[0] += 1
            if run[0] % 1 == 0:
                print(""run {}:"".format(run))
                print('Style Loss : {:4f} Content Loss: {:4f}'.format(
                    style_score.item(), content_score.item()))
                print()

            point_optimizer.step()
            color_optimizer.step()
            stroke_width_optimizers.step()

            for color in color_params:
                color.data.clamp_(0, 1)
            for w in stroke_width_params:
                w.data.clamp_(0.5, 4.0)

        return shapes, shape_groups

    shapes, shape_groups = run_style_transfer(\
        cnn, cnn_normalization_mean, cnn_normalization_std,
        content_img, style_img,
        canvas_width, canvas_height, shapes, shape_groups)

    scene_args = pydiffvg.RenderFunction.serialize_scene(shapes, shape_groups)
    render = pydiffvg.RenderFunction.apply
    img = render(canvas_width, # width
                 canvas_height, # height
                 2,   # num_samples_x
                 2,   # num_samples_y
                 0,   # seed
                 None,
                 *scene_args)
    # Transform to gamma space
    pydiffvg.imwrite(img.cpu(), 'results/style_transfer/output.png', gamma=1.0)",_705.py,241,run[0] % 1 == 0,not run[0] % 1
https://github.com/jazzband/django-admin2/tree/master/djadmin2/templatetags/admin2_tags.py,"def for_admin(permissions, admin):
    """"""
    Only useful in the permission handling. This filter binds a new admin to
    the permission handler to allow checking views of an arbitrary admin.
    """"""
    # some permission check has failed earlier, so we don't bother trying to
    # bind a new admin to it.
    if permissions == '':
        return permissions
    return permissions.bind_admin(admin)",_731.py,8,permissions == '',not permissions
https://github.com/r9y9/deepvoice3_pytorch/tree/master/vctk_preprocess/extract_feats.py,"def extract_intermediate_features(wav_path, txt_path, keep_silences=False,
                                  full_features=False, ehmm_max_n_itr=1):
    basedir = os.getcwd()
    latest_feature_dir = ""latest_features""
    if not os.path.exists(latest_feature_dir):
        os.mkdir(latest_feature_dir)

    os.chdir(latest_feature_dir)
    latest_feature_dir = os.getcwd()

    if not os.path.exists(""merlin""):
        clone_cmd = ""git clone https://github.com/kastnerkyle/merlin""
        pe(clone_cmd, shell=True)

    if keep_silences:
        # REMOVE SILENCES TO MATCH JOSE PREPROC
        os.chdir(""merlin/src"")
        pe(""sed -i.bak -e '708,712d;' run_merlin.py"", shell=True)
        pe(""sed -i.bak -e '695,706d;' run_merlin.py"", shell=True)
        os.chdir(latest_feature_dir)

    os.chdir(""merlin"")
    merlin_dir = os.getcwd()
    os.chdir(""egs/build_your_own_voice/s1"")
    experiment_dir = os.getcwd()

    if not os.path.exists(""database""):
        print(""Creating database and copying in files"")
        pe(""bash -x 01_setup.sh my_new_voice 2>&1"", shell=True)

        # Copy in wav files
        wav_partial_path = wav_path  # vctkdir + ""wav48/""
        """"""
        subfolders = sorted(os.listdir(wav_partial_path))
        # only p294 for now...
        subfolders = subfolder_select(subfolders)
        os.chdir(""database/wav"")
        for sf in subfolders:
            wav_path = wav_partial_path + sf + ""/*.wav""
            pe(""cp %s ."" % wav_path, shell=True)
        """"""
        to_copy = os.listdir(wav_partial_path)
        if len([tc for tc in to_copy if tc[-4:] == "".wav""]) == 0:
            raise IOError(
                ""Unable to find any wav files in %s, make sure the filenames end in .wav!"" % wav_partial_path)
        os.chdir(""database/wav"")
        if wav_partial_path[-1] != ""/"":
            wav_partial_path = wav_partial_path + ""/""
        wav_match_path = wav_partial_path + ""*.wav""
        for fi in glob.glob(wav_match_path):
            pe(""echo %s; cp %s ."" % (fi, fi), shell=True)
        # THIS MAY FAIL IF TOO MANY WAV FILES
        # pe(""cp %s ."" % wav_match_path, shell=True)
        for f in os.listdir("".""):
            # This is only necessary because of corrupted files...
            fs, d = wavfile.read(f)
            wavfile.write(f, fs, d)

        # downsample the files
        get_sr_cmd = 'file `ls *.wav | head -n 1` | cut -d "" "" -f 12'
        sr = pe(get_sr_cmd, shell=True)
        sr_int = int(sr[0].strip())
        print(""Got samplerate {}, converting to 16000"".format(sr_int))
        # was assuming all were 48000
        convert = estdir + \
            ""bin/ch_wave $i -o tmp_$i -itype wav -otype wav -F 16000 -f {}"".format(sr_int)
        pe(""for i in *.wav; do echo %s; %s; mv tmp_$i $i; done"" % (convert, convert), shell=True)

        os.chdir(experiment_dir)
        txt_partial_path = txt_path  # vctkdir + ""txt/""
        """"""
        subfolders = sorted(os.listdir(txt_partial_path))
        # only p294 for now...
        subfolders = subfolder_select(subfolders)
        os.chdir(""database/txt"")
        for sf in subfolders:
            txt_path = txt_partial_path + sf + ""/*.txt""
            pe(""cp %s ."" % txt_path, shell=True)
        """"""
        os.chdir(""database/txt"")
        to_copy = os.listdir(txt_partial_path)
        if len([tc for tc in to_copy if tc[-4:] == "".txt""]) == 0:
            raise IOError(
                ""Unable to find any txt files in %s. Be sure the filenames end in .txt!"" % txt_partial_path)
        txt_match_path = txt_partial_path + ""/*.txt""
        for fi in glob.glob(txt_match_path):
            # escape string...
            fi = re.escape(fi)
            try:
                pe(""echo %s; cp %s ."" % (fi, fi), shell=True)
            except:
                from IPython import embed
                embed()
                raise ValueError()

        #pe(""cp %s ."" % txt_match_path, shell=True)

    do_state_align = False
    if do_state_align:
        raise ValueError(""Replace these lies with something that points at the right place"")
        os.chdir(merlin_dir)
        os.chdir(""misc/scripts/alignment/state_align"")
        pe(""bash -x setup.sh 2>&1"", shell=True)

        with open(""config.cfg"", ""r"") as f:
            config_lines = f.readlines()

        # replace FESTDIR with the correct path
        festdir_replace_line = None
        for n, l in enumerate(config_lines):
            if ""FESTDIR="" in l:
                festdir_replace_line = n
                break

        config_lines[festdir_replace_line] = ""FESTDIR=%s\n"" % festdir

        # replace HTKDIR with the correct path
        htkdir_replace_line = None
        for n, l in enumerate(config_lines):
            if ""HTKDIR="" in l:
                htkdir_replace_line = n
                break

        config_lines[htkdir_replace_line] = ""HTKDIR=%s\n"" % htkdir

        with open(""config.cfg"", ""w"") as f:
            f.writelines(config_lines)

        pe(""bash -x run_aligner.sh config.cfg 2>&1"", shell=True)
    else:
        os.chdir(merlin_dir)
        if not os.path.exists(""misc/scripts/alignment/phone_align/full-context-labels/full""):
            os.chdir(""misc/scripts/alignment/phone_align"")
            pe(""bash -x setup.sh 2>&1"", shell=True)

            with open(""config.cfg"", ""r"") as f:
                config_lines = f.readlines()

            # replace ESTDIR with the correct path
            estdir_replace_line = None
            for n, l in enumerate(config_lines):
                if ""ESTDIR="" in l and l[0] == ""E"":
                    estdir_replace_line = n
                    break

            config_lines[estdir_replace_line] = ""ESTDIR=%s\n"" % estdir

            # replace FESTDIR with the correct path
            festdir_replace_line = None
            for n, l in enumerate(config_lines):
                # EST/FEST
                if ""FESTDIR="" in l and l[0] == ""F"":
                    festdir_replace_line = n
                    break

            config_lines[festdir_replace_line] = ""FESTDIR=%s\n"" % festdir

            # replace FESTVOXDIR with the correct path
            festvoxdir_replace_line = None
            for n, l in enumerate(config_lines):
                if ""FESTVOXDIR="" in l:
                    festvoxdir_replace_line = n
                    break

            config_lines[festvoxdir_replace_line] = ""FESTVOXDIR=%s\n"" % festvoxdir

            with open(""config.cfg"", ""w"") as f:
                f.writelines(config_lines)

            with open(""run_aligner.sh"", ""r"") as f:
                run_aligner_lines = f.readlines()

            replace_line = None
            for n, l in enumerate(run_aligner_lines):
                if ""cp ../cmuarctic.data"" in l:
                    replace_line = n
                    break

            run_aligner_lines[replace_line] = ""cp ../txt.done.data etc/txt.done.data\n""

            # Make the txt.done.data file
            def format_info_tup(info_tup):
                return ""( "" + str(info_tup[0]) + ' ""' + info_tup[1] + '"" )\n'

            # Now we need to get the text info
            txt_partial_path = txt_path  # vctkdir + ""txt/""
            cwd = os.getcwd()
            out_path = ""txt.done.data""
            out_file = open(out_path, ""w"")
            """"""
            subfolders = sorted(os.listdir(txt_partial_path))
            # TODO: Avoid this truncation and have an option to select subfolder(s)...
            subfolders = subfolder_select(subfolders)

            txt_ids = []
            for sf in subfolders:
                print(""Processing subfolder %s"" % sf)
                txt_sf_path = txt_partial_path + sf + ""/""
                for txtpath in os.listdir(txt_sf_path):
                    full_txtpath = txt_sf_path + txtpath
                    with open(full_txtpath, 'r') as f:
                        r = f.readlines()
                        assert len(r) == 1
                        # remove txt extension
                        name = txtpath.split(""."")[0]
                        text = r[0].strip()
                        info_tup = (name, text)
                        txt_ids.append(name)
                        out_file.writelines(format_info_tup(info_tup))
            """"""
            txt_ids = []
            txt_l_path = txt_partial_path
            for txtpath in os.listdir(txt_l_path):
                print(""Processing %s"" % txtpath)
                full_txtpath = txt_l_path + txtpath
                name = txtpath.split(""."")[0]
                wavpath_matches = [fname.split(""."")[0] for fname in os.listdir(wav_partial_path)
                                   if name in fname]
                for name in wavpath_matches:
                    # Need an extra level here for pavoque :/
                    with open(full_txtpath, 'r') as f:
                        r = f.readlines()
                    if len(r) == 0:
                        continue
                    if len(r) != 1:
                        new_r = []
                        for ri in r:
                            if ri != ""\n"":
                                new_r.append(ri)
                        r = new_r
                    if len(r) != 1:
                        print(""Something wrong in text extraction, cowardly bailing to IPython"")
                        from IPython import embed
                        embed()
                        raise ValueError()
                    assert len(r) == 1
                    # remove txt extension
                    text = r[0].strip()
                    info_tup = (name, text)
                    txt_ids.append(name)
                    out_file.writelines(format_info_tup(info_tup))
            out_file.close()
            pe(""cp %s %s/txt.done.data"" % (out_path, latest_feature_dir),
               shell=True)
            os.chdir(cwd)

            replace_line = None
            for n, l in enumerate(run_aligner_lines):
                if ""cp ../slt_wav/*.wav"" in l:
                    replace_line = n
                    break

            run_aligner_lines[replace_line] = ""cp ../wav/*.wav wav\n""

            # Put wav file in the correct place
            wav_partial_path = experiment_dir + ""/database/wav""
            """"""
            subfolders = sorted(os.listdir(wav_partial_path))
            """"""
            if not os.path.exists(""wav""):
                os.mkdir(""wav"")
            cwd = os.getcwd()
            os.chdir(""wav"")
            """"""
            for sf in subfolders:
                wav_path = wav_partial_path + ""/*.wav""
                pe(""cp %s ."" % wav_path, shell=True)
            """"""
            wav_match_path = wav_partial_path + ""/*.wav""
            for fi in glob.glob(wav_match_path):
                fi = re.escape(fi)
                try:
                    pe(""echo %s; cp %s ."" % (fi, fi), shell=True)
                except:
                    from IPython import embed
                    embed()
                    raise ValueError()
                #pe(""echo %s; cp %s ."" % (fi, fi), shell=True)
            #pe(""cp %s ."" % wav_match_path, shell=True)
            os.chdir(cwd)

            replace_line = None
            for n, l in enumerate(run_aligner_lines):
                if ""cat cmuarctic.data |"" in l:
                    replace_line = n
                    break

            run_aligner_lines[replace_line] = 'cat txt.done.data | cut -d "" "" -f 2 > file_id_list.scp\n'

            # FIXME
            # Hackaround to avoid harcoded 30 in festivox do_ehmm
            if not full_features:
                bdir = os.getcwd()

                # need to hack up run_aligner more..
                # do setup manually
                pe(""mkdir cmu_us_slt_arctic"", shell=True)
                os.chdir(""cmu_us_slt_arctic"")

                pe(""%s/src/clustergen/setup_cg cmu us slt_arctic"" % festvoxdir, shell=True)

                pe(""cp ../txt.done.data etc/txt.done.data"", shell=True)
                wmp = ""../wav/*.wav""
                for fi in glob.glob(wmp):
                    fi = re.escape(fi)
                    try:
                        pe(""echo %s; cp %s wav/"" % (fi, fi), shell=True)
                    except:
                        from IPython import embed
                        embed()
                        raise ValueError()
                    #pe(""echo %s; cp %s wav/"" % (fi, fi), shell=True)
                #pe(""cp ../wav/*.wav wav/"", shell=True)

                # remove top part but keep cd call
                run_aligner_lines = run_aligner_lines[:13] + \
                    [""cd cmu_us_slt_arctic\n""] + run_aligner_lines[35:]

                '''
                # need to change do_build
                # NO LONGER NECESSARY DUE TO FESTIVAL DEPENDENCE ON FILENAME

                os.chdir(""bin"")
                with open(""do_build"", ""r"") as f:
                    do_build_lines = f.readlines()

                replace_line = None
                for n, l in enumerate(do_build_lines):
                    if ""$FESTVOXDIR/src/ehmm/bin/do_ehmm"" in l:
                        replace_line = n
                        break

                do_build_lines[replace_line] = ""   $FESTVOXDIR/src/ehmm/bin/do_ehmm\n""

                # FIXME Why does this hang when not overwritten???
                with open(""edit_do_build"", ""w"") as f:
                    f.writelines(do_build_lines)
                '''

                # need to change do_ehmm
                os.chdir(festvoxdir)
                os.chdir(""src/ehmm/bin/"")

                # this is to fix festival if we somehow kill in the middle of training :(
                # all due to festival's apparent dependence on name of script!
                # really, really, REALLY weird
                if os.path.exists(""do_ehmm.bak""):
                    with open(""do_ehmm.bak"", ""r"") as f:
                        fix = f.readlines()

                    with open(""do_ehmm"", ""w"") as f:
                        f.writelines(fix)

                with open(""do_ehmm"", ""r"") as f:
                    do_ehmm_lines = f.readlines()

                with open(""do_ehmm.bak"", ""w"") as f:
                    f.writelines(do_ehmm_lines)

                replace_line = None
                for n, l in enumerate(do_ehmm_lines):
                    if ""$EHMMDIR/bin/ehmm ehmm/etc/ph_list.int"" in l:
                        replace_line = n
                        break

                max_n_itr = ehmm_max_n_itr
                do_ehmm_lines[replace_line] = ""    $EHMMDIR/bin/ehmm ehmm/etc/ph_list.int ehmm/etc/txt.phseq.data.int 1 0 ehmm/binfeat scaledft ehmm/mod 0 0 0 %s $num_cpus\n"" % str(
                    max_n_itr)

                # depends on *name* of the script?????????
                with open(""do_ehmm"", ""w"") as f:
                    f.writelines(do_ehmm_lines)

                # need to edit run_aligner....
                dbn = ""do_build""
                # FIXME
                # WHY DOES IT DEPEND ON FILENAME????!!!!!??????
                # should be able to call only edit_do_build label
                # but hangs indefinitely...
                replace_line = None
                for n, l in enumerate(run_aligner_lines):
                    if ""./bin/do_build build_prompts"" in l:
                        replace_line = n
                        break
                run_aligner_lines[replace_line] = ""./bin/%s build_prompts\n"" % dbn

                replace_line = None
                for n, l in enumerate(run_aligner_lines):
                    if ""./bin/do_build label"" in l:
                        replace_line = n
                        break
                run_aligner_lines[replace_line] = ""./bin/%s label\n"" % dbn

                replace_line = None
                for n, l in enumerate(run_aligner_lines):
                    if ""./bin/do_build build_utts"" in l:
                        replace_line = n
                        break
                run_aligner_lines[replace_line] = ""./bin/%s build_utts\n"" % dbn
                os.chdir(bdir)

            with open(""edit_run_aligner.sh"", ""w"") as f:
                f.writelines(run_aligner_lines)

            # 2>&1 needed to make it work?? really sketchy
            pe(""bash -x edit_run_aligner.sh config.cfg 2>&1"", shell=True)

    # compile vocoder
    os.chdir(merlin_dir)
    # set it to run on cpu
    pe(""sed -i.bak -e s/MERLIN_THEANO_FLAGS=.*/MERLIN_THEANO_FLAGS='device=cpu,floatX=float32,on_unused_input=ignore'/g src/setup_env.sh"", shell=True)
    os.chdir(""tools"")
    if not os.path.exists(""SPTK-3.9""):
        pe(""bash -x compile_tools.sh 2>&1"", shell=True)

    # slt_arctic stuff
    os.chdir(merlin_dir)
    os.chdir(""egs/slt_arctic/s1"")

    # This madness due to autogen configs...
    pe(""bash -x scripts/setup.sh slt_arctic_full 2>&1"", shell=True)

    global_config_file = ""conf/global_settings.cfg""
    replace_write(global_config_file, ""Labels"", ""phone_align"", replace_line=""%s=%s\n"")
    replace_write(global_config_file, ""Train"", ""1132"", replace_line=""%s=%s\n"")
    replace_write(global_config_file, ""Valid"", ""0"", replace_line=""%s=%s\n"")
    replace_write(global_config_file, ""Test"", ""0"", replace_line=""%s=%s\n"")

    pe(""bash -x scripts/prepare_config_files.sh %s 2>&1"" % global_config_file, shell=True)
    pe(""bash -x scripts/prepare_config_files_for_synthesis.sh %s 2>&1"" % global_config_file, shell=True)
    # delete the setup lines from run_full_voice.sh
    pe(""sed -i.bak -e '11d;12d;13d' run_full_voice.sh"", shell=True)

    pushd = os.getcwd()
    os.chdir(""conf"")

    acoustic_conf = ""acoustic_slt_arctic_full.conf""
    replace_write(acoustic_conf, ""train_file_number"", ""1132"")
    replace_write(acoustic_conf, ""valid_file_number"", ""0"")
    replace_write(acoustic_conf, ""test_file_number"", ""0"")

    replace_write(acoustic_conf, ""label_type"", ""phone_align"")
    replace_write(acoustic_conf, ""subphone_feats"", ""coarse_coding"")
    replace_write(acoustic_conf, ""dmgc"", ""60"")
    replace_write(acoustic_conf, ""dbap"", ""1"")
    # hack this to add an extra line in the config
    replace_write(acoustic_conf, ""dlf0"", ""1\ndo_MLPG: False"")

    if not full_features:
        replace_write(acoustic_conf, ""warmup_epoch"", ""1"")
        replace_write(acoustic_conf, ""training_epochs"", ""1"")
    replace_write(acoustic_conf, ""TRAINDNN"", ""False"")
    replace_write(acoustic_conf, ""DNNGEN"", ""False"")
    replace_write(acoustic_conf, ""GENWAV"", ""False"")
    replace_write(acoustic_conf, ""CALMCD"", ""False"")

    duration_conf = ""duration_slt_arctic_full.conf""
    replace_write(duration_conf, ""train_file_number"", ""1132"")
    replace_write(duration_conf, ""valid_file_number"", ""0"")
    replace_write(duration_conf, ""test_file_number"", ""0"")
    replace_write(duration_conf, ""label_type"", ""phone_align"")
    replace_write(duration_conf, ""dur"", ""1"")
    if not full_features:
        replace_write(duration_conf, ""warmup_epoch"", ""1"")
        replace_write(duration_conf, ""training_epochs"", ""1"")

    replace_write(duration_conf, ""TRAINDNN"", ""False"")
    replace_write(duration_conf, ""DNNGEN"", ""False"")
    replace_write(duration_conf, ""CALMCD"", ""False"")

    os.chdir(pushd)
    if not os.path.exists(""slt_arctic_full_data""):
        pe(""bash -x run_full_voice.sh 2>&1"", shell=True)

    pe(""mv run_full_voice.sh.bak run_full_voice.sh"", shell=True)

    os.chdir(merlin_dir)
    os.chdir(""misc/scripts/vocoder/world"")

    with open(""extract_features_for_merlin.sh"", ""r"") as f:
        ex_lines = f.readlines()

    ex_line_replace = None
    for n, l in enumerate(ex_lines):
        if ""merlin_dir="" in l:
            ex_line_replace = n
            break

    ex_lines[ex_line_replace] = 'merlin_dir=""%s""' % merlin_dir

    ex_line_replace = None
    for n, l in enumerate(ex_lines):
        if ""wav_dir="" in l:
            ex_line_replace = n
            break

    ex_lines[ex_line_replace] = 'wav_dir=""%s""' % (experiment_dir + ""/database/wav"")

    with open(""edit_extract_features_for_merlin.sh"", ""w"") as f:
        f.writelines(ex_lines)

    pe(""bash -x edit_extract_features_for_merlin.sh 2>&1"", shell=True)

    os.chdir(basedir)
    os.chdir(""latest_features"")
    os.symlink(merlin_dir + ""/egs/slt_arctic/s1/slt_arctic_full_data/feat"", ""audio_feat"")
    os.symlink(merlin_dir + ""/misc/scripts/alignment/phone_align/full-context-labels/full"", ""text_feat"")

    print(""Audio features in %s (and %s)"" % (os.getcwd() + ""/audio_feat"",
                                             merlin_dir + ""/egs/slt_arctic/s1/slt_arctic_full_data/feat""))
    print(""Text features in %s (and %s)"" % (os.getcwd() + ""/text_feat"", merlin_dir +
                                            ""/misc/scripts/alignment/phone_align/full-context-labels/full""))
    os.chdir(basedir)",_794.py,43,len([tc for tc in to_copy if tc[-4:] == '.wav']) == 0,not len([tc for tc in to_copy if tc[-4:] == '.wav'])
https://github.com/r9y9/deepvoice3_pytorch/tree/master/vctk_preprocess/extract_feats.py,"def extract_intermediate_features(wav_path, txt_path, keep_silences=False,
                                  full_features=False, ehmm_max_n_itr=1):
    basedir = os.getcwd()
    latest_feature_dir = ""latest_features""
    if not os.path.exists(latest_feature_dir):
        os.mkdir(latest_feature_dir)

    os.chdir(latest_feature_dir)
    latest_feature_dir = os.getcwd()

    if not os.path.exists(""merlin""):
        clone_cmd = ""git clone https://github.com/kastnerkyle/merlin""
        pe(clone_cmd, shell=True)

    if keep_silences:
        # REMOVE SILENCES TO MATCH JOSE PREPROC
        os.chdir(""merlin/src"")
        pe(""sed -i.bak -e '708,712d;' run_merlin.py"", shell=True)
        pe(""sed -i.bak -e '695,706d;' run_merlin.py"", shell=True)
        os.chdir(latest_feature_dir)

    os.chdir(""merlin"")
    merlin_dir = os.getcwd()
    os.chdir(""egs/build_your_own_voice/s1"")
    experiment_dir = os.getcwd()

    if not os.path.exists(""database""):
        print(""Creating database and copying in files"")
        pe(""bash -x 01_setup.sh my_new_voice 2>&1"", shell=True)

        # Copy in wav files
        wav_partial_path = wav_path  # vctkdir + ""wav48/""
        """"""
        subfolders = sorted(os.listdir(wav_partial_path))
        # only p294 for now...
        subfolders = subfolder_select(subfolders)
        os.chdir(""database/wav"")
        for sf in subfolders:
            wav_path = wav_partial_path + sf + ""/*.wav""
            pe(""cp %s ."" % wav_path, shell=True)
        """"""
        to_copy = os.listdir(wav_partial_path)
        if len([tc for tc in to_copy if tc[-4:] == "".wav""]) == 0:
            raise IOError(
                ""Unable to find any wav files in %s, make sure the filenames end in .wav!"" % wav_partial_path)
        os.chdir(""database/wav"")
        if wav_partial_path[-1] != ""/"":
            wav_partial_path = wav_partial_path + ""/""
        wav_match_path = wav_partial_path + ""*.wav""
        for fi in glob.glob(wav_match_path):
            pe(""echo %s; cp %s ."" % (fi, fi), shell=True)
        # THIS MAY FAIL IF TOO MANY WAV FILES
        # pe(""cp %s ."" % wav_match_path, shell=True)
        for f in os.listdir("".""):
            # This is only necessary because of corrupted files...
            fs, d = wavfile.read(f)
            wavfile.write(f, fs, d)

        # downsample the files
        get_sr_cmd = 'file `ls *.wav | head -n 1` | cut -d "" "" -f 12'
        sr = pe(get_sr_cmd, shell=True)
        sr_int = int(sr[0].strip())
        print(""Got samplerate {}, converting to 16000"".format(sr_int))
        # was assuming all were 48000
        convert = estdir + \
            ""bin/ch_wave $i -o tmp_$i -itype wav -otype wav -F 16000 -f {}"".format(sr_int)
        pe(""for i in *.wav; do echo %s; %s; mv tmp_$i $i; done"" % (convert, convert), shell=True)

        os.chdir(experiment_dir)
        txt_partial_path = txt_path  # vctkdir + ""txt/""
        """"""
        subfolders = sorted(os.listdir(txt_partial_path))
        # only p294 for now...
        subfolders = subfolder_select(subfolders)
        os.chdir(""database/txt"")
        for sf in subfolders:
            txt_path = txt_partial_path + sf + ""/*.txt""
            pe(""cp %s ."" % txt_path, shell=True)
        """"""
        os.chdir(""database/txt"")
        to_copy = os.listdir(txt_partial_path)
        if len([tc for tc in to_copy if tc[-4:] == "".txt""]) == 0:
            raise IOError(
                ""Unable to find any txt files in %s. Be sure the filenames end in .txt!"" % txt_partial_path)
        txt_match_path = txt_partial_path + ""/*.txt""
        for fi in glob.glob(txt_match_path):
            # escape string...
            fi = re.escape(fi)
            try:
                pe(""echo %s; cp %s ."" % (fi, fi), shell=True)
            except:
                from IPython import embed
                embed()
                raise ValueError()

        #pe(""cp %s ."" % txt_match_path, shell=True)

    do_state_align = False
    if do_state_align:
        raise ValueError(""Replace these lies with something that points at the right place"")
        os.chdir(merlin_dir)
        os.chdir(""misc/scripts/alignment/state_align"")
        pe(""bash -x setup.sh 2>&1"", shell=True)

        with open(""config.cfg"", ""r"") as f:
            config_lines = f.readlines()

        # replace FESTDIR with the correct path
        festdir_replace_line = None
        for n, l in enumerate(config_lines):
            if ""FESTDIR="" in l:
                festdir_replace_line = n
                break

        config_lines[festdir_replace_line] = ""FESTDIR=%s\n"" % festdir

        # replace HTKDIR with the correct path
        htkdir_replace_line = None
        for n, l in enumerate(config_lines):
            if ""HTKDIR="" in l:
                htkdir_replace_line = n
                break

        config_lines[htkdir_replace_line] = ""HTKDIR=%s\n"" % htkdir

        with open(""config.cfg"", ""w"") as f:
            f.writelines(config_lines)

        pe(""bash -x run_aligner.sh config.cfg 2>&1"", shell=True)
    else:
        os.chdir(merlin_dir)
        if not os.path.exists(""misc/scripts/alignment/phone_align/full-context-labels/full""):
            os.chdir(""misc/scripts/alignment/phone_align"")
            pe(""bash -x setup.sh 2>&1"", shell=True)

            with open(""config.cfg"", ""r"") as f:
                config_lines = f.readlines()

            # replace ESTDIR with the correct path
            estdir_replace_line = None
            for n, l in enumerate(config_lines):
                if ""ESTDIR="" in l and l[0] == ""E"":
                    estdir_replace_line = n
                    break

            config_lines[estdir_replace_line] = ""ESTDIR=%s\n"" % estdir

            # replace FESTDIR with the correct path
            festdir_replace_line = None
            for n, l in enumerate(config_lines):
                # EST/FEST
                if ""FESTDIR="" in l and l[0] == ""F"":
                    festdir_replace_line = n
                    break

            config_lines[festdir_replace_line] = ""FESTDIR=%s\n"" % festdir

            # replace FESTVOXDIR with the correct path
            festvoxdir_replace_line = None
            for n, l in enumerate(config_lines):
                if ""FESTVOXDIR="" in l:
                    festvoxdir_replace_line = n
                    break

            config_lines[festvoxdir_replace_line] = ""FESTVOXDIR=%s\n"" % festvoxdir

            with open(""config.cfg"", ""w"") as f:
                f.writelines(config_lines)

            with open(""run_aligner.sh"", ""r"") as f:
                run_aligner_lines = f.readlines()

            replace_line = None
            for n, l in enumerate(run_aligner_lines):
                if ""cp ../cmuarctic.data"" in l:
                    replace_line = n
                    break

            run_aligner_lines[replace_line] = ""cp ../txt.done.data etc/txt.done.data\n""

            # Make the txt.done.data file
            def format_info_tup(info_tup):
                return ""( "" + str(info_tup[0]) + ' ""' + info_tup[1] + '"" )\n'

            # Now we need to get the text info
            txt_partial_path = txt_path  # vctkdir + ""txt/""
            cwd = os.getcwd()
            out_path = ""txt.done.data""
            out_file = open(out_path, ""w"")
            """"""
            subfolders = sorted(os.listdir(txt_partial_path))
            # TODO: Avoid this truncation and have an option to select subfolder(s)...
            subfolders = subfolder_select(subfolders)

            txt_ids = []
            for sf in subfolders:
                print(""Processing subfolder %s"" % sf)
                txt_sf_path = txt_partial_path + sf + ""/""
                for txtpath in os.listdir(txt_sf_path):
                    full_txtpath = txt_sf_path + txtpath
                    with open(full_txtpath, 'r') as f:
                        r = f.readlines()
                        assert len(r) == 1
                        # remove txt extension
                        name = txtpath.split(""."")[0]
                        text = r[0].strip()
                        info_tup = (name, text)
                        txt_ids.append(name)
                        out_file.writelines(format_info_tup(info_tup))
            """"""
            txt_ids = []
            txt_l_path = txt_partial_path
            for txtpath in os.listdir(txt_l_path):
                print(""Processing %s"" % txtpath)
                full_txtpath = txt_l_path + txtpath
                name = txtpath.split(""."")[0]
                wavpath_matches = [fname.split(""."")[0] for fname in os.listdir(wav_partial_path)
                                   if name in fname]
                for name in wavpath_matches:
                    # Need an extra level here for pavoque :/
                    with open(full_txtpath, 'r') as f:
                        r = f.readlines()
                    if len(r) == 0:
                        continue
                    if len(r) != 1:
                        new_r = []
                        for ri in r:
                            if ri != ""\n"":
                                new_r.append(ri)
                        r = new_r
                    if len(r) != 1:
                        print(""Something wrong in text extraction, cowardly bailing to IPython"")
                        from IPython import embed
                        embed()
                        raise ValueError()
                    assert len(r) == 1
                    # remove txt extension
                    text = r[0].strip()
                    info_tup = (name, text)
                    txt_ids.append(name)
                    out_file.writelines(format_info_tup(info_tup))
            out_file.close()
            pe(""cp %s %s/txt.done.data"" % (out_path, latest_feature_dir),
               shell=True)
            os.chdir(cwd)

            replace_line = None
            for n, l in enumerate(run_aligner_lines):
                if ""cp ../slt_wav/*.wav"" in l:
                    replace_line = n
                    break

            run_aligner_lines[replace_line] = ""cp ../wav/*.wav wav\n""

            # Put wav file in the correct place
            wav_partial_path = experiment_dir + ""/database/wav""
            """"""
            subfolders = sorted(os.listdir(wav_partial_path))
            """"""
            if not os.path.exists(""wav""):
                os.mkdir(""wav"")
            cwd = os.getcwd()
            os.chdir(""wav"")
            """"""
            for sf in subfolders:
                wav_path = wav_partial_path + ""/*.wav""
                pe(""cp %s ."" % wav_path, shell=True)
            """"""
            wav_match_path = wav_partial_path + ""/*.wav""
            for fi in glob.glob(wav_match_path):
                fi = re.escape(fi)
                try:
                    pe(""echo %s; cp %s ."" % (fi, fi), shell=True)
                except:
                    from IPython import embed
                    embed()
                    raise ValueError()
                #pe(""echo %s; cp %s ."" % (fi, fi), shell=True)
            #pe(""cp %s ."" % wav_match_path, shell=True)
            os.chdir(cwd)

            replace_line = None
            for n, l in enumerate(run_aligner_lines):
                if ""cat cmuarctic.data |"" in l:
                    replace_line = n
                    break

            run_aligner_lines[replace_line] = 'cat txt.done.data | cut -d "" "" -f 2 > file_id_list.scp\n'

            # FIXME
            # Hackaround to avoid harcoded 30 in festivox do_ehmm
            if not full_features:
                bdir = os.getcwd()

                # need to hack up run_aligner more..
                # do setup manually
                pe(""mkdir cmu_us_slt_arctic"", shell=True)
                os.chdir(""cmu_us_slt_arctic"")

                pe(""%s/src/clustergen/setup_cg cmu us slt_arctic"" % festvoxdir, shell=True)

                pe(""cp ../txt.done.data etc/txt.done.data"", shell=True)
                wmp = ""../wav/*.wav""
                for fi in glob.glob(wmp):
                    fi = re.escape(fi)
                    try:
                        pe(""echo %s; cp %s wav/"" % (fi, fi), shell=True)
                    except:
                        from IPython import embed
                        embed()
                        raise ValueError()
                    #pe(""echo %s; cp %s wav/"" % (fi, fi), shell=True)
                #pe(""cp ../wav/*.wav wav/"", shell=True)

                # remove top part but keep cd call
                run_aligner_lines = run_aligner_lines[:13] + \
                    [""cd cmu_us_slt_arctic\n""] + run_aligner_lines[35:]

                '''
                # need to change do_build
                # NO LONGER NECESSARY DUE TO FESTIVAL DEPENDENCE ON FILENAME

                os.chdir(""bin"")
                with open(""do_build"", ""r"") as f:
                    do_build_lines = f.readlines()

                replace_line = None
                for n, l in enumerate(do_build_lines):
                    if ""$FESTVOXDIR/src/ehmm/bin/do_ehmm"" in l:
                        replace_line = n
                        break

                do_build_lines[replace_line] = ""   $FESTVOXDIR/src/ehmm/bin/do_ehmm\n""

                # FIXME Why does this hang when not overwritten???
                with open(""edit_do_build"", ""w"") as f:
                    f.writelines(do_build_lines)
                '''

                # need to change do_ehmm
                os.chdir(festvoxdir)
                os.chdir(""src/ehmm/bin/"")

                # this is to fix festival if we somehow kill in the middle of training :(
                # all due to festival's apparent dependence on name of script!
                # really, really, REALLY weird
                if os.path.exists(""do_ehmm.bak""):
                    with open(""do_ehmm.bak"", ""r"") as f:
                        fix = f.readlines()

                    with open(""do_ehmm"", ""w"") as f:
                        f.writelines(fix)

                with open(""do_ehmm"", ""r"") as f:
                    do_ehmm_lines = f.readlines()

                with open(""do_ehmm.bak"", ""w"") as f:
                    f.writelines(do_ehmm_lines)

                replace_line = None
                for n, l in enumerate(do_ehmm_lines):
                    if ""$EHMMDIR/bin/ehmm ehmm/etc/ph_list.int"" in l:
                        replace_line = n
                        break

                max_n_itr = ehmm_max_n_itr
                do_ehmm_lines[replace_line] = ""    $EHMMDIR/bin/ehmm ehmm/etc/ph_list.int ehmm/etc/txt.phseq.data.int 1 0 ehmm/binfeat scaledft ehmm/mod 0 0 0 %s $num_cpus\n"" % str(
                    max_n_itr)

                # depends on *name* of the script?????????
                with open(""do_ehmm"", ""w"") as f:
                    f.writelines(do_ehmm_lines)

                # need to edit run_aligner....
                dbn = ""do_build""
                # FIXME
                # WHY DOES IT DEPEND ON FILENAME????!!!!!??????
                # should be able to call only edit_do_build label
                # but hangs indefinitely...
                replace_line = None
                for n, l in enumerate(run_aligner_lines):
                    if ""./bin/do_build build_prompts"" in l:
                        replace_line = n
                        break
                run_aligner_lines[replace_line] = ""./bin/%s build_prompts\n"" % dbn

                replace_line = None
                for n, l in enumerate(run_aligner_lines):
                    if ""./bin/do_build label"" in l:
                        replace_line = n
                        break
                run_aligner_lines[replace_line] = ""./bin/%s label\n"" % dbn

                replace_line = None
                for n, l in enumerate(run_aligner_lines):
                    if ""./bin/do_build build_utts"" in l:
                        replace_line = n
                        break
                run_aligner_lines[replace_line] = ""./bin/%s build_utts\n"" % dbn
                os.chdir(bdir)

            with open(""edit_run_aligner.sh"", ""w"") as f:
                f.writelines(run_aligner_lines)

            # 2>&1 needed to make it work?? really sketchy
            pe(""bash -x edit_run_aligner.sh config.cfg 2>&1"", shell=True)

    # compile vocoder
    os.chdir(merlin_dir)
    # set it to run on cpu
    pe(""sed -i.bak -e s/MERLIN_THEANO_FLAGS=.*/MERLIN_THEANO_FLAGS='device=cpu,floatX=float32,on_unused_input=ignore'/g src/setup_env.sh"", shell=True)
    os.chdir(""tools"")
    if not os.path.exists(""SPTK-3.9""):
        pe(""bash -x compile_tools.sh 2>&1"", shell=True)

    # slt_arctic stuff
    os.chdir(merlin_dir)
    os.chdir(""egs/slt_arctic/s1"")

    # This madness due to autogen configs...
    pe(""bash -x scripts/setup.sh slt_arctic_full 2>&1"", shell=True)

    global_config_file = ""conf/global_settings.cfg""
    replace_write(global_config_file, ""Labels"", ""phone_align"", replace_line=""%s=%s\n"")
    replace_write(global_config_file, ""Train"", ""1132"", replace_line=""%s=%s\n"")
    replace_write(global_config_file, ""Valid"", ""0"", replace_line=""%s=%s\n"")
    replace_write(global_config_file, ""Test"", ""0"", replace_line=""%s=%s\n"")

    pe(""bash -x scripts/prepare_config_files.sh %s 2>&1"" % global_config_file, shell=True)
    pe(""bash -x scripts/prepare_config_files_for_synthesis.sh %s 2>&1"" % global_config_file, shell=True)
    # delete the setup lines from run_full_voice.sh
    pe(""sed -i.bak -e '11d;12d;13d' run_full_voice.sh"", shell=True)

    pushd = os.getcwd()
    os.chdir(""conf"")

    acoustic_conf = ""acoustic_slt_arctic_full.conf""
    replace_write(acoustic_conf, ""train_file_number"", ""1132"")
    replace_write(acoustic_conf, ""valid_file_number"", ""0"")
    replace_write(acoustic_conf, ""test_file_number"", ""0"")

    replace_write(acoustic_conf, ""label_type"", ""phone_align"")
    replace_write(acoustic_conf, ""subphone_feats"", ""coarse_coding"")
    replace_write(acoustic_conf, ""dmgc"", ""60"")
    replace_write(acoustic_conf, ""dbap"", ""1"")
    # hack this to add an extra line in the config
    replace_write(acoustic_conf, ""dlf0"", ""1\ndo_MLPG: False"")

    if not full_features:
        replace_write(acoustic_conf, ""warmup_epoch"", ""1"")
        replace_write(acoustic_conf, ""training_epochs"", ""1"")
    replace_write(acoustic_conf, ""TRAINDNN"", ""False"")
    replace_write(acoustic_conf, ""DNNGEN"", ""False"")
    replace_write(acoustic_conf, ""GENWAV"", ""False"")
    replace_write(acoustic_conf, ""CALMCD"", ""False"")

    duration_conf = ""duration_slt_arctic_full.conf""
    replace_write(duration_conf, ""train_file_number"", ""1132"")
    replace_write(duration_conf, ""valid_file_number"", ""0"")
    replace_write(duration_conf, ""test_file_number"", ""0"")
    replace_write(duration_conf, ""label_type"", ""phone_align"")
    replace_write(duration_conf, ""dur"", ""1"")
    if not full_features:
        replace_write(duration_conf, ""warmup_epoch"", ""1"")
        replace_write(duration_conf, ""training_epochs"", ""1"")

    replace_write(duration_conf, ""TRAINDNN"", ""False"")
    replace_write(duration_conf, ""DNNGEN"", ""False"")
    replace_write(duration_conf, ""CALMCD"", ""False"")

    os.chdir(pushd)
    if not os.path.exists(""slt_arctic_full_data""):
        pe(""bash -x run_full_voice.sh 2>&1"", shell=True)

    pe(""mv run_full_voice.sh.bak run_full_voice.sh"", shell=True)

    os.chdir(merlin_dir)
    os.chdir(""misc/scripts/vocoder/world"")

    with open(""extract_features_for_merlin.sh"", ""r"") as f:
        ex_lines = f.readlines()

    ex_line_replace = None
    for n, l in enumerate(ex_lines):
        if ""merlin_dir="" in l:
            ex_line_replace = n
            break

    ex_lines[ex_line_replace] = 'merlin_dir=""%s""' % merlin_dir

    ex_line_replace = None
    for n, l in enumerate(ex_lines):
        if ""wav_dir="" in l:
            ex_line_replace = n
            break

    ex_lines[ex_line_replace] = 'wav_dir=""%s""' % (experiment_dir + ""/database/wav"")

    with open(""edit_extract_features_for_merlin.sh"", ""w"") as f:
        f.writelines(ex_lines)

    pe(""bash -x edit_extract_features_for_merlin.sh 2>&1"", shell=True)

    os.chdir(basedir)
    os.chdir(""latest_features"")
    os.symlink(merlin_dir + ""/egs/slt_arctic/s1/slt_arctic_full_data/feat"", ""audio_feat"")
    os.symlink(merlin_dir + ""/misc/scripts/alignment/phone_align/full-context-labels/full"", ""text_feat"")

    print(""Audio features in %s (and %s)"" % (os.getcwd() + ""/audio_feat"",
                                             merlin_dir + ""/egs/slt_arctic/s1/slt_arctic_full_data/feat""))
    print(""Text features in %s (and %s)"" % (os.getcwd() + ""/text_feat"", merlin_dir +
                                            ""/misc/scripts/alignment/phone_align/full-context-labels/full""))
    os.chdir(basedir)",_794.py,82,len([tc for tc in to_copy if tc[-4:] == '.txt']) == 0,not len([tc for tc in to_copy if tc[-4:] == '.txt'])
https://github.com/r9y9/deepvoice3_pytorch/tree/master/vctk_preprocess/extract_feats.py,"def extract_intermediate_features(wav_path, txt_path, keep_silences=False,
                                  full_features=False, ehmm_max_n_itr=1):
    basedir = os.getcwd()
    latest_feature_dir = ""latest_features""
    if not os.path.exists(latest_feature_dir):
        os.mkdir(latest_feature_dir)

    os.chdir(latest_feature_dir)
    latest_feature_dir = os.getcwd()

    if not os.path.exists(""merlin""):
        clone_cmd = ""git clone https://github.com/kastnerkyle/merlin""
        pe(clone_cmd, shell=True)

    if keep_silences:
        # REMOVE SILENCES TO MATCH JOSE PREPROC
        os.chdir(""merlin/src"")
        pe(""sed -i.bak -e '708,712d;' run_merlin.py"", shell=True)
        pe(""sed -i.bak -e '695,706d;' run_merlin.py"", shell=True)
        os.chdir(latest_feature_dir)

    os.chdir(""merlin"")
    merlin_dir = os.getcwd()
    os.chdir(""egs/build_your_own_voice/s1"")
    experiment_dir = os.getcwd()

    if not os.path.exists(""database""):
        print(""Creating database and copying in files"")
        pe(""bash -x 01_setup.sh my_new_voice 2>&1"", shell=True)

        # Copy in wav files
        wav_partial_path = wav_path  # vctkdir + ""wav48/""
        """"""
        subfolders = sorted(os.listdir(wav_partial_path))
        # only p294 for now...
        subfolders = subfolder_select(subfolders)
        os.chdir(""database/wav"")
        for sf in subfolders:
            wav_path = wav_partial_path + sf + ""/*.wav""
            pe(""cp %s ."" % wav_path, shell=True)
        """"""
        to_copy = os.listdir(wav_partial_path)
        if len([tc for tc in to_copy if tc[-4:] == "".wav""]) == 0:
            raise IOError(
                ""Unable to find any wav files in %s, make sure the filenames end in .wav!"" % wav_partial_path)
        os.chdir(""database/wav"")
        if wav_partial_path[-1] != ""/"":
            wav_partial_path = wav_partial_path + ""/""
        wav_match_path = wav_partial_path + ""*.wav""
        for fi in glob.glob(wav_match_path):
            pe(""echo %s; cp %s ."" % (fi, fi), shell=True)
        # THIS MAY FAIL IF TOO MANY WAV FILES
        # pe(""cp %s ."" % wav_match_path, shell=True)
        for f in os.listdir("".""):
            # This is only necessary because of corrupted files...
            fs, d = wavfile.read(f)
            wavfile.write(f, fs, d)

        # downsample the files
        get_sr_cmd = 'file `ls *.wav | head -n 1` | cut -d "" "" -f 12'
        sr = pe(get_sr_cmd, shell=True)
        sr_int = int(sr[0].strip())
        print(""Got samplerate {}, converting to 16000"".format(sr_int))
        # was assuming all were 48000
        convert = estdir + \
            ""bin/ch_wave $i -o tmp_$i -itype wav -otype wav -F 16000 -f {}"".format(sr_int)
        pe(""for i in *.wav; do echo %s; %s; mv tmp_$i $i; done"" % (convert, convert), shell=True)

        os.chdir(experiment_dir)
        txt_partial_path = txt_path  # vctkdir + ""txt/""
        """"""
        subfolders = sorted(os.listdir(txt_partial_path))
        # only p294 for now...
        subfolders = subfolder_select(subfolders)
        os.chdir(""database/txt"")
        for sf in subfolders:
            txt_path = txt_partial_path + sf + ""/*.txt""
            pe(""cp %s ."" % txt_path, shell=True)
        """"""
        os.chdir(""database/txt"")
        to_copy = os.listdir(txt_partial_path)
        if len([tc for tc in to_copy if tc[-4:] == "".txt""]) == 0:
            raise IOError(
                ""Unable to find any txt files in %s. Be sure the filenames end in .txt!"" % txt_partial_path)
        txt_match_path = txt_partial_path + ""/*.txt""
        for fi in glob.glob(txt_match_path):
            # escape string...
            fi = re.escape(fi)
            try:
                pe(""echo %s; cp %s ."" % (fi, fi), shell=True)
            except:
                from IPython import embed
                embed()
                raise ValueError()

        #pe(""cp %s ."" % txt_match_path, shell=True)

    do_state_align = False
    if do_state_align:
        raise ValueError(""Replace these lies with something that points at the right place"")
        os.chdir(merlin_dir)
        os.chdir(""misc/scripts/alignment/state_align"")
        pe(""bash -x setup.sh 2>&1"", shell=True)

        with open(""config.cfg"", ""r"") as f:
            config_lines = f.readlines()

        # replace FESTDIR with the correct path
        festdir_replace_line = None
        for n, l in enumerate(config_lines):
            if ""FESTDIR="" in l:
                festdir_replace_line = n
                break

        config_lines[festdir_replace_line] = ""FESTDIR=%s\n"" % festdir

        # replace HTKDIR with the correct path
        htkdir_replace_line = None
        for n, l in enumerate(config_lines):
            if ""HTKDIR="" in l:
                htkdir_replace_line = n
                break

        config_lines[htkdir_replace_line] = ""HTKDIR=%s\n"" % htkdir

        with open(""config.cfg"", ""w"") as f:
            f.writelines(config_lines)

        pe(""bash -x run_aligner.sh config.cfg 2>&1"", shell=True)
    else:
        os.chdir(merlin_dir)
        if not os.path.exists(""misc/scripts/alignment/phone_align/full-context-labels/full""):
            os.chdir(""misc/scripts/alignment/phone_align"")
            pe(""bash -x setup.sh 2>&1"", shell=True)

            with open(""config.cfg"", ""r"") as f:
                config_lines = f.readlines()

            # replace ESTDIR with the correct path
            estdir_replace_line = None
            for n, l in enumerate(config_lines):
                if ""ESTDIR="" in l and l[0] == ""E"":
                    estdir_replace_line = n
                    break

            config_lines[estdir_replace_line] = ""ESTDIR=%s\n"" % estdir

            # replace FESTDIR with the correct path
            festdir_replace_line = None
            for n, l in enumerate(config_lines):
                # EST/FEST
                if ""FESTDIR="" in l and l[0] == ""F"":
                    festdir_replace_line = n
                    break

            config_lines[festdir_replace_line] = ""FESTDIR=%s\n"" % festdir

            # replace FESTVOXDIR with the correct path
            festvoxdir_replace_line = None
            for n, l in enumerate(config_lines):
                if ""FESTVOXDIR="" in l:
                    festvoxdir_replace_line = n
                    break

            config_lines[festvoxdir_replace_line] = ""FESTVOXDIR=%s\n"" % festvoxdir

            with open(""config.cfg"", ""w"") as f:
                f.writelines(config_lines)

            with open(""run_aligner.sh"", ""r"") as f:
                run_aligner_lines = f.readlines()

            replace_line = None
            for n, l in enumerate(run_aligner_lines):
                if ""cp ../cmuarctic.data"" in l:
                    replace_line = n
                    break

            run_aligner_lines[replace_line] = ""cp ../txt.done.data etc/txt.done.data\n""

            # Make the txt.done.data file
            def format_info_tup(info_tup):
                return ""( "" + str(info_tup[0]) + ' ""' + info_tup[1] + '"" )\n'

            # Now we need to get the text info
            txt_partial_path = txt_path  # vctkdir + ""txt/""
            cwd = os.getcwd()
            out_path = ""txt.done.data""
            out_file = open(out_path, ""w"")
            """"""
            subfolders = sorted(os.listdir(txt_partial_path))
            # TODO: Avoid this truncation and have an option to select subfolder(s)...
            subfolders = subfolder_select(subfolders)

            txt_ids = []
            for sf in subfolders:
                print(""Processing subfolder %s"" % sf)
                txt_sf_path = txt_partial_path + sf + ""/""
                for txtpath in os.listdir(txt_sf_path):
                    full_txtpath = txt_sf_path + txtpath
                    with open(full_txtpath, 'r') as f:
                        r = f.readlines()
                        assert len(r) == 1
                        # remove txt extension
                        name = txtpath.split(""."")[0]
                        text = r[0].strip()
                        info_tup = (name, text)
                        txt_ids.append(name)
                        out_file.writelines(format_info_tup(info_tup))
            """"""
            txt_ids = []
            txt_l_path = txt_partial_path
            for txtpath in os.listdir(txt_l_path):
                print(""Processing %s"" % txtpath)
                full_txtpath = txt_l_path + txtpath
                name = txtpath.split(""."")[0]
                wavpath_matches = [fname.split(""."")[0] for fname in os.listdir(wav_partial_path)
                                   if name in fname]
                for name in wavpath_matches:
                    # Need an extra level here for pavoque :/
                    with open(full_txtpath, 'r') as f:
                        r = f.readlines()
                    if len(r) == 0:
                        continue
                    if len(r) != 1:
                        new_r = []
                        for ri in r:
                            if ri != ""\n"":
                                new_r.append(ri)
                        r = new_r
                    if len(r) != 1:
                        print(""Something wrong in text extraction, cowardly bailing to IPython"")
                        from IPython import embed
                        embed()
                        raise ValueError()
                    assert len(r) == 1
                    # remove txt extension
                    text = r[0].strip()
                    info_tup = (name, text)
                    txt_ids.append(name)
                    out_file.writelines(format_info_tup(info_tup))
            out_file.close()
            pe(""cp %s %s/txt.done.data"" % (out_path, latest_feature_dir),
               shell=True)
            os.chdir(cwd)

            replace_line = None
            for n, l in enumerate(run_aligner_lines):
                if ""cp ../slt_wav/*.wav"" in l:
                    replace_line = n
                    break

            run_aligner_lines[replace_line] = ""cp ../wav/*.wav wav\n""

            # Put wav file in the correct place
            wav_partial_path = experiment_dir + ""/database/wav""
            """"""
            subfolders = sorted(os.listdir(wav_partial_path))
            """"""
            if not os.path.exists(""wav""):
                os.mkdir(""wav"")
            cwd = os.getcwd()
            os.chdir(""wav"")
            """"""
            for sf in subfolders:
                wav_path = wav_partial_path + ""/*.wav""
                pe(""cp %s ."" % wav_path, shell=True)
            """"""
            wav_match_path = wav_partial_path + ""/*.wav""
            for fi in glob.glob(wav_match_path):
                fi = re.escape(fi)
                try:
                    pe(""echo %s; cp %s ."" % (fi, fi), shell=True)
                except:
                    from IPython import embed
                    embed()
                    raise ValueError()
                #pe(""echo %s; cp %s ."" % (fi, fi), shell=True)
            #pe(""cp %s ."" % wav_match_path, shell=True)
            os.chdir(cwd)

            replace_line = None
            for n, l in enumerate(run_aligner_lines):
                if ""cat cmuarctic.data |"" in l:
                    replace_line = n
                    break

            run_aligner_lines[replace_line] = 'cat txt.done.data | cut -d "" "" -f 2 > file_id_list.scp\n'

            # FIXME
            # Hackaround to avoid harcoded 30 in festivox do_ehmm
            if not full_features:
                bdir = os.getcwd()

                # need to hack up run_aligner more..
                # do setup manually
                pe(""mkdir cmu_us_slt_arctic"", shell=True)
                os.chdir(""cmu_us_slt_arctic"")

                pe(""%s/src/clustergen/setup_cg cmu us slt_arctic"" % festvoxdir, shell=True)

                pe(""cp ../txt.done.data etc/txt.done.data"", shell=True)
                wmp = ""../wav/*.wav""
                for fi in glob.glob(wmp):
                    fi = re.escape(fi)
                    try:
                        pe(""echo %s; cp %s wav/"" % (fi, fi), shell=True)
                    except:
                        from IPython import embed
                        embed()
                        raise ValueError()
                    #pe(""echo %s; cp %s wav/"" % (fi, fi), shell=True)
                #pe(""cp ../wav/*.wav wav/"", shell=True)

                # remove top part but keep cd call
                run_aligner_lines = run_aligner_lines[:13] + \
                    [""cd cmu_us_slt_arctic\n""] + run_aligner_lines[35:]

                '''
                # need to change do_build
                # NO LONGER NECESSARY DUE TO FESTIVAL DEPENDENCE ON FILENAME

                os.chdir(""bin"")
                with open(""do_build"", ""r"") as f:
                    do_build_lines = f.readlines()

                replace_line = None
                for n, l in enumerate(do_build_lines):
                    if ""$FESTVOXDIR/src/ehmm/bin/do_ehmm"" in l:
                        replace_line = n
                        break

                do_build_lines[replace_line] = ""   $FESTVOXDIR/src/ehmm/bin/do_ehmm\n""

                # FIXME Why does this hang when not overwritten???
                with open(""edit_do_build"", ""w"") as f:
                    f.writelines(do_build_lines)
                '''

                # need to change do_ehmm
                os.chdir(festvoxdir)
                os.chdir(""src/ehmm/bin/"")

                # this is to fix festival if we somehow kill in the middle of training :(
                # all due to festival's apparent dependence on name of script!
                # really, really, REALLY weird
                if os.path.exists(""do_ehmm.bak""):
                    with open(""do_ehmm.bak"", ""r"") as f:
                        fix = f.readlines()

                    with open(""do_ehmm"", ""w"") as f:
                        f.writelines(fix)

                with open(""do_ehmm"", ""r"") as f:
                    do_ehmm_lines = f.readlines()

                with open(""do_ehmm.bak"", ""w"") as f:
                    f.writelines(do_ehmm_lines)

                replace_line = None
                for n, l in enumerate(do_ehmm_lines):
                    if ""$EHMMDIR/bin/ehmm ehmm/etc/ph_list.int"" in l:
                        replace_line = n
                        break

                max_n_itr = ehmm_max_n_itr
                do_ehmm_lines[replace_line] = ""    $EHMMDIR/bin/ehmm ehmm/etc/ph_list.int ehmm/etc/txt.phseq.data.int 1 0 ehmm/binfeat scaledft ehmm/mod 0 0 0 %s $num_cpus\n"" % str(
                    max_n_itr)

                # depends on *name* of the script?????????
                with open(""do_ehmm"", ""w"") as f:
                    f.writelines(do_ehmm_lines)

                # need to edit run_aligner....
                dbn = ""do_build""
                # FIXME
                # WHY DOES IT DEPEND ON FILENAME????!!!!!??????
                # should be able to call only edit_do_build label
                # but hangs indefinitely...
                replace_line = None
                for n, l in enumerate(run_aligner_lines):
                    if ""./bin/do_build build_prompts"" in l:
                        replace_line = n
                        break
                run_aligner_lines[replace_line] = ""./bin/%s build_prompts\n"" % dbn

                replace_line = None
                for n, l in enumerate(run_aligner_lines):
                    if ""./bin/do_build label"" in l:
                        replace_line = n
                        break
                run_aligner_lines[replace_line] = ""./bin/%s label\n"" % dbn

                replace_line = None
                for n, l in enumerate(run_aligner_lines):
                    if ""./bin/do_build build_utts"" in l:
                        replace_line = n
                        break
                run_aligner_lines[replace_line] = ""./bin/%s build_utts\n"" % dbn
                os.chdir(bdir)

            with open(""edit_run_aligner.sh"", ""w"") as f:
                f.writelines(run_aligner_lines)

            # 2>&1 needed to make it work?? really sketchy
            pe(""bash -x edit_run_aligner.sh config.cfg 2>&1"", shell=True)

    # compile vocoder
    os.chdir(merlin_dir)
    # set it to run on cpu
    pe(""sed -i.bak -e s/MERLIN_THEANO_FLAGS=.*/MERLIN_THEANO_FLAGS='device=cpu,floatX=float32,on_unused_input=ignore'/g src/setup_env.sh"", shell=True)
    os.chdir(""tools"")
    if not os.path.exists(""SPTK-3.9""):
        pe(""bash -x compile_tools.sh 2>&1"", shell=True)

    # slt_arctic stuff
    os.chdir(merlin_dir)
    os.chdir(""egs/slt_arctic/s1"")

    # This madness due to autogen configs...
    pe(""bash -x scripts/setup.sh slt_arctic_full 2>&1"", shell=True)

    global_config_file = ""conf/global_settings.cfg""
    replace_write(global_config_file, ""Labels"", ""phone_align"", replace_line=""%s=%s\n"")
    replace_write(global_config_file, ""Train"", ""1132"", replace_line=""%s=%s\n"")
    replace_write(global_config_file, ""Valid"", ""0"", replace_line=""%s=%s\n"")
    replace_write(global_config_file, ""Test"", ""0"", replace_line=""%s=%s\n"")

    pe(""bash -x scripts/prepare_config_files.sh %s 2>&1"" % global_config_file, shell=True)
    pe(""bash -x scripts/prepare_config_files_for_synthesis.sh %s 2>&1"" % global_config_file, shell=True)
    # delete the setup lines from run_full_voice.sh
    pe(""sed -i.bak -e '11d;12d;13d' run_full_voice.sh"", shell=True)

    pushd = os.getcwd()
    os.chdir(""conf"")

    acoustic_conf = ""acoustic_slt_arctic_full.conf""
    replace_write(acoustic_conf, ""train_file_number"", ""1132"")
    replace_write(acoustic_conf, ""valid_file_number"", ""0"")
    replace_write(acoustic_conf, ""test_file_number"", ""0"")

    replace_write(acoustic_conf, ""label_type"", ""phone_align"")
    replace_write(acoustic_conf, ""subphone_feats"", ""coarse_coding"")
    replace_write(acoustic_conf, ""dmgc"", ""60"")
    replace_write(acoustic_conf, ""dbap"", ""1"")
    # hack this to add an extra line in the config
    replace_write(acoustic_conf, ""dlf0"", ""1\ndo_MLPG: False"")

    if not full_features:
        replace_write(acoustic_conf, ""warmup_epoch"", ""1"")
        replace_write(acoustic_conf, ""training_epochs"", ""1"")
    replace_write(acoustic_conf, ""TRAINDNN"", ""False"")
    replace_write(acoustic_conf, ""DNNGEN"", ""False"")
    replace_write(acoustic_conf, ""GENWAV"", ""False"")
    replace_write(acoustic_conf, ""CALMCD"", ""False"")

    duration_conf = ""duration_slt_arctic_full.conf""
    replace_write(duration_conf, ""train_file_number"", ""1132"")
    replace_write(duration_conf, ""valid_file_number"", ""0"")
    replace_write(duration_conf, ""test_file_number"", ""0"")
    replace_write(duration_conf, ""label_type"", ""phone_align"")
    replace_write(duration_conf, ""dur"", ""1"")
    if not full_features:
        replace_write(duration_conf, ""warmup_epoch"", ""1"")
        replace_write(duration_conf, ""training_epochs"", ""1"")

    replace_write(duration_conf, ""TRAINDNN"", ""False"")
    replace_write(duration_conf, ""DNNGEN"", ""False"")
    replace_write(duration_conf, ""CALMCD"", ""False"")

    os.chdir(pushd)
    if not os.path.exists(""slt_arctic_full_data""):
        pe(""bash -x run_full_voice.sh 2>&1"", shell=True)

    pe(""mv run_full_voice.sh.bak run_full_voice.sh"", shell=True)

    os.chdir(merlin_dir)
    os.chdir(""misc/scripts/vocoder/world"")

    with open(""extract_features_for_merlin.sh"", ""r"") as f:
        ex_lines = f.readlines()

    ex_line_replace = None
    for n, l in enumerate(ex_lines):
        if ""merlin_dir="" in l:
            ex_line_replace = n
            break

    ex_lines[ex_line_replace] = 'merlin_dir=""%s""' % merlin_dir

    ex_line_replace = None
    for n, l in enumerate(ex_lines):
        if ""wav_dir="" in l:
            ex_line_replace = n
            break

    ex_lines[ex_line_replace] = 'wav_dir=""%s""' % (experiment_dir + ""/database/wav"")

    with open(""edit_extract_features_for_merlin.sh"", ""w"") as f:
        f.writelines(ex_lines)

    pe(""bash -x edit_extract_features_for_merlin.sh 2>&1"", shell=True)

    os.chdir(basedir)
    os.chdir(""latest_features"")
    os.symlink(merlin_dir + ""/egs/slt_arctic/s1/slt_arctic_full_data/feat"", ""audio_feat"")
    os.symlink(merlin_dir + ""/misc/scripts/alignment/phone_align/full-context-labels/full"", ""text_feat"")

    print(""Audio features in %s (and %s)"" % (os.getcwd() + ""/audio_feat"",
                                             merlin_dir + ""/egs/slt_arctic/s1/slt_arctic_full_data/feat""))
    print(""Text features in %s (and %s)"" % (os.getcwd() + ""/text_feat"", merlin_dir +
                                            ""/misc/scripts/alignment/phone_align/full-context-labels/full""))
    os.chdir(basedir)",_794.py,223,len(r) == 0,not len(r)
https://github.com/Zulko/moviepy/tree/master/moviepy/video/fx/rotate.py,"def rotate(
    clip,
    angle,
    unit=""deg"",
    resample=""bicubic"",
    expand=True,
    center=None,
    translate=None,
    bg_color=None,
):
    """"""
    Rotates the specified clip by ``angle`` degrees (or radians) anticlockwise
    If the angle is not a multiple of 90 (degrees) or ``center``, ``translate``,
    and ``bg_color`` are not ``None``, the package ``pillow`` must be installed,
    and there will be black borders. You can make them transparent with:

    >>> new_clip = clip.add_mask().rotate(72)

    Parameters
    ----------

    clip : VideoClip
      A video clip.

    angle : float
      Either a value or a function angle(t) representing the angle of rotation.

    unit : str, optional
      Unit of parameter `angle` (either ""deg"" for degrees or ""rad"" for radians).

    resample : str, optional
      An optional resampling filter. One of ""nearest"", ""bilinear"", or ""bicubic"".

    expand : bool, optional
      If true, expands the output image to make it large enough to hold the
      entire rotated image. If false or omitted, make the output image the same
      size as the input image.

    translate : tuple, optional
      An optional post-rotate translation (a 2-tuple).

    center : tuple, optional
      Optional center of rotation (a 2-tuple). Origin is the upper left corner.

    bg_color : tuple, optional
      An optional color for area outside the rotated image. Only has effect if
      ``expand`` is true.
    """"""
    if Image:
        try:
            resample = {
                ""bilinear"": Image.BILINEAR,
                ""nearest"": Image.NEAREST,
                ""bicubic"": Image.BICUBIC,
            }[resample]
        except KeyError:
            raise ValueError(
                ""'resample' argument must be either 'bilinear', 'nearest' or 'bicubic'""
            )

    if hasattr(angle, ""__call__""):
        get_angle = angle
    else:
        get_angle = lambda t: angle

    def filter(get_frame, t):
        angle = get_angle(t)
        im = get_frame(t)

        if unit == ""rad"":
            angle = math.degrees(angle)

        angle %= 360
        if not center and not translate and not bg_color:
            if (angle == 0) and expand:
                return im
            if (angle == 90) and expand:
                transpose = [1, 0] if len(im.shape) == 2 else [1, 0, 2]
                return np.transpose(im, axes=transpose)[::-1]
            elif (angle == 270) and expand:
                transpose = [1, 0] if len(im.shape) == 2 else [1, 0, 2]
                return np.transpose(im, axes=transpose)[:, ::-1]
            elif (angle == 180) and expand:
                return im[::-1, ::-1]

        if not Image:
            raise ValueError(
                'Without ""Pillow"" installed, only angles that are a multiple of 90'
                "" without centering, translation and background color transformations""
                ' are supported, please install ""Pillow"" with `pip install pillow`'
            )

        # build PIL.rotate kwargs
        kwargs, _locals = ({}, locals())
        for PIL_rotate_kw_name, (
            kw_name,
            supported,
            min_version,
        ) in PIL_rotate_kwargs_supported.items():
            # get the value passed to rotate FX from `locals()` dictionary
            kw_value = _locals[kw_name]

            if supported:  # if argument supported by PIL version
                kwargs[PIL_rotate_kw_name] = kw_value
            else:
                if kw_value is not None:  # if not default value
                    warnings.warn(
                        f""rotate '{kw_name}' argument is not supported""
                        "" by your Pillow version and is being ignored. Minimum""
                        "" Pillow version required:""
                        f"" v{'.'.join(str(n) for n in min_version)}"",
                        UserWarning,
                    )

        # PIL expects uint8 type data. However a mask image has values in the
        # range [0, 1] and is of float type.  To handle this we scale it up by
        # a factor 'a' for use with PIL and then back again by 'a' afterwards.
        if im.dtype == ""float64"":
            # this is a mask image
            a = 255.0
        else:
            a = 1

        # call PIL.rotate
        return (
            np.array(
                Image.fromarray(np.array(a * im).astype(np.uint8)).rotate(
                    angle, expand=expand, resample=resample, **kwargs
                )
            )
            / a
        )

    return clip.transform(filter, apply_to=[""mask""])",_801.py,75,angle == 0,not angle
https://github.com/Zulko/moviepy/tree/master/moviepy/video/fx/rotate.py,"def rotate(
    clip,
    angle,
    unit=""deg"",
    resample=""bicubic"",
    expand=True,
    center=None,
    translate=None,
    bg_color=None,
):
    """"""
    Rotates the specified clip by ``angle`` degrees (or radians) anticlockwise
    If the angle is not a multiple of 90 (degrees) or ``center``, ``translate``,
    and ``bg_color`` are not ``None``, the package ``pillow`` must be installed,
    and there will be black borders. You can make them transparent with:

    >>> new_clip = clip.add_mask().rotate(72)

    Parameters
    ----------

    clip : VideoClip
      A video clip.

    angle : float
      Either a value or a function angle(t) representing the angle of rotation.

    unit : str, optional
      Unit of parameter `angle` (either ""deg"" for degrees or ""rad"" for radians).

    resample : str, optional
      An optional resampling filter. One of ""nearest"", ""bilinear"", or ""bicubic"".

    expand : bool, optional
      If true, expands the output image to make it large enough to hold the
      entire rotated image. If false or omitted, make the output image the same
      size as the input image.

    translate : tuple, optional
      An optional post-rotate translation (a 2-tuple).

    center : tuple, optional
      Optional center of rotation (a 2-tuple). Origin is the upper left corner.

    bg_color : tuple, optional
      An optional color for area outside the rotated image. Only has effect if
      ``expand`` is true.
    """"""
    if Image:
        try:
            resample = {
                ""bilinear"": Image.BILINEAR,
                ""nearest"": Image.NEAREST,
                ""bicubic"": Image.BICUBIC,
            }[resample]
        except KeyError:
            raise ValueError(
                ""'resample' argument must be either 'bilinear', 'nearest' or 'bicubic'""
            )

    if hasattr(angle, ""__call__""):
        get_angle = angle
    else:
        get_angle = lambda t: angle

    def filter(get_frame, t):
        angle = get_angle(t)
        im = get_frame(t)

        if unit == ""rad"":
            angle = math.degrees(angle)

        angle %= 360
        if not center and not translate and not bg_color:
            if (angle == 0) and expand:
                return im
            if (angle == 90) and expand:
                transpose = [1, 0] if len(im.shape) == 2 else [1, 0, 2]
                return np.transpose(im, axes=transpose)[::-1]
            elif (angle == 270) and expand:
                transpose = [1, 0] if len(im.shape) == 2 else [1, 0, 2]
                return np.transpose(im, axes=transpose)[:, ::-1]
            elif (angle == 180) and expand:
                return im[::-1, ::-1]

        if not Image:
            raise ValueError(
                'Without ""Pillow"" installed, only angles that are a multiple of 90'
                "" without centering, translation and background color transformations""
                ' are supported, please install ""Pillow"" with `pip install pillow`'
            )

        # build PIL.rotate kwargs
        kwargs, _locals = ({}, locals())
        for PIL_rotate_kw_name, (
            kw_name,
            supported,
            min_version,
        ) in PIL_rotate_kwargs_supported.items():
            # get the value passed to rotate FX from `locals()` dictionary
            kw_value = _locals[kw_name]

            if supported:  # if argument supported by PIL version
                kwargs[PIL_rotate_kw_name] = kw_value
            else:
                if kw_value is not None:  # if not default value
                    warnings.warn(
                        f""rotate '{kw_name}' argument is not supported""
                        "" by your Pillow version and is being ignored. Minimum""
                        "" Pillow version required:""
                        f"" v{'.'.join(str(n) for n in min_version)}"",
                        UserWarning,
                    )

        # PIL expects uint8 type data. However a mask image has values in the
        # range [0, 1] and is of float type.  To handle this we scale it up by
        # a factor 'a' for use with PIL and then back again by 'a' afterwards.
        if im.dtype == ""float64"":
            # this is a mask image
            a = 255.0
        else:
            a = 1

        # call PIL.rotate
        return (
            np.array(
                Image.fromarray(np.array(a * im).astype(np.uint8)).rotate(
                    angle, expand=expand, resample=resample, **kwargs
                )
            )
            / a
        )

    return clip.transform(filter, apply_to=[""mask""])",_801.py,75,angle == 0,not angle
https://github.com/Cisco-Talos/GhIDA/tree/master/ghida_plugin/idaxml.py,"def write_attribute(self, name, value):
        """"""
        Outputs an attribute (name and value) for an element.

        Args:
            name: String representing attribute name.
            value: String representing attribute value.
        """"""
        if name == None or value == None:
            return
        if (len(name) == 0) or (len(value) == 0):
            return
        attr = "" "" + name + '=""' + self.check_for_entities(value) + '""'
        self.write_to_xmlfile(attr)",_809.py,9,name == None,not name
https://github.com/Cisco-Talos/GhIDA/tree/master/ghida_plugin/idaxml.py,"def write_attribute(self, name, value):
        """"""
        Outputs an attribute (name and value) for an element.

        Args:
            name: String representing attribute name.
            value: String representing attribute value.
        """"""
        if name == None or value == None:
            return
        if (len(name) == 0) or (len(value) == 0):
            return
        attr = "" "" + name + '=""' + self.check_for_entities(value) + '""'
        self.write_to_xmlfile(attr)",_809.py,9,value == None,not value
https://github.com/Cisco-Talos/GhIDA/tree/master/ghida_plugin/idaxml.py,"def write_attribute(self, name, value):
        """"""
        Outputs an attribute (name and value) for an element.

        Args:
            name: String representing attribute name.
            value: String representing attribute value.
        """"""
        if name == None or value == None:
            return
        if (len(name) == 0) or (len(value) == 0):
            return
        attr = "" "" + name + '=""' + self.check_for_entities(value) + '""'
        self.write_to_xmlfile(attr)",_809.py,11,len(name) == 0,not len(name)
https://github.com/Cisco-Talos/GhIDA/tree/master/ghida_plugin/idaxml.py,"def write_attribute(self, name, value):
        """"""
        Outputs an attribute (name and value) for an element.

        Args:
            name: String representing attribute name.
            value: String representing attribute value.
        """"""
        if name == None or value == None:
            return
        if (len(name) == 0) or (len(value) == 0):
            return
        attr = "" "" + name + '=""' + self.check_for_entities(value) + '""'
        self.write_to_xmlfile(attr)",_809.py,11,len(value) == 0,not len(value)
https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/framework.py,"def _convert_to_pdf(dot_file_path):
            pdf_save_path = os.path.splitext(dot_file_path)[0] + '.pdf'
            exited_code = subprocess.call(
                'dot -Tpdf ' + dot_file_path + ' -o ' + pdf_save_path,
                shell=True,
            )
            if exited_code != 0:
                print('The dot command is needed for creating pdf files.')
                print(
                    'The {} is saved as the dot filetype.'.format(dot_file_path)
                )",_837.py,7,exited_code != 0,exited_code
https://github.com/evhub/coconut/tree/master/coconut/compiler/util.py,"def __new__(cls, action, original, loc, tokens, ignore_no_tokens=False, ignore_one_token=False, greedy=False, trim_arity=True):
        """"""Create a ComputionNode to return from a parse action.

        If ignore_no_tokens, then don't call the action if there are no tokens.
        If ignore_one_token, then don't call the action if there is only one token.
        If greedy, then never defer the action until later.""""""
        if ignore_no_tokens and len(tokens) == 0:
            return []
        elif ignore_one_token and len(tokens) == 1:
            return tokens[0]  # could be a ComputationNode, so we can't have an __init__
        else:
            self = super(ComputationNode, cls).__new__(cls)
            if trim_arity:
                self.action = _trim_arity(action)
            else:
                self.action = action
            self.original = original
            self.loc = loc
            self.tokens = tokens
            if DEVELOP:
                self.been_called = False
            if greedy:
                return self.evaluate()
            else:
                return self",_877.py,7,len(tokens) == 0,not len(tokens)
https://github.com/PyCQA/pydocstyle/tree/master/src/tests/test_integration.py,"def test_only_comment_with_noqa_file(env):
    """"""Test that file with noqa and only comments does not cause errors.""""""
    with env.open('comments.py', 'wt') as comments:
        comments.write(
            '#!/usr/bin/env python3\n'
            '# -*- coding: utf-8 -*-\n'
            '# Useless comment\n'
            '# Just another useless comment\n'
            '# noqa: D100\n'
        )

    out, _, code = env.invoke()
    assert 'D100' not in out
    assert code == 0",_878.py,14,code == 0,not code
https://github.com/MhLiao/MaskTextSpotterV3/tree/master/maskrcnn_benchmark/utils/comm.py,"def reduce_dict(input_dict, average=True):
    """"""
    Args:
        input_dict (dict): all the values will be reduced
        average (bool): whether to do average or sum
    Reduce the values in the dictionary from all processes so that process with rank
    0 has the averaged results. Returns a dict with the same fields as
    input_dict, after reduction.
    """"""
    world_size = get_world_size()
    if world_size < 2:
        return input_dict
    with torch.no_grad():
        names = []
        values = []
        # sort the keys so that they are consistent across processes
        for k in sorted(input_dict.keys()):
            names.append(k)
            values.append(input_dict[k])
        values = torch.stack(values, dim=0)
        dist.reduce(values, dst=0)
        if dist.get_rank() == 0 and average:
            # only main process gets accumulated, so only divide by
            # world_size in this case
            values /= world_size
        reduced_dict = {k: v for k, v in zip(names, values)}
    return reduced_dict",_886.py,22,dist.get_rank() == 0,not dist.get_rank()
https://github.com/django/django/tree/master/django/http/multipartparser.py,"def parse(self):
        """"""
        Parse the POST data and break it into a FILES MultiValueDict and a POST
        MultiValueDict.

        Return a tuple containing the POST and FILES dictionary, respectively.
        """"""
        from django.http import QueryDict

        encoding = self._encoding
        handlers = self._upload_handlers

        # HTTP spec says that Content-Length >= 0 is valid
        # handling content-length == 0 before continuing
        if self._content_length == 0:
            return QueryDict(encoding=self._encoding), MultiValueDict()

        # See if any of the handlers take care of the parsing.
        # This allows overriding everything if need be.
        for handler in handlers:
            result = handler.handle_raw_input(
                self._input_data,
                self._meta,
                self._content_length,
                self._boundary,
                encoding,
            )
            # Check to see if it was handled
            if result is not None:
                return result[0], result[1]

        # Create the data structures to be used later.
        self._post = QueryDict(mutable=True)
        self._files = MultiValueDict()

        # Instantiate the parser and stream:
        stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))

        # Whether or not to signal a file-completion at the beginning of the loop.
        old_field_name = None
        counters = [0] * len(handlers)

        # Number of bytes that have been read.
        num_bytes_read = 0
        # To count the number of keys in the request.
        num_post_keys = 0
        # To limit the amount of data read from the request.
        read_size = None
        # Whether a file upload is finished.
        uploaded_file = True

        try:
            for item_type, meta_data, field_stream in Parser(stream, self._boundary):
                if old_field_name:
                    # We run this at the beginning of the next loop
                    # since we cannot be sure a file is complete until
                    # we hit the next boundary/part of the multipart content.
                    self.handle_file_complete(old_field_name, counters)
                    old_field_name = None
                    uploaded_file = True

                try:
                    disposition = meta_data['content-disposition'][1]
                    field_name = disposition['name'].strip()
                except (KeyError, IndexError, AttributeError):
                    continue

                transfer_encoding = meta_data.get('content-transfer-encoding')
                if transfer_encoding is not None:
                    transfer_encoding = transfer_encoding[0].strip()
                field_name = force_str(field_name, encoding, errors='replace')

                if item_type == FIELD:
                    # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.
                    num_post_keys += 1
                    if (settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None and
                            settings.DATA_UPLOAD_MAX_NUMBER_FIELDS < num_post_keys):
                        raise TooManyFieldsSent(
                            'The number of GET/POST parameters exceeded '
                            'settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.'
                        )

                    # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.
                    if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:
                        read_size = settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read

                    # This is a post field, we can just set it in the post
                    if transfer_encoding == 'base64':
                        raw_data = field_stream.read(size=read_size)
                        num_bytes_read += len(raw_data)
                        try:
                            data = base64.b64decode(raw_data)
                        except binascii.Error:
                            data = raw_data
                    else:
                        data = field_stream.read(size=read_size)
                        num_bytes_read += len(data)

                    # Add two here to make the check consistent with the
                    # x-www-form-urlencoded check that includes '&='.
                    num_bytes_read += len(field_name) + 2
                    if (settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None and
                            num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE):
                        raise RequestDataTooBig('Request body exceeded settings.DATA_UPLOAD_MAX_MEMORY_SIZE.')

                    self._post.appendlist(field_name, force_str(data, encoding, errors='replace'))
                elif item_type == FILE:
                    # This is a file, use the handler...
                    file_name = disposition.get('filename')
                    if file_name:
                        file_name = force_str(file_name, encoding, errors='replace')
                        file_name = self.sanitize_file_name(file_name)
                    if not file_name:
                        continue

                    content_type, content_type_extra = meta_data.get('content-type', ('', {}))
                    content_type = content_type.strip()
                    charset = content_type_extra.get('charset')

                    try:
                        content_length = int(meta_data.get('content-length')[0])
                    except (IndexError, TypeError, ValueError):
                        content_length = None

                    counters = [0] * len(handlers)
                    uploaded_file = False
                    try:
                        for handler in handlers:
                            try:
                                handler.new_file(
                                    field_name, file_name, content_type,
                                    content_length, charset, content_type_extra,
                                )
                            except StopFutureHandlers:
                                break

                        for chunk in field_stream:
                            if transfer_encoding == 'base64':
                                # We only special-case base64 transfer encoding
                                # We should always decode base64 chunks by multiple of 4,
                                # ignoring whitespace.

                                stripped_chunk = b"""".join(chunk.split())

                                remaining = len(stripped_chunk) % 4
                                while remaining != 0:
                                    over_chunk = field_stream.read(4 - remaining)
                                    stripped_chunk += b"""".join(over_chunk.split())
                                    remaining = len(stripped_chunk) % 4

                                try:
                                    chunk = base64.b64decode(stripped_chunk)
                                except Exception as exc:
                                    # Since this is only a chunk, any error is an unfixable error.
                                    raise MultiPartParserError(""Could not decode base64 data."") from exc

                            for i, handler in enumerate(handlers):
                                chunk_length = len(chunk)
                                chunk = handler.receive_data_chunk(chunk, counters[i])
                                counters[i] += chunk_length
                                if chunk is None:
                                    # Don't continue if the chunk received by
                                    # the handler is None.
                                    break

                    except SkipFile:
                        self._close_files()
                        # Just use up the rest of this file...
                        exhaust(field_stream)
                    else:
                        # Handle file upload completions on next iteration.
                        old_field_name = field_name
                else:
                    # If this is neither a FIELD or a FILE, just exhaust the stream.
                    exhaust(stream)
        except StopUpload as e:
            self._close_files()
            if not e.connection_reset:
                exhaust(self._input_data)
        else:
            if not uploaded_file:
                for handler in handlers:
                    handler.upload_interrupted()
            # Make sure that the request data is all fed
            exhaust(self._input_data)

        # Signal that the upload has completed.
        # any() shortcircuits if a handler's upload_complete() returns a value.
        any(handler.upload_complete() for handler in handlers)
        self._post._mutable = False
        return self._post, self._files",_896.py,15,self._content_length == 0,not self._content_length
https://github.com/django/django/tree/master/django/http/multipartparser.py,"def parse(self):
        """"""
        Parse the POST data and break it into a FILES MultiValueDict and a POST
        MultiValueDict.

        Return a tuple containing the POST and FILES dictionary, respectively.
        """"""
        from django.http import QueryDict

        encoding = self._encoding
        handlers = self._upload_handlers

        # HTTP spec says that Content-Length >= 0 is valid
        # handling content-length == 0 before continuing
        if self._content_length == 0:
            return QueryDict(encoding=self._encoding), MultiValueDict()

        # See if any of the handlers take care of the parsing.
        # This allows overriding everything if need be.
        for handler in handlers:
            result = handler.handle_raw_input(
                self._input_data,
                self._meta,
                self._content_length,
                self._boundary,
                encoding,
            )
            # Check to see if it was handled
            if result is not None:
                return result[0], result[1]

        # Create the data structures to be used later.
        self._post = QueryDict(mutable=True)
        self._files = MultiValueDict()

        # Instantiate the parser and stream:
        stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))

        # Whether or not to signal a file-completion at the beginning of the loop.
        old_field_name = None
        counters = [0] * len(handlers)

        # Number of bytes that have been read.
        num_bytes_read = 0
        # To count the number of keys in the request.
        num_post_keys = 0
        # To limit the amount of data read from the request.
        read_size = None
        # Whether a file upload is finished.
        uploaded_file = True

        try:
            for item_type, meta_data, field_stream in Parser(stream, self._boundary):
                if old_field_name:
                    # We run this at the beginning of the next loop
                    # since we cannot be sure a file is complete until
                    # we hit the next boundary/part of the multipart content.
                    self.handle_file_complete(old_field_name, counters)
                    old_field_name = None
                    uploaded_file = True

                try:
                    disposition = meta_data['content-disposition'][1]
                    field_name = disposition['name'].strip()
                except (KeyError, IndexError, AttributeError):
                    continue

                transfer_encoding = meta_data.get('content-transfer-encoding')
                if transfer_encoding is not None:
                    transfer_encoding = transfer_encoding[0].strip()
                field_name = force_str(field_name, encoding, errors='replace')

                if item_type == FIELD:
                    # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.
                    num_post_keys += 1
                    if (settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None and
                            settings.DATA_UPLOAD_MAX_NUMBER_FIELDS < num_post_keys):
                        raise TooManyFieldsSent(
                            'The number of GET/POST parameters exceeded '
                            'settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.'
                        )

                    # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.
                    if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:
                        read_size = settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read

                    # This is a post field, we can just set it in the post
                    if transfer_encoding == 'base64':
                        raw_data = field_stream.read(size=read_size)
                        num_bytes_read += len(raw_data)
                        try:
                            data = base64.b64decode(raw_data)
                        except binascii.Error:
                            data = raw_data
                    else:
                        data = field_stream.read(size=read_size)
                        num_bytes_read += len(data)

                    # Add two here to make the check consistent with the
                    # x-www-form-urlencoded check that includes '&='.
                    num_bytes_read += len(field_name) + 2
                    if (settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None and
                            num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE):
                        raise RequestDataTooBig('Request body exceeded settings.DATA_UPLOAD_MAX_MEMORY_SIZE.')

                    self._post.appendlist(field_name, force_str(data, encoding, errors='replace'))
                elif item_type == FILE:
                    # This is a file, use the handler...
                    file_name = disposition.get('filename')
                    if file_name:
                        file_name = force_str(file_name, encoding, errors='replace')
                        file_name = self.sanitize_file_name(file_name)
                    if not file_name:
                        continue

                    content_type, content_type_extra = meta_data.get('content-type', ('', {}))
                    content_type = content_type.strip()
                    charset = content_type_extra.get('charset')

                    try:
                        content_length = int(meta_data.get('content-length')[0])
                    except (IndexError, TypeError, ValueError):
                        content_length = None

                    counters = [0] * len(handlers)
                    uploaded_file = False
                    try:
                        for handler in handlers:
                            try:
                                handler.new_file(
                                    field_name, file_name, content_type,
                                    content_length, charset, content_type_extra,
                                )
                            except StopFutureHandlers:
                                break

                        for chunk in field_stream:
                            if transfer_encoding == 'base64':
                                # We only special-case base64 transfer encoding
                                # We should always decode base64 chunks by multiple of 4,
                                # ignoring whitespace.

                                stripped_chunk = b"""".join(chunk.split())

                                remaining = len(stripped_chunk) % 4
                                while remaining != 0:
                                    over_chunk = field_stream.read(4 - remaining)
                                    stripped_chunk += b"""".join(over_chunk.split())
                                    remaining = len(stripped_chunk) % 4

                                try:
                                    chunk = base64.b64decode(stripped_chunk)
                                except Exception as exc:
                                    # Since this is only a chunk, any error is an unfixable error.
                                    raise MultiPartParserError(""Could not decode base64 data."") from exc

                            for i, handler in enumerate(handlers):
                                chunk_length = len(chunk)
                                chunk = handler.receive_data_chunk(chunk, counters[i])
                                counters[i] += chunk_length
                                if chunk is None:
                                    # Don't continue if the chunk received by
                                    # the handler is None.
                                    break

                    except SkipFile:
                        self._close_files()
                        # Just use up the rest of this file...
                        exhaust(field_stream)
                    else:
                        # Handle file upload completions on next iteration.
                        old_field_name = field_name
                else:
                    # If this is neither a FIELD or a FILE, just exhaust the stream.
                    exhaust(stream)
        except StopUpload as e:
            self._close_files()
            if not e.connection_reset:
                exhaust(self._input_data)
        else:
            if not uploaded_file:
                for handler in handlers:
                    handler.upload_interrupted()
            # Make sure that the request data is all fed
            exhaust(self._input_data)

        # Signal that the upload has completed.
        # any() shortcircuits if a handler's upload_complete() returns a value.
        any(handler.upload_complete() for handler in handlers)
        self._post._mutable = False
        return self._post, self._files",_896.py,146,remaining != 0,remaining
https://github.com/shuup/shuup/tree/master/shuup_tests/discounts/test_happy_hours.py,"def test_happy_hour(rf):
    happy_hour = init_test()

    discount = happy_hour.discounts.first()
    shop = discount.shop
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    w_today = timezone.now().date().weekday()
    w_tomorrow = (timezone.now() + datetime.timedelta(days=1)).date().weekday()
    w_future = (timezone.now() + datetime.timedelta(days=2)).date().weekday()
    matching_days = "","".join(map(str, [w_today]))
    non_matching_days = "","".join(map(str, [w_tomorrow, w_future]))

    # Matching time range
    hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    hour_end = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    set_valid_times_condition(happy_hour, hour_start, hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, hour_start, hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Hour end shouldn't cause a match. Should be obvious that if the
    # merchant set start time 8:00 AM and end time 10:00 AM th campaign is no more
    # at 10:10 AM
    new_hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    new_hour_end = timezone.now().time()  # 10:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in future shouldn't match
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=4)).time()  # 14:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in past shouldn't match
    new_hour_start = (timezone.now() - datetime.timedelta(hours=3)).time()  # 7:00 AM
    new_hour_end = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should match)
    new_hour_start = timezone.now().time()  # 10:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should not match)
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Lastly few timezone tests (LA it is monday and time is 2:00 AM.)
    with override_settings(TIME_ZONE=""America/Los_Angeles""):
        # Timezone needs to be activated to current one because some old timezone can still be active
        timezone.activate(pytz.timezone(""America/Los_Angeles""))

        # So the 10:00 AM shouldn't match at all
        new_hour_start = (timezone.now() - datetime.timedelta(hours=1)).time()  # 9:00 AM
        new_hour_end = (timezone.now() + datetime.timedelta(hours=1)).time()  # 11:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0

        # Instead around 2:00 AM we will find a match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=7)).time()  # 3:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 1
        assert Discount.objects.available(shop).count() == 1

        # Make sure that the hour end doesn't cause match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=8)).time()  # 2:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0",_913.py,23,Discount.objects.available().count() == 0,not Discount.objects.available().count()
https://github.com/shuup/shuup/tree/master/shuup_tests/discounts/test_happy_hours.py,"def test_happy_hour(rf):
    happy_hour = init_test()

    discount = happy_hour.discounts.first()
    shop = discount.shop
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    w_today = timezone.now().date().weekday()
    w_tomorrow = (timezone.now() + datetime.timedelta(days=1)).date().weekday()
    w_future = (timezone.now() + datetime.timedelta(days=2)).date().weekday()
    matching_days = "","".join(map(str, [w_today]))
    non_matching_days = "","".join(map(str, [w_tomorrow, w_future]))

    # Matching time range
    hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    hour_end = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    set_valid_times_condition(happy_hour, hour_start, hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, hour_start, hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Hour end shouldn't cause a match. Should be obvious that if the
    # merchant set start time 8:00 AM and end time 10:00 AM th campaign is no more
    # at 10:10 AM
    new_hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    new_hour_end = timezone.now().time()  # 10:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in future shouldn't match
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=4)).time()  # 14:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in past shouldn't match
    new_hour_start = (timezone.now() - datetime.timedelta(hours=3)).time()  # 7:00 AM
    new_hour_end = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should match)
    new_hour_start = timezone.now().time()  # 10:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should not match)
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Lastly few timezone tests (LA it is monday and time is 2:00 AM.)
    with override_settings(TIME_ZONE=""America/Los_Angeles""):
        # Timezone needs to be activated to current one because some old timezone can still be active
        timezone.activate(pytz.timezone(""America/Los_Angeles""))

        # So the 10:00 AM shouldn't match at all
        new_hour_start = (timezone.now() - datetime.timedelta(hours=1)).time()  # 9:00 AM
        new_hour_end = (timezone.now() + datetime.timedelta(hours=1)).time()  # 11:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0

        # Instead around 2:00 AM we will find a match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=7)).time()  # 3:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 1
        assert Discount.objects.available(shop).count() == 1

        # Make sure that the hour end doesn't cause match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=8)).time()  # 2:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0",_913.py,24,Discount.objects.available(shop).count() == 0,not Discount.objects.available(shop).count()
https://github.com/shuup/shuup/tree/master/shuup_tests/discounts/test_happy_hours.py,"def test_happy_hour(rf):
    happy_hour = init_test()

    discount = happy_hour.discounts.first()
    shop = discount.shop
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    w_today = timezone.now().date().weekday()
    w_tomorrow = (timezone.now() + datetime.timedelta(days=1)).date().weekday()
    w_future = (timezone.now() + datetime.timedelta(days=2)).date().weekday()
    matching_days = "","".join(map(str, [w_today]))
    non_matching_days = "","".join(map(str, [w_tomorrow, w_future]))

    # Matching time range
    hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    hour_end = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    set_valid_times_condition(happy_hour, hour_start, hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, hour_start, hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Hour end shouldn't cause a match. Should be obvious that if the
    # merchant set start time 8:00 AM and end time 10:00 AM th campaign is no more
    # at 10:10 AM
    new_hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    new_hour_end = timezone.now().time()  # 10:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in future shouldn't match
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=4)).time()  # 14:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in past shouldn't match
    new_hour_start = (timezone.now() - datetime.timedelta(hours=3)).time()  # 7:00 AM
    new_hour_end = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should match)
    new_hour_start = timezone.now().time()  # 10:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should not match)
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Lastly few timezone tests (LA it is monday and time is 2:00 AM.)
    with override_settings(TIME_ZONE=""America/Los_Angeles""):
        # Timezone needs to be activated to current one because some old timezone can still be active
        timezone.activate(pytz.timezone(""America/Los_Angeles""))

        # So the 10:00 AM shouldn't match at all
        new_hour_start = (timezone.now() - datetime.timedelta(hours=1)).time()  # 9:00 AM
        new_hour_end = (timezone.now() + datetime.timedelta(hours=1)).time()  # 11:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0

        # Instead around 2:00 AM we will find a match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=7)).time()  # 3:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 1
        assert Discount.objects.available(shop).count() == 1

        # Make sure that the hour end doesn't cause match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=8)).time()  # 2:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0",_913.py,32,Discount.objects.available().count() == 0,not Discount.objects.available().count()
https://github.com/shuup/shuup/tree/master/shuup_tests/discounts/test_happy_hours.py,"def test_happy_hour(rf):
    happy_hour = init_test()

    discount = happy_hour.discounts.first()
    shop = discount.shop
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    w_today = timezone.now().date().weekday()
    w_tomorrow = (timezone.now() + datetime.timedelta(days=1)).date().weekday()
    w_future = (timezone.now() + datetime.timedelta(days=2)).date().weekday()
    matching_days = "","".join(map(str, [w_today]))
    non_matching_days = "","".join(map(str, [w_tomorrow, w_future]))

    # Matching time range
    hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    hour_end = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    set_valid_times_condition(happy_hour, hour_start, hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, hour_start, hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Hour end shouldn't cause a match. Should be obvious that if the
    # merchant set start time 8:00 AM and end time 10:00 AM th campaign is no more
    # at 10:10 AM
    new_hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    new_hour_end = timezone.now().time()  # 10:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in future shouldn't match
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=4)).time()  # 14:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in past shouldn't match
    new_hour_start = (timezone.now() - datetime.timedelta(hours=3)).time()  # 7:00 AM
    new_hour_end = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should match)
    new_hour_start = timezone.now().time()  # 10:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should not match)
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Lastly few timezone tests (LA it is monday and time is 2:00 AM.)
    with override_settings(TIME_ZONE=""America/Los_Angeles""):
        # Timezone needs to be activated to current one because some old timezone can still be active
        timezone.activate(pytz.timezone(""America/Los_Angeles""))

        # So the 10:00 AM shouldn't match at all
        new_hour_start = (timezone.now() - datetime.timedelta(hours=1)).time()  # 9:00 AM
        new_hour_end = (timezone.now() + datetime.timedelta(hours=1)).time()  # 11:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0

        # Instead around 2:00 AM we will find a match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=7)).time()  # 3:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 1
        assert Discount.objects.available(shop).count() == 1

        # Make sure that the hour end doesn't cause match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=8)).time()  # 2:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0",_913.py,33,Discount.objects.available(shop).count() == 0,not Discount.objects.available(shop).count()
https://github.com/shuup/shuup/tree/master/shuup_tests/discounts/test_happy_hours.py,"def test_happy_hour(rf):
    happy_hour = init_test()

    discount = happy_hour.discounts.first()
    shop = discount.shop
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    w_today = timezone.now().date().weekday()
    w_tomorrow = (timezone.now() + datetime.timedelta(days=1)).date().weekday()
    w_future = (timezone.now() + datetime.timedelta(days=2)).date().weekday()
    matching_days = "","".join(map(str, [w_today]))
    non_matching_days = "","".join(map(str, [w_tomorrow, w_future]))

    # Matching time range
    hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    hour_end = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    set_valid_times_condition(happy_hour, hour_start, hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, hour_start, hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Hour end shouldn't cause a match. Should be obvious that if the
    # merchant set start time 8:00 AM and end time 10:00 AM th campaign is no more
    # at 10:10 AM
    new_hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    new_hour_end = timezone.now().time()  # 10:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in future shouldn't match
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=4)).time()  # 14:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in past shouldn't match
    new_hour_start = (timezone.now() - datetime.timedelta(hours=3)).time()  # 7:00 AM
    new_hour_end = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should match)
    new_hour_start = timezone.now().time()  # 10:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should not match)
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Lastly few timezone tests (LA it is monday and time is 2:00 AM.)
    with override_settings(TIME_ZONE=""America/Los_Angeles""):
        # Timezone needs to be activated to current one because some old timezone can still be active
        timezone.activate(pytz.timezone(""America/Los_Angeles""))

        # So the 10:00 AM shouldn't match at all
        new_hour_start = (timezone.now() - datetime.timedelta(hours=1)).time()  # 9:00 AM
        new_hour_end = (timezone.now() + datetime.timedelta(hours=1)).time()  # 11:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0

        # Instead around 2:00 AM we will find a match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=7)).time()  # 3:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 1
        assert Discount.objects.available(shop).count() == 1

        # Make sure that the hour end doesn't cause match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=8)).time()  # 2:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0",_913.py,39,Discount.objects.available().count() == 0,not Discount.objects.available().count()
https://github.com/shuup/shuup/tree/master/shuup_tests/discounts/test_happy_hours.py,"def test_happy_hour(rf):
    happy_hour = init_test()

    discount = happy_hour.discounts.first()
    shop = discount.shop
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    w_today = timezone.now().date().weekday()
    w_tomorrow = (timezone.now() + datetime.timedelta(days=1)).date().weekday()
    w_future = (timezone.now() + datetime.timedelta(days=2)).date().weekday()
    matching_days = "","".join(map(str, [w_today]))
    non_matching_days = "","".join(map(str, [w_tomorrow, w_future]))

    # Matching time range
    hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    hour_end = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    set_valid_times_condition(happy_hour, hour_start, hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, hour_start, hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Hour end shouldn't cause a match. Should be obvious that if the
    # merchant set start time 8:00 AM and end time 10:00 AM th campaign is no more
    # at 10:10 AM
    new_hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    new_hour_end = timezone.now().time()  # 10:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in future shouldn't match
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=4)).time()  # 14:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in past shouldn't match
    new_hour_start = (timezone.now() - datetime.timedelta(hours=3)).time()  # 7:00 AM
    new_hour_end = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should match)
    new_hour_start = timezone.now().time()  # 10:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should not match)
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Lastly few timezone tests (LA it is monday and time is 2:00 AM.)
    with override_settings(TIME_ZONE=""America/Los_Angeles""):
        # Timezone needs to be activated to current one because some old timezone can still be active
        timezone.activate(pytz.timezone(""America/Los_Angeles""))

        # So the 10:00 AM shouldn't match at all
        new_hour_start = (timezone.now() - datetime.timedelta(hours=1)).time()  # 9:00 AM
        new_hour_end = (timezone.now() + datetime.timedelta(hours=1)).time()  # 11:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0

        # Instead around 2:00 AM we will find a match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=7)).time()  # 3:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 1
        assert Discount.objects.available(shop).count() == 1

        # Make sure that the hour end doesn't cause match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=8)).time()  # 2:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0",_913.py,40,Discount.objects.available(shop).count() == 0,not Discount.objects.available(shop).count()
https://github.com/shuup/shuup/tree/master/shuup_tests/discounts/test_happy_hours.py,"def test_happy_hour(rf):
    happy_hour = init_test()

    discount = happy_hour.discounts.first()
    shop = discount.shop
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    w_today = timezone.now().date().weekday()
    w_tomorrow = (timezone.now() + datetime.timedelta(days=1)).date().weekday()
    w_future = (timezone.now() + datetime.timedelta(days=2)).date().weekday()
    matching_days = "","".join(map(str, [w_today]))
    non_matching_days = "","".join(map(str, [w_tomorrow, w_future]))

    # Matching time range
    hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    hour_end = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    set_valid_times_condition(happy_hour, hour_start, hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, hour_start, hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Hour end shouldn't cause a match. Should be obvious that if the
    # merchant set start time 8:00 AM and end time 10:00 AM th campaign is no more
    # at 10:10 AM
    new_hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    new_hour_end = timezone.now().time()  # 10:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in future shouldn't match
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=4)).time()  # 14:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in past shouldn't match
    new_hour_start = (timezone.now() - datetime.timedelta(hours=3)).time()  # 7:00 AM
    new_hour_end = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should match)
    new_hour_start = timezone.now().time()  # 10:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should not match)
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Lastly few timezone tests (LA it is monday and time is 2:00 AM.)
    with override_settings(TIME_ZONE=""America/Los_Angeles""):
        # Timezone needs to be activated to current one because some old timezone can still be active
        timezone.activate(pytz.timezone(""America/Los_Angeles""))

        # So the 10:00 AM shouldn't match at all
        new_hour_start = (timezone.now() - datetime.timedelta(hours=1)).time()  # 9:00 AM
        new_hour_end = (timezone.now() + datetime.timedelta(hours=1)).time()  # 11:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0

        # Instead around 2:00 AM we will find a match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=7)).time()  # 3:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 1
        assert Discount.objects.available(shop).count() == 1

        # Make sure that the hour end doesn't cause match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=8)).time()  # 2:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0",_913.py,46,Discount.objects.available().count() == 0,not Discount.objects.available().count()
https://github.com/shuup/shuup/tree/master/shuup_tests/discounts/test_happy_hours.py,"def test_happy_hour(rf):
    happy_hour = init_test()

    discount = happy_hour.discounts.first()
    shop = discount.shop
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    w_today = timezone.now().date().weekday()
    w_tomorrow = (timezone.now() + datetime.timedelta(days=1)).date().weekday()
    w_future = (timezone.now() + datetime.timedelta(days=2)).date().weekday()
    matching_days = "","".join(map(str, [w_today]))
    non_matching_days = "","".join(map(str, [w_tomorrow, w_future]))

    # Matching time range
    hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    hour_end = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    set_valid_times_condition(happy_hour, hour_start, hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, hour_start, hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Hour end shouldn't cause a match. Should be obvious that if the
    # merchant set start time 8:00 AM and end time 10:00 AM th campaign is no more
    # at 10:10 AM
    new_hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    new_hour_end = timezone.now().time()  # 10:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in future shouldn't match
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=4)).time()  # 14:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in past shouldn't match
    new_hour_start = (timezone.now() - datetime.timedelta(hours=3)).time()  # 7:00 AM
    new_hour_end = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should match)
    new_hour_start = timezone.now().time()  # 10:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should not match)
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Lastly few timezone tests (LA it is monday and time is 2:00 AM.)
    with override_settings(TIME_ZONE=""America/Los_Angeles""):
        # Timezone needs to be activated to current one because some old timezone can still be active
        timezone.activate(pytz.timezone(""America/Los_Angeles""))

        # So the 10:00 AM shouldn't match at all
        new_hour_start = (timezone.now() - datetime.timedelta(hours=1)).time()  # 9:00 AM
        new_hour_end = (timezone.now() + datetime.timedelta(hours=1)).time()  # 11:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0

        # Instead around 2:00 AM we will find a match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=7)).time()  # 3:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 1
        assert Discount.objects.available(shop).count() == 1

        # Make sure that the hour end doesn't cause match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=8)).time()  # 2:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0",_913.py,47,Discount.objects.available(shop).count() == 0,not Discount.objects.available(shop).count()
https://github.com/shuup/shuup/tree/master/shuup_tests/discounts/test_happy_hours.py,"def test_happy_hour(rf):
    happy_hour = init_test()

    discount = happy_hour.discounts.first()
    shop = discount.shop
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    w_today = timezone.now().date().weekday()
    w_tomorrow = (timezone.now() + datetime.timedelta(days=1)).date().weekday()
    w_future = (timezone.now() + datetime.timedelta(days=2)).date().weekday()
    matching_days = "","".join(map(str, [w_today]))
    non_matching_days = "","".join(map(str, [w_tomorrow, w_future]))

    # Matching time range
    hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    hour_end = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    set_valid_times_condition(happy_hour, hour_start, hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, hour_start, hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Hour end shouldn't cause a match. Should be obvious that if the
    # merchant set start time 8:00 AM and end time 10:00 AM th campaign is no more
    # at 10:10 AM
    new_hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    new_hour_end = timezone.now().time()  # 10:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in future shouldn't match
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=4)).time()  # 14:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in past shouldn't match
    new_hour_start = (timezone.now() - datetime.timedelta(hours=3)).time()  # 7:00 AM
    new_hour_end = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should match)
    new_hour_start = timezone.now().time()  # 10:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should not match)
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Lastly few timezone tests (LA it is monday and time is 2:00 AM.)
    with override_settings(TIME_ZONE=""America/Los_Angeles""):
        # Timezone needs to be activated to current one because some old timezone can still be active
        timezone.activate(pytz.timezone(""America/Los_Angeles""))

        # So the 10:00 AM shouldn't match at all
        new_hour_start = (timezone.now() - datetime.timedelta(hours=1)).time()  # 9:00 AM
        new_hour_end = (timezone.now() + datetime.timedelta(hours=1)).time()  # 11:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0

        # Instead around 2:00 AM we will find a match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=7)).time()  # 3:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 1
        assert Discount.objects.available(shop).count() == 1

        # Make sure that the hour end doesn't cause match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=8)).time()  # 2:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0",_913.py,57,Discount.objects.available().count() == 0,not Discount.objects.available().count()
https://github.com/shuup/shuup/tree/master/shuup_tests/discounts/test_happy_hours.py,"def test_happy_hour(rf):
    happy_hour = init_test()

    discount = happy_hour.discounts.first()
    shop = discount.shop
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    w_today = timezone.now().date().weekday()
    w_tomorrow = (timezone.now() + datetime.timedelta(days=1)).date().weekday()
    w_future = (timezone.now() + datetime.timedelta(days=2)).date().weekday()
    matching_days = "","".join(map(str, [w_today]))
    non_matching_days = "","".join(map(str, [w_tomorrow, w_future]))

    # Matching time range
    hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    hour_end = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    set_valid_times_condition(happy_hour, hour_start, hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, hour_start, hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Hour end shouldn't cause a match. Should be obvious that if the
    # merchant set start time 8:00 AM and end time 10:00 AM th campaign is no more
    # at 10:10 AM
    new_hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    new_hour_end = timezone.now().time()  # 10:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in future shouldn't match
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=4)).time()  # 14:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in past shouldn't match
    new_hour_start = (timezone.now() - datetime.timedelta(hours=3)).time()  # 7:00 AM
    new_hour_end = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should match)
    new_hour_start = timezone.now().time()  # 10:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should not match)
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Lastly few timezone tests (LA it is monday and time is 2:00 AM.)
    with override_settings(TIME_ZONE=""America/Los_Angeles""):
        # Timezone needs to be activated to current one because some old timezone can still be active
        timezone.activate(pytz.timezone(""America/Los_Angeles""))

        # So the 10:00 AM shouldn't match at all
        new_hour_start = (timezone.now() - datetime.timedelta(hours=1)).time()  # 9:00 AM
        new_hour_end = (timezone.now() + datetime.timedelta(hours=1)).time()  # 11:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0

        # Instead around 2:00 AM we will find a match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=7)).time()  # 3:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 1
        assert Discount.objects.available(shop).count() == 1

        # Make sure that the hour end doesn't cause match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=8)).time()  # 2:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0",_913.py,58,Discount.objects.available(shop).count() == 0,not Discount.objects.available(shop).count()
https://github.com/shuup/shuup/tree/master/shuup_tests/discounts/test_happy_hours.py,"def test_happy_hour(rf):
    happy_hour = init_test()

    discount = happy_hour.discounts.first()
    shop = discount.shop
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    w_today = timezone.now().date().weekday()
    w_tomorrow = (timezone.now() + datetime.timedelta(days=1)).date().weekday()
    w_future = (timezone.now() + datetime.timedelta(days=2)).date().weekday()
    matching_days = "","".join(map(str, [w_today]))
    non_matching_days = "","".join(map(str, [w_tomorrow, w_future]))

    # Matching time range
    hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    hour_end = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    set_valid_times_condition(happy_hour, hour_start, hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, hour_start, hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Hour end shouldn't cause a match. Should be obvious that if the
    # merchant set start time 8:00 AM and end time 10:00 AM th campaign is no more
    # at 10:10 AM
    new_hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    new_hour_end = timezone.now().time()  # 10:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in future shouldn't match
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=4)).time()  # 14:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in past shouldn't match
    new_hour_start = (timezone.now() - datetime.timedelta(hours=3)).time()  # 7:00 AM
    new_hour_end = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should match)
    new_hour_start = timezone.now().time()  # 10:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should not match)
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Lastly few timezone tests (LA it is monday and time is 2:00 AM.)
    with override_settings(TIME_ZONE=""America/Los_Angeles""):
        # Timezone needs to be activated to current one because some old timezone can still be active
        timezone.activate(pytz.timezone(""America/Los_Angeles""))

        # So the 10:00 AM shouldn't match at all
        new_hour_start = (timezone.now() - datetime.timedelta(hours=1)).time()  # 9:00 AM
        new_hour_end = (timezone.now() + datetime.timedelta(hours=1)).time()  # 11:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0

        # Instead around 2:00 AM we will find a match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=7)).time()  # 3:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 1
        assert Discount.objects.available(shop).count() == 1

        # Make sure that the hour end doesn't cause match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=8)).time()  # 2:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0",_913.py,64,Discount.objects.available().count() == 0,not Discount.objects.available().count()
https://github.com/shuup/shuup/tree/master/shuup_tests/discounts/test_happy_hours.py,"def test_happy_hour(rf):
    happy_hour = init_test()

    discount = happy_hour.discounts.first()
    shop = discount.shop
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    w_today = timezone.now().date().weekday()
    w_tomorrow = (timezone.now() + datetime.timedelta(days=1)).date().weekday()
    w_future = (timezone.now() + datetime.timedelta(days=2)).date().weekday()
    matching_days = "","".join(map(str, [w_today]))
    non_matching_days = "","".join(map(str, [w_tomorrow, w_future]))

    # Matching time range
    hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    hour_end = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    set_valid_times_condition(happy_hour, hour_start, hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, hour_start, hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Hour end shouldn't cause a match. Should be obvious that if the
    # merchant set start time 8:00 AM and end time 10:00 AM th campaign is no more
    # at 10:10 AM
    new_hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    new_hour_end = timezone.now().time()  # 10:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in future shouldn't match
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=4)).time()  # 14:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in past shouldn't match
    new_hour_start = (timezone.now() - datetime.timedelta(hours=3)).time()  # 7:00 AM
    new_hour_end = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should match)
    new_hour_start = timezone.now().time()  # 10:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should not match)
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Lastly few timezone tests (LA it is monday and time is 2:00 AM.)
    with override_settings(TIME_ZONE=""America/Los_Angeles""):
        # Timezone needs to be activated to current one because some old timezone can still be active
        timezone.activate(pytz.timezone(""America/Los_Angeles""))

        # So the 10:00 AM shouldn't match at all
        new_hour_start = (timezone.now() - datetime.timedelta(hours=1)).time()  # 9:00 AM
        new_hour_end = (timezone.now() + datetime.timedelta(hours=1)).time()  # 11:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0

        # Instead around 2:00 AM we will find a match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=7)).time()  # 3:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 1
        assert Discount.objects.available(shop).count() == 1

        # Make sure that the hour end doesn't cause match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=8)).time()  # 2:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0",_913.py,65,Discount.objects.available(shop).count() == 0,not Discount.objects.available(shop).count()
https://github.com/shuup/shuup/tree/master/shuup_tests/discounts/test_happy_hours.py,"def test_happy_hour(rf):
    happy_hour = init_test()

    discount = happy_hour.discounts.first()
    shop = discount.shop
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    w_today = timezone.now().date().weekday()
    w_tomorrow = (timezone.now() + datetime.timedelta(days=1)).date().weekday()
    w_future = (timezone.now() + datetime.timedelta(days=2)).date().weekday()
    matching_days = "","".join(map(str, [w_today]))
    non_matching_days = "","".join(map(str, [w_tomorrow, w_future]))

    # Matching time range
    hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    hour_end = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    set_valid_times_condition(happy_hour, hour_start, hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, hour_start, hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Hour end shouldn't cause a match. Should be obvious that if the
    # merchant set start time 8:00 AM and end time 10:00 AM th campaign is no more
    # at 10:10 AM
    new_hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    new_hour_end = timezone.now().time()  # 10:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in future shouldn't match
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=4)).time()  # 14:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in past shouldn't match
    new_hour_start = (timezone.now() - datetime.timedelta(hours=3)).time()  # 7:00 AM
    new_hour_end = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should match)
    new_hour_start = timezone.now().time()  # 10:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should not match)
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Lastly few timezone tests (LA it is monday and time is 2:00 AM.)
    with override_settings(TIME_ZONE=""America/Los_Angeles""):
        # Timezone needs to be activated to current one because some old timezone can still be active
        timezone.activate(pytz.timezone(""America/Los_Angeles""))

        # So the 10:00 AM shouldn't match at all
        new_hour_start = (timezone.now() - datetime.timedelta(hours=1)).time()  # 9:00 AM
        new_hour_end = (timezone.now() + datetime.timedelta(hours=1)).time()  # 11:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0

        # Instead around 2:00 AM we will find a match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=7)).time()  # 3:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 1
        assert Discount.objects.available(shop).count() == 1

        # Make sure that the hour end doesn't cause match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=8)).time()  # 2:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0",_913.py,76,Discount.objects.available().count() == 0,not Discount.objects.available().count()
https://github.com/shuup/shuup/tree/master/shuup_tests/discounts/test_happy_hours.py,"def test_happy_hour(rf):
    happy_hour = init_test()

    discount = happy_hour.discounts.first()
    shop = discount.shop
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    w_today = timezone.now().date().weekday()
    w_tomorrow = (timezone.now() + datetime.timedelta(days=1)).date().weekday()
    w_future = (timezone.now() + datetime.timedelta(days=2)).date().weekday()
    matching_days = "","".join(map(str, [w_today]))
    non_matching_days = "","".join(map(str, [w_tomorrow, w_future]))

    # Matching time range
    hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    hour_end = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    set_valid_times_condition(happy_hour, hour_start, hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, hour_start, hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Hour end shouldn't cause a match. Should be obvious that if the
    # merchant set start time 8:00 AM and end time 10:00 AM th campaign is no more
    # at 10:10 AM
    new_hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    new_hour_end = timezone.now().time()  # 10:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in future shouldn't match
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=4)).time()  # 14:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in past shouldn't match
    new_hour_start = (timezone.now() - datetime.timedelta(hours=3)).time()  # 7:00 AM
    new_hour_end = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should match)
    new_hour_start = timezone.now().time()  # 10:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should not match)
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Lastly few timezone tests (LA it is monday and time is 2:00 AM.)
    with override_settings(TIME_ZONE=""America/Los_Angeles""):
        # Timezone needs to be activated to current one because some old timezone can still be active
        timezone.activate(pytz.timezone(""America/Los_Angeles""))

        # So the 10:00 AM shouldn't match at all
        new_hour_start = (timezone.now() - datetime.timedelta(hours=1)).time()  # 9:00 AM
        new_hour_end = (timezone.now() + datetime.timedelta(hours=1)).time()  # 11:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0

        # Instead around 2:00 AM we will find a match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=7)).time()  # 3:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 1
        assert Discount.objects.available(shop).count() == 1

        # Make sure that the hour end doesn't cause match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=8)).time()  # 2:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0",_913.py,77,Discount.objects.available(shop).count() == 0,not Discount.objects.available(shop).count()
https://github.com/shuup/shuup/tree/master/shuup_tests/discounts/test_happy_hours.py,"def test_happy_hour(rf):
    happy_hour = init_test()

    discount = happy_hour.discounts.first()
    shop = discount.shop
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    w_today = timezone.now().date().weekday()
    w_tomorrow = (timezone.now() + datetime.timedelta(days=1)).date().weekday()
    w_future = (timezone.now() + datetime.timedelta(days=2)).date().weekday()
    matching_days = "","".join(map(str, [w_today]))
    non_matching_days = "","".join(map(str, [w_tomorrow, w_future]))

    # Matching time range
    hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    hour_end = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    set_valid_times_condition(happy_hour, hour_start, hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, hour_start, hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Hour end shouldn't cause a match. Should be obvious that if the
    # merchant set start time 8:00 AM and end time 10:00 AM th campaign is no more
    # at 10:10 AM
    new_hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    new_hour_end = timezone.now().time()  # 10:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in future shouldn't match
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=4)).time()  # 14:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in past shouldn't match
    new_hour_start = (timezone.now() - datetime.timedelta(hours=3)).time()  # 7:00 AM
    new_hour_end = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should match)
    new_hour_start = timezone.now().time()  # 10:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should not match)
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Lastly few timezone tests (LA it is monday and time is 2:00 AM.)
    with override_settings(TIME_ZONE=""America/Los_Angeles""):
        # Timezone needs to be activated to current one because some old timezone can still be active
        timezone.activate(pytz.timezone(""America/Los_Angeles""))

        # So the 10:00 AM shouldn't match at all
        new_hour_start = (timezone.now() - datetime.timedelta(hours=1)).time()  # 9:00 AM
        new_hour_end = (timezone.now() + datetime.timedelta(hours=1)).time()  # 11:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0

        # Instead around 2:00 AM we will find a match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=7)).time()  # 3:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 1
        assert Discount.objects.available(shop).count() == 1

        # Make sure that the hour end doesn't cause match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=8)).time()  # 2:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0",_913.py,90,Discount.objects.available().count() == 0,not Discount.objects.available().count()
https://github.com/shuup/shuup/tree/master/shuup_tests/discounts/test_happy_hours.py,"def test_happy_hour(rf):
    happy_hour = init_test()

    discount = happy_hour.discounts.first()
    shop = discount.shop
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    w_today = timezone.now().date().weekday()
    w_tomorrow = (timezone.now() + datetime.timedelta(days=1)).date().weekday()
    w_future = (timezone.now() + datetime.timedelta(days=2)).date().weekday()
    matching_days = "","".join(map(str, [w_today]))
    non_matching_days = "","".join(map(str, [w_tomorrow, w_future]))

    # Matching time range
    hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    hour_end = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    set_valid_times_condition(happy_hour, hour_start, hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, hour_start, hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Hour end shouldn't cause a match. Should be obvious that if the
    # merchant set start time 8:00 AM and end time 10:00 AM th campaign is no more
    # at 10:10 AM
    new_hour_start = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    new_hour_end = timezone.now().time()  # 10:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in future shouldn't match
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 PM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=4)).time()  # 14:00 PM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # time in past shouldn't match
    new_hour_start = (timezone.now() - datetime.timedelta(hours=3)).time()  # 7:00 AM
    new_hour_end = (timezone.now() - datetime.timedelta(hours=2)).time()  # 8:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should match)
    new_hour_start = timezone.now().time()  # 10:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 1
    assert Discount.objects.available(shop).count() == 1

    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, non_matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Special times (should not match)
    new_hour_start = (timezone.now() + datetime.timedelta(hours=2)).time()  # 12:00 AM
    new_hour_end = (timezone.now() + datetime.timedelta(hours=14)).time()  # 0:00 AM
    set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
    assert Discount.objects.available().count() == 0
    assert Discount.objects.available(shop).count() == 0

    # Lastly few timezone tests (LA it is monday and time is 2:00 AM.)
    with override_settings(TIME_ZONE=""America/Los_Angeles""):
        # Timezone needs to be activated to current one because some old timezone can still be active
        timezone.activate(pytz.timezone(""America/Los_Angeles""))

        # So the 10:00 AM shouldn't match at all
        new_hour_start = (timezone.now() - datetime.timedelta(hours=1)).time()  # 9:00 AM
        new_hour_end = (timezone.now() + datetime.timedelta(hours=1)).time()  # 11:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0

        # Instead around 2:00 AM we will find a match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=7)).time()  # 3:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 1
        assert Discount.objects.available(shop).count() == 1

        # Make sure that the hour end doesn't cause match
        new_hour_start = (timezone.now() - datetime.timedelta(hours=9)).time()  # 1:00 AM
        new_hour_end = (timezone.now() - datetime.timedelta(hours=8)).time()  # 2:00 AM
        set_valid_times_condition(happy_hour, new_hour_start, new_hour_end, matching_days)
        assert Discount.objects.available().count() == 0
        assert Discount.objects.available(shop).count() == 0",_913.py,91,Discount.objects.available(shop).count() == 0,not Discount.objects.available(shop).count()
https://github.com/saltstack/salt/tree/master/tests/pytests/unit/beacons/test_btmp.py,"def test_no_match():
    config = [
        {
            ""users"": {
                ""gareth"": {
                    ""time_range"": {
                        ""end"": ""09-22-2017 5pm"",
                        ""start"": ""09-22-2017 3pm"",
                    }
                }
            }
        }
    ]

    ret = btmp.validate(config)

    assert ret == (True, ""Valid beacon configuration"")

    with patch(""salt.utils.files.fopen"", mock_open(b"""")) as m_open:
        ret = btmp.beacon(config)
        call_args = next(iter(m_open.filehandles.values()))[0].call.args
        assert call_args == (btmp.BTMP, ""rb""), call_args
        assert ret == [], ret",_944.py,23,ret == [],not ret
https://github.com/openstack/nova/tree/master/nova/utils.py,"def sanitize_hostname(hostname, default_name=None):
    """"""Sanitize a given hostname.

    Return a hostname which conforms to RFC-952 and RFC-1123 specs except the
    length of hostname. Window, Linux, and dnsmasq has different limitation:

    - Windows: 255 (net_bios limits to 15, but window will truncate it)
    - Linux: 64
    - dnsmasq: 63

    We choose the lowest of these (so 63).
    """"""

    def truncate_hostname(name):
        if len(name) > 63:
            LOG.warning(""Hostname %(hostname)s is longer than 63, ""
                        ""truncate it to %(truncated_name)s"",
                        {'hostname': name, 'truncated_name': name[:63]})
        return name[:63]

    if isinstance(hostname, str):
        # Remove characters outside the Unicode range U+0000-U+00FF
        hostname = hostname.encode('latin-1', 'ignore').decode('latin-1')

    hostname = truncate_hostname(hostname)
    hostname = re.sub(r'[ _\.]', '-', hostname)
    hostname = re.sub(r'[^\w.-]+', '', hostname)
    hostname = hostname.lower()
    hostname = hostname.strip('.-')
    # NOTE(eliqiao): set hostname to default_display_name to avoid
    # empty hostname
    if hostname == """" and default_name is not None:
        return truncate_hostname(default_name)
    return hostname",_1011.py,32,hostname == '',not hostname
https://github.com/DeepLabCut/DeepLabCut/tree/master/deeplabcut/pose_estimation_tensorflow/datasets/pose_deterministic.py,"def next_training_sample(self):
        if self.curr_img == 0 and self.shuffle:
            self.shuffle_images()

        curr_img = self.curr_img
        self.curr_img = (self.curr_img + 1) % self.num_training_samples()

        imidx = self.image_indices[curr_img]
        mirror = self.cfg[""mirror""] and self.mirrored[curr_img]

        return imidx, mirror",_1052.py,2,self.curr_img == 0,not self.curr_img
https://github.com/aerkalov/ebooklib/tree/master/ebooklib/utils.py,"def get_pages(item):
    body = parse_html_string(item.get_body_content())
    pages = []

    for elem in body.iter():
        if 'epub:type' in elem.attrib:
            if elem.get('id') is not None:
                _text = None
                
                if elem.text is not None and elem.text.strip() != '':
                    _text = elem.text.strip()

                if _text is None:
                    _text = elem.get('aria-label')

                if _text is None:
                    _text = get_headers(elem)

                pages.append((item.get_name(), elem.get('id'), _text or elem.get('id')))

    return pages",_1113.py,10,elem.text.strip() != '',elem.text.strip()
https://github.com/elyra-ai/elyra/tree/master/elyra/tests/pipeline/test_validation.py,"def test_valid_node_property_shared_mem_size(validation_manager):
    """"""
    Verify that valid shared memory definitions pass validation
    """"""
    response = ValidationResponse()
    node_dict = {""id"": ""test-id"", ""app_data"": {""label"": ""test"", ""ui_data"": {}, ""component_parameters"": {}}}

    # test size
    for size in [None, 0, 3.1415, 64]:
        shared_mem_size = CustomSharedMemorySize(size=size, units=""G"")
        node_dict[""app_data""][""component_parameters""][KUBERNETES_SHARED_MEM_SIZE] = shared_mem_size

        node = Node(node_dict)
        validation_manager._validate_elyra_owned_property(
            node_id=node.id, node_label=node.label, node=node, param_name=KUBERNETES_SHARED_MEM_SIZE, response=response
        )
        issues = response.to_json().get(""issues"")
        assert len(issues) == 0, issues

    # test units
    for unit in [""G"", None, """"]:
        shared_mem_size = CustomSharedMemorySize(size=0, units=unit)
        node_dict[""app_data""][""component_parameters""][KUBERNETES_SHARED_MEM_SIZE] = shared_mem_size

        node = Node(node_dict)
        validation_manager._validate_elyra_owned_property(
            node_id=node.id, node_label=node.label, node=node, param_name=KUBERNETES_SHARED_MEM_SIZE, response=response
        )
        issues = response.to_json().get(""issues"")
        assert len(issues) == 0, issues",_1120.py,18,len(issues) == 0,not len(issues)
https://github.com/elyra-ai/elyra/tree/master/elyra/tests/pipeline/test_validation.py,"def test_valid_node_property_shared_mem_size(validation_manager):
    """"""
    Verify that valid shared memory definitions pass validation
    """"""
    response = ValidationResponse()
    node_dict = {""id"": ""test-id"", ""app_data"": {""label"": ""test"", ""ui_data"": {}, ""component_parameters"": {}}}

    # test size
    for size in [None, 0, 3.1415, 64]:
        shared_mem_size = CustomSharedMemorySize(size=size, units=""G"")
        node_dict[""app_data""][""component_parameters""][KUBERNETES_SHARED_MEM_SIZE] = shared_mem_size

        node = Node(node_dict)
        validation_manager._validate_elyra_owned_property(
            node_id=node.id, node_label=node.label, node=node, param_name=KUBERNETES_SHARED_MEM_SIZE, response=response
        )
        issues = response.to_json().get(""issues"")
        assert len(issues) == 0, issues

    # test units
    for unit in [""G"", None, """"]:
        shared_mem_size = CustomSharedMemorySize(size=0, units=unit)
        node_dict[""app_data""][""component_parameters""][KUBERNETES_SHARED_MEM_SIZE] = shared_mem_size

        node = Node(node_dict)
        validation_manager._validate_elyra_owned_property(
            node_id=node.id, node_label=node.label, node=node, param_name=KUBERNETES_SHARED_MEM_SIZE, response=response
        )
        issues = response.to_json().get(""issues"")
        assert len(issues) == 0, issues",_1120.py,30,len(issues) == 0,not len(issues)
https://github.com/scwuaptx/Pwngdb/tree/master/angelheap/angelheap.py,"def infoprocmap():
    """""" Use gdb command 'info proc map' to get the memory mapping """"""
    """""" Notice: No permission info """"""
    resp = gdb.execute(""info proc map"", to_string=True).split(""\n"")
    resp = '\n'.join(resp[i] for i  in range(4, len(resp))).strip().split(""\n"")
    infomap = """"
    for l in resp:
        line = """"
        first = True
        for sep in l.split("" ""):
            if len(sep) != 0:
                if first: # start address
                    line += sep + ""-""
                    first = False
                else:
                    line += sep + "" ""
        line = line.strip() + ""\n""
        infomap += line
    return infomap",_1146.py,11,len(sep) != 0,len(sep)
https://github.com/shadowsocksr-backup/shadowsocksr/tree/master/shadowsocks/udprelay.py,"def _handle_server(self):
        server = self._server_socket
        data, r_addr = server.recvfrom(BUF_SIZE)
        ogn_data = data
        if not data:
            logging.debug('UDP handle_server: data is empty')
        if self._stat_callback:
            self._stat_callback(self._listen_port, len(data))
        if self._is_local:
            frag = common.ord(data[2])
            if frag != 0:
                logging.warn('drop a message since frag is not 0')
                return
            else:
                data = data[3:]
        else:
            data = encrypt.encrypt_all(self._password, self._method, 0, data)
            # decrypt data
            if not data:
                logging.debug('UDP handle_server: data is empty after decrypt')
                return

        #logging.info(""UDP data %s"" % (binascii.hexlify(data),))
        if not self._is_local:
            data = pre_parse_header(data)

            data = self._pre_parse_udp_header(data)
            if data is None:
                return

            if type(data) is tuple:
                #(cmd, request_id, data)
                #logging.info(""UDP data %d %d %s"" % (data[0], data[1], binascii.hexlify(data[2])))
                try:
                    if data[0] == 0:
                        if len(data[2]) >= 4:
                            for i in range(64):
                                req_id = random.randint(1, 65535)
                                if req_id not in self._reqid_to_hd:
                                    break
                            if req_id in self._reqid_to_hd:
                                for i in range(64):
                                    req_id = random.randint(1, 65535)
                                    if type(self._reqid_to_hd[req_id]) is tuple:
                                        break
                            # return req id
                            self._reqid_to_hd[req_id] = (data[2][0:4], None)
                            rsp_data = self._pack_rsp_data(CMD_RSP_CONNECT, req_id, RSP_STATE_CONNECTED)
                            data_to_send = encrypt.encrypt_all(self._password, self._method, 1, rsp_data)
                            self.write_to_server_socket(data_to_send, r_addr)
                    elif data[0] == CMD_CONNECT_REMOTE:
                        if len(data[2]) > 4 and data[1] in self._reqid_to_hd:
                            # create
                            if type(self._reqid_to_hd[data[1]]) is tuple:
                                if data[2][0:4] == self._reqid_to_hd[data[1]][0]:
                                    handle = TCPRelayHandler(self, self._reqid_to_hd, self._fd_to_handlers,
                                        self._eventloop, self._server_socket,
                                        self._reqid_to_hd[data[1]][0], self._reqid_to_hd[data[1]][1],
                                        self._config, self._dns_resolver, self._is_local)
                                    self._reqid_to_hd[data[1]] = handle
                                    handle.handle_client(r_addr, CMD_CONNECT, data[1], data[2])
                                    handle.handle_client(r_addr, *data)
                                    self.update_activity(handle)
                                else:
                                    # disconnect
                                    rsp_data = self._pack_rsp_data(CMD_DISCONNECT, data[1], RSP_STATE_EMPTY)
                                    data_to_send = encrypt.encrypt_all(self._password, self._method, 1, rsp_data)
                                    self.write_to_server_socket(data_to_send, r_addr)
                            else:
                                self.update_activity(self._reqid_to_hd[data[1]])
                                self._reqid_to_hd[data[1]].handle_client(r_addr, *data)
                        else:
                            # disconnect
                            rsp_data = self._pack_rsp_data(CMD_DISCONNECT, data[1], RSP_STATE_EMPTY)
                            data_to_send = encrypt.encrypt_all(self._password, self._method, 1, rsp_data)
                            self.write_to_server_socket(data_to_send, r_addr)
                    elif data[0] > CMD_CONNECT_REMOTE and data[0] <= CMD_DISCONNECT:
                        if data[1] in self._reqid_to_hd:
                            if type(self._reqid_to_hd[data[1]]) is tuple:
                                pass
                            else:
                                self.update_activity(self._reqid_to_hd[data[1]])
                                self._reqid_to_hd[data[1]].handle_client(r_addr, *data)
                        else:
                            # disconnect
                            rsp_data = self._pack_rsp_data(CMD_DISCONNECT, data[1], RSP_STATE_EMPTY)
                            data_to_send = encrypt.encrypt_all(self._password, self._method, 1, rsp_data)
                            self.write_to_server_socket(data_to_send, r_addr)
                    return
                except Exception as e:
                    trace = traceback.format_exc()
                    logging.error(trace)
                    return

        try:
            header_result = parse_header(data)
        except:
            self._handel_protocol_error(r_addr, ogn_data)
            return

        if header_result is None:
            self._handel_protocol_error(r_addr, ogn_data)
            return
        connecttype, dest_addr, dest_port, header_length = header_result

        if self._is_local:
            server_addr, server_port = self._get_a_server()
        else:
            server_addr, server_port = dest_addr, dest_port

        addrs = self._dns_cache.get(server_addr, None)
        if addrs is None:
            addrs = socket.getaddrinfo(server_addr, server_port, 0,
                                       socket.SOCK_DGRAM, socket.SOL_UDP)
            if not addrs:
                # drop
                return
            else:
                self._dns_cache[server_addr] = addrs

        af, socktype, proto, canonname, sa = addrs[0]
        key = client_key(r_addr, af)
        client = self._cache.get(key, None)
        if not client:
            # TODO async getaddrinfo
            if self._forbidden_iplist:
                if common.to_str(sa[0]) in self._forbidden_iplist:
                    logging.debug('IP %s is in forbidden list, drop' %
                                  common.to_str(sa[0]))
                    # drop
                    return
            client = socket.socket(af, socktype, proto)
            client.setblocking(False)
            self._cache[key] = client
            self._client_fd_to_server_addr[client.fileno()] = r_addr

            self._sockets.add(client.fileno())
            self._eventloop.add(client, eventloop.POLL_IN, self)

            logging.debug('UDP port %5d sockets %d' % (self._listen_port, len(self._sockets)))

            logging.info('UDP data to %s:%d from %s:%d' %
                        (common.to_str(server_addr), server_port,
                            r_addr[0], r_addr[1]))

        if self._is_local:
            data = encrypt.encrypt_all(self._password, self._method, 1, data)
            if not data:
                return
        else:
            data = data[header_length:]
        if not data:
            return
        try:
            #logging.info('UDP handle_server sendto %s:%d %d bytes' % (common.to_str(server_addr), server_port, len(data)))
            client.sendto(data, (server_addr, server_port))
        except IOError as e:
            err = eventloop.errno_from_exception(e)
            if err in (errno.EINPROGRESS, errno.EAGAIN):
                pass
            else:
                shell.print_exception(e)",_1154.py,11,frag != 0,frag
https://github.com/shadowsocksr-backup/shadowsocksr/tree/master/shadowsocks/udprelay.py,"def _handle_server(self):
        server = self._server_socket
        data, r_addr = server.recvfrom(BUF_SIZE)
        ogn_data = data
        if not data:
            logging.debug('UDP handle_server: data is empty')
        if self._stat_callback:
            self._stat_callback(self._listen_port, len(data))
        if self._is_local:
            frag = common.ord(data[2])
            if frag != 0:
                logging.warn('drop a message since frag is not 0')
                return
            else:
                data = data[3:]
        else:
            data = encrypt.encrypt_all(self._password, self._method, 0, data)
            # decrypt data
            if not data:
                logging.debug('UDP handle_server: data is empty after decrypt')
                return

        #logging.info(""UDP data %s"" % (binascii.hexlify(data),))
        if not self._is_local:
            data = pre_parse_header(data)

            data = self._pre_parse_udp_header(data)
            if data is None:
                return

            if type(data) is tuple:
                #(cmd, request_id, data)
                #logging.info(""UDP data %d %d %s"" % (data[0], data[1], binascii.hexlify(data[2])))
                try:
                    if data[0] == 0:
                        if len(data[2]) >= 4:
                            for i in range(64):
                                req_id = random.randint(1, 65535)
                                if req_id not in self._reqid_to_hd:
                                    break
                            if req_id in self._reqid_to_hd:
                                for i in range(64):
                                    req_id = random.randint(1, 65535)
                                    if type(self._reqid_to_hd[req_id]) is tuple:
                                        break
                            # return req id
                            self._reqid_to_hd[req_id] = (data[2][0:4], None)
                            rsp_data = self._pack_rsp_data(CMD_RSP_CONNECT, req_id, RSP_STATE_CONNECTED)
                            data_to_send = encrypt.encrypt_all(self._password, self._method, 1, rsp_data)
                            self.write_to_server_socket(data_to_send, r_addr)
                    elif data[0] == CMD_CONNECT_REMOTE:
                        if len(data[2]) > 4 and data[1] in self._reqid_to_hd:
                            # create
                            if type(self._reqid_to_hd[data[1]]) is tuple:
                                if data[2][0:4] == self._reqid_to_hd[data[1]][0]:
                                    handle = TCPRelayHandler(self, self._reqid_to_hd, self._fd_to_handlers,
                                        self._eventloop, self._server_socket,
                                        self._reqid_to_hd[data[1]][0], self._reqid_to_hd[data[1]][1],
                                        self._config, self._dns_resolver, self._is_local)
                                    self._reqid_to_hd[data[1]] = handle
                                    handle.handle_client(r_addr, CMD_CONNECT, data[1], data[2])
                                    handle.handle_client(r_addr, *data)
                                    self.update_activity(handle)
                                else:
                                    # disconnect
                                    rsp_data = self._pack_rsp_data(CMD_DISCONNECT, data[1], RSP_STATE_EMPTY)
                                    data_to_send = encrypt.encrypt_all(self._password, self._method, 1, rsp_data)
                                    self.write_to_server_socket(data_to_send, r_addr)
                            else:
                                self.update_activity(self._reqid_to_hd[data[1]])
                                self._reqid_to_hd[data[1]].handle_client(r_addr, *data)
                        else:
                            # disconnect
                            rsp_data = self._pack_rsp_data(CMD_DISCONNECT, data[1], RSP_STATE_EMPTY)
                            data_to_send = encrypt.encrypt_all(self._password, self._method, 1, rsp_data)
                            self.write_to_server_socket(data_to_send, r_addr)
                    elif data[0] > CMD_CONNECT_REMOTE and data[0] <= CMD_DISCONNECT:
                        if data[1] in self._reqid_to_hd:
                            if type(self._reqid_to_hd[data[1]]) is tuple:
                                pass
                            else:
                                self.update_activity(self._reqid_to_hd[data[1]])
                                self._reqid_to_hd[data[1]].handle_client(r_addr, *data)
                        else:
                            # disconnect
                            rsp_data = self._pack_rsp_data(CMD_DISCONNECT, data[1], RSP_STATE_EMPTY)
                            data_to_send = encrypt.encrypt_all(self._password, self._method, 1, rsp_data)
                            self.write_to_server_socket(data_to_send, r_addr)
                    return
                except Exception as e:
                    trace = traceback.format_exc()
                    logging.error(trace)
                    return

        try:
            header_result = parse_header(data)
        except:
            self._handel_protocol_error(r_addr, ogn_data)
            return

        if header_result is None:
            self._handel_protocol_error(r_addr, ogn_data)
            return
        connecttype, dest_addr, dest_port, header_length = header_result

        if self._is_local:
            server_addr, server_port = self._get_a_server()
        else:
            server_addr, server_port = dest_addr, dest_port

        addrs = self._dns_cache.get(server_addr, None)
        if addrs is None:
            addrs = socket.getaddrinfo(server_addr, server_port, 0,
                                       socket.SOCK_DGRAM, socket.SOL_UDP)
            if not addrs:
                # drop
                return
            else:
                self._dns_cache[server_addr] = addrs

        af, socktype, proto, canonname, sa = addrs[0]
        key = client_key(r_addr, af)
        client = self._cache.get(key, None)
        if not client:
            # TODO async getaddrinfo
            if self._forbidden_iplist:
                if common.to_str(sa[0]) in self._forbidden_iplist:
                    logging.debug('IP %s is in forbidden list, drop' %
                                  common.to_str(sa[0]))
                    # drop
                    return
            client = socket.socket(af, socktype, proto)
            client.setblocking(False)
            self._cache[key] = client
            self._client_fd_to_server_addr[client.fileno()] = r_addr

            self._sockets.add(client.fileno())
            self._eventloop.add(client, eventloop.POLL_IN, self)

            logging.debug('UDP port %5d sockets %d' % (self._listen_port, len(self._sockets)))

            logging.info('UDP data to %s:%d from %s:%d' %
                        (common.to_str(server_addr), server_port,
                            r_addr[0], r_addr[1]))

        if self._is_local:
            data = encrypt.encrypt_all(self._password, self._method, 1, data)
            if not data:
                return
        else:
            data = data[header_length:]
        if not data:
            return
        try:
            #logging.info('UDP handle_server sendto %s:%d %d bytes' % (common.to_str(server_addr), server_port, len(data)))
            client.sendto(data, (server_addr, server_port))
        except IOError as e:
            err = eventloop.errno_from_exception(e)
            if err in (errno.EINPROGRESS, errno.EAGAIN):
                pass
            else:
                shell.print_exception(e)",_1154.py,35,data[0] == 0,not data[0]
https://github.com/gugarosa/opytimizer/tree/master/tests/opytimizer/core/test_agent.py,"def test_agent_fit_setter():
    new_agent = agent.Agent(1, 1, 0, 1)

    try:
        new_agent.fit = np.array([0])
    except:
        new_agent.fit = 0

    assert new_agent.fit == 0",_1247.py,9,new_agent.fit == 0,not new_agent.fit
https://github.com/ivre/ivre/tree/master/ivre/utils.py,"def guess_srv_port(port1: int, port2: int, proto: str = ""tcp"") -> int:
    """"""Returns 1 when port1 is probably the server port, -1 when that's
    port2, and 0 when we cannot tell.

    """"""
    if not _PORTS_POPULATED:
        _set_ports()
    ports = _PORTS.get(proto, {})
    val1, val2 = ports.get(port1, 0), ports.get(port2, 0)
    cmpval = (val1 > val2) - (val1 < val2)
    if cmpval == 0:
        return (port2 > port1) - (port2 < port1)
    return cmpval",_1260.py,11,cmpval == 0,not cmpval
https://github.com/camelot-dev/camelot/tree/master/camelot/parsers/stream.py,"def _generate_columns_and_rows(self, table_idx, tk):
        # select elements which lie within table_bbox
        t_bbox = {}
        t_bbox[""horizontal""] = text_in_bbox(tk, self.horizontal_text)
        t_bbox[""vertical""] = text_in_bbox(tk, self.vertical_text)

        t_bbox[""horizontal""].sort(key=lambda x: (-x.y0, x.x0))
        t_bbox[""vertical""].sort(key=lambda x: (x.x0, -x.y0))

        self.t_bbox = t_bbox

        text_x_min, text_y_min, text_x_max, text_y_max = self._text_bbox(self.t_bbox)
        rows_grouped = self._group_rows(self.t_bbox[""horizontal""], row_tol=self.row_tol)
        rows = self._join_rows(rows_grouped, text_y_max, text_y_min)
        elements = [len(r) for r in rows_grouped]

        if self.columns is not None and self.columns[table_idx] != """":
            # user has to input boundary columns too
            # take (0, pdf_width) by default
            # similar to else condition
            # len can't be 1
            cols = self.columns[table_idx].split("","")
            cols = [float(c) for c in cols]
            cols.insert(0, text_x_min)
            cols.append(text_x_max)
            cols = [(cols[i], cols[i + 1]) for i in range(0, len(cols) - 1)]
        else:
            # calculate mode of the list of number of elements in
            # each row to guess the number of columns
            if not len(elements):
                cols = [(text_x_min, text_x_max)]
            else:
                ncols = max(set(elements), key=elements.count)
                if ncols == 1:
                    # if mode is 1, the page usually contains not tables
                    # but there can be cases where the list can be skewed,
                    # try to remove all 1s from list in this case and
                    # see if the list contains elements, if yes, then use
                    # the mode after removing 1s
                    elements = list(filter(lambda x: x != 1, elements))
                    if len(elements):
                        ncols = max(set(elements), key=elements.count)
                    else:
                        warnings.warn(f""No tables found in table area {table_idx + 1}"")
                cols = [
                    (t.x0, t.x1) for r in rows_grouped if len(r) == ncols for t in r
                ]
                cols = self._merge_columns(sorted(cols), column_tol=self.column_tol)
                inner_text = []
                for i in range(1, len(cols)):
                    left = cols[i - 1][1]
                    right = cols[i][0]
                    inner_text.extend(
                        [
                            t
                            for direction in self.t_bbox
                            for t in self.t_bbox[direction]
                            if t.x0 > left and t.x1 < right
                        ]
                    )
                outer_text = [
                    t
                    for direction in self.t_bbox
                    for t in self.t_bbox[direction]
                    if t.x0 > cols[-1][1] or t.x1 < cols[0][0]
                ]
                inner_text.extend(outer_text)
                cols = self._add_columns(cols, inner_text, self.row_tol)
                cols = self._join_columns(cols, text_x_min, text_x_max)

        return cols, rows",_1266.py,17,self.columns[table_idx] != '',self.columns[table_idx]
https://github.com/cloud-custodian/cloud-custodian/tree/master/tests/test_iam.py,"def test_iam_user_boundary_remove(self):
        factory = self.replay_flight_data('test_iam_user_remove_boundary')
        p = self.load_policy({
            'name': 'boundary',
            'resource': 'iam-user',
            'filters': [
                {'UserName': 'devbot'},
                {'PermissionsBoundary': 'present'}],
            'actions': [{
                'type': 'set-boundary', 'state': 'absent'}]},
            session_factory=factory)

        resources = p.run()
        assert len(resources) == 1
        assert resources[0]['UserName'] == 'devbot'

        if self.recording:
            time.sleep(1)
        client = factory().client('iam')
        assert client.get_user(UserName='devbot')['User'].get('PermissionsBoundary', {}) == {}",_1277.py,20,"client.get_user(UserName='devbot')['User'].get('PermissionsBoundary', {}) == {}","not client.get_user(UserName='devbot')['User'].get('PermissionsBoundary', {})"
https://github.com/netzob/netzob/tree/master/netzob/src/netzob/Inference/Vocabulary/FormatOperations/FieldSplitAligned/FieldSplitAligned.py,"def _alignData(self, values, semanticTags=None):
        """"""Align the specified data with respect to the semantic tags
        identified over the data.

        :parameter values: values to align
        :type values: a list of hexastring.
        :keyword semanticTags: semantic tags to consider when aligning
        :type semanticTags: a dict of :class:`netzob.Model.Vocabulary.SemanticTag.SemanticTag`
        :return: the alignment, its score and the semantic tags
        :rtype: a tupple (alignement, semanticTags, score)
        """"""
        if values is None or len(values) == 0:
            raise TypeError(""At least one value must be provided."")

        for val in values:
            if val is None or not isinstance(val, bytes):
                raise TypeError(
                    ""At least one value is None or not an str which is not authorized.""
                )

        if semanticTags is None:
            semanticTags = [OrderedDict() for v in values]

        if len(semanticTags) != len(values):
            raise TypeError(
                ""There should be a list of semantic tags for each value"")

        # Prepare the argument to send to the C wrapper
        toSend = [(values[iValue], semanticTags[iValue])
                  for iValue in range(len(values))]

        wrapper = WrapperArgsFactory(""_libNeedleman.alignMessages"")
        wrapper.typeList[wrapper.function](toSend)

        debug = False
        (score1, score2, score3, regex, mask,
         semanticTags) = _libNeedleman.alignMessages(
             self.doInternalSlick, self._cb_executionStatus, debug, wrapper)
        scores = (score1, score2, score3)

        # Deserialize returned info
        alignment = self._deserializeAlignment(regex, mask, self.unitSize)
        semanticTags = self._deserializeSemanticTags(semanticTags,
                                                     self.unitSize)
        return (alignment, semanticTags, scores)",_1292.py,12,len(values) == 0,not len(values)
https://github.com/imageio/imageio/tree/master/imageio/plugins/freeimage.py,"def _get_data(self, index):
            if index != 0:
                raise IndexError(""This format only supports singleton images."")
            return self._bm.get_image_data(), self._bm.get_meta_data()",_1395.py,2,index != 0,index
https://github.com/coqui-ai/TTS/tree/master/tests/vocoder_tests/test_wavegrad.py,"def test_train_step(self):  # pylint: disable=no-self-use
        """"""Test if all layers are updated in a basic training cycle""""""
        input_dummy = torch.rand(8, 1, 20 * 300).to(device)
        mel_spec = torch.rand(8, 80, 20).to(device)

        criterion = torch.nn.L1Loss().to(device)
        args = WavegradArgs(
            in_channels=80,
            out_channels=1,
            upsample_factors=[5, 5, 3, 2, 2],
            upsample_dilations=[[1, 2, 1, 2], [1, 2, 1, 2], [1, 2, 4, 8], [1, 2, 4, 8], [1, 2, 4, 8]],
        )
        config = WavegradConfig(model_params=args)
        model = Wavegrad(config)

        model_ref = Wavegrad(config)
        model.train()
        model.to(device)
        betas = np.linspace(1e-6, 1e-2, 1000)
        model.compute_noise_level(betas)
        model_ref.load_state_dict(model.state_dict())
        model_ref.to(device)
        count = 0
        for param, param_ref in zip(model.parameters(), model_ref.parameters()):
            assert (param - param_ref).sum() == 0, param
            count += 1
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        for i in range(5):
            y_hat = model.forward(input_dummy, mel_spec, torch.rand(8).to(device))
            optimizer.zero_grad()
            loss = criterion(y_hat, input_dummy)
            loss.backward()
            optimizer.step()
        # check parameter changes
        count = 0
        for param, param_ref in zip(model.parameters(), model_ref.parameters()):
            # ignore pre-higway layer since it works conditional
            # if count not in [145, 59]:
            assert (param != param_ref).any(), ""param {} with shape {} not updated!! \n{}\n{}"".format(
                count, param.shape, param, param_ref
            )
            count += 1",_1411.py,25,(param - param_ref).sum() == 0,not (param - param_ref).sum()
https://github.com/crossbario/crossbar/tree/master/crossbar/shell/reflection/Type.py,"def Element(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
        if o != 0:
            return self._tab.Get(flatbuffers.number_types.Int8Flags, o + self._tab.Pos)
        return 0",_1447.py,3,o != 0,o
https://github.com/tonybaloney/wily/tree/master/test/integration/test_rank.py,"def test_rank_single_file_default_metric(builddir):
    """"""Test the rank feature with default (AimLow) metric on a single file""""""
    runner = CliRunner()
    result = runner.invoke(main.cli, [""--path"", builddir, ""rank"", ""src/test.py""])
    assert result.exit_code == 0, result.stdout",_1470.py,5,result.exit_code == 0,not result.exit_code
https://github.com/yt-dlp/yt-dlp/tree/master/yt_dlp/utils.py,"def handle_starttag(self, tag, attrs):
        if tag == 'li' and self._level == 0:
            self.items.append(dict(attrs))
        self._level += 1",_1476.py,2,self._level == 0,not self._level
https://github.com/bbfamily/abu/tree/master/abupy/MLBu/ABuMLExecute.py,"def plot_boundaries(i, p_xlim, p_ylim):
        """"""
        针对有tree_属性的学习器绘制决策边界
        :param i: 内部递归调用使用ree_inner.children_left[i]和tree_inner.children_right[i]
        :param p_xlim: 原始参数使用plt.xlim()
        :param p_ylim: 原始参数使用plt.ylim()
        """"""
        if i < 0:
            return
        # 拿到tree_使用plot_boundaries继续递归绘制
        tree_inner = estimator.tree_

        if tree_inner.feature[i] == 0:
            # 绘制0的边界
            plt.plot([tree_inner.threshold[i], tree_inner.threshold[i]], p_ylim, '-k')
            # 即x轴固定p_ylim，xlim=[p_xlim[0], tree_inner.threshold[i]], [tree_inner.threshold[i], p_xlim[1]]
            plot_boundaries(tree_inner.children_left[i],
                            [p_xlim[0], tree_inner.threshold[i]], p_ylim)
            plot_boundaries(tree_inner.children_right[i],
                            [tree_inner.threshold[i], p_xlim[1]], p_ylim)
        elif tree_inner.feature[i] == 1:
            # 绘制1的边界
            plt.plot(p_xlim, [tree_inner.threshold[i], tree_inner.threshold[i]], '-k')
            # 即y轴固定p_xlim，ylim=[p_ylim[0], tree_inner.threshold[i]], [tree_inner.threshold[i], p_ylim[1]]
            plot_boundaries(tree_inner.children_left[i], p_xlim,
                            [p_ylim[0], tree_inner.threshold[i]])
            plot_boundaries(tree_inner.children_right[i], p_xlim,
                            [tree_inner.threshold[i], p_ylim[1]])",_1479.py,13,tree_inner.feature[i] == 0,not tree_inner.feature[i]
https://github.com/MilesCranmer/PySR/tree/master/pysr/sr.py,"def _validate_and_set_init_params(self):
        """"""
        Ensure parameters passed at initialization are valid.

        Also returns a dictionary of parameters to update from their
        values given at initialization.

        Returns
        -------
        packed_modified_params : dict
            Dictionary of parameters to modify from their initialized
            values. For example, default parameters are set here
            when a parameter is left set to `None`.
        """"""
        # Immutable parameter validation
        # Ensure instance parameters are allowable values:
        if self.tournament_selection_n > self.population_size:
            raise ValueError(
                ""tournament_selection_n parameter must be smaller than population_size.""
            )

        if self.maxsize > 40:
            warnings.warn(
                ""Note: Using a large maxsize for the equation search will be ""
                ""exponentially slower and use significant memory. You should consider ""
                ""turning `use_frequency` to False, and perhaps use `warmup_maxsize_by`.""
            )
        elif self.maxsize < 7:
            raise ValueError(""PySR requires a maxsize of at least 7"")

        if self.deterministic and not (
            self.multithreading in [False, None]
            and self.procs == 0
            and self.random_state is not None
        ):
            raise ValueError(
                ""To ensure deterministic searches, you must set `random_state` to a seed, ""
                ""`procs` to `0`, and `multithreading` to `False` or `None`.""
            )

        if self.random_state is not None and (
            not self.deterministic or self.procs != 0
        ):
            warnings.warn(
                ""Note: Setting `random_state` without also setting `deterministic` ""
                ""to True and `procs` to 0 will result in non-deterministic searches. ""
            )

        # NotImplementedError - Values that could be supported at a later time
        if self.optimizer_algorithm not in VALID_OPTIMIZER_ALGORITHMS:
            raise NotImplementedError(
                f""PySR currently only supports the following optimizer algorithms: {VALID_OPTIMIZER_ALGORITHMS}""
            )

        # 'Mutable' parameter validation
        buffer_available = ""buffer"" in sys.stdout.__dir__()
        # Params and their default values, if None is given:
        default_param_mapping = {
            ""binary_operators"": ""+ * - /"".split("" ""),
            ""unary_operators"": [],
            ""maxdepth"": self.maxsize,
            ""constraints"": {},
            ""multithreading"": self.procs != 0 and self.cluster_manager is None,
            ""batch_size"": 1,
            ""update_verbosity"": self.verbosity,
            ""progress"": buffer_available,
        }
        packed_modified_params = {}
        for parameter, default_value in default_param_mapping.items():
            parameter_value = getattr(self, parameter)
            if parameter_value is None:
                parameter_value = default_value
            else:
                # Special cases such as when binary_operators is a string
                if parameter in [""binary_operators"", ""unary_operators""] and isinstance(
                    parameter_value, str
                ):
                    parameter_value = [parameter_value]
                elif parameter == ""batch_size"" and parameter_value < 1:
                    warnings.warn(
                        ""Given `batch_size` must be greater than or equal to one. ""
                        ""`batch_size` has been increased to equal one.""
                    )
                    parameter_value = 1
                elif parameter == ""progress"" and not buffer_available:
                    warnings.warn(
                        ""Note: it looks like you are running in Jupyter. ""
                        ""The progress bar will be turned off.""
                    )
                    parameter_value = False
            packed_modified_params[parameter] = parameter_value

        assert (
            len(packed_modified_params[""binary_operators""])
            + len(packed_modified_params[""unary_operators""])
            > 0
        )

        julia_kwargs = {}
        if self.julia_kwargs is not None:
            for key, value in self.julia_kwargs.items():
                julia_kwargs[key] = value
        if ""optimize"" not in julia_kwargs:
            julia_kwargs[""optimize""] = 3
        if ""threads"" not in julia_kwargs and packed_modified_params[""multithreading""]:
            julia_kwargs[""threads""] = self.procs
        packed_modified_params[""julia_kwargs""] = julia_kwargs

        return packed_modified_params",_1496.py,42,self.procs != 0,self.procs
https://github.com/MilesCranmer/PySR/tree/master/pysr/sr.py,"def _validate_and_set_init_params(self):
        """"""
        Ensure parameters passed at initialization are valid.

        Also returns a dictionary of parameters to update from their
        values given at initialization.

        Returns
        -------
        packed_modified_params : dict
            Dictionary of parameters to modify from their initialized
            values. For example, default parameters are set here
            when a parameter is left set to `None`.
        """"""
        # Immutable parameter validation
        # Ensure instance parameters are allowable values:
        if self.tournament_selection_n > self.population_size:
            raise ValueError(
                ""tournament_selection_n parameter must be smaller than population_size.""
            )

        if self.maxsize > 40:
            warnings.warn(
                ""Note: Using a large maxsize for the equation search will be ""
                ""exponentially slower and use significant memory. You should consider ""
                ""turning `use_frequency` to False, and perhaps use `warmup_maxsize_by`.""
            )
        elif self.maxsize < 7:
            raise ValueError(""PySR requires a maxsize of at least 7"")

        if self.deterministic and not (
            self.multithreading in [False, None]
            and self.procs == 0
            and self.random_state is not None
        ):
            raise ValueError(
                ""To ensure deterministic searches, you must set `random_state` to a seed, ""
                ""`procs` to `0`, and `multithreading` to `False` or `None`.""
            )

        if self.random_state is not None and (
            not self.deterministic or self.procs != 0
        ):
            warnings.warn(
                ""Note: Setting `random_state` without also setting `deterministic` ""
                ""to True and `procs` to 0 will result in non-deterministic searches. ""
            )

        # NotImplementedError - Values that could be supported at a later time
        if self.optimizer_algorithm not in VALID_OPTIMIZER_ALGORITHMS:
            raise NotImplementedError(
                f""PySR currently only supports the following optimizer algorithms: {VALID_OPTIMIZER_ALGORITHMS}""
            )

        # 'Mutable' parameter validation
        buffer_available = ""buffer"" in sys.stdout.__dir__()
        # Params and their default values, if None is given:
        default_param_mapping = {
            ""binary_operators"": ""+ * - /"".split("" ""),
            ""unary_operators"": [],
            ""maxdepth"": self.maxsize,
            ""constraints"": {},
            ""multithreading"": self.procs != 0 and self.cluster_manager is None,
            ""batch_size"": 1,
            ""update_verbosity"": self.verbosity,
            ""progress"": buffer_available,
        }
        packed_modified_params = {}
        for parameter, default_value in default_param_mapping.items():
            parameter_value = getattr(self, parameter)
            if parameter_value is None:
                parameter_value = default_value
            else:
                # Special cases such as when binary_operators is a string
                if parameter in [""binary_operators"", ""unary_operators""] and isinstance(
                    parameter_value, str
                ):
                    parameter_value = [parameter_value]
                elif parameter == ""batch_size"" and parameter_value < 1:
                    warnings.warn(
                        ""Given `batch_size` must be greater than or equal to one. ""
                        ""`batch_size` has been increased to equal one.""
                    )
                    parameter_value = 1
                elif parameter == ""progress"" and not buffer_available:
                    warnings.warn(
                        ""Note: it looks like you are running in Jupyter. ""
                        ""The progress bar will be turned off.""
                    )
                    parameter_value = False
            packed_modified_params[parameter] = parameter_value

        assert (
            len(packed_modified_params[""binary_operators""])
            + len(packed_modified_params[""unary_operators""])
            > 0
        )

        julia_kwargs = {}
        if self.julia_kwargs is not None:
            for key, value in self.julia_kwargs.items():
                julia_kwargs[key] = value
        if ""optimize"" not in julia_kwargs:
            julia_kwargs[""optimize""] = 3
        if ""threads"" not in julia_kwargs and packed_modified_params[""multithreading""]:
            julia_kwargs[""threads""] = self.procs
        packed_modified_params[""julia_kwargs""] = julia_kwargs

        return packed_modified_params",_1496.py,63,self.procs != 0,self.procs
https://github.com/MilesCranmer/PySR/tree/master/pysr/sr.py,"def _validate_and_set_init_params(self):
        """"""
        Ensure parameters passed at initialization are valid.

        Also returns a dictionary of parameters to update from their
        values given at initialization.

        Returns
        -------
        packed_modified_params : dict
            Dictionary of parameters to modify from their initialized
            values. For example, default parameters are set here
            when a parameter is left set to `None`.
        """"""
        # Immutable parameter validation
        # Ensure instance parameters are allowable values:
        if self.tournament_selection_n > self.population_size:
            raise ValueError(
                ""tournament_selection_n parameter must be smaller than population_size.""
            )

        if self.maxsize > 40:
            warnings.warn(
                ""Note: Using a large maxsize for the equation search will be ""
                ""exponentially slower and use significant memory. You should consider ""
                ""turning `use_frequency` to False, and perhaps use `warmup_maxsize_by`.""
            )
        elif self.maxsize < 7:
            raise ValueError(""PySR requires a maxsize of at least 7"")

        if self.deterministic and not (
            self.multithreading in [False, None]
            and self.procs == 0
            and self.random_state is not None
        ):
            raise ValueError(
                ""To ensure deterministic searches, you must set `random_state` to a seed, ""
                ""`procs` to `0`, and `multithreading` to `False` or `None`.""
            )

        if self.random_state is not None and (
            not self.deterministic or self.procs != 0
        ):
            warnings.warn(
                ""Note: Setting `random_state` without also setting `deterministic` ""
                ""to True and `procs` to 0 will result in non-deterministic searches. ""
            )

        # NotImplementedError - Values that could be supported at a later time
        if self.optimizer_algorithm not in VALID_OPTIMIZER_ALGORITHMS:
            raise NotImplementedError(
                f""PySR currently only supports the following optimizer algorithms: {VALID_OPTIMIZER_ALGORITHMS}""
            )

        # 'Mutable' parameter validation
        buffer_available = ""buffer"" in sys.stdout.__dir__()
        # Params and their default values, if None is given:
        default_param_mapping = {
            ""binary_operators"": ""+ * - /"".split("" ""),
            ""unary_operators"": [],
            ""maxdepth"": self.maxsize,
            ""constraints"": {},
            ""multithreading"": self.procs != 0 and self.cluster_manager is None,
            ""batch_size"": 1,
            ""update_verbosity"": self.verbosity,
            ""progress"": buffer_available,
        }
        packed_modified_params = {}
        for parameter, default_value in default_param_mapping.items():
            parameter_value = getattr(self, parameter)
            if parameter_value is None:
                parameter_value = default_value
            else:
                # Special cases such as when binary_operators is a string
                if parameter in [""binary_operators"", ""unary_operators""] and isinstance(
                    parameter_value, str
                ):
                    parameter_value = [parameter_value]
                elif parameter == ""batch_size"" and parameter_value < 1:
                    warnings.warn(
                        ""Given `batch_size` must be greater than or equal to one. ""
                        ""`batch_size` has been increased to equal one.""
                    )
                    parameter_value = 1
                elif parameter == ""progress"" and not buffer_available:
                    warnings.warn(
                        ""Note: it looks like you are running in Jupyter. ""
                        ""The progress bar will be turned off.""
                    )
                    parameter_value = False
            packed_modified_params[parameter] = parameter_value

        assert (
            len(packed_modified_params[""binary_operators""])
            + len(packed_modified_params[""unary_operators""])
            > 0
        )

        julia_kwargs = {}
        if self.julia_kwargs is not None:
            for key, value in self.julia_kwargs.items():
                julia_kwargs[key] = value
        if ""optimize"" not in julia_kwargs:
            julia_kwargs[""optimize""] = 3
        if ""threads"" not in julia_kwargs and packed_modified_params[""multithreading""]:
            julia_kwargs[""threads""] = self.procs
        packed_modified_params[""julia_kwargs""] = julia_kwargs

        return packed_modified_params",_1496.py,33,self.procs == 0,not self.procs
https://github.com/maqp/tfc/tree/master/src/relay/server.py,"def flask_server(queues:               'QueueDict',
                 url_token_public_key: str,
                 unit_test:            bool = False
                 ) -> Optional[Flask]:
    """"""Run Flask web server for outgoing messages.

    This process runs Flask web server from where clients of contacts
    can load messages sent to them. Making such requests requires the
    clients know the secret path (i.e. URL token), that is, the X448
    shared secret derived from Relay Program's private key, and the
    public key obtained from the Onion Service of the contact.

    Note that this private key is not part of E2EE of messages, it only
    manages E2EE sessions between Relay Programs of conversing parties.
    It prevents anyone without the Relay Program's ephemeral private key
    from requesting ciphertexts from contact that do not belong to the
    user.

    The connection between the requests client and Flask server is
    end-to-end encrypted by the Tor Onion Service protocol: No Tor relay
    between them can see the content of the traffic; With Onion
    Services, there is no exit node. The connection is strongly
    authenticated by the Onion Service domain name, that is, the TFC
    account pinned by the user.
    """"""
    app          = Flask(__name__)
    pub_key_dict = dict()  # type: Dict[str, bytes]

    buf_key_queue = queues[RX_BUF_KEY_QUEUE]

    while buf_key_queue.qsize() == 0:
        time.sleep(0.01)
    buf_key = buf_key_queue.get()

    @app.route('/')
    def index() -> str:
        """"""Return the URL token public key to contacts that know the .onion address.""""""
        return url_token_public_key

    @app.route('/contact_request/<string:purp_onion_address>')
    def contact_request(purp_onion_address: str) -> str:
        """"""Pass contact request to `c_req_manager`.""""""
        queues[CONTACT_REQ_QUEUE].put(purp_onion_address)
        return 'OK'

    @app.route('/<purp_url_token>/files/')
    def file_get(purp_url_token: str) -> Any:
        """"""Validate the URL token and return a queued file.""""""
        return get_file(purp_url_token, queues, pub_key_dict, buf_key)

    @app.route(""/<purp_url_token>/messages/"")
    def message_get(purp_url_token: str) -> str:
        """"""Validate the URL token and return queued messages.""""""
        return get_message(purp_url_token, queues, pub_key_dict, buf_key)

    # --------------------------------------------------------------------------

    log = logging.getLogger('werkzeug')
    log.setLevel(logging.ERROR)

    if unit_test:
        return app
    else:  # pragma: no cover
        app.run()
        return None",_1526.py,31,buf_key_queue.qsize() == 0,not buf_key_queue.qsize()
https://github.com/ndee85/coa_tools/tree/master/Blender/coa_tools/addon_updater_ops.py,"def draw(self, context):
        layout = self.layout
        row = layout.row()
        if updater.invalidupdater == True:
            layout.label(""Updater module error"")
            return
        if updater.update_ready == True:
            col = layout.column()
            col.scale_y = 0.7
            col.label(""Update ready! Press OK to install ""\
                        +str(updater.update_version),icon=""LOOP_FORWARDS"")
            col.label(""or click outside window to defer"",icon=""BLANK1"")
            # could offer to remove popups here, but window will not redraw
            # so may be confusing to the user/look like a bug
            # row = layout.row()
            # row.label(""Prevent future popups:"")
            # row.operator(addon_updater_ignore.bl_idname,text=""Ignore update"")
        elif updater.update_ready == False:
            col = layout.column()
            col.scale_y = 0.7
            col.label(""No updates available"")
            col.label(""Press okay to dismiss dialog"")
            # add option to force install
        else:
            # case: updater.update_ready = None
            # we have not yet checked for the update
            layout.label(""Check for update now?"")",_1554.py,4,updater.invalidupdater == True,updater.invalidupdater
https://github.com/ndee85/coa_tools/tree/master/Blender/coa_tools/addon_updater_ops.py,"def draw(self, context):
        layout = self.layout
        row = layout.row()
        if updater.invalidupdater == True:
            layout.label(""Updater module error"")
            return
        if updater.update_ready == True:
            col = layout.column()
            col.scale_y = 0.7
            col.label(""Update ready! Press OK to install ""\
                        +str(updater.update_version),icon=""LOOP_FORWARDS"")
            col.label(""or click outside window to defer"",icon=""BLANK1"")
            # could offer to remove popups here, but window will not redraw
            # so may be confusing to the user/look like a bug
            # row = layout.row()
            # row.label(""Prevent future popups:"")
            # row.operator(addon_updater_ignore.bl_idname,text=""Ignore update"")
        elif updater.update_ready == False:
            col = layout.column()
            col.scale_y = 0.7
            col.label(""No updates available"")
            col.label(""Press okay to dismiss dialog"")
            # add option to force install
        else:
            # case: updater.update_ready = None
            # we have not yet checked for the update
            layout.label(""Check for update now?"")",_1554.py,7,updater.update_ready == True,updater.update_ready
https://github.com/ndee85/coa_tools/tree/master/Blender/coa_tools/addon_updater_ops.py,"def draw(self, context):
        layout = self.layout
        row = layout.row()
        if updater.invalidupdater == True:
            layout.label(""Updater module error"")
            return
        if updater.update_ready == True:
            col = layout.column()
            col.scale_y = 0.7
            col.label(""Update ready! Press OK to install ""\
                        +str(updater.update_version),icon=""LOOP_FORWARDS"")
            col.label(""or click outside window to defer"",icon=""BLANK1"")
            # could offer to remove popups here, but window will not redraw
            # so may be confusing to the user/look like a bug
            # row = layout.row()
            # row.label(""Prevent future popups:"")
            # row.operator(addon_updater_ignore.bl_idname,text=""Ignore update"")
        elif updater.update_ready == False:
            col = layout.column()
            col.scale_y = 0.7
            col.label(""No updates available"")
            col.label(""Press okay to dismiss dialog"")
            # add option to force install
        else:
            # case: updater.update_ready = None
            # we have not yet checked for the update
            layout.label(""Check for update now?"")",_1554.py,18,updater.update_ready == False,not updater.update_ready
https://github.com/openvinotoolkit/open_model_zoo/tree/master/tools/accuracy_checker/openvino/tools/accuracy_checker/annotation_converters/wider.py,"def generate_meta(self):
        if not self.dataset_meta:
            if self.label_start != 0:
                return {'label_map': {0: '__background__', self.label_start: 'face'}, 'background_label': 0}
            return {'label_map': {self.label_start: 'face'}}
        dataset_meta = read_json(self.dataset_meta)
        background_label = dataset_meta.get('background_label', -1)
        labels = dataset_meta.get('labels')
        label_map = {0: '__background__', self.label_start: 'face'}
        if labels:
            label_map = {
                label_id + self.label_start if label not in ('background', '__background__') else 0: label
                for label_id, label in enumerate(labels)
            }
            label_map[background_label] = '__background__'

        label_map = verify_label_map(dataset_meta.get('label_map', label_map))
        valid_labels = [key for key in label_map if key != background_label]
        self.background_label = background_label
        self.label_start = sorted(valid_labels)[0]
        meta = {'label_map': label_map}
        if background_label != -1:
            meta['background_label'] = background_label

        return meta",_1568.py,3,self.label_start != 0,self.label_start
https://github.com/commixproject/commix/tree/master/src/core/injections/controller/checks.py,"def print_ps_version(ps_version, filename, _):
  try:
    settings.PS_ENABLED = True
    ps_version = """".join(str(p) for p in ps_version)
    if settings.VERBOSITY_LEVEL == 0 and _:
      print(settings.SINGLE_WHITESPACE)
    # Output PowerShell's version number
    info_msg = ""Powershell version: "" + ps_version
    print(settings.print_bold_info_msg(info_msg))
    # Add infos to logs file. 
    output_file = open(filename, ""a"")
    if not menu.options.no_logging:
      info_msg = ""Powershell version: "" + ps_version + ""\n""
      output_file.write(re.compile(re.compile(settings.ANSI_COLOR_REMOVAL)).sub("""",settings.INFO_BOLD_SIGN) + info_msg)
    output_file.close()
  except ValueError:
    warn_msg = ""Heuristics have failed to identify the version of Powershell, ""
    warn_msg += ""which means that some payloads or injection techniques may be failed."" 
    print(settings.print_warning_msg(warn_msg))
    settings.PS_ENABLED = False
    ps_check_failed()",_1572.py,5,settings.VERBOSITY_LEVEL == 0,not settings.VERBOSITY_LEVEL
https://github.com/jay0lee/GAM/tree/master/src/gam/gapi/cloudidentity/groups.py,"def print_():
    ci = gapi_cloudidentity.build('cloudidentity_beta')
    i = 3
    members = False
    membersCountOnly = False
    managers = False
    managersCountOnly = False
    owners = False
    ownersCountOnly = False
    memberRestrictions = False
    gapi_directory_customer.setTrueCustomerId()
    parent = f'customers/{GC_Values[GC_CUSTOMER_ID]}'
    usemember = None
    memberDelimiter = '\n'
    todrive = False
    titles = []
    csvRows = []
    roles = []
    sortHeaders = False
    while i < len(sys.argv):
        myarg = sys.argv[i].lower()
        if myarg == 'todrive':
            todrive = True
            i += 1
        elif myarg == 'enterprisemember':
            member = gam.convertUIDtoEmailAddress(sys.argv[i + 1], email_types=['user', 'group'])
            usemember = f""member_key_id == '{member}' && 'cloudidentity.googleapis.com/groups.discussion_forum' in labels""
            i += 2
        elif myarg == 'delimiter':
            memberDelimiter = sys.argv[i + 1]
            i += 2
        elif myarg == 'sortheaders':
            sortHeaders = True
            i += 1
        elif myarg in ['members', 'memberscount']:
            roles.append(ROLE_MEMBER)
            members = True
            if myarg == 'memberscount':
                membersCountOnly = True
            i += 1
        elif myarg in ['owners', 'ownerscount']:
            roles.append(ROLE_OWNER)
            owners = True
            if myarg == 'ownerscount':
                ownersCountOnly = True
            i += 1
        elif myarg in ['managers', 'managerscount']:
            roles.append(ROLE_MANAGER)
            managers = True
            if myarg == 'managerscount':
                managersCountOnly = True
            i += 1
        elif myarg in ['memberrestrictions']:
            memberRestrictions = True
            display.add_titles_to_csv_file(
                    ['memberRestrictionQuery',],
                    titles)
            display.add_titles_to_csv_file(
                    ['memberRestrictionEvaluation',],
                    titles)
            i += 1
        else:
            controlflow.invalid_argument_exit(sys.argv[i], 'gam print cigroups')
    if roles:
        if members:
            display.add_titles_to_csv_file([
                'MembersCount',
            ], titles)
            if not membersCountOnly:
                display.add_titles_to_csv_file([
                    'Members',
                ], titles)
        if managers:
            display.add_titles_to_csv_file([
                'ManagersCount',
            ], titles)
            if not managersCountOnly:
                display.add_titles_to_csv_file([
                    'Managers',
                ], titles)
        if owners:
            display.add_titles_to_csv_file([
                'OwnersCount',
            ], titles)
            if not ownersCountOnly:
                display.add_titles_to_csv_file([
                    'Owners',
                ], titles)
    gam.printGettingAllItems('Groups', usemember)
    page_message = gapi.got_total_items_first_last_msg('Groups')
    if usemember:
        try:
            result = gapi.get_all_pages(ci.groups().memberships(),
                                        'searchTransitiveGroups',
                                        'memberships',
                                        throw_reasons=[gapi_errors.ErrorReason.FOUR_O_O],
                                        page_message=page_message,
                                        message_attribute=['groupKey', 'id'],
                                        parent='groups/-', query=usemember,
                                        fields='nextPageToken,memberships(group,groupKey(id),relationType)',
                                        pageSize=1000)
        except googleapiclient.errors.HttpError:
            controlflow.system_error_exit(
                2,
                'enterprisemember requires Enterprise license')
        entityList = []
        for entity in result:
            if entity['relationType'] == 'DIRECT':
                entityList.append(gapi.call(ci.groups(), 'get', name=entity['group']))
    else:
        entityList = gapi.get_all_pages(ci.groups(),
                                        'list',
                                        'groups',
                                        page_message=page_message,
                                        message_attribute=['groupKey', 'id'],
                                        parent=parent,
                                        view='FULL',
                                        pageSize=500)
    i = 0
    count = len(entityList)
    for groupEntity in entityList:
        i += 1
        groupEmail = groupEntity['groupKey']['id']
        for k, v in iter(groupEntity.pop('labels', {}).items()):
            if v == '':
                groupEntity[f'labels.{k}'] = True
            else:
                groupEntity[f'labels.{k}'] = v
        group = utils.flatten_json(groupEntity)
        for a_key in group:
            if a_key not in titles:
                titles.append(a_key)
        groupKey_id = groupEntity['name']
        if roles:
            sys.stderr.write(
                f' Getting {roles} for {groupEmail}{gam.currentCountNL(i, count)}'
            )
            page_message = gapi.got_total_items_first_last_msg('Members')
            validRoles, _, _ = gam._getRoleVerification(
                '.'.join(roles), 'nextPageToken,members(email,id,role)')
            groupMembers = gapi.get_all_pages(ci.groups().memberships(),
                                              'list',
                                              'memberships',
                                              page_message=page_message,
                                              message_attribute=['preferredMemberKey', 'id'],
                                              soft_errors=True,
                                              parent=groupKey_id,
                                              view='BASIC')
            if members:
                membersList = []
                membersCount = 0
            if managers:
                managersList = []
                managersCount = 0
            if owners:
                ownersList = []
                ownersCount = 0
            for member in groupMembers:
                member_email = member['preferredMemberKey']['id']
                role = get_single_role(member.get('roles', []))
                if not validRoles or role in validRoles:
                    if role == ROLE_MEMBER:
                        if members:
                            membersCount += 1
                            if not membersCountOnly:
                                membersList.append(member_email)
                    elif role == ROLE_MANAGER:
                        if managers:
                            managersCount += 1
                            if not managersCountOnly:
                                managersList.append(member_email)
                    elif role == ROLE_OWNER:
                        if owners:
                            ownersCount += 1
                            if not ownersCountOnly:
                                ownersList.append(member_email)
                    elif members:
                        membersCount += 1
                        if not membersCountOnly:
                            membersList.append(member_email)
            if members:
                group['MembersCount'] = membersCount
                if not membersCountOnly:
                    group['Members'] = memberDelimiter.join(membersList)
            if managers:
                group['ManagersCount'] = managersCount
                if not managersCountOnly:
                    group['Managers'] = memberDelimiter.join(managersList)
            if owners:
                group['OwnersCount'] = ownersCount
                if not ownersCountOnly:
                    group['Owners'] = memberDelimiter.join(ownersList)
        if memberRestrictions:
           name = f'{groupKey_id}/securitySettings'
           print(f'Getting member restrictions for {groupEmail} ({i}/{count}')
           sec_info = gapi.call(ci.groups(),
                                'getSecuritySettings',
                                name=name,
                                readMask='*')
           if 'memberRestriction' in sec_info:
               group['memberRestrictionQuery'] = sec_info['memberRestriction'].get('query', '')
               group['memberRestrictionEvaluation'] = sec_info['memberRestriction'].get('evaluation', {}).get('state', '')
        csvRows.append(group)
    if sortHeaders:
        display.sort_csv_titles([
            'name', 'groupKey.id'
        ], titles)
    display.write_csv_file(csvRows, titles, 'Groups', todrive)",_1576.py,125,v == '',not v
https://github.com/arsenetar/dupeguru/tree/master/hscommon/geometry.py,"def slope(self):
        if self.dx() == 0:
            return INF if self.dy() > 0 else -INF
        else:
            return self.dy() / self.dx()",_1604.py,2,self.dx() == 0,not self.dx()
https://github.com/PokeAPI/pokeapi/tree/master/data/v2/build.py,"def csv_record_to_objects(info):
        yield Move(
            id=int(info[0]),
            name=info[1],
            generation_id=int(info[2]),
            type_id=int(info[3]),
            power=int(info[4]) if info[4] != """" else None,
            pp=int(info[5]) if info[5] != """" else None,
            accuracy=int(info[6]) if info[6] != """" else None,
            priority=int(info[7]) if info[7] != """" else None,
            move_target_id=int(info[8]) if info[8] != """" else None,
            move_damage_class_id=int(info[9]) if info[9] != """" else None,
            move_effect_id=int(info[10]) if info[10] != """" else None,
            move_effect_chance=int(info[11]) if info[11] != """" else None,
            contest_type_id=int(info[12]) if info[12] != """" else None,
            contest_effect_id=int(info[13]) if info[13] != """" else None,
            super_contest_effect_id=int(info[14]) if info[14] != """" else None,
        )",_1608.py,7,info[4] != '',info[4]
https://github.com/PokeAPI/pokeapi/tree/master/data/v2/build.py,"def csv_record_to_objects(info):
        yield Move(
            id=int(info[0]),
            name=info[1],
            generation_id=int(info[2]),
            type_id=int(info[3]),
            power=int(info[4]) if info[4] != """" else None,
            pp=int(info[5]) if info[5] != """" else None,
            accuracy=int(info[6]) if info[6] != """" else None,
            priority=int(info[7]) if info[7] != """" else None,
            move_target_id=int(info[8]) if info[8] != """" else None,
            move_damage_class_id=int(info[9]) if info[9] != """" else None,
            move_effect_id=int(info[10]) if info[10] != """" else None,
            move_effect_chance=int(info[11]) if info[11] != """" else None,
            contest_type_id=int(info[12]) if info[12] != """" else None,
            contest_effect_id=int(info[13]) if info[13] != """" else None,
            super_contest_effect_id=int(info[14]) if info[14] != """" else None,
        )",_1608.py,8,info[5] != '',info[5]
https://github.com/PokeAPI/pokeapi/tree/master/data/v2/build.py,"def csv_record_to_objects(info):
        yield Move(
            id=int(info[0]),
            name=info[1],
            generation_id=int(info[2]),
            type_id=int(info[3]),
            power=int(info[4]) if info[4] != """" else None,
            pp=int(info[5]) if info[5] != """" else None,
            accuracy=int(info[6]) if info[6] != """" else None,
            priority=int(info[7]) if info[7] != """" else None,
            move_target_id=int(info[8]) if info[8] != """" else None,
            move_damage_class_id=int(info[9]) if info[9] != """" else None,
            move_effect_id=int(info[10]) if info[10] != """" else None,
            move_effect_chance=int(info[11]) if info[11] != """" else None,
            contest_type_id=int(info[12]) if info[12] != """" else None,
            contest_effect_id=int(info[13]) if info[13] != """" else None,
            super_contest_effect_id=int(info[14]) if info[14] != """" else None,
        )",_1608.py,9,info[6] != '',info[6]
https://github.com/PokeAPI/pokeapi/tree/master/data/v2/build.py,"def csv_record_to_objects(info):
        yield Move(
            id=int(info[0]),
            name=info[1],
            generation_id=int(info[2]),
            type_id=int(info[3]),
            power=int(info[4]) if info[4] != """" else None,
            pp=int(info[5]) if info[5] != """" else None,
            accuracy=int(info[6]) if info[6] != """" else None,
            priority=int(info[7]) if info[7] != """" else None,
            move_target_id=int(info[8]) if info[8] != """" else None,
            move_damage_class_id=int(info[9]) if info[9] != """" else None,
            move_effect_id=int(info[10]) if info[10] != """" else None,
            move_effect_chance=int(info[11]) if info[11] != """" else None,
            contest_type_id=int(info[12]) if info[12] != """" else None,
            contest_effect_id=int(info[13]) if info[13] != """" else None,
            super_contest_effect_id=int(info[14]) if info[14] != """" else None,
        )",_1608.py,10,info[7] != '',info[7]
https://github.com/PokeAPI/pokeapi/tree/master/data/v2/build.py,"def csv_record_to_objects(info):
        yield Move(
            id=int(info[0]),
            name=info[1],
            generation_id=int(info[2]),
            type_id=int(info[3]),
            power=int(info[4]) if info[4] != """" else None,
            pp=int(info[5]) if info[5] != """" else None,
            accuracy=int(info[6]) if info[6] != """" else None,
            priority=int(info[7]) if info[7] != """" else None,
            move_target_id=int(info[8]) if info[8] != """" else None,
            move_damage_class_id=int(info[9]) if info[9] != """" else None,
            move_effect_id=int(info[10]) if info[10] != """" else None,
            move_effect_chance=int(info[11]) if info[11] != """" else None,
            contest_type_id=int(info[12]) if info[12] != """" else None,
            contest_effect_id=int(info[13]) if info[13] != """" else None,
            super_contest_effect_id=int(info[14]) if info[14] != """" else None,
        )",_1608.py,11,info[8] != '',info[8]
https://github.com/PokeAPI/pokeapi/tree/master/data/v2/build.py,"def csv_record_to_objects(info):
        yield Move(
            id=int(info[0]),
            name=info[1],
            generation_id=int(info[2]),
            type_id=int(info[3]),
            power=int(info[4]) if info[4] != """" else None,
            pp=int(info[5]) if info[5] != """" else None,
            accuracy=int(info[6]) if info[6] != """" else None,
            priority=int(info[7]) if info[7] != """" else None,
            move_target_id=int(info[8]) if info[8] != """" else None,
            move_damage_class_id=int(info[9]) if info[9] != """" else None,
            move_effect_id=int(info[10]) if info[10] != """" else None,
            move_effect_chance=int(info[11]) if info[11] != """" else None,
            contest_type_id=int(info[12]) if info[12] != """" else None,
            contest_effect_id=int(info[13]) if info[13] != """" else None,
            super_contest_effect_id=int(info[14]) if info[14] != """" else None,
        )",_1608.py,12,info[9] != '',info[9]
https://github.com/PokeAPI/pokeapi/tree/master/data/v2/build.py,"def csv_record_to_objects(info):
        yield Move(
            id=int(info[0]),
            name=info[1],
            generation_id=int(info[2]),
            type_id=int(info[3]),
            power=int(info[4]) if info[4] != """" else None,
            pp=int(info[5]) if info[5] != """" else None,
            accuracy=int(info[6]) if info[6] != """" else None,
            priority=int(info[7]) if info[7] != """" else None,
            move_target_id=int(info[8]) if info[8] != """" else None,
            move_damage_class_id=int(info[9]) if info[9] != """" else None,
            move_effect_id=int(info[10]) if info[10] != """" else None,
            move_effect_chance=int(info[11]) if info[11] != """" else None,
            contest_type_id=int(info[12]) if info[12] != """" else None,
            contest_effect_id=int(info[13]) if info[13] != """" else None,
            super_contest_effect_id=int(info[14]) if info[14] != """" else None,
        )",_1608.py,13,info[10] != '',info[10]
https://github.com/PokeAPI/pokeapi/tree/master/data/v2/build.py,"def csv_record_to_objects(info):
        yield Move(
            id=int(info[0]),
            name=info[1],
            generation_id=int(info[2]),
            type_id=int(info[3]),
            power=int(info[4]) if info[4] != """" else None,
            pp=int(info[5]) if info[5] != """" else None,
            accuracy=int(info[6]) if info[6] != """" else None,
            priority=int(info[7]) if info[7] != """" else None,
            move_target_id=int(info[8]) if info[8] != """" else None,
            move_damage_class_id=int(info[9]) if info[9] != """" else None,
            move_effect_id=int(info[10]) if info[10] != """" else None,
            move_effect_chance=int(info[11]) if info[11] != """" else None,
            contest_type_id=int(info[12]) if info[12] != """" else None,
            contest_effect_id=int(info[13]) if info[13] != """" else None,
            super_contest_effect_id=int(info[14]) if info[14] != """" else None,
        )",_1608.py,14,info[11] != '',info[11]
https://github.com/PokeAPI/pokeapi/tree/master/data/v2/build.py,"def csv_record_to_objects(info):
        yield Move(
            id=int(info[0]),
            name=info[1],
            generation_id=int(info[2]),
            type_id=int(info[3]),
            power=int(info[4]) if info[4] != """" else None,
            pp=int(info[5]) if info[5] != """" else None,
            accuracy=int(info[6]) if info[6] != """" else None,
            priority=int(info[7]) if info[7] != """" else None,
            move_target_id=int(info[8]) if info[8] != """" else None,
            move_damage_class_id=int(info[9]) if info[9] != """" else None,
            move_effect_id=int(info[10]) if info[10] != """" else None,
            move_effect_chance=int(info[11]) if info[11] != """" else None,
            contest_type_id=int(info[12]) if info[12] != """" else None,
            contest_effect_id=int(info[13]) if info[13] != """" else None,
            super_contest_effect_id=int(info[14]) if info[14] != """" else None,
        )",_1608.py,15,info[12] != '',info[12]
https://github.com/PokeAPI/pokeapi/tree/master/data/v2/build.py,"def csv_record_to_objects(info):
        yield Move(
            id=int(info[0]),
            name=info[1],
            generation_id=int(info[2]),
            type_id=int(info[3]),
            power=int(info[4]) if info[4] != """" else None,
            pp=int(info[5]) if info[5] != """" else None,
            accuracy=int(info[6]) if info[6] != """" else None,
            priority=int(info[7]) if info[7] != """" else None,
            move_target_id=int(info[8]) if info[8] != """" else None,
            move_damage_class_id=int(info[9]) if info[9] != """" else None,
            move_effect_id=int(info[10]) if info[10] != """" else None,
            move_effect_chance=int(info[11]) if info[11] != """" else None,
            contest_type_id=int(info[12]) if info[12] != """" else None,
            contest_effect_id=int(info[13]) if info[13] != """" else None,
            super_contest_effect_id=int(info[14]) if info[14] != """" else None,
        )",_1608.py,16,info[13] != '',info[13]
https://github.com/PokeAPI/pokeapi/tree/master/data/v2/build.py,"def csv_record_to_objects(info):
        yield Move(
            id=int(info[0]),
            name=info[1],
            generation_id=int(info[2]),
            type_id=int(info[3]),
            power=int(info[4]) if info[4] != """" else None,
            pp=int(info[5]) if info[5] != """" else None,
            accuracy=int(info[6]) if info[6] != """" else None,
            priority=int(info[7]) if info[7] != """" else None,
            move_target_id=int(info[8]) if info[8] != """" else None,
            move_damage_class_id=int(info[9]) if info[9] != """" else None,
            move_effect_id=int(info[10]) if info[10] != """" else None,
            move_effect_chance=int(info[11]) if info[11] != """" else None,
            contest_type_id=int(info[12]) if info[12] != """" else None,
            contest_effect_id=int(info[13]) if info[13] != """" else None,
            super_contest_effect_id=int(info[14]) if info[14] != """" else None,
        )",_1608.py,17,info[14] != '',info[14]
https://github.com/ansible/ansible-modules-core/tree/master/system/user.py,"def create_user(self):
        cmd = [
            self.module.get_bin_path('pw', True),
            'useradd',
            '-n',
            self.name,
        ]

        if self.uid is not None:
            cmd.append('-u')
            cmd.append(self.uid)

            if self.non_unique:
                cmd.append('-o')

        if self.comment is not None:
            cmd.append('-c')
            cmd.append(self.comment)

        if self.home is not None:
            cmd.append('-d')
            cmd.append(self.home)

        if self.group is not None:
            if not self.group_exists(self.group):
                self.module.fail_json(msg=""Group %s does not exist"" % self.group)
            cmd.append('-g')
            cmd.append(self.group)

        if self.groups is not None:
            groups = self.get_groups_set()
            cmd.append('-G')
            cmd.append(','.join(groups))

        if self.createhome:
            cmd.append('-m')

            if self.skeleton is not None:
                cmd.append('-k')
                cmd.append(self.skeleton)

        if self.shell is not None:
            cmd.append('-s')
            cmd.append(self.shell)

        if self.login_class is not None:
            cmd.append('-L')
            cmd.append(self.login_class)

        if self.expires:
            days =( time.mktime(self.expires) - time.time() ) / 86400
            cmd.append('-e')
            cmd.append(str(int(days)))

        # system cannot be handled currently - should we error if its requested?
        # create the user
        (rc, out, err) = self.execute_command(cmd)
        if rc is not None and rc != 0:
            self.module.fail_json(name=self.name, msg=err, rc=rc)

        # we have to set the password in a second command
        if self.password is not None:
            cmd = [
                self.module.get_bin_path('chpass', True),
                '-p',
                self.password,
                self.name
            ]
            return self.execute_command(cmd)

        return (rc, out, err)",_1614.py,58,rc != 0,rc
https://github.com/google-research/pegasus/tree/master/pegasus/layers/attention.py,"def __init__(self, hidden_size, num_heads, attention_dropout):
    if hidden_size % num_heads != 0:
      raise ValueError(""Number of attention heads must divide hidden size"")

    self._q_layer = tf.layers.Dense(hidden_size, use_bias=False, name=""q_proj"")
    self._k_layer = tf.layers.Dense(hidden_size, use_bias=False, name=""k_proj"")
    self._v_layer = tf.layers.Dense(hidden_size, use_bias=False, name=""v_proj"")
    self._output_layer = tf.layers.Dense(
        hidden_size, use_bias=False, name=""output_proj"")
    self._num_heads = num_heads
    self._hidden_size = hidden_size
    self._attention_dropout = attention_dropout",_1665.py,2,hidden_size % num_heads != 0,hidden_size % num_heads
https://github.com/rll/rllab/tree/master/rllab/viskit/core.py,"def to_json(stub_object):
    from rllab.misc.instrument import StubObject
    from rllab.misc.instrument import StubAttr
    if isinstance(stub_object, StubObject):
        assert len(stub_object.args) == 0
        data = dict()
        for k, v in stub_object.kwargs.items():
            data[k] = to_json(v)
        data[""_name""] = stub_object.proxy_class.__module__ + \
                        ""."" + stub_object.proxy_class.__name__
        return data
    elif isinstance(stub_object, StubAttr):
        return dict(
            obj=to_json(stub_object.obj),
            attr=to_json(stub_object.attr_name)
        )
    return stub_object",_1735.py,5,len(stub_object.args) == 0,not len(stub_object.args)
https://github.com/bitbrute/evillimiter/tree/master/evillimiter/menus/parser.py,"def parse(self, command):
        """"""
        Parses a given list of arguments
        """"""
        names = [x.name for x in (self._flag_commands + self._parameter_commands)]
        result_dict = dict.fromkeys(names, None)

        # indicates whether or not to skip the next command argument
        skip_next = False

        for i, arg in enumerate(command):
            if skip_next:
                skip_next = False
                continue

            if i == 0:
                # check if the first argument is a subparser
                for sp in self._subparsers:
                    if sp.identifier == arg:
                        # if subparser present, parse arguments there
                        result = sp.subparser.parse(command[(i + 1):])
                        if result is not None and sp.handler is not None:
                            # call the subparser's handler if available
                            sp.handler(result)

                        return result
            
            # indicates whether or not the argument has been processed
            is_arg_processed = False

            for cmd in self._flag_commands:
                if cmd.identifier == arg:
                    if cmd.type == CommandParser.CommandType.FLAG_COMMAND:
                        # if its a flag, set the flag to true
                        result_dict[cmd.name] = True
                        is_arg_processed = True
                        break
                    elif cmd.type == CommandParser.CommandType.PARAMETERIZED_FLAG_COMMAND:
                        if (len(command) - 1) < (i + 1):
                            # no more command arguments to process
                            IO.error('parameter for flag {}{}{} is missing'.format(IO.Fore.LIGHTYELLOW_EX, cmd.name, IO.Style.RESET_ALL))
                            return

                        # if parameterized flag, set value to next argument
                        value = command[i + 1]
                        result_dict[cmd.name] = value

                        # skip the next argument (already processed)
                        skip_next = True

                        is_arg_processed = True
                        break
            
            if not is_arg_processed:
                for cmd in self._parameter_commands:
                    # parameter command, since a flag could not be found
                    if result_dict[cmd.name] is None:
                        # set parameter value
                        result_dict[cmd.name] = arg
                        is_arg_processed = True
                        break

            if not is_arg_processed:
                IO.error('{}{}{} is an unknown command.'.format(IO.Fore.LIGHTYELLOW_EX, arg, IO.Style.RESET_ALL))
                return

        # check if there are any parameters missing
        for cmd in self._parameter_commands:
            if result_dict[cmd.name] is None:
                IO.error('parameter {}{}{} is missing'.format(IO.Fore.LIGHTYELLOW_EX, cmd.name, IO.Style.RESET_ALL))
                return

        # set unspecified flags to False instead of None
        for cmd in self._flag_commands:
            if cmd.type == CommandParser.CommandType.FLAG_COMMAND:
                if result_dict[cmd.name] is None:
                    result_dict[cmd.name] = False

        result_tuple = collections.namedtuple('ParseResult', sorted(result_dict))
        return result_tuple(**result_dict)",_1978.py,16,i == 0,not i
https://github.com/joblib/joblib/tree/master/joblib/test/test_memory.py,"def test__get_items_to_delete(tmpdir):
    memory, expected_hash_cachedirs, _ = _setup_toy_cache(tmpdir)
    items = memory.store_backend.get_items()
    # bytes_limit set to keep only one cache item (each hash cache
    # folder is about 1000 bytes + metadata)
    items_to_delete = memory.store_backend._get_items_to_delete('2K')
    nb_hashes = len(expected_hash_cachedirs)
    assert set.issubset(set(items_to_delete), set(items))
    assert len(items_to_delete) == nb_hashes - 1

    # Sanity check bytes_limit=2048 is the same as bytes_limit='2K'
    items_to_delete_2048b = memory.store_backend._get_items_to_delete(2048)
    assert sorted(items_to_delete) == sorted(items_to_delete_2048b)

    # bytes_limit greater than the size of the cache
    items_to_delete_empty = memory.store_backend._get_items_to_delete('1M')
    assert items_to_delete_empty == []

    # All the cache items need to be deleted
    bytes_limit_too_small = 500
    items_to_delete_500b = memory.store_backend._get_items_to_delete(
        bytes_limit_too_small)
    assert set(items_to_delete_500b), set(items)

    # Test LRU property: surviving cache items should all have a more
    # recent last_access that the ones that have been deleted
    items_to_delete_6000b = memory.store_backend._get_items_to_delete(6000)
    surviving_items = set(items).difference(items_to_delete_6000b)

    assert (max(ci.last_access for ci in items_to_delete_6000b) <=
            min(ci.last_access for ci in surviving_items))",_1993.py,17,items_to_delete_empty == [],not items_to_delete_empty
https://github.com/PaddlePaddle/PaddleGAN/tree/master/ppgan/apps/first_order_predictor.py,"def make_animation(self,
                       source_image,
                       driving_video,
                       generator,
                       kp_detector,
                       relative=True,
                       adapt_movement_scale=True):
        with paddle.no_grad():
            predictions = []
            source = paddle.to_tensor(source_image[np.newaxis].astype(
                np.float32)).transpose([0, 3, 1, 2])

            driving_video_np = np.array(driving_video).astype(np.float32)
            driving_n, driving_h, driving_w, driving_c = driving_video_np.shape

            driving_slices = []

            if self.slice_size != 0:
                batch_count_in_slice = int(
                    np.floor(
                        float(self.slice_size) /
                        (self.batch_size * driving_h * driving_w * driving_c)))
                assert batch_count_in_slice > 0, ""batch_count_in_slice is 0, use smaller batch_size or bigger slice_size""
                frame_count_in_slice = batch_count_in_slice * self.batch_size
                for slice_start in range(0, driving_n, frame_count_in_slice):
                    slice_end = slice_start + min(frame_count_in_slice,
                                                  driving_n - slice_start)
                    current_slice = paddle.to_tensor(
                        driving_video_np[slice_start:slice_end, ]).transpose(
                            [0, 3, 1, 2])
                    driving_slices.append(current_slice)
            else:
                # whole driving as a single slice
                driving = paddle.to_tensor(
                    np.array(driving_video).astype(np.float32)).transpose(
                        [0, 3, 1, 2])
                frame_count_in_slice = driving_n
                driving_slices.append(driving)

            kp_source = kp_detector(source)
            kp_driving_initial = kp_detector(driving_slices[0][0:1])
            kp_source_batch = {}
            kp_source_batch[""value""] = paddle.tile(
                kp_source[""value""], repeat_times=[self.batch_size, 1, 1])
            kp_source_batch[""jacobian""] = paddle.tile(
                kp_source[""jacobian""], repeat_times=[self.batch_size, 1, 1, 1])
            source = paddle.tile(source,
                                 repeat_times=[self.batch_size, 1, 1, 1])
            begin_idx = 0
            for frame_idx in tqdm(
                    range(int(np.ceil(float(driving_n) / self.batch_size)))):
                frame_num = min(self.batch_size, driving_n - begin_idx)
                slice_id = int(frame_idx * self.batch_size /
                               frame_count_in_slice)

                internal_start = frame_idx - slice_id * frame_count_in_slice
                internal_end = frame_idx - slice_id * frame_count_in_slice + frame_num

                driving_frame = driving_slices[slice_id][
                    internal_start:internal_end]

                kp_driving = kp_detector(driving_frame)
                kp_source_img = {}
                kp_source_img[""value""] = kp_source_batch[""value""][0:frame_num]
                kp_source_img[""jacobian""] = kp_source_batch[""jacobian""][
                    0:frame_num]

                kp_norm = normalize_kp(
                    kp_source=kp_source,
                    kp_driving=kp_driving,
                    kp_driving_initial=kp_driving_initial,
                    use_relative_movement=relative,
                    use_relative_jacobian=relative,
                    adapt_movement_scale=adapt_movement_scale)

                out = generator(source[0:frame_num],
                                kp_source=kp_source_img,
                                kp_driving=kp_norm)
                img = np.transpose(out['prediction'].numpy(),
                                   [0, 2, 3, 1]) * 255.0

                if self.face_enhancement:
                    img = self.faceenhancer.enhance_from_batch(img)

                predictions.append(img)
                begin_idx += frame_num
        return np.concatenate(predictions)",_2033.py,18,self.slice_size != 0,self.slice_size
https://github.com/sympy/sympy/tree/master/sympy/core/tests/test_arit.py,"def test_Mod():
    assert Mod(x, 1).func is Mod
    assert pi % pi is S.Zero
    assert Mod(5, 3) == 2
    assert Mod(-5, 3) == 1
    assert Mod(5, -3) == -1
    assert Mod(-5, -3) == -2
    assert type(Mod(3.2, 2, evaluate=False)) == Mod
    assert 5 % x == Mod(5, x)
    assert x % 5 == Mod(x, 5)
    assert x % y == Mod(x, y)
    assert (x % y).subs({x: 5, y: 3}) == 2
    assert Mod(nan, 1) is nan
    assert Mod(1, nan) is nan
    assert Mod(nan, nan) is nan

    assert Mod(0, x) == 0
    with raises(ZeroDivisionError):
        Mod(x, 0)

    k = Symbol('k', integer=True)
    m = Symbol('m', integer=True, positive=True)
    assert (x**m % x).func is Mod
    assert (k**(-m) % k).func is Mod
    assert k**m % k == 0
    assert (-2*k)**m % k == 0

    # Float handling
    point3 = Float(3.3) % 1
    assert (x - 3.3) % 1 == Mod(1.*x + 1 - point3, 1)
    assert Mod(-3.3, 1) == 1 - point3
    assert Mod(0.7, 1) == Float(0.7)
    e = Mod(1.3, 1)
    assert comp(e, .3) and e.is_Float
    e = Mod(1.3, .7)
    assert comp(e, .6) and e.is_Float
    e = Mod(1.3, Rational(7, 10))
    assert comp(e, .6) and e.is_Float
    e = Mod(Rational(13, 10), 0.7)
    assert comp(e, .6) and e.is_Float
    e = Mod(Rational(13, 10), Rational(7, 10))
    assert comp(e, .6) and e.is_Rational

    # check that sign is right
    r2 = sqrt(2)
    r3 = sqrt(3)
    for i in [-r3, -r2, r2, r3]:
        for j in [-r3, -r2, r2, r3]:
            assert verify_numerically(i % j, i.n() % j.n())
    for _x in range(4):
        for _y in range(9):
            reps = [(x, _x), (y, _y)]
            assert Mod(3*x + y, 9).subs(reps) == (3*_x + _y) % 9

    # denesting
    t = Symbol('t', real=True)
    assert Mod(Mod(x, t), t) == Mod(x, t)
    assert Mod(-Mod(x, t), t) == Mod(-x, t)
    assert Mod(Mod(x, 2*t), t) == Mod(x, t)
    assert Mod(-Mod(x, 2*t), t) == Mod(-x, t)
    assert Mod(Mod(x, t), 2*t) == Mod(x, t)
    assert Mod(-Mod(x, t), -2*t) == -Mod(x, t)
    for i in [-4, -2, 2, 4]:
        for j in [-4, -2, 2, 4]:
            for k in range(4):
                assert Mod(Mod(x, i), j).subs({x: k}) == (k % i) % j
                assert Mod(-Mod(x, i), j).subs({x: k}) == -(k % i) % j

    # known difference
    assert Mod(5*sqrt(2), sqrt(5)) == 5*sqrt(2) - 3*sqrt(5)
    p = symbols('p', positive=True)
    assert Mod(2, p + 3) == 2
    assert Mod(-2, p + 3) == p + 1
    assert Mod(2, -p - 3) == -p - 1
    assert Mod(-2, -p - 3) == -2
    assert Mod(p + 5, p + 3) == 2
    assert Mod(-p - 5, p + 3) == p + 1
    assert Mod(p + 5, -p - 3) == -p - 1
    assert Mod(-p - 5, -p - 3) == -2
    assert Mod(p + 1, p - 1).func is Mod

    # handling sums
    assert (x + 3) % 1 == Mod(x, 1)
    assert (x + 3.0) % 1 == Mod(1.*x, 1)
    assert (x - S(33)/10) % 1 == Mod(x + S(7)/10, 1)

    a = Mod(.6*x + y, .3*y)
    b = Mod(0.1*y + 0.6*x, 0.3*y)
    # Test that a, b are equal, with 1e-14 accuracy in coefficients
    eps = 1e-14
    assert abs((a.args[0] - b.args[0]).subs({x: 1, y: 1})) < eps
    assert abs((a.args[1] - b.args[1]).subs({x: 1, y: 1})) < eps

    assert (x + 1) % x == 1 % x
    assert (x + y) % x == y % x
    assert (x + y + 2) % x == (y + 2) % x
    assert (a + 3*x + 1) % (2*x) == Mod(a + x + 1, 2*x)
    assert (12*x + 18*y) % (3*x) == 3*Mod(6*y, x)

    # gcd extraction
    assert (-3*x) % (-2*y) == -Mod(3*x, 2*y)
    assert (.6*pi) % (.3*x*pi) == 0.3*pi*Mod(2, x)
    assert (.6*pi) % (.31*x*pi) == pi*Mod(0.6, 0.31*x)
    assert (6*pi) % (.3*x*pi) == 0.3*pi*Mod(20, x)
    assert (6*pi) % (.31*x*pi) == pi*Mod(6, 0.31*x)
    assert (6*pi) % (.42*x*pi) == pi*Mod(6, 0.42*x)
    assert (12*x) % (2*y) == 2*Mod(6*x, y)
    assert (12*x) % (3*5*y) == 3*Mod(4*x, 5*y)
    assert (12*x) % (15*x*y) == 3*x*Mod(4, 5*y)
    assert (-2*pi) % (3*pi) == pi
    assert (2*x + 2) % (x + 1) == 0
    assert (x*(x + 1)) % (x + 1) == (x + 1)*Mod(x, 1)
    assert Mod(5.0*x, 0.1*y) == 0.1*Mod(50*x, y)
    i = Symbol('i', integer=True)
    assert (3*i*x) % (2*i*y) == i*Mod(3*x, 2*y)
    assert Mod(4*i, 4) == 0

    # issue 8677
    n = Symbol('n', integer=True, positive=True)
    assert factorial(n) % n == 0
    assert factorial(n + 2) % n == 0
    assert (factorial(n + 4) % (n + 5)).func is Mod

    # Wilson's theorem
    assert factorial(18042, evaluate=False) % 18043 == 18042
    p = Symbol('n', prime=True)
    assert factorial(p - 1) % p == p - 1
    assert factorial(p - 1) % -p == -1
    assert (factorial(3, evaluate=False) % 4).doit() == 2
    n = Symbol('n', composite=True, odd=True)
    assert factorial(n - 1) % n == 0

    # symbolic with known parity
    n = Symbol('n', even=True)
    assert Mod(n, 2) == 0
    n = Symbol('n', odd=True)
    assert Mod(n, 2) == 1

    # issue 10963
    assert (x**6000%400).args[1] == 400

    #issue 13543
    assert Mod(Mod(x + 1, 2) + 1, 2) == Mod(x, 2)

    assert Mod(Mod(x + 2, 4)*(x + 4), 4) == Mod(x*(x + 2), 4)
    assert Mod(Mod(x + 2, 4)*4, 4) == 0

    # issue 15493
    i, j = symbols('i j', integer=True, positive=True)
    assert Mod(3*i, 2) == Mod(i, 2)
    assert Mod(8*i/j, 4) == 4*Mod(2*i/j, 1)
    assert Mod(8*i, 4) == 0

    # rewrite
    assert Mod(x, y).rewrite(floor) == x - y*floor(x/y)
    assert ((x - Mod(x, y))/y).rewrite(floor) == floor(x/y)

    # issue 21373
    from sympy.functions.elementary.trigonometric import sinh
    from sympy.functions.elementary.piecewise import Piecewise

    x_r, y_r = symbols('x_r y_r', real=True)
    assert (Piecewise((x_r, y_r > x_r), (y_r, True)) / z) % 1
    expr = exp(sinh(Piecewise((x_r, y_r > x_r), (y_r, True)) / z))
    expr.subs({1: 1.0})
    sinh(Piecewise((x_r, y_r > x_r), (y_r, True)) * z ** -1.0).is_zero",_2048.py,17,"Mod(0, x) == 0","not Mod(0, x)"
https://github.com/sympy/sympy/tree/master/sympy/core/tests/test_arit.py,"def test_Mod():
    assert Mod(x, 1).func is Mod
    assert pi % pi is S.Zero
    assert Mod(5, 3) == 2
    assert Mod(-5, 3) == 1
    assert Mod(5, -3) == -1
    assert Mod(-5, -3) == -2
    assert type(Mod(3.2, 2, evaluate=False)) == Mod
    assert 5 % x == Mod(5, x)
    assert x % 5 == Mod(x, 5)
    assert x % y == Mod(x, y)
    assert (x % y).subs({x: 5, y: 3}) == 2
    assert Mod(nan, 1) is nan
    assert Mod(1, nan) is nan
    assert Mod(nan, nan) is nan

    assert Mod(0, x) == 0
    with raises(ZeroDivisionError):
        Mod(x, 0)

    k = Symbol('k', integer=True)
    m = Symbol('m', integer=True, positive=True)
    assert (x**m % x).func is Mod
    assert (k**(-m) % k).func is Mod
    assert k**m % k == 0
    assert (-2*k)**m % k == 0

    # Float handling
    point3 = Float(3.3) % 1
    assert (x - 3.3) % 1 == Mod(1.*x + 1 - point3, 1)
    assert Mod(-3.3, 1) == 1 - point3
    assert Mod(0.7, 1) == Float(0.7)
    e = Mod(1.3, 1)
    assert comp(e, .3) and e.is_Float
    e = Mod(1.3, .7)
    assert comp(e, .6) and e.is_Float
    e = Mod(1.3, Rational(7, 10))
    assert comp(e, .6) and e.is_Float
    e = Mod(Rational(13, 10), 0.7)
    assert comp(e, .6) and e.is_Float
    e = Mod(Rational(13, 10), Rational(7, 10))
    assert comp(e, .6) and e.is_Rational

    # check that sign is right
    r2 = sqrt(2)
    r3 = sqrt(3)
    for i in [-r3, -r2, r2, r3]:
        for j in [-r3, -r2, r2, r3]:
            assert verify_numerically(i % j, i.n() % j.n())
    for _x in range(4):
        for _y in range(9):
            reps = [(x, _x), (y, _y)]
            assert Mod(3*x + y, 9).subs(reps) == (3*_x + _y) % 9

    # denesting
    t = Symbol('t', real=True)
    assert Mod(Mod(x, t), t) == Mod(x, t)
    assert Mod(-Mod(x, t), t) == Mod(-x, t)
    assert Mod(Mod(x, 2*t), t) == Mod(x, t)
    assert Mod(-Mod(x, 2*t), t) == Mod(-x, t)
    assert Mod(Mod(x, t), 2*t) == Mod(x, t)
    assert Mod(-Mod(x, t), -2*t) == -Mod(x, t)
    for i in [-4, -2, 2, 4]:
        for j in [-4, -2, 2, 4]:
            for k in range(4):
                assert Mod(Mod(x, i), j).subs({x: k}) == (k % i) % j
                assert Mod(-Mod(x, i), j).subs({x: k}) == -(k % i) % j

    # known difference
    assert Mod(5*sqrt(2), sqrt(5)) == 5*sqrt(2) - 3*sqrt(5)
    p = symbols('p', positive=True)
    assert Mod(2, p + 3) == 2
    assert Mod(-2, p + 3) == p + 1
    assert Mod(2, -p - 3) == -p - 1
    assert Mod(-2, -p - 3) == -2
    assert Mod(p + 5, p + 3) == 2
    assert Mod(-p - 5, p + 3) == p + 1
    assert Mod(p + 5, -p - 3) == -p - 1
    assert Mod(-p - 5, -p - 3) == -2
    assert Mod(p + 1, p - 1).func is Mod

    # handling sums
    assert (x + 3) % 1 == Mod(x, 1)
    assert (x + 3.0) % 1 == Mod(1.*x, 1)
    assert (x - S(33)/10) % 1 == Mod(x + S(7)/10, 1)

    a = Mod(.6*x + y, .3*y)
    b = Mod(0.1*y + 0.6*x, 0.3*y)
    # Test that a, b are equal, with 1e-14 accuracy in coefficients
    eps = 1e-14
    assert abs((a.args[0] - b.args[0]).subs({x: 1, y: 1})) < eps
    assert abs((a.args[1] - b.args[1]).subs({x: 1, y: 1})) < eps

    assert (x + 1) % x == 1 % x
    assert (x + y) % x == y % x
    assert (x + y + 2) % x == (y + 2) % x
    assert (a + 3*x + 1) % (2*x) == Mod(a + x + 1, 2*x)
    assert (12*x + 18*y) % (3*x) == 3*Mod(6*y, x)

    # gcd extraction
    assert (-3*x) % (-2*y) == -Mod(3*x, 2*y)
    assert (.6*pi) % (.3*x*pi) == 0.3*pi*Mod(2, x)
    assert (.6*pi) % (.31*x*pi) == pi*Mod(0.6, 0.31*x)
    assert (6*pi) % (.3*x*pi) == 0.3*pi*Mod(20, x)
    assert (6*pi) % (.31*x*pi) == pi*Mod(6, 0.31*x)
    assert (6*pi) % (.42*x*pi) == pi*Mod(6, 0.42*x)
    assert (12*x) % (2*y) == 2*Mod(6*x, y)
    assert (12*x) % (3*5*y) == 3*Mod(4*x, 5*y)
    assert (12*x) % (15*x*y) == 3*x*Mod(4, 5*y)
    assert (-2*pi) % (3*pi) == pi
    assert (2*x + 2) % (x + 1) == 0
    assert (x*(x + 1)) % (x + 1) == (x + 1)*Mod(x, 1)
    assert Mod(5.0*x, 0.1*y) == 0.1*Mod(50*x, y)
    i = Symbol('i', integer=True)
    assert (3*i*x) % (2*i*y) == i*Mod(3*x, 2*y)
    assert Mod(4*i, 4) == 0

    # issue 8677
    n = Symbol('n', integer=True, positive=True)
    assert factorial(n) % n == 0
    assert factorial(n + 2) % n == 0
    assert (factorial(n + 4) % (n + 5)).func is Mod

    # Wilson's theorem
    assert factorial(18042, evaluate=False) % 18043 == 18042
    p = Symbol('n', prime=True)
    assert factorial(p - 1) % p == p - 1
    assert factorial(p - 1) % -p == -1
    assert (factorial(3, evaluate=False) % 4).doit() == 2
    n = Symbol('n', composite=True, odd=True)
    assert factorial(n - 1) % n == 0

    # symbolic with known parity
    n = Symbol('n', even=True)
    assert Mod(n, 2) == 0
    n = Symbol('n', odd=True)
    assert Mod(n, 2) == 1

    # issue 10963
    assert (x**6000%400).args[1] == 400

    #issue 13543
    assert Mod(Mod(x + 1, 2) + 1, 2) == Mod(x, 2)

    assert Mod(Mod(x + 2, 4)*(x + 4), 4) == Mod(x*(x + 2), 4)
    assert Mod(Mod(x + 2, 4)*4, 4) == 0

    # issue 15493
    i, j = symbols('i j', integer=True, positive=True)
    assert Mod(3*i, 2) == Mod(i, 2)
    assert Mod(8*i/j, 4) == 4*Mod(2*i/j, 1)
    assert Mod(8*i, 4) == 0

    # rewrite
    assert Mod(x, y).rewrite(floor) == x - y*floor(x/y)
    assert ((x - Mod(x, y))/y).rewrite(floor) == floor(x/y)

    # issue 21373
    from sympy.functions.elementary.trigonometric import sinh
    from sympy.functions.elementary.piecewise import Piecewise

    x_r, y_r = symbols('x_r y_r', real=True)
    assert (Piecewise((x_r, y_r > x_r), (y_r, True)) / z) % 1
    expr = exp(sinh(Piecewise((x_r, y_r > x_r), (y_r, True)) / z))
    expr.subs({1: 1.0})
    sinh(Piecewise((x_r, y_r > x_r), (y_r, True)) * z ** -1.0).is_zero",_2048.py,25,k ** m % k == 0,not k ** m % k
https://github.com/sympy/sympy/tree/master/sympy/core/tests/test_arit.py,"def test_Mod():
    assert Mod(x, 1).func is Mod
    assert pi % pi is S.Zero
    assert Mod(5, 3) == 2
    assert Mod(-5, 3) == 1
    assert Mod(5, -3) == -1
    assert Mod(-5, -3) == -2
    assert type(Mod(3.2, 2, evaluate=False)) == Mod
    assert 5 % x == Mod(5, x)
    assert x % 5 == Mod(x, 5)
    assert x % y == Mod(x, y)
    assert (x % y).subs({x: 5, y: 3}) == 2
    assert Mod(nan, 1) is nan
    assert Mod(1, nan) is nan
    assert Mod(nan, nan) is nan

    assert Mod(0, x) == 0
    with raises(ZeroDivisionError):
        Mod(x, 0)

    k = Symbol('k', integer=True)
    m = Symbol('m', integer=True, positive=True)
    assert (x**m % x).func is Mod
    assert (k**(-m) % k).func is Mod
    assert k**m % k == 0
    assert (-2*k)**m % k == 0

    # Float handling
    point3 = Float(3.3) % 1
    assert (x - 3.3) % 1 == Mod(1.*x + 1 - point3, 1)
    assert Mod(-3.3, 1) == 1 - point3
    assert Mod(0.7, 1) == Float(0.7)
    e = Mod(1.3, 1)
    assert comp(e, .3) and e.is_Float
    e = Mod(1.3, .7)
    assert comp(e, .6) and e.is_Float
    e = Mod(1.3, Rational(7, 10))
    assert comp(e, .6) and e.is_Float
    e = Mod(Rational(13, 10), 0.7)
    assert comp(e, .6) and e.is_Float
    e = Mod(Rational(13, 10), Rational(7, 10))
    assert comp(e, .6) and e.is_Rational

    # check that sign is right
    r2 = sqrt(2)
    r3 = sqrt(3)
    for i in [-r3, -r2, r2, r3]:
        for j in [-r3, -r2, r2, r3]:
            assert verify_numerically(i % j, i.n() % j.n())
    for _x in range(4):
        for _y in range(9):
            reps = [(x, _x), (y, _y)]
            assert Mod(3*x + y, 9).subs(reps) == (3*_x + _y) % 9

    # denesting
    t = Symbol('t', real=True)
    assert Mod(Mod(x, t), t) == Mod(x, t)
    assert Mod(-Mod(x, t), t) == Mod(-x, t)
    assert Mod(Mod(x, 2*t), t) == Mod(x, t)
    assert Mod(-Mod(x, 2*t), t) == Mod(-x, t)
    assert Mod(Mod(x, t), 2*t) == Mod(x, t)
    assert Mod(-Mod(x, t), -2*t) == -Mod(x, t)
    for i in [-4, -2, 2, 4]:
        for j in [-4, -2, 2, 4]:
            for k in range(4):
                assert Mod(Mod(x, i), j).subs({x: k}) == (k % i) % j
                assert Mod(-Mod(x, i), j).subs({x: k}) == -(k % i) % j

    # known difference
    assert Mod(5*sqrt(2), sqrt(5)) == 5*sqrt(2) - 3*sqrt(5)
    p = symbols('p', positive=True)
    assert Mod(2, p + 3) == 2
    assert Mod(-2, p + 3) == p + 1
    assert Mod(2, -p - 3) == -p - 1
    assert Mod(-2, -p - 3) == -2
    assert Mod(p + 5, p + 3) == 2
    assert Mod(-p - 5, p + 3) == p + 1
    assert Mod(p + 5, -p - 3) == -p - 1
    assert Mod(-p - 5, -p - 3) == -2
    assert Mod(p + 1, p - 1).func is Mod

    # handling sums
    assert (x + 3) % 1 == Mod(x, 1)
    assert (x + 3.0) % 1 == Mod(1.*x, 1)
    assert (x - S(33)/10) % 1 == Mod(x + S(7)/10, 1)

    a = Mod(.6*x + y, .3*y)
    b = Mod(0.1*y + 0.6*x, 0.3*y)
    # Test that a, b are equal, with 1e-14 accuracy in coefficients
    eps = 1e-14
    assert abs((a.args[0] - b.args[0]).subs({x: 1, y: 1})) < eps
    assert abs((a.args[1] - b.args[1]).subs({x: 1, y: 1})) < eps

    assert (x + 1) % x == 1 % x
    assert (x + y) % x == y % x
    assert (x + y + 2) % x == (y + 2) % x
    assert (a + 3*x + 1) % (2*x) == Mod(a + x + 1, 2*x)
    assert (12*x + 18*y) % (3*x) == 3*Mod(6*y, x)

    # gcd extraction
    assert (-3*x) % (-2*y) == -Mod(3*x, 2*y)
    assert (.6*pi) % (.3*x*pi) == 0.3*pi*Mod(2, x)
    assert (.6*pi) % (.31*x*pi) == pi*Mod(0.6, 0.31*x)
    assert (6*pi) % (.3*x*pi) == 0.3*pi*Mod(20, x)
    assert (6*pi) % (.31*x*pi) == pi*Mod(6, 0.31*x)
    assert (6*pi) % (.42*x*pi) == pi*Mod(6, 0.42*x)
    assert (12*x) % (2*y) == 2*Mod(6*x, y)
    assert (12*x) % (3*5*y) == 3*Mod(4*x, 5*y)
    assert (12*x) % (15*x*y) == 3*x*Mod(4, 5*y)
    assert (-2*pi) % (3*pi) == pi
    assert (2*x + 2) % (x + 1) == 0
    assert (x*(x + 1)) % (x + 1) == (x + 1)*Mod(x, 1)
    assert Mod(5.0*x, 0.1*y) == 0.1*Mod(50*x, y)
    i = Symbol('i', integer=True)
    assert (3*i*x) % (2*i*y) == i*Mod(3*x, 2*y)
    assert Mod(4*i, 4) == 0

    # issue 8677
    n = Symbol('n', integer=True, positive=True)
    assert factorial(n) % n == 0
    assert factorial(n + 2) % n == 0
    assert (factorial(n + 4) % (n + 5)).func is Mod

    # Wilson's theorem
    assert factorial(18042, evaluate=False) % 18043 == 18042
    p = Symbol('n', prime=True)
    assert factorial(p - 1) % p == p - 1
    assert factorial(p - 1) % -p == -1
    assert (factorial(3, evaluate=False) % 4).doit() == 2
    n = Symbol('n', composite=True, odd=True)
    assert factorial(n - 1) % n == 0

    # symbolic with known parity
    n = Symbol('n', even=True)
    assert Mod(n, 2) == 0
    n = Symbol('n', odd=True)
    assert Mod(n, 2) == 1

    # issue 10963
    assert (x**6000%400).args[1] == 400

    #issue 13543
    assert Mod(Mod(x + 1, 2) + 1, 2) == Mod(x, 2)

    assert Mod(Mod(x + 2, 4)*(x + 4), 4) == Mod(x*(x + 2), 4)
    assert Mod(Mod(x + 2, 4)*4, 4) == 0

    # issue 15493
    i, j = symbols('i j', integer=True, positive=True)
    assert Mod(3*i, 2) == Mod(i, 2)
    assert Mod(8*i/j, 4) == 4*Mod(2*i/j, 1)
    assert Mod(8*i, 4) == 0

    # rewrite
    assert Mod(x, y).rewrite(floor) == x - y*floor(x/y)
    assert ((x - Mod(x, y))/y).rewrite(floor) == floor(x/y)

    # issue 21373
    from sympy.functions.elementary.trigonometric import sinh
    from sympy.functions.elementary.piecewise import Piecewise

    x_r, y_r = symbols('x_r y_r', real=True)
    assert (Piecewise((x_r, y_r > x_r), (y_r, True)) / z) % 1
    expr = exp(sinh(Piecewise((x_r, y_r > x_r), (y_r, True)) / z))
    expr.subs({1: 1.0})
    sinh(Piecewise((x_r, y_r > x_r), (y_r, True)) * z ** -1.0).is_zero",_2048.py,26,(-2 * k) ** m % k == 0,not (-2 * k) ** m % k
https://github.com/sympy/sympy/tree/master/sympy/core/tests/test_arit.py,"def test_Mod():
    assert Mod(x, 1).func is Mod
    assert pi % pi is S.Zero
    assert Mod(5, 3) == 2
    assert Mod(-5, 3) == 1
    assert Mod(5, -3) == -1
    assert Mod(-5, -3) == -2
    assert type(Mod(3.2, 2, evaluate=False)) == Mod
    assert 5 % x == Mod(5, x)
    assert x % 5 == Mod(x, 5)
    assert x % y == Mod(x, y)
    assert (x % y).subs({x: 5, y: 3}) == 2
    assert Mod(nan, 1) is nan
    assert Mod(1, nan) is nan
    assert Mod(nan, nan) is nan

    assert Mod(0, x) == 0
    with raises(ZeroDivisionError):
        Mod(x, 0)

    k = Symbol('k', integer=True)
    m = Symbol('m', integer=True, positive=True)
    assert (x**m % x).func is Mod
    assert (k**(-m) % k).func is Mod
    assert k**m % k == 0
    assert (-2*k)**m % k == 0

    # Float handling
    point3 = Float(3.3) % 1
    assert (x - 3.3) % 1 == Mod(1.*x + 1 - point3, 1)
    assert Mod(-3.3, 1) == 1 - point3
    assert Mod(0.7, 1) == Float(0.7)
    e = Mod(1.3, 1)
    assert comp(e, .3) and e.is_Float
    e = Mod(1.3, .7)
    assert comp(e, .6) and e.is_Float
    e = Mod(1.3, Rational(7, 10))
    assert comp(e, .6) and e.is_Float
    e = Mod(Rational(13, 10), 0.7)
    assert comp(e, .6) and e.is_Float
    e = Mod(Rational(13, 10), Rational(7, 10))
    assert comp(e, .6) and e.is_Rational

    # check that sign is right
    r2 = sqrt(2)
    r3 = sqrt(3)
    for i in [-r3, -r2, r2, r3]:
        for j in [-r3, -r2, r2, r3]:
            assert verify_numerically(i % j, i.n() % j.n())
    for _x in range(4):
        for _y in range(9):
            reps = [(x, _x), (y, _y)]
            assert Mod(3*x + y, 9).subs(reps) == (3*_x + _y) % 9

    # denesting
    t = Symbol('t', real=True)
    assert Mod(Mod(x, t), t) == Mod(x, t)
    assert Mod(-Mod(x, t), t) == Mod(-x, t)
    assert Mod(Mod(x, 2*t), t) == Mod(x, t)
    assert Mod(-Mod(x, 2*t), t) == Mod(-x, t)
    assert Mod(Mod(x, t), 2*t) == Mod(x, t)
    assert Mod(-Mod(x, t), -2*t) == -Mod(x, t)
    for i in [-4, -2, 2, 4]:
        for j in [-4, -2, 2, 4]:
            for k in range(4):
                assert Mod(Mod(x, i), j).subs({x: k}) == (k % i) % j
                assert Mod(-Mod(x, i), j).subs({x: k}) == -(k % i) % j

    # known difference
    assert Mod(5*sqrt(2), sqrt(5)) == 5*sqrt(2) - 3*sqrt(5)
    p = symbols('p', positive=True)
    assert Mod(2, p + 3) == 2
    assert Mod(-2, p + 3) == p + 1
    assert Mod(2, -p - 3) == -p - 1
    assert Mod(-2, -p - 3) == -2
    assert Mod(p + 5, p + 3) == 2
    assert Mod(-p - 5, p + 3) == p + 1
    assert Mod(p + 5, -p - 3) == -p - 1
    assert Mod(-p - 5, -p - 3) == -2
    assert Mod(p + 1, p - 1).func is Mod

    # handling sums
    assert (x + 3) % 1 == Mod(x, 1)
    assert (x + 3.0) % 1 == Mod(1.*x, 1)
    assert (x - S(33)/10) % 1 == Mod(x + S(7)/10, 1)

    a = Mod(.6*x + y, .3*y)
    b = Mod(0.1*y + 0.6*x, 0.3*y)
    # Test that a, b are equal, with 1e-14 accuracy in coefficients
    eps = 1e-14
    assert abs((a.args[0] - b.args[0]).subs({x: 1, y: 1})) < eps
    assert abs((a.args[1] - b.args[1]).subs({x: 1, y: 1})) < eps

    assert (x + 1) % x == 1 % x
    assert (x + y) % x == y % x
    assert (x + y + 2) % x == (y + 2) % x
    assert (a + 3*x + 1) % (2*x) == Mod(a + x + 1, 2*x)
    assert (12*x + 18*y) % (3*x) == 3*Mod(6*y, x)

    # gcd extraction
    assert (-3*x) % (-2*y) == -Mod(3*x, 2*y)
    assert (.6*pi) % (.3*x*pi) == 0.3*pi*Mod(2, x)
    assert (.6*pi) % (.31*x*pi) == pi*Mod(0.6, 0.31*x)
    assert (6*pi) % (.3*x*pi) == 0.3*pi*Mod(20, x)
    assert (6*pi) % (.31*x*pi) == pi*Mod(6, 0.31*x)
    assert (6*pi) % (.42*x*pi) == pi*Mod(6, 0.42*x)
    assert (12*x) % (2*y) == 2*Mod(6*x, y)
    assert (12*x) % (3*5*y) == 3*Mod(4*x, 5*y)
    assert (12*x) % (15*x*y) == 3*x*Mod(4, 5*y)
    assert (-2*pi) % (3*pi) == pi
    assert (2*x + 2) % (x + 1) == 0
    assert (x*(x + 1)) % (x + 1) == (x + 1)*Mod(x, 1)
    assert Mod(5.0*x, 0.1*y) == 0.1*Mod(50*x, y)
    i = Symbol('i', integer=True)
    assert (3*i*x) % (2*i*y) == i*Mod(3*x, 2*y)
    assert Mod(4*i, 4) == 0

    # issue 8677
    n = Symbol('n', integer=True, positive=True)
    assert factorial(n) % n == 0
    assert factorial(n + 2) % n == 0
    assert (factorial(n + 4) % (n + 5)).func is Mod

    # Wilson's theorem
    assert factorial(18042, evaluate=False) % 18043 == 18042
    p = Symbol('n', prime=True)
    assert factorial(p - 1) % p == p - 1
    assert factorial(p - 1) % -p == -1
    assert (factorial(3, evaluate=False) % 4).doit() == 2
    n = Symbol('n', composite=True, odd=True)
    assert factorial(n - 1) % n == 0

    # symbolic with known parity
    n = Symbol('n', even=True)
    assert Mod(n, 2) == 0
    n = Symbol('n', odd=True)
    assert Mod(n, 2) == 1

    # issue 10963
    assert (x**6000%400).args[1] == 400

    #issue 13543
    assert Mod(Mod(x + 1, 2) + 1, 2) == Mod(x, 2)

    assert Mod(Mod(x + 2, 4)*(x + 4), 4) == Mod(x*(x + 2), 4)
    assert Mod(Mod(x + 2, 4)*4, 4) == 0

    # issue 15493
    i, j = symbols('i j', integer=True, positive=True)
    assert Mod(3*i, 2) == Mod(i, 2)
    assert Mod(8*i/j, 4) == 4*Mod(2*i/j, 1)
    assert Mod(8*i, 4) == 0

    # rewrite
    assert Mod(x, y).rewrite(floor) == x - y*floor(x/y)
    assert ((x - Mod(x, y))/y).rewrite(floor) == floor(x/y)

    # issue 21373
    from sympy.functions.elementary.trigonometric import sinh
    from sympy.functions.elementary.piecewise import Piecewise

    x_r, y_r = symbols('x_r y_r', real=True)
    assert (Piecewise((x_r, y_r > x_r), (y_r, True)) / z) % 1
    expr = exp(sinh(Piecewise((x_r, y_r > x_r), (y_r, True)) / z))
    expr.subs({1: 1.0})
    sinh(Piecewise((x_r, y_r > x_r), (y_r, True)) * z ** -1.0).is_zero",_2048.py,111,(2 * x + 2) % (x + 1) == 0,not (2 * x + 2) % (x + 1)
https://github.com/sympy/sympy/tree/master/sympy/core/tests/test_arit.py,"def test_Mod():
    assert Mod(x, 1).func is Mod
    assert pi % pi is S.Zero
    assert Mod(5, 3) == 2
    assert Mod(-5, 3) == 1
    assert Mod(5, -3) == -1
    assert Mod(-5, -3) == -2
    assert type(Mod(3.2, 2, evaluate=False)) == Mod
    assert 5 % x == Mod(5, x)
    assert x % 5 == Mod(x, 5)
    assert x % y == Mod(x, y)
    assert (x % y).subs({x: 5, y: 3}) == 2
    assert Mod(nan, 1) is nan
    assert Mod(1, nan) is nan
    assert Mod(nan, nan) is nan

    assert Mod(0, x) == 0
    with raises(ZeroDivisionError):
        Mod(x, 0)

    k = Symbol('k', integer=True)
    m = Symbol('m', integer=True, positive=True)
    assert (x**m % x).func is Mod
    assert (k**(-m) % k).func is Mod
    assert k**m % k == 0
    assert (-2*k)**m % k == 0

    # Float handling
    point3 = Float(3.3) % 1
    assert (x - 3.3) % 1 == Mod(1.*x + 1 - point3, 1)
    assert Mod(-3.3, 1) == 1 - point3
    assert Mod(0.7, 1) == Float(0.7)
    e = Mod(1.3, 1)
    assert comp(e, .3) and e.is_Float
    e = Mod(1.3, .7)
    assert comp(e, .6) and e.is_Float
    e = Mod(1.3, Rational(7, 10))
    assert comp(e, .6) and e.is_Float
    e = Mod(Rational(13, 10), 0.7)
    assert comp(e, .6) and e.is_Float
    e = Mod(Rational(13, 10), Rational(7, 10))
    assert comp(e, .6) and e.is_Rational

    # check that sign is right
    r2 = sqrt(2)
    r3 = sqrt(3)
    for i in [-r3, -r2, r2, r3]:
        for j in [-r3, -r2, r2, r3]:
            assert verify_numerically(i % j, i.n() % j.n())
    for _x in range(4):
        for _y in range(9):
            reps = [(x, _x), (y, _y)]
            assert Mod(3*x + y, 9).subs(reps) == (3*_x + _y) % 9

    # denesting
    t = Symbol('t', real=True)
    assert Mod(Mod(x, t), t) == Mod(x, t)
    assert Mod(-Mod(x, t), t) == Mod(-x, t)
    assert Mod(Mod(x, 2*t), t) == Mod(x, t)
    assert Mod(-Mod(x, 2*t), t) == Mod(-x, t)
    assert Mod(Mod(x, t), 2*t) == Mod(x, t)
    assert Mod(-Mod(x, t), -2*t) == -Mod(x, t)
    for i in [-4, -2, 2, 4]:
        for j in [-4, -2, 2, 4]:
            for k in range(4):
                assert Mod(Mod(x, i), j).subs({x: k}) == (k % i) % j
                assert Mod(-Mod(x, i), j).subs({x: k}) == -(k % i) % j

    # known difference
    assert Mod(5*sqrt(2), sqrt(5)) == 5*sqrt(2) - 3*sqrt(5)
    p = symbols('p', positive=True)
    assert Mod(2, p + 3) == 2
    assert Mod(-2, p + 3) == p + 1
    assert Mod(2, -p - 3) == -p - 1
    assert Mod(-2, -p - 3) == -2
    assert Mod(p + 5, p + 3) == 2
    assert Mod(-p - 5, p + 3) == p + 1
    assert Mod(p + 5, -p - 3) == -p - 1
    assert Mod(-p - 5, -p - 3) == -2
    assert Mod(p + 1, p - 1).func is Mod

    # handling sums
    assert (x + 3) % 1 == Mod(x, 1)
    assert (x + 3.0) % 1 == Mod(1.*x, 1)
    assert (x - S(33)/10) % 1 == Mod(x + S(7)/10, 1)

    a = Mod(.6*x + y, .3*y)
    b = Mod(0.1*y + 0.6*x, 0.3*y)
    # Test that a, b are equal, with 1e-14 accuracy in coefficients
    eps = 1e-14
    assert abs((a.args[0] - b.args[0]).subs({x: 1, y: 1})) < eps
    assert abs((a.args[1] - b.args[1]).subs({x: 1, y: 1})) < eps

    assert (x + 1) % x == 1 % x
    assert (x + y) % x == y % x
    assert (x + y + 2) % x == (y + 2) % x
    assert (a + 3*x + 1) % (2*x) == Mod(a + x + 1, 2*x)
    assert (12*x + 18*y) % (3*x) == 3*Mod(6*y, x)

    # gcd extraction
    assert (-3*x) % (-2*y) == -Mod(3*x, 2*y)
    assert (.6*pi) % (.3*x*pi) == 0.3*pi*Mod(2, x)
    assert (.6*pi) % (.31*x*pi) == pi*Mod(0.6, 0.31*x)
    assert (6*pi) % (.3*x*pi) == 0.3*pi*Mod(20, x)
    assert (6*pi) % (.31*x*pi) == pi*Mod(6, 0.31*x)
    assert (6*pi) % (.42*x*pi) == pi*Mod(6, 0.42*x)
    assert (12*x) % (2*y) == 2*Mod(6*x, y)
    assert (12*x) % (3*5*y) == 3*Mod(4*x, 5*y)
    assert (12*x) % (15*x*y) == 3*x*Mod(4, 5*y)
    assert (-2*pi) % (3*pi) == pi
    assert (2*x + 2) % (x + 1) == 0
    assert (x*(x + 1)) % (x + 1) == (x + 1)*Mod(x, 1)
    assert Mod(5.0*x, 0.1*y) == 0.1*Mod(50*x, y)
    i = Symbol('i', integer=True)
    assert (3*i*x) % (2*i*y) == i*Mod(3*x, 2*y)
    assert Mod(4*i, 4) == 0

    # issue 8677
    n = Symbol('n', integer=True, positive=True)
    assert factorial(n) % n == 0
    assert factorial(n + 2) % n == 0
    assert (factorial(n + 4) % (n + 5)).func is Mod

    # Wilson's theorem
    assert factorial(18042, evaluate=False) % 18043 == 18042
    p = Symbol('n', prime=True)
    assert factorial(p - 1) % p == p - 1
    assert factorial(p - 1) % -p == -1
    assert (factorial(3, evaluate=False) % 4).doit() == 2
    n = Symbol('n', composite=True, odd=True)
    assert factorial(n - 1) % n == 0

    # symbolic with known parity
    n = Symbol('n', even=True)
    assert Mod(n, 2) == 0
    n = Symbol('n', odd=True)
    assert Mod(n, 2) == 1

    # issue 10963
    assert (x**6000%400).args[1] == 400

    #issue 13543
    assert Mod(Mod(x + 1, 2) + 1, 2) == Mod(x, 2)

    assert Mod(Mod(x + 2, 4)*(x + 4), 4) == Mod(x*(x + 2), 4)
    assert Mod(Mod(x + 2, 4)*4, 4) == 0

    # issue 15493
    i, j = symbols('i j', integer=True, positive=True)
    assert Mod(3*i, 2) == Mod(i, 2)
    assert Mod(8*i/j, 4) == 4*Mod(2*i/j, 1)
    assert Mod(8*i, 4) == 0

    # rewrite
    assert Mod(x, y).rewrite(floor) == x - y*floor(x/y)
    assert ((x - Mod(x, y))/y).rewrite(floor) == floor(x/y)

    # issue 21373
    from sympy.functions.elementary.trigonometric import sinh
    from sympy.functions.elementary.piecewise import Piecewise

    x_r, y_r = symbols('x_r y_r', real=True)
    assert (Piecewise((x_r, y_r > x_r), (y_r, True)) / z) % 1
    expr = exp(sinh(Piecewise((x_r, y_r > x_r), (y_r, True)) / z))
    expr.subs({1: 1.0})
    sinh(Piecewise((x_r, y_r > x_r), (y_r, True)) * z ** -1.0).is_zero",_2048.py,116,"Mod(4 * i, 4) == 0","not Mod(4 * i, 4)"
https://github.com/sympy/sympy/tree/master/sympy/core/tests/test_arit.py,"def test_Mod():
    assert Mod(x, 1).func is Mod
    assert pi % pi is S.Zero
    assert Mod(5, 3) == 2
    assert Mod(-5, 3) == 1
    assert Mod(5, -3) == -1
    assert Mod(-5, -3) == -2
    assert type(Mod(3.2, 2, evaluate=False)) == Mod
    assert 5 % x == Mod(5, x)
    assert x % 5 == Mod(x, 5)
    assert x % y == Mod(x, y)
    assert (x % y).subs({x: 5, y: 3}) == 2
    assert Mod(nan, 1) is nan
    assert Mod(1, nan) is nan
    assert Mod(nan, nan) is nan

    assert Mod(0, x) == 0
    with raises(ZeroDivisionError):
        Mod(x, 0)

    k = Symbol('k', integer=True)
    m = Symbol('m', integer=True, positive=True)
    assert (x**m % x).func is Mod
    assert (k**(-m) % k).func is Mod
    assert k**m % k == 0
    assert (-2*k)**m % k == 0

    # Float handling
    point3 = Float(3.3) % 1
    assert (x - 3.3) % 1 == Mod(1.*x + 1 - point3, 1)
    assert Mod(-3.3, 1) == 1 - point3
    assert Mod(0.7, 1) == Float(0.7)
    e = Mod(1.3, 1)
    assert comp(e, .3) and e.is_Float
    e = Mod(1.3, .7)
    assert comp(e, .6) and e.is_Float
    e = Mod(1.3, Rational(7, 10))
    assert comp(e, .6) and e.is_Float
    e = Mod(Rational(13, 10), 0.7)
    assert comp(e, .6) and e.is_Float
    e = Mod(Rational(13, 10), Rational(7, 10))
    assert comp(e, .6) and e.is_Rational

    # check that sign is right
    r2 = sqrt(2)
    r3 = sqrt(3)
    for i in [-r3, -r2, r2, r3]:
        for j in [-r3, -r2, r2, r3]:
            assert verify_numerically(i % j, i.n() % j.n())
    for _x in range(4):
        for _y in range(9):
            reps = [(x, _x), (y, _y)]
            assert Mod(3*x + y, 9).subs(reps) == (3*_x + _y) % 9

    # denesting
    t = Symbol('t', real=True)
    assert Mod(Mod(x, t), t) == Mod(x, t)
    assert Mod(-Mod(x, t), t) == Mod(-x, t)
    assert Mod(Mod(x, 2*t), t) == Mod(x, t)
    assert Mod(-Mod(x, 2*t), t) == Mod(-x, t)
    assert Mod(Mod(x, t), 2*t) == Mod(x, t)
    assert Mod(-Mod(x, t), -2*t) == -Mod(x, t)
    for i in [-4, -2, 2, 4]:
        for j in [-4, -2, 2, 4]:
            for k in range(4):
                assert Mod(Mod(x, i), j).subs({x: k}) == (k % i) % j
                assert Mod(-Mod(x, i), j).subs({x: k}) == -(k % i) % j

    # known difference
    assert Mod(5*sqrt(2), sqrt(5)) == 5*sqrt(2) - 3*sqrt(5)
    p = symbols('p', positive=True)
    assert Mod(2, p + 3) == 2
    assert Mod(-2, p + 3) == p + 1
    assert Mod(2, -p - 3) == -p - 1
    assert Mod(-2, -p - 3) == -2
    assert Mod(p + 5, p + 3) == 2
    assert Mod(-p - 5, p + 3) == p + 1
    assert Mod(p + 5, -p - 3) == -p - 1
    assert Mod(-p - 5, -p - 3) == -2
    assert Mod(p + 1, p - 1).func is Mod

    # handling sums
    assert (x + 3) % 1 == Mod(x, 1)
    assert (x + 3.0) % 1 == Mod(1.*x, 1)
    assert (x - S(33)/10) % 1 == Mod(x + S(7)/10, 1)

    a = Mod(.6*x + y, .3*y)
    b = Mod(0.1*y + 0.6*x, 0.3*y)
    # Test that a, b are equal, with 1e-14 accuracy in coefficients
    eps = 1e-14
    assert abs((a.args[0] - b.args[0]).subs({x: 1, y: 1})) < eps
    assert abs((a.args[1] - b.args[1]).subs({x: 1, y: 1})) < eps

    assert (x + 1) % x == 1 % x
    assert (x + y) % x == y % x
    assert (x + y + 2) % x == (y + 2) % x
    assert (a + 3*x + 1) % (2*x) == Mod(a + x + 1, 2*x)
    assert (12*x + 18*y) % (3*x) == 3*Mod(6*y, x)

    # gcd extraction
    assert (-3*x) % (-2*y) == -Mod(3*x, 2*y)
    assert (.6*pi) % (.3*x*pi) == 0.3*pi*Mod(2, x)
    assert (.6*pi) % (.31*x*pi) == pi*Mod(0.6, 0.31*x)
    assert (6*pi) % (.3*x*pi) == 0.3*pi*Mod(20, x)
    assert (6*pi) % (.31*x*pi) == pi*Mod(6, 0.31*x)
    assert (6*pi) % (.42*x*pi) == pi*Mod(6, 0.42*x)
    assert (12*x) % (2*y) == 2*Mod(6*x, y)
    assert (12*x) % (3*5*y) == 3*Mod(4*x, 5*y)
    assert (12*x) % (15*x*y) == 3*x*Mod(4, 5*y)
    assert (-2*pi) % (3*pi) == pi
    assert (2*x + 2) % (x + 1) == 0
    assert (x*(x + 1)) % (x + 1) == (x + 1)*Mod(x, 1)
    assert Mod(5.0*x, 0.1*y) == 0.1*Mod(50*x, y)
    i = Symbol('i', integer=True)
    assert (3*i*x) % (2*i*y) == i*Mod(3*x, 2*y)
    assert Mod(4*i, 4) == 0

    # issue 8677
    n = Symbol('n', integer=True, positive=True)
    assert factorial(n) % n == 0
    assert factorial(n + 2) % n == 0
    assert (factorial(n + 4) % (n + 5)).func is Mod

    # Wilson's theorem
    assert factorial(18042, evaluate=False) % 18043 == 18042
    p = Symbol('n', prime=True)
    assert factorial(p - 1) % p == p - 1
    assert factorial(p - 1) % -p == -1
    assert (factorial(3, evaluate=False) % 4).doit() == 2
    n = Symbol('n', composite=True, odd=True)
    assert factorial(n - 1) % n == 0

    # symbolic with known parity
    n = Symbol('n', even=True)
    assert Mod(n, 2) == 0
    n = Symbol('n', odd=True)
    assert Mod(n, 2) == 1

    # issue 10963
    assert (x**6000%400).args[1] == 400

    #issue 13543
    assert Mod(Mod(x + 1, 2) + 1, 2) == Mod(x, 2)

    assert Mod(Mod(x + 2, 4)*(x + 4), 4) == Mod(x*(x + 2), 4)
    assert Mod(Mod(x + 2, 4)*4, 4) == 0

    # issue 15493
    i, j = symbols('i j', integer=True, positive=True)
    assert Mod(3*i, 2) == Mod(i, 2)
    assert Mod(8*i/j, 4) == 4*Mod(2*i/j, 1)
    assert Mod(8*i, 4) == 0

    # rewrite
    assert Mod(x, y).rewrite(floor) == x - y*floor(x/y)
    assert ((x - Mod(x, y))/y).rewrite(floor) == floor(x/y)

    # issue 21373
    from sympy.functions.elementary.trigonometric import sinh
    from sympy.functions.elementary.piecewise import Piecewise

    x_r, y_r = symbols('x_r y_r', real=True)
    assert (Piecewise((x_r, y_r > x_r), (y_r, True)) / z) % 1
    expr = exp(sinh(Piecewise((x_r, y_r > x_r), (y_r, True)) / z))
    expr.subs({1: 1.0})
    sinh(Piecewise((x_r, y_r > x_r), (y_r, True)) * z ** -1.0).is_zero",_2048.py,120,factorial(n) % n == 0,not factorial(n) % n
https://github.com/sympy/sympy/tree/master/sympy/core/tests/test_arit.py,"def test_Mod():
    assert Mod(x, 1).func is Mod
    assert pi % pi is S.Zero
    assert Mod(5, 3) == 2
    assert Mod(-5, 3) == 1
    assert Mod(5, -3) == -1
    assert Mod(-5, -3) == -2
    assert type(Mod(3.2, 2, evaluate=False)) == Mod
    assert 5 % x == Mod(5, x)
    assert x % 5 == Mod(x, 5)
    assert x % y == Mod(x, y)
    assert (x % y).subs({x: 5, y: 3}) == 2
    assert Mod(nan, 1) is nan
    assert Mod(1, nan) is nan
    assert Mod(nan, nan) is nan

    assert Mod(0, x) == 0
    with raises(ZeroDivisionError):
        Mod(x, 0)

    k = Symbol('k', integer=True)
    m = Symbol('m', integer=True, positive=True)
    assert (x**m % x).func is Mod
    assert (k**(-m) % k).func is Mod
    assert k**m % k == 0
    assert (-2*k)**m % k == 0

    # Float handling
    point3 = Float(3.3) % 1
    assert (x - 3.3) % 1 == Mod(1.*x + 1 - point3, 1)
    assert Mod(-3.3, 1) == 1 - point3
    assert Mod(0.7, 1) == Float(0.7)
    e = Mod(1.3, 1)
    assert comp(e, .3) and e.is_Float
    e = Mod(1.3, .7)
    assert comp(e, .6) and e.is_Float
    e = Mod(1.3, Rational(7, 10))
    assert comp(e, .6) and e.is_Float
    e = Mod(Rational(13, 10), 0.7)
    assert comp(e, .6) and e.is_Float
    e = Mod(Rational(13, 10), Rational(7, 10))
    assert comp(e, .6) and e.is_Rational

    # check that sign is right
    r2 = sqrt(2)
    r3 = sqrt(3)
    for i in [-r3, -r2, r2, r3]:
        for j in [-r3, -r2, r2, r3]:
            assert verify_numerically(i % j, i.n() % j.n())
    for _x in range(4):
        for _y in range(9):
            reps = [(x, _x), (y, _y)]
            assert Mod(3*x + y, 9).subs(reps) == (3*_x + _y) % 9

    # denesting
    t = Symbol('t', real=True)
    assert Mod(Mod(x, t), t) == Mod(x, t)
    assert Mod(-Mod(x, t), t) == Mod(-x, t)
    assert Mod(Mod(x, 2*t), t) == Mod(x, t)
    assert Mod(-Mod(x, 2*t), t) == Mod(-x, t)
    assert Mod(Mod(x, t), 2*t) == Mod(x, t)
    assert Mod(-Mod(x, t), -2*t) == -Mod(x, t)
    for i in [-4, -2, 2, 4]:
        for j in [-4, -2, 2, 4]:
            for k in range(4):
                assert Mod(Mod(x, i), j).subs({x: k}) == (k % i) % j
                assert Mod(-Mod(x, i), j).subs({x: k}) == -(k % i) % j

    # known difference
    assert Mod(5*sqrt(2), sqrt(5)) == 5*sqrt(2) - 3*sqrt(5)
    p = symbols('p', positive=True)
    assert Mod(2, p + 3) == 2
    assert Mod(-2, p + 3) == p + 1
    assert Mod(2, -p - 3) == -p - 1
    assert Mod(-2, -p - 3) == -2
    assert Mod(p + 5, p + 3) == 2
    assert Mod(-p - 5, p + 3) == p + 1
    assert Mod(p + 5, -p - 3) == -p - 1
    assert Mod(-p - 5, -p - 3) == -2
    assert Mod(p + 1, p - 1).func is Mod

    # handling sums
    assert (x + 3) % 1 == Mod(x, 1)
    assert (x + 3.0) % 1 == Mod(1.*x, 1)
    assert (x - S(33)/10) % 1 == Mod(x + S(7)/10, 1)

    a = Mod(.6*x + y, .3*y)
    b = Mod(0.1*y + 0.6*x, 0.3*y)
    # Test that a, b are equal, with 1e-14 accuracy in coefficients
    eps = 1e-14
    assert abs((a.args[0] - b.args[0]).subs({x: 1, y: 1})) < eps
    assert abs((a.args[1] - b.args[1]).subs({x: 1, y: 1})) < eps

    assert (x + 1) % x == 1 % x
    assert (x + y) % x == y % x
    assert (x + y + 2) % x == (y + 2) % x
    assert (a + 3*x + 1) % (2*x) == Mod(a + x + 1, 2*x)
    assert (12*x + 18*y) % (3*x) == 3*Mod(6*y, x)

    # gcd extraction
    assert (-3*x) % (-2*y) == -Mod(3*x, 2*y)
    assert (.6*pi) % (.3*x*pi) == 0.3*pi*Mod(2, x)
    assert (.6*pi) % (.31*x*pi) == pi*Mod(0.6, 0.31*x)
    assert (6*pi) % (.3*x*pi) == 0.3*pi*Mod(20, x)
    assert (6*pi) % (.31*x*pi) == pi*Mod(6, 0.31*x)
    assert (6*pi) % (.42*x*pi) == pi*Mod(6, 0.42*x)
    assert (12*x) % (2*y) == 2*Mod(6*x, y)
    assert (12*x) % (3*5*y) == 3*Mod(4*x, 5*y)
    assert (12*x) % (15*x*y) == 3*x*Mod(4, 5*y)
    assert (-2*pi) % (3*pi) == pi
    assert (2*x + 2) % (x + 1) == 0
    assert (x*(x + 1)) % (x + 1) == (x + 1)*Mod(x, 1)
    assert Mod(5.0*x, 0.1*y) == 0.1*Mod(50*x, y)
    i = Symbol('i', integer=True)
    assert (3*i*x) % (2*i*y) == i*Mod(3*x, 2*y)
    assert Mod(4*i, 4) == 0

    # issue 8677
    n = Symbol('n', integer=True, positive=True)
    assert factorial(n) % n == 0
    assert factorial(n + 2) % n == 0
    assert (factorial(n + 4) % (n + 5)).func is Mod

    # Wilson's theorem
    assert factorial(18042, evaluate=False) % 18043 == 18042
    p = Symbol('n', prime=True)
    assert factorial(p - 1) % p == p - 1
    assert factorial(p - 1) % -p == -1
    assert (factorial(3, evaluate=False) % 4).doit() == 2
    n = Symbol('n', composite=True, odd=True)
    assert factorial(n - 1) % n == 0

    # symbolic with known parity
    n = Symbol('n', even=True)
    assert Mod(n, 2) == 0
    n = Symbol('n', odd=True)
    assert Mod(n, 2) == 1

    # issue 10963
    assert (x**6000%400).args[1] == 400

    #issue 13543
    assert Mod(Mod(x + 1, 2) + 1, 2) == Mod(x, 2)

    assert Mod(Mod(x + 2, 4)*(x + 4), 4) == Mod(x*(x + 2), 4)
    assert Mod(Mod(x + 2, 4)*4, 4) == 0

    # issue 15493
    i, j = symbols('i j', integer=True, positive=True)
    assert Mod(3*i, 2) == Mod(i, 2)
    assert Mod(8*i/j, 4) == 4*Mod(2*i/j, 1)
    assert Mod(8*i, 4) == 0

    # rewrite
    assert Mod(x, y).rewrite(floor) == x - y*floor(x/y)
    assert ((x - Mod(x, y))/y).rewrite(floor) == floor(x/y)

    # issue 21373
    from sympy.functions.elementary.trigonometric import sinh
    from sympy.functions.elementary.piecewise import Piecewise

    x_r, y_r = symbols('x_r y_r', real=True)
    assert (Piecewise((x_r, y_r > x_r), (y_r, True)) / z) % 1
    expr = exp(sinh(Piecewise((x_r, y_r > x_r), (y_r, True)) / z))
    expr.subs({1: 1.0})
    sinh(Piecewise((x_r, y_r > x_r), (y_r, True)) * z ** -1.0).is_zero",_2048.py,121,factorial(n + 2) % n == 0,not factorial(n + 2) % n
https://github.com/sympy/sympy/tree/master/sympy/core/tests/test_arit.py,"def test_Mod():
    assert Mod(x, 1).func is Mod
    assert pi % pi is S.Zero
    assert Mod(5, 3) == 2
    assert Mod(-5, 3) == 1
    assert Mod(5, -3) == -1
    assert Mod(-5, -3) == -2
    assert type(Mod(3.2, 2, evaluate=False)) == Mod
    assert 5 % x == Mod(5, x)
    assert x % 5 == Mod(x, 5)
    assert x % y == Mod(x, y)
    assert (x % y).subs({x: 5, y: 3}) == 2
    assert Mod(nan, 1) is nan
    assert Mod(1, nan) is nan
    assert Mod(nan, nan) is nan

    assert Mod(0, x) == 0
    with raises(ZeroDivisionError):
        Mod(x, 0)

    k = Symbol('k', integer=True)
    m = Symbol('m', integer=True, positive=True)
    assert (x**m % x).func is Mod
    assert (k**(-m) % k).func is Mod
    assert k**m % k == 0
    assert (-2*k)**m % k == 0

    # Float handling
    point3 = Float(3.3) % 1
    assert (x - 3.3) % 1 == Mod(1.*x + 1 - point3, 1)
    assert Mod(-3.3, 1) == 1 - point3
    assert Mod(0.7, 1) == Float(0.7)
    e = Mod(1.3, 1)
    assert comp(e, .3) and e.is_Float
    e = Mod(1.3, .7)
    assert comp(e, .6) and e.is_Float
    e = Mod(1.3, Rational(7, 10))
    assert comp(e, .6) and e.is_Float
    e = Mod(Rational(13, 10), 0.7)
    assert comp(e, .6) and e.is_Float
    e = Mod(Rational(13, 10), Rational(7, 10))
    assert comp(e, .6) and e.is_Rational

    # check that sign is right
    r2 = sqrt(2)
    r3 = sqrt(3)
    for i in [-r3, -r2, r2, r3]:
        for j in [-r3, -r2, r2, r3]:
            assert verify_numerically(i % j, i.n() % j.n())
    for _x in range(4):
        for _y in range(9):
            reps = [(x, _x), (y, _y)]
            assert Mod(3*x + y, 9).subs(reps) == (3*_x + _y) % 9

    # denesting
    t = Symbol('t', real=True)
    assert Mod(Mod(x, t), t) == Mod(x, t)
    assert Mod(-Mod(x, t), t) == Mod(-x, t)
    assert Mod(Mod(x, 2*t), t) == Mod(x, t)
    assert Mod(-Mod(x, 2*t), t) == Mod(-x, t)
    assert Mod(Mod(x, t), 2*t) == Mod(x, t)
    assert Mod(-Mod(x, t), -2*t) == -Mod(x, t)
    for i in [-4, -2, 2, 4]:
        for j in [-4, -2, 2, 4]:
            for k in range(4):
                assert Mod(Mod(x, i), j).subs({x: k}) == (k % i) % j
                assert Mod(-Mod(x, i), j).subs({x: k}) == -(k % i) % j

    # known difference
    assert Mod(5*sqrt(2), sqrt(5)) == 5*sqrt(2) - 3*sqrt(5)
    p = symbols('p', positive=True)
    assert Mod(2, p + 3) == 2
    assert Mod(-2, p + 3) == p + 1
    assert Mod(2, -p - 3) == -p - 1
    assert Mod(-2, -p - 3) == -2
    assert Mod(p + 5, p + 3) == 2
    assert Mod(-p - 5, p + 3) == p + 1
    assert Mod(p + 5, -p - 3) == -p - 1
    assert Mod(-p - 5, -p - 3) == -2
    assert Mod(p + 1, p - 1).func is Mod

    # handling sums
    assert (x + 3) % 1 == Mod(x, 1)
    assert (x + 3.0) % 1 == Mod(1.*x, 1)
    assert (x - S(33)/10) % 1 == Mod(x + S(7)/10, 1)

    a = Mod(.6*x + y, .3*y)
    b = Mod(0.1*y + 0.6*x, 0.3*y)
    # Test that a, b are equal, with 1e-14 accuracy in coefficients
    eps = 1e-14
    assert abs((a.args[0] - b.args[0]).subs({x: 1, y: 1})) < eps
    assert abs((a.args[1] - b.args[1]).subs({x: 1, y: 1})) < eps

    assert (x + 1) % x == 1 % x
    assert (x + y) % x == y % x
    assert (x + y + 2) % x == (y + 2) % x
    assert (a + 3*x + 1) % (2*x) == Mod(a + x + 1, 2*x)
    assert (12*x + 18*y) % (3*x) == 3*Mod(6*y, x)

    # gcd extraction
    assert (-3*x) % (-2*y) == -Mod(3*x, 2*y)
    assert (.6*pi) % (.3*x*pi) == 0.3*pi*Mod(2, x)
    assert (.6*pi) % (.31*x*pi) == pi*Mod(0.6, 0.31*x)
    assert (6*pi) % (.3*x*pi) == 0.3*pi*Mod(20, x)
    assert (6*pi) % (.31*x*pi) == pi*Mod(6, 0.31*x)
    assert (6*pi) % (.42*x*pi) == pi*Mod(6, 0.42*x)
    assert (12*x) % (2*y) == 2*Mod(6*x, y)
    assert (12*x) % (3*5*y) == 3*Mod(4*x, 5*y)
    assert (12*x) % (15*x*y) == 3*x*Mod(4, 5*y)
    assert (-2*pi) % (3*pi) == pi
    assert (2*x + 2) % (x + 1) == 0
    assert (x*(x + 1)) % (x + 1) == (x + 1)*Mod(x, 1)
    assert Mod(5.0*x, 0.1*y) == 0.1*Mod(50*x, y)
    i = Symbol('i', integer=True)
    assert (3*i*x) % (2*i*y) == i*Mod(3*x, 2*y)
    assert Mod(4*i, 4) == 0

    # issue 8677
    n = Symbol('n', integer=True, positive=True)
    assert factorial(n) % n == 0
    assert factorial(n + 2) % n == 0
    assert (factorial(n + 4) % (n + 5)).func is Mod

    # Wilson's theorem
    assert factorial(18042, evaluate=False) % 18043 == 18042
    p = Symbol('n', prime=True)
    assert factorial(p - 1) % p == p - 1
    assert factorial(p - 1) % -p == -1
    assert (factorial(3, evaluate=False) % 4).doit() == 2
    n = Symbol('n', composite=True, odd=True)
    assert factorial(n - 1) % n == 0

    # symbolic with known parity
    n = Symbol('n', even=True)
    assert Mod(n, 2) == 0
    n = Symbol('n', odd=True)
    assert Mod(n, 2) == 1

    # issue 10963
    assert (x**6000%400).args[1] == 400

    #issue 13543
    assert Mod(Mod(x + 1, 2) + 1, 2) == Mod(x, 2)

    assert Mod(Mod(x + 2, 4)*(x + 4), 4) == Mod(x*(x + 2), 4)
    assert Mod(Mod(x + 2, 4)*4, 4) == 0

    # issue 15493
    i, j = symbols('i j', integer=True, positive=True)
    assert Mod(3*i, 2) == Mod(i, 2)
    assert Mod(8*i/j, 4) == 4*Mod(2*i/j, 1)
    assert Mod(8*i, 4) == 0

    # rewrite
    assert Mod(x, y).rewrite(floor) == x - y*floor(x/y)
    assert ((x - Mod(x, y))/y).rewrite(floor) == floor(x/y)

    # issue 21373
    from sympy.functions.elementary.trigonometric import sinh
    from sympy.functions.elementary.piecewise import Piecewise

    x_r, y_r = symbols('x_r y_r', real=True)
    assert (Piecewise((x_r, y_r > x_r), (y_r, True)) / z) % 1
    expr = exp(sinh(Piecewise((x_r, y_r > x_r), (y_r, True)) / z))
    expr.subs({1: 1.0})
    sinh(Piecewise((x_r, y_r > x_r), (y_r, True)) * z ** -1.0).is_zero",_2048.py,131,factorial(n - 1) % n == 0,not factorial(n - 1) % n
https://github.com/sympy/sympy/tree/master/sympy/core/tests/test_arit.py,"def test_Mod():
    assert Mod(x, 1).func is Mod
    assert pi % pi is S.Zero
    assert Mod(5, 3) == 2
    assert Mod(-5, 3) == 1
    assert Mod(5, -3) == -1
    assert Mod(-5, -3) == -2
    assert type(Mod(3.2, 2, evaluate=False)) == Mod
    assert 5 % x == Mod(5, x)
    assert x % 5 == Mod(x, 5)
    assert x % y == Mod(x, y)
    assert (x % y).subs({x: 5, y: 3}) == 2
    assert Mod(nan, 1) is nan
    assert Mod(1, nan) is nan
    assert Mod(nan, nan) is nan

    assert Mod(0, x) == 0
    with raises(ZeroDivisionError):
        Mod(x, 0)

    k = Symbol('k', integer=True)
    m = Symbol('m', integer=True, positive=True)
    assert (x**m % x).func is Mod
    assert (k**(-m) % k).func is Mod
    assert k**m % k == 0
    assert (-2*k)**m % k == 0

    # Float handling
    point3 = Float(3.3) % 1
    assert (x - 3.3) % 1 == Mod(1.*x + 1 - point3, 1)
    assert Mod(-3.3, 1) == 1 - point3
    assert Mod(0.7, 1) == Float(0.7)
    e = Mod(1.3, 1)
    assert comp(e, .3) and e.is_Float
    e = Mod(1.3, .7)
    assert comp(e, .6) and e.is_Float
    e = Mod(1.3, Rational(7, 10))
    assert comp(e, .6) and e.is_Float
    e = Mod(Rational(13, 10), 0.7)
    assert comp(e, .6) and e.is_Float
    e = Mod(Rational(13, 10), Rational(7, 10))
    assert comp(e, .6) and e.is_Rational

    # check that sign is right
    r2 = sqrt(2)
    r3 = sqrt(3)
    for i in [-r3, -r2, r2, r3]:
        for j in [-r3, -r2, r2, r3]:
            assert verify_numerically(i % j, i.n() % j.n())
    for _x in range(4):
        for _y in range(9):
            reps = [(x, _x), (y, _y)]
            assert Mod(3*x + y, 9).subs(reps) == (3*_x + _y) % 9

    # denesting
    t = Symbol('t', real=True)
    assert Mod(Mod(x, t), t) == Mod(x, t)
    assert Mod(-Mod(x, t), t) == Mod(-x, t)
    assert Mod(Mod(x, 2*t), t) == Mod(x, t)
    assert Mod(-Mod(x, 2*t), t) == Mod(-x, t)
    assert Mod(Mod(x, t), 2*t) == Mod(x, t)
    assert Mod(-Mod(x, t), -2*t) == -Mod(x, t)
    for i in [-4, -2, 2, 4]:
        for j in [-4, -2, 2, 4]:
            for k in range(4):
                assert Mod(Mod(x, i), j).subs({x: k}) == (k % i) % j
                assert Mod(-Mod(x, i), j).subs({x: k}) == -(k % i) % j

    # known difference
    assert Mod(5*sqrt(2), sqrt(5)) == 5*sqrt(2) - 3*sqrt(5)
    p = symbols('p', positive=True)
    assert Mod(2, p + 3) == 2
    assert Mod(-2, p + 3) == p + 1
    assert Mod(2, -p - 3) == -p - 1
    assert Mod(-2, -p - 3) == -2
    assert Mod(p + 5, p + 3) == 2
    assert Mod(-p - 5, p + 3) == p + 1
    assert Mod(p + 5, -p - 3) == -p - 1
    assert Mod(-p - 5, -p - 3) == -2
    assert Mod(p + 1, p - 1).func is Mod

    # handling sums
    assert (x + 3) % 1 == Mod(x, 1)
    assert (x + 3.0) % 1 == Mod(1.*x, 1)
    assert (x - S(33)/10) % 1 == Mod(x + S(7)/10, 1)

    a = Mod(.6*x + y, .3*y)
    b = Mod(0.1*y + 0.6*x, 0.3*y)
    # Test that a, b are equal, with 1e-14 accuracy in coefficients
    eps = 1e-14
    assert abs((a.args[0] - b.args[0]).subs({x: 1, y: 1})) < eps
    assert abs((a.args[1] - b.args[1]).subs({x: 1, y: 1})) < eps

    assert (x + 1) % x == 1 % x
    assert (x + y) % x == y % x
    assert (x + y + 2) % x == (y + 2) % x
    assert (a + 3*x + 1) % (2*x) == Mod(a + x + 1, 2*x)
    assert (12*x + 18*y) % (3*x) == 3*Mod(6*y, x)

    # gcd extraction
    assert (-3*x) % (-2*y) == -Mod(3*x, 2*y)
    assert (.6*pi) % (.3*x*pi) == 0.3*pi*Mod(2, x)
    assert (.6*pi) % (.31*x*pi) == pi*Mod(0.6, 0.31*x)
    assert (6*pi) % (.3*x*pi) == 0.3*pi*Mod(20, x)
    assert (6*pi) % (.31*x*pi) == pi*Mod(6, 0.31*x)
    assert (6*pi) % (.42*x*pi) == pi*Mod(6, 0.42*x)
    assert (12*x) % (2*y) == 2*Mod(6*x, y)
    assert (12*x) % (3*5*y) == 3*Mod(4*x, 5*y)
    assert (12*x) % (15*x*y) == 3*x*Mod(4, 5*y)
    assert (-2*pi) % (3*pi) == pi
    assert (2*x + 2) % (x + 1) == 0
    assert (x*(x + 1)) % (x + 1) == (x + 1)*Mod(x, 1)
    assert Mod(5.0*x, 0.1*y) == 0.1*Mod(50*x, y)
    i = Symbol('i', integer=True)
    assert (3*i*x) % (2*i*y) == i*Mod(3*x, 2*y)
    assert Mod(4*i, 4) == 0

    # issue 8677
    n = Symbol('n', integer=True, positive=True)
    assert factorial(n) % n == 0
    assert factorial(n + 2) % n == 0
    assert (factorial(n + 4) % (n + 5)).func is Mod

    # Wilson's theorem
    assert factorial(18042, evaluate=False) % 18043 == 18042
    p = Symbol('n', prime=True)
    assert factorial(p - 1) % p == p - 1
    assert factorial(p - 1) % -p == -1
    assert (factorial(3, evaluate=False) % 4).doit() == 2
    n = Symbol('n', composite=True, odd=True)
    assert factorial(n - 1) % n == 0

    # symbolic with known parity
    n = Symbol('n', even=True)
    assert Mod(n, 2) == 0
    n = Symbol('n', odd=True)
    assert Mod(n, 2) == 1

    # issue 10963
    assert (x**6000%400).args[1] == 400

    #issue 13543
    assert Mod(Mod(x + 1, 2) + 1, 2) == Mod(x, 2)

    assert Mod(Mod(x + 2, 4)*(x + 4), 4) == Mod(x*(x + 2), 4)
    assert Mod(Mod(x + 2, 4)*4, 4) == 0

    # issue 15493
    i, j = symbols('i j', integer=True, positive=True)
    assert Mod(3*i, 2) == Mod(i, 2)
    assert Mod(8*i/j, 4) == 4*Mod(2*i/j, 1)
    assert Mod(8*i, 4) == 0

    # rewrite
    assert Mod(x, y).rewrite(floor) == x - y*floor(x/y)
    assert ((x - Mod(x, y))/y).rewrite(floor) == floor(x/y)

    # issue 21373
    from sympy.functions.elementary.trigonometric import sinh
    from sympy.functions.elementary.piecewise import Piecewise

    x_r, y_r = symbols('x_r y_r', real=True)
    assert (Piecewise((x_r, y_r > x_r), (y_r, True)) / z) % 1
    expr = exp(sinh(Piecewise((x_r, y_r > x_r), (y_r, True)) / z))
    expr.subs({1: 1.0})
    sinh(Piecewise((x_r, y_r > x_r), (y_r, True)) * z ** -1.0).is_zero",_2048.py,135,"Mod(n, 2) == 0","not Mod(n, 2)"
https://github.com/sympy/sympy/tree/master/sympy/core/tests/test_arit.py,"def test_Mod():
    assert Mod(x, 1).func is Mod
    assert pi % pi is S.Zero
    assert Mod(5, 3) == 2
    assert Mod(-5, 3) == 1
    assert Mod(5, -3) == -1
    assert Mod(-5, -3) == -2
    assert type(Mod(3.2, 2, evaluate=False)) == Mod
    assert 5 % x == Mod(5, x)
    assert x % 5 == Mod(x, 5)
    assert x % y == Mod(x, y)
    assert (x % y).subs({x: 5, y: 3}) == 2
    assert Mod(nan, 1) is nan
    assert Mod(1, nan) is nan
    assert Mod(nan, nan) is nan

    assert Mod(0, x) == 0
    with raises(ZeroDivisionError):
        Mod(x, 0)

    k = Symbol('k', integer=True)
    m = Symbol('m', integer=True, positive=True)
    assert (x**m % x).func is Mod
    assert (k**(-m) % k).func is Mod
    assert k**m % k == 0
    assert (-2*k)**m % k == 0

    # Float handling
    point3 = Float(3.3) % 1
    assert (x - 3.3) % 1 == Mod(1.*x + 1 - point3, 1)
    assert Mod(-3.3, 1) == 1 - point3
    assert Mod(0.7, 1) == Float(0.7)
    e = Mod(1.3, 1)
    assert comp(e, .3) and e.is_Float
    e = Mod(1.3, .7)
    assert comp(e, .6) and e.is_Float
    e = Mod(1.3, Rational(7, 10))
    assert comp(e, .6) and e.is_Float
    e = Mod(Rational(13, 10), 0.7)
    assert comp(e, .6) and e.is_Float
    e = Mod(Rational(13, 10), Rational(7, 10))
    assert comp(e, .6) and e.is_Rational

    # check that sign is right
    r2 = sqrt(2)
    r3 = sqrt(3)
    for i in [-r3, -r2, r2, r3]:
        for j in [-r3, -r2, r2, r3]:
            assert verify_numerically(i % j, i.n() % j.n())
    for _x in range(4):
        for _y in range(9):
            reps = [(x, _x), (y, _y)]
            assert Mod(3*x + y, 9).subs(reps) == (3*_x + _y) % 9

    # denesting
    t = Symbol('t', real=True)
    assert Mod(Mod(x, t), t) == Mod(x, t)
    assert Mod(-Mod(x, t), t) == Mod(-x, t)
    assert Mod(Mod(x, 2*t), t) == Mod(x, t)
    assert Mod(-Mod(x, 2*t), t) == Mod(-x, t)
    assert Mod(Mod(x, t), 2*t) == Mod(x, t)
    assert Mod(-Mod(x, t), -2*t) == -Mod(x, t)
    for i in [-4, -2, 2, 4]:
        for j in [-4, -2, 2, 4]:
            for k in range(4):
                assert Mod(Mod(x, i), j).subs({x: k}) == (k % i) % j
                assert Mod(-Mod(x, i), j).subs({x: k}) == -(k % i) % j

    # known difference
    assert Mod(5*sqrt(2), sqrt(5)) == 5*sqrt(2) - 3*sqrt(5)
    p = symbols('p', positive=True)
    assert Mod(2, p + 3) == 2
    assert Mod(-2, p + 3) == p + 1
    assert Mod(2, -p - 3) == -p - 1
    assert Mod(-2, -p - 3) == -2
    assert Mod(p + 5, p + 3) == 2
    assert Mod(-p - 5, p + 3) == p + 1
    assert Mod(p + 5, -p - 3) == -p - 1
    assert Mod(-p - 5, -p - 3) == -2
    assert Mod(p + 1, p - 1).func is Mod

    # handling sums
    assert (x + 3) % 1 == Mod(x, 1)
    assert (x + 3.0) % 1 == Mod(1.*x, 1)
    assert (x - S(33)/10) % 1 == Mod(x + S(7)/10, 1)

    a = Mod(.6*x + y, .3*y)
    b = Mod(0.1*y + 0.6*x, 0.3*y)
    # Test that a, b are equal, with 1e-14 accuracy in coefficients
    eps = 1e-14
    assert abs((a.args[0] - b.args[0]).subs({x: 1, y: 1})) < eps
    assert abs((a.args[1] - b.args[1]).subs({x: 1, y: 1})) < eps

    assert (x + 1) % x == 1 % x
    assert (x + y) % x == y % x
    assert (x + y + 2) % x == (y + 2) % x
    assert (a + 3*x + 1) % (2*x) == Mod(a + x + 1, 2*x)
    assert (12*x + 18*y) % (3*x) == 3*Mod(6*y, x)

    # gcd extraction
    assert (-3*x) % (-2*y) == -Mod(3*x, 2*y)
    assert (.6*pi) % (.3*x*pi) == 0.3*pi*Mod(2, x)
    assert (.6*pi) % (.31*x*pi) == pi*Mod(0.6, 0.31*x)
    assert (6*pi) % (.3*x*pi) == 0.3*pi*Mod(20, x)
    assert (6*pi) % (.31*x*pi) == pi*Mod(6, 0.31*x)
    assert (6*pi) % (.42*x*pi) == pi*Mod(6, 0.42*x)
    assert (12*x) % (2*y) == 2*Mod(6*x, y)
    assert (12*x) % (3*5*y) == 3*Mod(4*x, 5*y)
    assert (12*x) % (15*x*y) == 3*x*Mod(4, 5*y)
    assert (-2*pi) % (3*pi) == pi
    assert (2*x + 2) % (x + 1) == 0
    assert (x*(x + 1)) % (x + 1) == (x + 1)*Mod(x, 1)
    assert Mod(5.0*x, 0.1*y) == 0.1*Mod(50*x, y)
    i = Symbol('i', integer=True)
    assert (3*i*x) % (2*i*y) == i*Mod(3*x, 2*y)
    assert Mod(4*i, 4) == 0

    # issue 8677
    n = Symbol('n', integer=True, positive=True)
    assert factorial(n) % n == 0
    assert factorial(n + 2) % n == 0
    assert (factorial(n + 4) % (n + 5)).func is Mod

    # Wilson's theorem
    assert factorial(18042, evaluate=False) % 18043 == 18042
    p = Symbol('n', prime=True)
    assert factorial(p - 1) % p == p - 1
    assert factorial(p - 1) % -p == -1
    assert (factorial(3, evaluate=False) % 4).doit() == 2
    n = Symbol('n', composite=True, odd=True)
    assert factorial(n - 1) % n == 0

    # symbolic with known parity
    n = Symbol('n', even=True)
    assert Mod(n, 2) == 0
    n = Symbol('n', odd=True)
    assert Mod(n, 2) == 1

    # issue 10963
    assert (x**6000%400).args[1] == 400

    #issue 13543
    assert Mod(Mod(x + 1, 2) + 1, 2) == Mod(x, 2)

    assert Mod(Mod(x + 2, 4)*(x + 4), 4) == Mod(x*(x + 2), 4)
    assert Mod(Mod(x + 2, 4)*4, 4) == 0

    # issue 15493
    i, j = symbols('i j', integer=True, positive=True)
    assert Mod(3*i, 2) == Mod(i, 2)
    assert Mod(8*i/j, 4) == 4*Mod(2*i/j, 1)
    assert Mod(8*i, 4) == 0

    # rewrite
    assert Mod(x, y).rewrite(floor) == x - y*floor(x/y)
    assert ((x - Mod(x, y))/y).rewrite(floor) == floor(x/y)

    # issue 21373
    from sympy.functions.elementary.trigonometric import sinh
    from sympy.functions.elementary.piecewise import Piecewise

    x_r, y_r = symbols('x_r y_r', real=True)
    assert (Piecewise((x_r, y_r > x_r), (y_r, True)) / z) % 1
    expr = exp(sinh(Piecewise((x_r, y_r > x_r), (y_r, True)) / z))
    expr.subs({1: 1.0})
    sinh(Piecewise((x_r, y_r > x_r), (y_r, True)) * z ** -1.0).is_zero",_2048.py,146,"Mod(Mod(x + 2, 4) * 4, 4) == 0","not Mod(Mod(x + 2, 4) * 4, 4)"
https://github.com/sympy/sympy/tree/master/sympy/core/tests/test_arit.py,"def test_Mod():
    assert Mod(x, 1).func is Mod
    assert pi % pi is S.Zero
    assert Mod(5, 3) == 2
    assert Mod(-5, 3) == 1
    assert Mod(5, -3) == -1
    assert Mod(-5, -3) == -2
    assert type(Mod(3.2, 2, evaluate=False)) == Mod
    assert 5 % x == Mod(5, x)
    assert x % 5 == Mod(x, 5)
    assert x % y == Mod(x, y)
    assert (x % y).subs({x: 5, y: 3}) == 2
    assert Mod(nan, 1) is nan
    assert Mod(1, nan) is nan
    assert Mod(nan, nan) is nan

    assert Mod(0, x) == 0
    with raises(ZeroDivisionError):
        Mod(x, 0)

    k = Symbol('k', integer=True)
    m = Symbol('m', integer=True, positive=True)
    assert (x**m % x).func is Mod
    assert (k**(-m) % k).func is Mod
    assert k**m % k == 0
    assert (-2*k)**m % k == 0

    # Float handling
    point3 = Float(3.3) % 1
    assert (x - 3.3) % 1 == Mod(1.*x + 1 - point3, 1)
    assert Mod(-3.3, 1) == 1 - point3
    assert Mod(0.7, 1) == Float(0.7)
    e = Mod(1.3, 1)
    assert comp(e, .3) and e.is_Float
    e = Mod(1.3, .7)
    assert comp(e, .6) and e.is_Float
    e = Mod(1.3, Rational(7, 10))
    assert comp(e, .6) and e.is_Float
    e = Mod(Rational(13, 10), 0.7)
    assert comp(e, .6) and e.is_Float
    e = Mod(Rational(13, 10), Rational(7, 10))
    assert comp(e, .6) and e.is_Rational

    # check that sign is right
    r2 = sqrt(2)
    r3 = sqrt(3)
    for i in [-r3, -r2, r2, r3]:
        for j in [-r3, -r2, r2, r3]:
            assert verify_numerically(i % j, i.n() % j.n())
    for _x in range(4):
        for _y in range(9):
            reps = [(x, _x), (y, _y)]
            assert Mod(3*x + y, 9).subs(reps) == (3*_x + _y) % 9

    # denesting
    t = Symbol('t', real=True)
    assert Mod(Mod(x, t), t) == Mod(x, t)
    assert Mod(-Mod(x, t), t) == Mod(-x, t)
    assert Mod(Mod(x, 2*t), t) == Mod(x, t)
    assert Mod(-Mod(x, 2*t), t) == Mod(-x, t)
    assert Mod(Mod(x, t), 2*t) == Mod(x, t)
    assert Mod(-Mod(x, t), -2*t) == -Mod(x, t)
    for i in [-4, -2, 2, 4]:
        for j in [-4, -2, 2, 4]:
            for k in range(4):
                assert Mod(Mod(x, i), j).subs({x: k}) == (k % i) % j
                assert Mod(-Mod(x, i), j).subs({x: k}) == -(k % i) % j

    # known difference
    assert Mod(5*sqrt(2), sqrt(5)) == 5*sqrt(2) - 3*sqrt(5)
    p = symbols('p', positive=True)
    assert Mod(2, p + 3) == 2
    assert Mod(-2, p + 3) == p + 1
    assert Mod(2, -p - 3) == -p - 1
    assert Mod(-2, -p - 3) == -2
    assert Mod(p + 5, p + 3) == 2
    assert Mod(-p - 5, p + 3) == p + 1
    assert Mod(p + 5, -p - 3) == -p - 1
    assert Mod(-p - 5, -p - 3) == -2
    assert Mod(p + 1, p - 1).func is Mod

    # handling sums
    assert (x + 3) % 1 == Mod(x, 1)
    assert (x + 3.0) % 1 == Mod(1.*x, 1)
    assert (x - S(33)/10) % 1 == Mod(x + S(7)/10, 1)

    a = Mod(.6*x + y, .3*y)
    b = Mod(0.1*y + 0.6*x, 0.3*y)
    # Test that a, b are equal, with 1e-14 accuracy in coefficients
    eps = 1e-14
    assert abs((a.args[0] - b.args[0]).subs({x: 1, y: 1})) < eps
    assert abs((a.args[1] - b.args[1]).subs({x: 1, y: 1})) < eps

    assert (x + 1) % x == 1 % x
    assert (x + y) % x == y % x
    assert (x + y + 2) % x == (y + 2) % x
    assert (a + 3*x + 1) % (2*x) == Mod(a + x + 1, 2*x)
    assert (12*x + 18*y) % (3*x) == 3*Mod(6*y, x)

    # gcd extraction
    assert (-3*x) % (-2*y) == -Mod(3*x, 2*y)
    assert (.6*pi) % (.3*x*pi) == 0.3*pi*Mod(2, x)
    assert (.6*pi) % (.31*x*pi) == pi*Mod(0.6, 0.31*x)
    assert (6*pi) % (.3*x*pi) == 0.3*pi*Mod(20, x)
    assert (6*pi) % (.31*x*pi) == pi*Mod(6, 0.31*x)
    assert (6*pi) % (.42*x*pi) == pi*Mod(6, 0.42*x)
    assert (12*x) % (2*y) == 2*Mod(6*x, y)
    assert (12*x) % (3*5*y) == 3*Mod(4*x, 5*y)
    assert (12*x) % (15*x*y) == 3*x*Mod(4, 5*y)
    assert (-2*pi) % (3*pi) == pi
    assert (2*x + 2) % (x + 1) == 0
    assert (x*(x + 1)) % (x + 1) == (x + 1)*Mod(x, 1)
    assert Mod(5.0*x, 0.1*y) == 0.1*Mod(50*x, y)
    i = Symbol('i', integer=True)
    assert (3*i*x) % (2*i*y) == i*Mod(3*x, 2*y)
    assert Mod(4*i, 4) == 0

    # issue 8677
    n = Symbol('n', integer=True, positive=True)
    assert factorial(n) % n == 0
    assert factorial(n + 2) % n == 0
    assert (factorial(n + 4) % (n + 5)).func is Mod

    # Wilson's theorem
    assert factorial(18042, evaluate=False) % 18043 == 18042
    p = Symbol('n', prime=True)
    assert factorial(p - 1) % p == p - 1
    assert factorial(p - 1) % -p == -1
    assert (factorial(3, evaluate=False) % 4).doit() == 2
    n = Symbol('n', composite=True, odd=True)
    assert factorial(n - 1) % n == 0

    # symbolic with known parity
    n = Symbol('n', even=True)
    assert Mod(n, 2) == 0
    n = Symbol('n', odd=True)
    assert Mod(n, 2) == 1

    # issue 10963
    assert (x**6000%400).args[1] == 400

    #issue 13543
    assert Mod(Mod(x + 1, 2) + 1, 2) == Mod(x, 2)

    assert Mod(Mod(x + 2, 4)*(x + 4), 4) == Mod(x*(x + 2), 4)
    assert Mod(Mod(x + 2, 4)*4, 4) == 0

    # issue 15493
    i, j = symbols('i j', integer=True, positive=True)
    assert Mod(3*i, 2) == Mod(i, 2)
    assert Mod(8*i/j, 4) == 4*Mod(2*i/j, 1)
    assert Mod(8*i, 4) == 0

    # rewrite
    assert Mod(x, y).rewrite(floor) == x - y*floor(x/y)
    assert ((x - Mod(x, y))/y).rewrite(floor) == floor(x/y)

    # issue 21373
    from sympy.functions.elementary.trigonometric import sinh
    from sympy.functions.elementary.piecewise import Piecewise

    x_r, y_r = symbols('x_r y_r', real=True)
    assert (Piecewise((x_r, y_r > x_r), (y_r, True)) / z) % 1
    expr = exp(sinh(Piecewise((x_r, y_r > x_r), (y_r, True)) / z))
    expr.subs({1: 1.0})
    sinh(Piecewise((x_r, y_r > x_r), (y_r, True)) * z ** -1.0).is_zero",_2048.py,152,"Mod(8 * i, 4) == 0","not Mod(8 * i, 4)"
https://github.com/jaraco/keyring/tree/master/tests/backends/test_kwallet.py,"def check_set_get(self, service, username, password):
        keyring = self.keyring

        # for the non-existent password
        assert keyring.get_password(service, username) is None

        # common usage
        self.set_password(service, username, password, True)
        # re-init keyring to force migration
        self.keyring = keyring = self.init_keyring()
        ret_password = keyring.get_password(service, username)
        assert ret_password == password, (
            ""Incorrect password for username: '%s' ""
            ""on service: '%s'. '%s' != '%s'""
            % (service, username, ret_password, password),
        )

        # for the empty password
        self.set_password(service, username, """", True)
        # re-init keyring to force migration
        self.keyring = keyring = self.init_keyring()
        ret_password = keyring.get_password(service, username)
        assert ret_password == """", (
            ""Incorrect password for username: '%s' ""
            ""on service: '%s'. '%s' != '%s'"" % (service, username, ret_password, """"),
        )
        ret_password = keyring.get_password('Python', username + '@' + service)
        assert ret_password is None, (
            ""Not 'None' password returned for username: '%s' ""
            ""on service: '%s'. '%s' != '%s'. Passwords from old ""
            ""folder should be deleted during migration.""
            % (service, username, ret_password, None),
        )",_2054.py,23,ret_password == '',not ret_password
https://github.com/nucleic/enaml/tree/master/tests/test_stylesheet.py,"def test_name_selector():
    from enaml.styling import StyleCache
    source = dedent(""""""\
    from enaml.widgets.api import Window, Container, PushButton
    from enaml.styling import StyleSheet, Style, Setter

    enamldef Sheet(StyleSheet):
        Style:
            object_name = 'button'
            Setter:
                field = 'background'
                value = 'blue'

    enamldef Main(Window):
        alias button
        alias other
        Sheet:
            pass
        Container:
            PushButton: button:
                name = 'button'
            PushButton: other:
                style_class = 'button'

    """""")
    main = compile_source(source, 'Main')()
    assert len(StyleCache.styles(main.button)) == 1
    assert len(StyleCache.styles(main.other)) == 0",_2056.py,28,len(StyleCache.styles(main.other)) == 0,not len(StyleCache.styles(main.other))
https://github.com/DataDog/integrations-core/tree/master/datadog_checks_dev/datadog_checks/dev/tooling/commands/meta/scripts/github_user.py,"def email2ghuser(email):
    """"""Given an email, attempt to find a Github username
       associated with the email.

    `$ ddev meta scripts email2ghuser example@datadoghq.com`
    """"""

    try:
        response = requests.get(f'https://api.github.com/search/users?q={email}')
        response.raise_for_status()
        content = response.json()

        if content.get('total_count') == 0:
            abort(f'No username found for email {email}')

        user = content.get('items')[0]
        username = user.get('login')

        echo_success(f'Found username ""{username}"" associated with email {email}')

    except Exception as e:
        abort(str(e))",_2099.py,13,content.get('total_count') == 0,not content.get('total_count')
https://github.com/dask/dask/tree/master/dask/dataframe/multi.py,"def concat(
    dfs,
    axis=0,
    join=""outer"",
    interleave_partitions=False,
    ignore_unknown_divisions=False,
    ignore_order=False,
    **kwargs,
):
    """"""Concatenate DataFrames along rows.

    - When axis=0 (default), concatenate DataFrames row-wise:

      - If all divisions are known and ordered, concatenate DataFrames keeping
        divisions. When divisions are not ordered, specifying
        interleave_partition=True allows concatenate divisions each by each.

      - If any of division is unknown, concatenate DataFrames resetting its
        division to unknown (None)

    - When axis=1, concatenate DataFrames column-wise:

      - Allowed if all divisions are known.

      - If any of division is unknown, it raises ValueError.

    Parameters
    ----------
    dfs : list
        List of dask.DataFrames to be concatenated
    axis : {0, 1, 'index', 'columns'}, default 0
        The axis to concatenate along
    join : {'inner', 'outer'}, default 'outer'
        How to handle indexes on other axis
    interleave_partitions : bool, default False
        Whether to concatenate DataFrames ignoring its order. If True, every
        divisions are concatenated each by each.
    ignore_unknown_divisions : bool, default False
        By default a warning is raised if any input has unknown divisions.
        Set to True to disable this warning.
    ignore_order : bool, default False
        Whether to ignore order when doing the union of categoricals.

    Notes
    -----
    This differs in from ``pd.concat`` in the when concatenating Categoricals
    with different categories. Pandas currently coerces those to objects
    before concatenating. Coercing to objects is very expensive for large
    arrays, so dask preserves the Categoricals by taking the union of
    the categories.

    Examples
    --------
    If all divisions are known and ordered, divisions are kept.

    >>> import dask.dataframe as dd
    >>> a                                               # doctest: +SKIP
    dd.DataFrame<x, divisions=(1, 3, 5)>
    >>> b                                               # doctest: +SKIP
    dd.DataFrame<y, divisions=(6, 8, 10)>
    >>> dd.concat([a, b])                               # doctest: +SKIP
    dd.DataFrame<concat-..., divisions=(1, 3, 6, 8, 10)>

    Unable to concatenate if divisions are not ordered.

    >>> a                                               # doctest: +SKIP
    dd.DataFrame<x, divisions=(1, 3, 5)>
    >>> b                                               # doctest: +SKIP
    dd.DataFrame<y, divisions=(2, 3, 6)>
    >>> dd.concat([a, b])                               # doctest: +SKIP
    ValueError: All inputs have known divisions which cannot be concatenated
    in order. Specify interleave_partitions=True to ignore order

    Specify interleave_partitions=True to ignore the division order.

    >>> dd.concat([a, b], interleave_partitions=True)   # doctest: +SKIP
    dd.DataFrame<concat-..., divisions=(1, 2, 3, 5, 6)>

    If any of division is unknown, the result division will be unknown

    >>> a                                               # doctest: +SKIP
    dd.DataFrame<x, divisions=(None, None)>
    >>> b                                               # doctest: +SKIP
    dd.DataFrame<y, divisions=(1, 4, 10)>
    >>> dd.concat([a, b])                               # doctest: +SKIP
    dd.DataFrame<concat-..., divisions=(None, None, None, None)>

    By default concatenating with unknown divisions will raise a warning.
    Set ``ignore_unknown_divisions=True`` to disable this:

    >>> dd.concat([a, b], ignore_unknown_divisions=True)# doctest: +SKIP
    dd.DataFrame<concat-..., divisions=(None, None, None, None)>

    Different categoricals are unioned

    >>> dd.concat([
    ...     dd.from_pandas(pd.Series(['a', 'b'], dtype='category'), 1),
    ...     dd.from_pandas(pd.Series(['a', 'c'], dtype='category'), 1),
    ... ], interleave_partitions=True).dtype
    CategoricalDtype(categories=['a', 'b', 'c'], ordered=False)
    """"""

    if not isinstance(dfs, list):
        raise TypeError(""dfs must be a list of DataFrames/Series objects"")
    if len(dfs) == 0:
        raise ValueError(""No objects to concatenate"")
    if len(dfs) == 1:
        if axis == 1 and isinstance(dfs[0], Series):
            return dfs[0].to_frame()
        else:
            return dfs[0]

    if join not in (""inner"", ""outer""):
        raise ValueError(""'join' must be 'inner' or 'outer'"")

    axis = DataFrame._validate_axis(axis)
    try:
        # remove any empty DataFrames
        dfs = [df for df in dfs if bool(len(df.columns))]
    except AttributeError:
        # 'Series' object has no attribute 'columns'
        pass
    dasks = [df for df in dfs if isinstance(df, _Frame)]
    dfs = _maybe_from_pandas(dfs)

    if axis == 1:
        if all(df.known_divisions for df in dasks):
            return concat_indexed_dataframes(
                dfs, axis=axis, join=join, ignore_order=ignore_order, **kwargs
            )
        elif (
            len(dasks) == len(dfs)
            and all(not df.known_divisions for df in dfs)
            and len({df.npartitions for df in dasks}) == 1
        ):
            if not ignore_unknown_divisions:
                warnings.warn(
                    ""Concatenating dataframes with unknown divisions.\n""
                    ""We're assuming that the indices of each dataframes""
                    "" are \n aligned. This assumption is not generally ""
                    ""safe.""
                )
            return concat_unindexed_dataframes(dfs, ignore_order=ignore_order, **kwargs)
        else:
            raise ValueError(
                ""Unable to concatenate DataFrame with unknown ""
                ""division specifying axis=1""
            )
    else:
        if all(df.known_divisions for df in dasks):
            # each DataFrame's division must be greater than previous one
            if all(
                dfs[i].divisions[-1] < dfs[i + 1].divisions[0]
                for i in range(len(dfs) - 1)
            ):
                divisions = []
                for df in dfs[:-1]:
                    # remove last to concatenate with next
                    divisions += df.divisions[:-1]
                divisions += dfs[-1].divisions
                return stack_partitions(
                    dfs, divisions, join=join, ignore_order=ignore_order, **kwargs
                )
            elif interleave_partitions:
                return concat_indexed_dataframes(
                    dfs, join=join, ignore_order=ignore_order, **kwargs
                )
            else:
                divisions = [None] * (sum(df.npartitions for df in dfs) + 1)
                return stack_partitions(
                    dfs, divisions, join=join, ignore_order=ignore_order, **kwargs
                )
        else:
            divisions = [None] * (sum(df.npartitions for df in dfs) + 1)
            return stack_partitions(
                dfs, divisions, join=join, ignore_order=ignore_order, **kwargs
            )",_2102.py,105,len(dfs) == 0,not len(dfs)
https://github.com/ucbdrive/3d-vehicle-tracking/tree/master/3d-tracking/tools/object-ap-eval/eval.py,"def get_split_parts(num, num_part):
    assert num_part > 0, ""Invalid number of parts""
    same_part = num // num_part
    remain_num = num % num_part
    num_ = num_part * (same_part > 0)
    if remain_num == 0:
        return [same_part] * num_ 
    else:
        return [same_part] * num_ + [remain_num]",_2129.py,6,remain_num == 0,not remain_num
https://github.com/e2nIEE/pandapower/tree/master/pandapower/pypower/opf_execute.py,"def opf_execute(om, ppopt):
    """"""Executes the OPF specified by an OPF model object.

    C{results} are returned with internal indexing, all equipment
    in-service, etc.

    @see: L{opf}, L{opf_setup}

    @author: Ray Zimmerman (PSERC Cornell)
    @author: Richard Lincoln
    """"""
    ##-----  setup  -----
    ## options
    dc  = ppopt['PF_DC']        ## 1 = DC OPF, 0 = AC OPF
    alg = ppopt['OPF_ALG']
    verbose = ppopt['VERBOSE']

    ## build user-defined costs
    om.build_cost_params()

    ## get indexing
    vv, ll, nn, _ = om.get_idx()

    if verbose > 0:
        v = ppver('all')
        stdout.write('PYPOWER Version %s, %s' % (v['Version'], v['Date']))

    ##-----  run DC OPF solver  -----
    if dc:
        if verbose > 0:
            stdout.write(' -- DC Optimal Power Flow\n')

        results, success, raw = dcopf_solver(om, ppopt)
    else:
        ##-----  run AC OPF solver  -----
        if verbose > 0:
            stdout.write(' -- AC Optimal Power Flow\n')

        ## if OPF_ALG not set, choose best available option
        if alg == 0:
            alg = 560                ## MIPS

        ## update deprecated algorithm codes to new, generalized formulation equivalents
        if alg == 100 | alg == 200:        ## CONSTR
            alg = 300
        elif alg == 120 | alg == 220:      ## dense LP
            alg = 320
        elif alg == 140 | alg == 240:      ## sparse (relaxed) LP
            alg = 340
        elif alg == 160 | alg == 260:      ## sparse (full) LP
            alg = 360

        ppopt['OPF_ALG_POLY'] = alg

        ## run specific AC OPF solver
        if alg == 560 or alg == 565:                   ## PIPS
            results, success, raw = pipsopf_solver(om, ppopt)
#        elif alg == 580:                              ## IPOPT # pragma: no cover
#            try:
#                __import__('pyipopt')
#                results, success, raw = ipoptopf_solver(om, ppopt)
#            except ImportError:
#                raise ImportError('OPF_ALG %d requires IPOPT '
#                                  '(see https://projects.coin-or.org/Ipopt/)' %
#                                  alg)
        else:
            stderr.write('opf_execute: OPF_ALG %d is not a valid algorithm code\n' % alg)

    if ('output' not in raw) or ('alg' not in raw['output']):
        raw['output']['alg'] = alg

    if success:
        if not dc:
            ## copy bus voltages back to gen matrix
            results['gen'][:, VG] = results['bus'][results['gen'][:, GEN_BUS].astype(int), VM]

            ## gen PQ capability curve multipliers
            if (ll['N']['PQh'] > 0) | (ll['N']['PQl'] > 0): # pragma: no cover
                mu_PQh = results['mu']['lin']['l'][ll['i1']['PQh']:ll['iN']['PQh']] - results['mu']['lin']['u'][ll['i1']['PQh']:ll['iN']['PQh']]
                mu_PQl = results['mu']['lin']['l'][ll['i1']['PQl']:ll['iN']['PQl']] - results['mu']['lin']['u'][ll['i1']['PQl']:ll['iN']['PQl']]
                Apqdata = om.userdata('Apqdata')
                results['gen'] = update_mupq(results['baseMVA'], results['gen'], mu_PQh, mu_PQl, Apqdata)

            ## compute g, dg, f, df, d2f if requested by RETURN_RAW_DER = 1
            if ppopt['RETURN_RAW_DER']: # pragma: no cover
                ## move from results to raw if using v4.0 of MINOPF or TSPOPF
                if 'dg' in results:
                    raw = {}
                    raw['dg'] = results['dg']
                    raw['g'] = results['g']

                ## compute g, dg, unless already done by post-v4.0 MINOPF or TSPOPF
                if 'dg' not in raw:
                    ppc = om.get_ppc()
                    Ybus, Yf, Yt = makeYbus(ppc['baseMVA'], ppc['bus'], ppc['branch'])
                    g, geq, dg, dgeq = opf_consfcn(results['x'], om, Ybus, Yf, Yt, ppopt)
                    raw['g'] = r_[geq, g]
                    raw['dg'] = r_[dgeq.T, dg.T]   ## true Jacobian organization

                ## compute df, d2f
                _, df, d2f = opf_costfcn(results['x'], om, True)
                raw['df'] = df
                raw['d2f'] = d2f

        ## delete g and dg fieldsfrom results if using v4.0 of MINOPF or TSPOPF
        if 'dg' in results:
            del results['dg']
            del results['g']

        ## angle limit constraint multipliers
        if ll['N']['ang'] > 0:
            iang = om.userdata('iang')
            results['branch'][iang, MU_ANGMIN] = results['mu']['lin']['l'][ll['i1']['ang']:ll['iN']['ang']] * pi / 180
            results['branch'][iang, MU_ANGMAX] = results['mu']['lin']['u'][ll['i1']['ang']:ll['iN']['ang']] * pi / 180
    else:
        ## assign empty g, dg, f, df, d2f if requested by RETURN_RAW_DER = 1
        if not dc and ppopt['RETURN_RAW_DER']:
            raw['dg'] = array([])
            raw['g'] = array([])
            raw['df'] = array([])
            raw['d2f'] = array([])

    ## assign values and limit shadow prices for variables
    if om.var['order']:
        results['var'] = {'val': {}, 'mu': {'l': {}, 'u': {}}}
    for name in om.var['order']:
        if om.getN('var', name):
            idx = arange(vv['i1'][name], vv['iN'][name])
            results['var']['val'][name] = results['x'][idx]
            results['var']['mu']['l'][name] = results['mu']['var']['l'][idx]
            results['var']['mu']['u'][name] = results['mu']['var']['u'][idx]

    ## assign shadow prices for linear constraints
    if om.lin['order']:
        results['lin'] = {'mu': {'l': {}, 'u': {}}}
    for name in om.lin['order']:
        if om.getN('lin', name):
            idx = arange(ll['i1'][name], ll['iN'][name])
            results['lin']['mu']['l'][name] = results['mu']['lin']['l'][idx]
            results['lin']['mu']['u'][name] = results['mu']['lin']['u'][idx]

    ## assign shadow prices for nonlinear constraints
    if not dc:
        if om.nln['order']:
            results['nln'] = {'mu': {'l': {}, 'u': {}}}
        for name in om.nln['order']:
            if om.getN('nln', name):
                idx = arange(nn['i1'][name], nn['iN'][name])
                results['nln']['mu']['l'][name] = results['mu']['nln']['l'][idx]
                results['nln']['mu']['u'][name] = results['mu']['nln']['u'][idx]

    ## assign values for components of user cost
    if om.cost['order']:
        results['cost'] = {}
    for name in om.cost['order']:
        if om.getN('cost', name):
            results['cost'][name] = om.compute_cost(results['x'], name)

    ## if single-block PWL costs were converted to POLY, insert dummy y into x
    ## Note: The ""y"" portion of x will be nonsense, but everything should at
    ##       least be in the expected locations.
    pwl1 = om.userdata('pwl1')
    if (len(pwl1) > 0) and (alg != 545) and (alg != 550):
        ## get indexing
        vv, _, _, _ = om.get_idx()
        if dc:
            nx = vv['iN']['Pg']
        else:
            nx = vv['iN']['Qg']

        y = zeros(len(pwl1))
        raw['xr'] = r_[raw['xr'][:nx], y, raw['xr'][nx:]]
        results['x'] = r_[results['x'][:nx], y, results['x'][nx:]]

    return results, success, raw",_2131.py,40,alg == 0,not alg
https://github.com/ansible/ansible-modules-extras/tree/master/system/zfs.py,"def create(self):
        if self.module.check_mode:
            self.changed = True
            return
        properties = self.properties
        volsize = properties.pop('volsize', None)
        volblocksize = properties.pop('volblocksize', None)
        origin = properties.pop('origin', None)
        cmd = [self.zfs_cmd]

        if ""@"" in self.name:
            action = 'snapshot'
        elif origin:
            action = 'clone'
        else:
            action = 'create'

        cmd.append(action)

        if action in ['create', 'clone']:
            cmd += ['-p']

        if volsize:
            cmd += ['-V', volsize]
        if volblocksize:
            cmd += ['-b', 'volblocksize']
        if properties:
            for prop, value in properties.iteritems():
                cmd += ['-o', '%s=""%s""' % (prop, value)]
        if origin:
            cmd.append(origin)
        cmd.append(self.name)
        (rc, out, err) = self.module.run_command(' '.join(cmd))
        if rc == 0:
            self.changed = True
        else:
            self.module.fail_json(msg=err)",_2146.py,34,rc == 0,not rc
https://github.com/ansible/galaxy/tree/master/lib/galaxy/visualization/data_providers/phyloviz/newickparser.py,"def parseNode(self, string, depth):
        """"""
        Recursive method for parsing newick string, works by stripping down the string into substring
        of newick contained with brackers, which is used to call itself.

        Eg ... ( A, B, (D, E)C, F, G ) ...

        We will make the preceeding nodes first A, B, then the internal node C, its children D, E,
        and finally the succeeding nodes F, G
        """"""

        # Base case where there is only an empty string
        if string == """":
            return
            # Base case there it's only an internal claude
        if string.find(""("") == -1:
            return self._makeNodesFromString(string, depth)

        nodes = []      # nodes refer to the nodes on this level
        start = 0
        lenOfPreceedingInternalNodeString = 0
        bracketStack = []

        for j in range(len(string)):
            if string[j] == ""("":    # finding the positions of all the open brackets
                bracketStack.append(j)
                continue
            if string[j] == "")"":    # finding the positions of all the closed brackets to extract claude
                i = bracketStack.pop()

                if len(bracketStack) == 0:  # is child of current node

                    InternalNode = None

                    # First flat call to make nodes of the same depth but from the preceeding string.
                    startSubstring = string[start + lenOfPreceedingInternalNodeString: i]
                    preceedingNodes = self._makeNodesFromString(startSubstring, depth)
                    nodes += preceedingNodes

                    # Then We will try to see if the substring has any internal nodes first, make it then make nodes preceeding it and succeeding it.
                    if j + 1 < len(string):
                        stringRightOfBracket = string[j + 1:]      # Eg. '(b:0.4,a:0.3)c:0.3, stringRightOfBracket = c:0.3
                        match = re.search(r""[\)\,\(]"", stringRightOfBracket)
                        if match:
                            indexOfNextSymbol = match.start()
                            stringRepOfInternalNode = stringRightOfBracket[:indexOfNextSymbol]
                            internalNodes = self._makeNodesFromString(stringRepOfInternalNode, depth)
                            if len(internalNodes) > 0:
                                InternalNode = internalNodes[0]
                            lenOfPreceedingInternalNodeString = len(stringRepOfInternalNode)
                        else:   # sometimes the node can be the last element of a string
                            InternalNode = self._makeNodesFromString(string[j + 1:], depth)[0]
                            lenOfPreceedingInternalNodeString = len(string) - j
                    if InternalNode is None:       # creating a generic node if it is unnamed
                        InternalNode = self.phyloTree.makeNode("""", depth=depth, isInternal=True)  # ""internal-"" + str(depth)
                        lenOfPreceedingInternalNodeString = 0

                    # recussive call to make the internal claude
                    childSubString = string[i + 1:j]
                    InternalNode.addChildNode(self.parseNode(childSubString, depth + 1))

                    nodes.append(InternalNode)  # we append the internal node later to preserve order

                    start = j + 1
                continue

        if depth == 0:    # if it's the root node, we do nothing about it and return
            return nodes[0]

        # Adding last most set of children
        endString = string[start:]
        if string[start - 1] == "")"":  # if the symbol belongs to an internal node which is created previously, then we remove it from the string left to parse
            match = re.search(r""[\)\,\(]"", endString)
            if match:
                endOfNodeName = start + match.start() + 1
                endString = string[endOfNodeName:]
                nodes += self._makeNodesFromString(endString, depth)

        return nodes",_2158.py,13,string == '',not string
https://github.com/ansible/galaxy/tree/master/lib/galaxy/visualization/data_providers/phyloviz/newickparser.py,"def parseNode(self, string, depth):
        """"""
        Recursive method for parsing newick string, works by stripping down the string into substring
        of newick contained with brackers, which is used to call itself.

        Eg ... ( A, B, (D, E)C, F, G ) ...

        We will make the preceeding nodes first A, B, then the internal node C, its children D, E,
        and finally the succeeding nodes F, G
        """"""

        # Base case where there is only an empty string
        if string == """":
            return
            # Base case there it's only an internal claude
        if string.find(""("") == -1:
            return self._makeNodesFromString(string, depth)

        nodes = []      # nodes refer to the nodes on this level
        start = 0
        lenOfPreceedingInternalNodeString = 0
        bracketStack = []

        for j in range(len(string)):
            if string[j] == ""("":    # finding the positions of all the open brackets
                bracketStack.append(j)
                continue
            if string[j] == "")"":    # finding the positions of all the closed brackets to extract claude
                i = bracketStack.pop()

                if len(bracketStack) == 0:  # is child of current node

                    InternalNode = None

                    # First flat call to make nodes of the same depth but from the preceeding string.
                    startSubstring = string[start + lenOfPreceedingInternalNodeString: i]
                    preceedingNodes = self._makeNodesFromString(startSubstring, depth)
                    nodes += preceedingNodes

                    # Then We will try to see if the substring has any internal nodes first, make it then make nodes preceeding it and succeeding it.
                    if j + 1 < len(string):
                        stringRightOfBracket = string[j + 1:]      # Eg. '(b:0.4,a:0.3)c:0.3, stringRightOfBracket = c:0.3
                        match = re.search(r""[\)\,\(]"", stringRightOfBracket)
                        if match:
                            indexOfNextSymbol = match.start()
                            stringRepOfInternalNode = stringRightOfBracket[:indexOfNextSymbol]
                            internalNodes = self._makeNodesFromString(stringRepOfInternalNode, depth)
                            if len(internalNodes) > 0:
                                InternalNode = internalNodes[0]
                            lenOfPreceedingInternalNodeString = len(stringRepOfInternalNode)
                        else:   # sometimes the node can be the last element of a string
                            InternalNode = self._makeNodesFromString(string[j + 1:], depth)[0]
                            lenOfPreceedingInternalNodeString = len(string) - j
                    if InternalNode is None:       # creating a generic node if it is unnamed
                        InternalNode = self.phyloTree.makeNode("""", depth=depth, isInternal=True)  # ""internal-"" + str(depth)
                        lenOfPreceedingInternalNodeString = 0

                    # recussive call to make the internal claude
                    childSubString = string[i + 1:j]
                    InternalNode.addChildNode(self.parseNode(childSubString, depth + 1))

                    nodes.append(InternalNode)  # we append the internal node later to preserve order

                    start = j + 1
                continue

        if depth == 0:    # if it's the root node, we do nothing about it and return
            return nodes[0]

        # Adding last most set of children
        endString = string[start:]
        if string[start - 1] == "")"":  # if the symbol belongs to an internal node which is created previously, then we remove it from the string left to parse
            match = re.search(r""[\)\,\(]"", endString)
            if match:
                endOfNodeName = start + match.start() + 1
                endString = string[endOfNodeName:]
                nodes += self._makeNodesFromString(endString, depth)

        return nodes",_2158.py,67,depth == 0,not depth
https://github.com/ansible/galaxy/tree/master/lib/galaxy/visualization/data_providers/phyloviz/newickparser.py,"def parseNode(self, string, depth):
        """"""
        Recursive method for parsing newick string, works by stripping down the string into substring
        of newick contained with brackers, which is used to call itself.

        Eg ... ( A, B, (D, E)C, F, G ) ...

        We will make the preceeding nodes first A, B, then the internal node C, its children D, E,
        and finally the succeeding nodes F, G
        """"""

        # Base case where there is only an empty string
        if string == """":
            return
            # Base case there it's only an internal claude
        if string.find(""("") == -1:
            return self._makeNodesFromString(string, depth)

        nodes = []      # nodes refer to the nodes on this level
        start = 0
        lenOfPreceedingInternalNodeString = 0
        bracketStack = []

        for j in range(len(string)):
            if string[j] == ""("":    # finding the positions of all the open brackets
                bracketStack.append(j)
                continue
            if string[j] == "")"":    # finding the positions of all the closed brackets to extract claude
                i = bracketStack.pop()

                if len(bracketStack) == 0:  # is child of current node

                    InternalNode = None

                    # First flat call to make nodes of the same depth but from the preceeding string.
                    startSubstring = string[start + lenOfPreceedingInternalNodeString: i]
                    preceedingNodes = self._makeNodesFromString(startSubstring, depth)
                    nodes += preceedingNodes

                    # Then We will try to see if the substring has any internal nodes first, make it then make nodes preceeding it and succeeding it.
                    if j + 1 < len(string):
                        stringRightOfBracket = string[j + 1:]      # Eg. '(b:0.4,a:0.3)c:0.3, stringRightOfBracket = c:0.3
                        match = re.search(r""[\)\,\(]"", stringRightOfBracket)
                        if match:
                            indexOfNextSymbol = match.start()
                            stringRepOfInternalNode = stringRightOfBracket[:indexOfNextSymbol]
                            internalNodes = self._makeNodesFromString(stringRepOfInternalNode, depth)
                            if len(internalNodes) > 0:
                                InternalNode = internalNodes[0]
                            lenOfPreceedingInternalNodeString = len(stringRepOfInternalNode)
                        else:   # sometimes the node can be the last element of a string
                            InternalNode = self._makeNodesFromString(string[j + 1:], depth)[0]
                            lenOfPreceedingInternalNodeString = len(string) - j
                    if InternalNode is None:       # creating a generic node if it is unnamed
                        InternalNode = self.phyloTree.makeNode("""", depth=depth, isInternal=True)  # ""internal-"" + str(depth)
                        lenOfPreceedingInternalNodeString = 0

                    # recussive call to make the internal claude
                    childSubString = string[i + 1:j]
                    InternalNode.addChildNode(self.parseNode(childSubString, depth + 1))

                    nodes.append(InternalNode)  # we append the internal node later to preserve order

                    start = j + 1
                continue

        if depth == 0:    # if it's the root node, we do nothing about it and return
            return nodes[0]

        # Adding last most set of children
        endString = string[start:]
        if string[start - 1] == "")"":  # if the symbol belongs to an internal node which is created previously, then we remove it from the string left to parse
            match = re.search(r""[\)\,\(]"", endString)
            if match:
                endOfNodeName = start + match.start() + 1
                endString = string[endOfNodeName:]
                nodes += self._makeNodesFromString(endString, depth)

        return nodes",_2158.py,31,len(bracketStack) == 0,not len(bracketStack)
https://github.com/nlp-uoregon/trankit/tree/master/trankit/adapter_transformers/modeling_tf_utils.py,"def _use_cache(self, outputs, use_cache):
        """"""During generation, decide whether to pass the `past` variable to the next forward pass.""""""
        if len(outputs) <= 1 or use_cache is False:
            return False
        if hasattr(self.config, ""mem_len"") and self.config.mem_len == 0:
            return False
        return True",_2160.py,5,self.config.mem_len == 0,not self.config.mem_len
https://github.com/partho-maple/coding-interview-gym/tree/master/leetcode.com/python/416_Partition_Equal_Subset_Sum.py,"def canPartition(self, nums):
        """"""
        :type nums: List[int]
        :rtype: bool
        """"""
        s = sum(nums)
        if s % 2 != 0:              # if 's' is a an odd number, we can't have two subsets with equal sum
            return False
        return self.canPartitionHelper(nums, s/2, 0)",_2166.py,7,s % 2 != 0,s % 2
https://github.com/lunixbochs/ActualVim/tree/master/lib/asyncio/selector_events.py,"def sock_connect(self, sock, address):
        """"""Connect to a remote socket at address.

        This method is a coroutine.
        """"""
        if self._debug and sock.gettimeout() != 0:
            raise ValueError(""the socket must be non-blocking"")

        if not hasattr(socket, 'AF_UNIX') or sock.family != socket.AF_UNIX:
            resolved = base_events._ensure_resolved(
                address, family=sock.family, proto=sock.proto, loop=self)
            if not resolved.done():
                yield from resolved
            _, _, _, _, address = resolved.result()[0]

        fut = self.create_future()
        self._sock_connect(fut, sock, address)
        return (yield from fut)",_2169.py,6,sock.gettimeout() != 0,sock.gettimeout()
https://github.com/osmr/imgclsmob/tree/master/gluon/gluoncv2/models/seresnet.py,"def __init__(self,
                 channels,
                 init_block_channels,
                 bottleneck,
                 conv1_stride,
                 bn_use_global_stats=False,
                 in_channels=3,
                 in_size=(224, 224),
                 classes=1000,
                 **kwargs):
        super(SEResNet, self).__init__(**kwargs)
        self.in_size = in_size
        self.classes = classes

        with self.name_scope():
            self.features = nn.HybridSequential(prefix="""")
            self.features.add(ResInitBlock(
                in_channels=in_channels,
                out_channels=init_block_channels,
                bn_use_global_stats=bn_use_global_stats))
            in_channels = init_block_channels
            for i, channels_per_stage in enumerate(channels):
                stage = nn.HybridSequential(prefix=""stage{}_"".format(i + 1))
                with stage.name_scope():
                    for j, out_channels in enumerate(channels_per_stage):
                        strides = 2 if (j == 0) and (i != 0) else 1
                        stage.add(SEResUnit(
                            in_channels=in_channels,
                            out_channels=out_channels,
                            strides=strides,
                            bn_use_global_stats=bn_use_global_stats,
                            bottleneck=bottleneck,
                            conv1_stride=conv1_stride))
                        in_channels = out_channels
                self.features.add(stage)
            self.features.add(nn.AvgPool2D(
                pool_size=7,
                strides=1))

            self.output = nn.HybridSequential(prefix="""")
            self.output.add(nn.Flatten())
            self.output.add(nn.Dense(
                units=classes,
                in_units=in_channels))",_2173.py,26,j == 0,not j
https://github.com/osmr/imgclsmob/tree/master/gluon/gluoncv2/models/seresnet.py,"def __init__(self,
                 channels,
                 init_block_channels,
                 bottleneck,
                 conv1_stride,
                 bn_use_global_stats=False,
                 in_channels=3,
                 in_size=(224, 224),
                 classes=1000,
                 **kwargs):
        super(SEResNet, self).__init__(**kwargs)
        self.in_size = in_size
        self.classes = classes

        with self.name_scope():
            self.features = nn.HybridSequential(prefix="""")
            self.features.add(ResInitBlock(
                in_channels=in_channels,
                out_channels=init_block_channels,
                bn_use_global_stats=bn_use_global_stats))
            in_channels = init_block_channels
            for i, channels_per_stage in enumerate(channels):
                stage = nn.HybridSequential(prefix=""stage{}_"".format(i + 1))
                with stage.name_scope():
                    for j, out_channels in enumerate(channels_per_stage):
                        strides = 2 if (j == 0) and (i != 0) else 1
                        stage.add(SEResUnit(
                            in_channels=in_channels,
                            out_channels=out_channels,
                            strides=strides,
                            bn_use_global_stats=bn_use_global_stats,
                            bottleneck=bottleneck,
                            conv1_stride=conv1_stride))
                        in_channels = out_channels
                self.features.add(stage)
            self.features.add(nn.AvgPool2D(
                pool_size=7,
                strides=1))

            self.output = nn.HybridSequential(prefix="""")
            self.output.add(nn.Flatten())
            self.output.add(nn.Dense(
                units=classes,
                in_units=in_channels))",_2173.py,26,i != 0,i
https://github.com/scholarly-python-package/scholarly/tree/master/scholarly/publication_parser.py,"def _citation_pub(self, __data, publication: Publication):
        # create the bib entry in the dictionary
        publication['bib']['title'] = __data.find('a', class_='gsc_a_at').text
        publication['author_pub_id'] = re.findall(_CITATIONPUBRE, __data.find(
            'a', class_='gsc_a_at')['href'])[0]
        citedby = __data.find(class_='gsc_a_ac')

        publication[""num_citations""] = 0
        if citedby and not (citedby.text.isspace() or citedby.text == ''):
            publication[""num_citations""] = int(citedby.text.strip())
            publication[""citedby_url""] = citedby[""href""]
            publication[""cites_id""] = re.findall(_SCHOLARPUBRE, citedby[""href""])[0].split(',')

        year = __data.find(class_='gsc_a_h')
        if (year and year.text
                and not year.text.isspace()
                and len(year.text) > 0):
            publication['bib']['pub_year'] = year.text.strip()

        return publication",_2188.py,9,citedby.text == '',not citedby.text
https://github.com/saltstack/salt/tree/master/salt/modules/dracr.py,"def list_slotnames(host=None, admin_username=None, admin_password=None):
    """"""
    List the names of all slots in the chassis.

    host
        The chassis host.

    admin_username
        The username used to access the chassis.

    admin_password
        The password used to access the chassis.

    CLI Example:

    .. code-block:: bash

        salt-call --local dracr.list_slotnames host=111.222.333.444
            admin_username=root admin_password=secret

    """"""
    slotraw = __execute_ret(
        ""getslotname"",
        host=host,
        admin_username=admin_username,
        admin_password=admin_password,
    )

    if slotraw[""retcode""] != 0:
        return slotraw
    slots = {}
    stripheader = True
    for l in slotraw[""stdout""].splitlines():
        if l.startswith(""<""):
            stripheader = False
            continue
        if stripheader:
            continue
        fields = l.split()
        slots[fields[0]] = {}
        slots[fields[0]][""slot""] = fields[0]
        if len(fields) > 1:
            slots[fields[0]][""slotname""] = fields[1]
        else:
            slots[fields[0]][""slotname""] = """"
        if len(fields) > 2:
            slots[fields[0]][""hostname""] = fields[2]
        else:
            slots[fields[0]][""hostname""] = """"

    return slots",_2244.py,29,slotraw['retcode'] != 0,slotraw['retcode']
https://github.com/beerfactory/hbmqtt/tree/master/hbmqtt/broker.py,"def client_connected(self, listener_name, reader: ReaderAdapter, writer: WriterAdapter):
        # Wait for connection available on listener
        server = self._servers.get(listener_name, None)
        if not server:
            raise BrokerException(""Invalid listener name '%s'"" % listener_name)
        yield from server.acquire_connection()

        remote_address, remote_port = writer.get_peer_info()
        self.logger.info(""Connection from %s:%d on listener '%s'"" % (remote_address, remote_port, listener_name))

        # Wait for first packet and expect a CONNECT
        try:
            handler, client_session = yield from BrokerProtocolHandler.init_from_connect(reader, writer, self.plugins_manager, loop=self._loop)
        except HBMQTTException as exc:
            self.logger.warning(""[MQTT-3.1.0-1] %s: Can't read first packet an CONNECT: %s"" %
                                (format_client_message(address=remote_address, port=remote_port), exc))
            #yield from writer.close()
            self.logger.debug(""Connection closed"")
            return
        except MQTTException as me:
            self.logger.error('Invalid connection from %s : %s' %
                              (format_client_message(address=remote_address, port=remote_port), me))
            yield from writer.close()
            self.logger.debug(""Connection closed"")
            return

        if client_session.clean_session:
            # Delete existing session and create a new one
            if client_session.client_id is not None and client_session.client_id != """":
                self.delete_session(client_session.client_id)
            else:
                client_session.client_id = gen_client_id()
            client_session.parent = 0
        else:
            # Get session from cache
            if client_session.client_id in self._sessions:
                self.logger.debug(""Found old session %s"" % repr(self._sessions[client_session.client_id]))
                (client_session, h) = self._sessions[client_session.client_id]
                client_session.parent = 1
            else:
                client_session.parent = 0
        if client_session.keep_alive > 0:
            client_session.keep_alive += self.config['timeout-disconnect-delay']
        self.logger.debug(""Keep-alive timeout=%d"" % client_session.keep_alive)

        handler.attach(client_session, reader, writer)
        self._sessions[client_session.client_id] = (client_session, handler)

        authenticated = yield from self.authenticate(client_session, self.listeners_config[listener_name])
        if not authenticated:
            yield from writer.close()
            server.release_connection()  # Delete client from connections list
            return

        while True:
            try:
                client_session.transitions.connect()
                break
            except (MachineError, ValueError):
                # Backwards compat: MachineError is raised by transitions < 0.5.0.
                self.logger.warning(""Client %s is reconnecting too quickly, make it wait"" % client_session.client_id)
                # Wait a bit may be client is reconnecting too fast
                yield from asyncio.sleep(1, loop=self._loop)
        yield from handler.mqtt_connack_authorize(authenticated)

        yield from self.plugins_manager.fire_event(EVENT_BROKER_CLIENT_CONNECTED, client_id=client_session.client_id)

        self.logger.debug(""%s Start messages handling"" % client_session.client_id)
        yield from handler.start()
        self.logger.debug(""Retained messages queue size: %d"" % client_session.retained_messages.qsize())
        yield from self.publish_session_retained_messages(client_session)

        # Init and start loop for handling client messages (publish, subscribe/unsubscribe, disconnect)
        disconnect_waiter = asyncio.ensure_future(handler.wait_disconnect(), loop=self._loop)
        subscribe_waiter = asyncio.ensure_future(handler.get_next_pending_subscription(), loop=self._loop)
        unsubscribe_waiter = asyncio.ensure_future(handler.get_next_pending_unsubscription(), loop=self._loop)
        wait_deliver = asyncio.ensure_future(handler.mqtt_deliver_next_message(), loop=self._loop)
        connected = True
        while connected:
            try:
                done, pending = yield from asyncio.wait(
                    [disconnect_waiter, subscribe_waiter, unsubscribe_waiter, wait_deliver],
                    return_when=asyncio.FIRST_COMPLETED, loop=self._loop)
                if disconnect_waiter in done:
                    result = disconnect_waiter.result()
                    self.logger.debug(""%s Result from wait_diconnect: %s"" % (client_session.client_id, result))
                    if result is None:
                        self.logger.debug(""Will flag: %s"" % client_session.will_flag)
                        # Connection closed anormally, send will message
                        if client_session.will_flag:
                            self.logger.debug(""Client %s disconnected abnormally, sending will message"" %
                                              format_client_message(client_session))
                            yield from self._broadcast_message(
                                client_session,
                                client_session.will_topic,
                                client_session.will_message,
                                client_session.will_qos)
                            if client_session.will_retain:
                                self.retain_message(client_session,
                                                    client_session.will_topic,
                                                    client_session.will_message,
                                                    client_session.will_qos)
                    self.logger.debug(""%s Disconnecting session"" % client_session.client_id)
                    yield from self._stop_handler(handler)
                    client_session.transitions.disconnect()
                    yield from self.plugins_manager.fire_event(EVENT_BROKER_CLIENT_DISCONNECTED, client_id=client_session.client_id)
                    connected = False
                if unsubscribe_waiter in done:
                    self.logger.debug(""%s handling unsubscription"" % client_session.client_id)
                    unsubscription = unsubscribe_waiter.result()
                    for topic in unsubscription['topics']:
                        self._del_subscription(topic, client_session)
                        yield from self.plugins_manager.fire_event(
                            EVENT_BROKER_CLIENT_UNSUBSCRIBED,
                            client_id=client_session.client_id,
                            topic=topic)
                    yield from handler.mqtt_acknowledge_unsubscription(unsubscription['packet_id'])
                    unsubscribe_waiter = asyncio.Task(handler.get_next_pending_unsubscription(), loop=self._loop)
                if subscribe_waiter in done:
                    self.logger.debug(""%s handling subscription"" % client_session.client_id)
                    subscriptions = subscribe_waiter.result()
                    return_codes = []
                    for subscription in subscriptions['topics']:
                        result = yield from self.add_subscription(subscription, client_session)
                        return_codes.append(result)
                    yield from handler.mqtt_acknowledge_subscription(subscriptions['packet_id'], return_codes)
                    for index, subscription in enumerate(subscriptions['topics']):
                        if return_codes[index] != 0x80:
                            yield from self.plugins_manager.fire_event(
                                EVENT_BROKER_CLIENT_SUBSCRIBED,
                                client_id=client_session.client_id,
                                topic=subscription[0],
                                qos=subscription[1])
                            yield from self.publish_retained_messages_for_subscription(subscription, client_session)
                    subscribe_waiter = asyncio.Task(handler.get_next_pending_subscription(), loop=self._loop)
                    self.logger.debug(repr(self._subscriptions))
                if wait_deliver in done:
                    if self.logger.isEnabledFor(logging.DEBUG):
                        self.logger.debug(""%s handling message delivery"" % client_session.client_id)
                    app_message = wait_deliver.result()
                    if not app_message.topic:
                        self.logger.warning(""[MQTT-4.7.3-1] - %s invalid TOPIC sent in PUBLISH message, closing connection"" % client_session.client_id)
                        break
                    if ""#"" in app_message.topic or ""+"" in app_message.topic:
                        self.logger.warning(""[MQTT-3.3.2-2] - %s invalid TOPIC sent in PUBLISH message, closing connection"" % client_session.client_id)
                        break
                    yield from self.plugins_manager.fire_event(EVENT_BROKER_MESSAGE_RECEIVED,
                                                               client_id=client_session.client_id,
                                                               message=app_message)
                    yield from self._broadcast_message(client_session, app_message.topic, app_message.data)
                    if app_message.publish_packet.retain_flag:
                        self.retain_message(client_session, app_message.topic, app_message.data, app_message.qos)
                    wait_deliver = asyncio.Task(handler.mqtt_deliver_next_message(), loop=self._loop)
            except asyncio.CancelledError:
                self.logger.debug(""Client loop cancelled"")
                break
        disconnect_waiter.cancel()
        subscribe_waiter.cancel()
        unsubscribe_waiter.cancel()
        wait_deliver.cancel()

        self.logger.debug(""%s Client disconnected"" % client_session.client_id)
        server.release_connection()",_2280.py,29,client_session.client_id != '',client_session.client_id
https://github.com/kizniche/Mycodo/tree/master/mycodo/mycodo_flask/utils/utils_conditional.py,"def check_cond_conditions(cond, error):
    """"""Checks if the saved variables have any errors.""""""
    if (cond.condition_type == 'measurement' and
            (not cond.measurement or cond.measurement == '')):
        error.append(
            ""Measurement must be set. Condition with ID starting with {id} ""
            ""is not set."".format(id=cond.unique_id.split('-')[0]))
    if (cond.condition_type == 'output_state' and
            (not cond.output_id or cond.output_id == '')):
        error.append(
            ""Output must be set. Condition with ID starting with {id} ""
            ""is not set."".format(id=cond.unique_id.split('-')[0]))
    if cond.condition_type == 'gpio_state' and not cond.gpio_pin:
        error.append(
            ""GPIO Pin must be set. Condition with ID starting with {id} ""
            ""is not set."".format(id=cond.unique_id.split('-')[0]))
    if not cond.max_age or cond.max_age <= 0:
        error.append(""Max Age must be greater than 0"")
    return error",_2303.py,4,cond.measurement == '',not cond.measurement
https://github.com/kizniche/Mycodo/tree/master/mycodo/mycodo_flask/utils/utils_conditional.py,"def check_cond_conditions(cond, error):
    """"""Checks if the saved variables have any errors.""""""
    if (cond.condition_type == 'measurement' and
            (not cond.measurement or cond.measurement == '')):
        error.append(
            ""Measurement must be set. Condition with ID starting with {id} ""
            ""is not set."".format(id=cond.unique_id.split('-')[0]))
    if (cond.condition_type == 'output_state' and
            (not cond.output_id or cond.output_id == '')):
        error.append(
            ""Output must be set. Condition with ID starting with {id} ""
            ""is not set."".format(id=cond.unique_id.split('-')[0]))
    if cond.condition_type == 'gpio_state' and not cond.gpio_pin:
        error.append(
            ""GPIO Pin must be set. Condition with ID starting with {id} ""
            ""is not set."".format(id=cond.unique_id.split('-')[0]))
    if not cond.max_age or cond.max_age <= 0:
        error.append(""Max Age must be greater than 0"")
    return error",_2303.py,9,cond.output_id == '',not cond.output_id
https://github.com/deluge-torrent/deluge/tree/master/deluge/ui/console/modes/addtorrents.py,"def refresh(self, lines=None):
        if self.mode_paused():
            return

        # Update the status bars
        self.stdscr.erase()
        self.draw_statusbars()

        off = 1

        # Render breadcrumbs
        s = 'Location: '
        for i, e in enumerate(self.path_stack):
            if e == '/':
                if i == self.path_stack_pos - 1:
                    s += '{!black,red,bold!}root'
                else:
                    s += '{!red,black,bold!}root'
            else:
                if i == self.path_stack_pos - 1:
                    s += '{!black,white,bold!}%s' % e
                else:
                    s += '{!white,black,bold!}%s' % e

            if e != len(self.path_stack) - 1:
                s += '{!white,black!}/'

        self.add_string(off, s)
        off += 1

        # Render header
        cols = ['Name', 'Contents', 'Modification time']
        widths = [self.cols - 35, 12, 23]
        s = ''
        for i, (c, w) in enumerate(zip(cols, widths)):
            cn = ''
            if i == 0:
                cn = 'name'
            elif i == 2:
                cn = 'date'

            if cn == self.sort_column:
                s += '{!black,green,bold!}' + c.ljust(w - 2)
                if self.reverse_sort:
                    s += '^ '
                else:
                    s += 'v '
            else:
                s += '{!green,black,bold!}' + c.ljust(w)
        self.add_string(off, s)
        off += 1

        # Render files and folders
        for i, row in enumerate(self.formatted_rows[self.view_offset :]):
            i += self.view_offset
            # It's a folder
            color_string = ''
            if self.raw_rows[i][4]:
                if self.raw_rows[i][1] == -1:
                    if i == self.cursel:
                        color_string = '{!black,red,bold!}'
                    else:
                        color_string = '{!red,black!}'
                else:
                    if i == self.cursel:
                        color_string = '{!black,cyan,bold!}'
                    else:
                        color_string = '{!cyan,black!}'

            elif i == self.cursel:
                if self.raw_rows[i][0] in self.marked:
                    color_string = '{!blue,white,bold!}'
                else:
                    color_string = '{!black,white,bold!}'
            elif self.raw_rows[i][0] in self.marked:
                color_string = '{!white,blue,bold!}'

            self.add_string(off, color_string + row)
            off += 1

            if off > self.rows - 2:
                break

        if not component.get('ConsoleUI').is_active_mode(self):
            return

        self.stdscr.noutrefresh()

        if self.popup:
            self.popup.refresh()

        curses.doupdate()",_2361.py,37,i == 0,not i
https://github.com/hydrusnetwork/hydrus/tree/master/hydrus/client/ClientData.py,"def MergeCounts( min_a, max_a, min_b, max_b ):
    
    # 100-None and 100-None returns 100-200
    # 1-None and 4-5 returns 5-6
    # 1-2, and 5-7 returns 6, 9
    
    if min_a == 0:
        
        ( min_answer, max_answer ) = ( min_b, max_b )
        
    elif min_b == 0:
        
        ( min_answer, max_answer ) = ( min_a, max_a )
        
    else:
        
        if max_a is None:
            
            max_a = min_a
            
        
        if max_b is None:
            
            max_b = min_b
            
        
        min_answer = max( min_a, min_b )
        max_answer = max_a + max_b
        
    
    return ( min_answer, max_answer )",_2453.py,7,min_a == 0,not min_a
https://github.com/hydrusnetwork/hydrus/tree/master/hydrus/client/ClientData.py,"def MergeCounts( min_a, max_a, min_b, max_b ):
    
    # 100-None and 100-None returns 100-200
    # 1-None and 4-5 returns 5-6
    # 1-2, and 5-7 returns 6, 9
    
    if min_a == 0:
        
        ( min_answer, max_answer ) = ( min_b, max_b )
        
    elif min_b == 0:
        
        ( min_answer, max_answer ) = ( min_a, max_a )
        
    else:
        
        if max_a is None:
            
            max_a = min_a
            
        
        if max_b is None:
            
            max_b = min_b
            
        
        min_answer = max( min_a, min_b )
        max_answer = max_a + max_b
        
    
    return ( min_answer, max_answer )",_2453.py,11,min_b == 0,not min_b
https://github.com/openvinotoolkit/open_model_zoo/tree/master/tools/accuracy_checker/openvino/tools/accuracy_checker/annotation_converters/cvat_facial_landmarks.py,"def convert(self, check_content=False, progress_callback=None, progress_interval=100, **kwargs):
        annotation = read_xml(self.annotation_file)
        meta = annotation.find('meta')
        size = int(meta.find('task').find('size').text)
        num_landmarks = len(list(meta.iter('label')))
        annotations = []
        content_errors = None if not check_content else []
        for image_id, image in enumerate(annotation.iter('image')):
            identifier = image.attrib['name'].split('/')[-1]
            if check_content:
                if not check_file_existence(self.images_dir / identifier):
                    content_errors.append('{}: does not exist'.format(self.images_dir / identifier))
            landmarks_x, landmarks_y = self.get_landmarks(image, num_landmarks)
            landmarks_annotation = FacialLandmarksAnnotation(identifier, np.array(landmarks_x), np.array(landmarks_y))
            landmarks_annotation.metadata['left_eye'] = [1, 0]
            landmarks_annotation.metadata['right_eye'] = [2, 3]
            annotations.append(landmarks_annotation)
            if progress_callback is not None and image_id % progress_interval == 0:
                progress_callback(image_id * 100 / size)

        return ConverterReturn(annotations, None, content_errors)",_2462.py,18,image_id % progress_interval == 0,not image_id % progress_interval
https://github.com/Megvii-BaseDetection/cvpods/tree/master/cvpods/data/datasets/crowdhuman.py,"def _load_annotations(self, json_file, image_root):
        """"""
        Load a json file with CrowdHuman's instances annotation format.
        Currently supports instance detection, instance segmentation,
        and person keypoints annotations.

        Args:
            json_file (str): full path to the json file in CrowdHuman instances annotation format.
            image_root (str): the directory where the images in this json file exists.

        Returns:
            list[dict]: a list of dicts in cvpods standard format. (See
            `Using Custom Datasets </tutorials/datasets.html>`_ )

        Notes:
            1. This function does not read the image files.
               The results do not have the ""image"" field.
        """"""
        timer = Timer()
        # json_file = PathManager.get_local_path(json_file)
        with open(json_file, 'r') as file:
            gt_records = file.readlines()
        if timer.seconds() > 1:
            logger.info(""Loading {} takes {:.2f} seconds."".format(
                json_file, timer.seconds()))

        logger.info(""Loaded {} images in CrowdHuman format from {}"".format(
            len(gt_records), json_file))

        dataset_dicts = []

        ann_keys = [""tag"", ""hbox"", ""vbox"", ""head_attr"", ""extra""]
        for anno_str in gt_records:
            anno_dict = json.loads(anno_str)

            record = {}
            record[""file_name""] = os.path.join(image_root, ""{}.jpg"".format(anno_dict[""ID""]))
            record[""height""] = anno_dict[""height""]
            record[""width""] = anno_dict[""width""]
            record[""image_id""] = anno_dict[""ID""]

            objs = []
            for anno in anno_dict['gtboxes']:
                # Check that the image_id in this annotation is the same as
                # the image_id we're looking at.
                # This fails only when the data parsing logic or the annotation file is buggy.

                # The original COCO valminusminival2014 & minival2014 annotation files
                # actually contains bugs that, together with certain ways of using COCO API,
                # can trigger this assertion.
                obj = {key: anno[key] for key in ann_keys if key in anno}
                obj[""bbox""] = anno[""fbox""]
                obj[""category_id""] = 0

                if 'extra' in anno and 'ignore' in anno['extra'] and anno['extra']['ignore'] != 0:
                    obj[""category_id""] = -1

                obj[""bbox_mode""] = BoxMode.XYWH_ABS
                objs.append(obj)
            record[""annotations""] = objs
            dataset_dicts.append(record)

        return dataset_dicts",_2464.py,55,anno['extra']['ignore'] != 0,anno['extra']['ignore']
https://github.com/DreamSourceLab/DSView/tree/master/libsigrokdecode4DSL/decoders/amulet_ascii/pd.py,"def decode(self, ss, es, data):
        ptype, rxtx, pdata = data

        self.ss, self.es = ss, es

        if ptype != 'DATA':
            return

        # Handle commands.
        try:
            abort_current = (0xD0 <= pdata[0] <= 0xF7) and \
                (not (self.state in cmds_with_high_bytes)) and \
                self.state != None
            if abort_current:
                self.putx([Ann.WARN, ['Command aborted by invalid byte', 'Abort']])
                self.state = pdata[0]
                self.emit_cmd_byte()
                self.cmdstate = 1
            if self.state is None:
                self.state = pdata[0]
                self.emit_cmd_byte()
                self.cmdstate = 1
            self.cmd_handlers[self.state](pdata[0])
        except KeyError:
            self.putx([Ann.WARN, ['Unknown command: 0x%02x' % pdata[0]]])
            self.state = None",_2488.py,13,self.state != None,self.state
https://github.com/quay/quay/tree/master/buildman/jobutil/buildjob.py,"def extract_dockerfile_args(self):
        dockerfile_path = self.build_config.get(""build_subdir"", """")
        context = self.build_config.get(""context"", """")
        if not (dockerfile_path == """" or context == """"):
            # This should not happen and can be removed when we centralize validating build_config
            dockerfile_abspath = slash_join("""", dockerfile_path)
            if "".."" in os.path.relpath(dockerfile_abspath, context):
                return os.path.split(dockerfile_path)
            dockerfile_path = os.path.relpath(dockerfile_abspath, context)

        return context, dockerfile_path",_2516.py,4,dockerfile_path == '',not dockerfile_path
https://github.com/quay/quay/tree/master/buildman/jobutil/buildjob.py,"def extract_dockerfile_args(self):
        dockerfile_path = self.build_config.get(""build_subdir"", """")
        context = self.build_config.get(""context"", """")
        if not (dockerfile_path == """" or context == """"):
            # This should not happen and can be removed when we centralize validating build_config
            dockerfile_abspath = slash_join("""", dockerfile_path)
            if "".."" in os.path.relpath(dockerfile_abspath, context):
                return os.path.split(dockerfile_path)
            dockerfile_path = os.path.relpath(dockerfile_abspath, context)

        return context, dockerfile_path",_2516.py,4,context == '',not context
https://github.com/plasma-disassembler/plasma/tree/master/plasma/lib/disassembler.py,"def dump_asm(self, ctx, lines=NB_LINES_TO_DISASM, until=-1):
        ARCH = self.load_arch_module()
        ARCH_OUTPUT = ARCH.output
        ARCH_UTILS = ARCH.utils

        ad = ctx.entry
        s = self.binary.get_section(ad)

        if s is None:
            # until is != -1 only from the visual mode
            # It allows to not go before the first section.
            if until != -1:
                return None
            # Get the next section, it's not mandatory that sections
            # are consecutives !
            s = self.binary.get_next_section(ad)
            if s is None:
                return None
            ad = s.start

        o = ARCH_OUTPUT.Output(ctx)
        o._new_line()
        o.curr_section = s
        o.mode_dump = True
        l = 0
        api = ctx.gctx.api

        # For mips: after a jump we add a newline, but for mips we should
        # add this newline after the prefetch instruction.
        prefetch_after_branch = False

        while 1:
            if ad == s.start:
                if not o.last_2_lines_are_empty():
                    o._new_line()
                o._dash()
                o._section(s.name)
                o._add(""  0x%x -> 0x%x"" % (s.start, s.end))
                o._new_line()
                o._new_line()

            while ((l < lines and until == -1) or (ad < until and until != -1)) \
                    and ad <= s.end:

                ty = self.mem.get_type(ad)

                # A PE import should not be displayed as a subroutine
                if not(self.binary.type == T_BIN_PE and ad in self.binary.imports) \
                        and self.mem.is_code(ad):

                    is_func = ad in self.functions

                    if is_func:
                        if not o.last_2_lines_are_empty():
                            o._new_line()
                        o._dash()
                        o._user_comment(""; SUBROUTINE"")
                        o._new_line()
                        o._dash()

                    i = self.lazy_disasm(ad, s.start)

                    if not is_func and ad in self.xrefs and \
                            not o.last_2_lines_are_empty():
                        o._new_line()

                    o._asm_inst(i)

                    is_end = ad in self.end_functions

                    # mips
                    if prefetch_after_branch:
                        prefetch_after_branch = False
                        if not is_end:
                            o._new_line()

                    if is_end:
                        for fad in self.end_functions[ad]:
                            sy = api.get_symbol(fad)
                            o._user_comment(""; end function %s"" % sy)
                            o._new_line()
                        o._new_line()

                    elif ARCH_UTILS.is_uncond_jump(i) or ARCH_UTILS.is_ret(i):
                        if self.is_mips:
                            prefetch_after_branch = True
                        else:
                            o._new_line()

                    elif ARCH_UTILS.is_call(i):
                        op = i.operands[0]
                        if op.type == self.capstone.CS_OP_IMM:
                            imm = unsigned(op.value.imm)
                            if imm in self.functions and self.is_noreturn(imm):
                                if self.is_mips:
                                    prefetch_after_branch = True
                                else:
                                    o._new_line()

                    ad += i.size

                elif MEM_WOFFSET <= ty <= MEM_QOFFSET:
                    prefetch_after_branch = False
                    o._label_and_address(ad)
                    o.set_line(ad)
                    sz = self.mem.get_size(ad)
                    off = s.read_int(ad, sz)
                    if off is None:
                        continue
                    if ctx.gctx.print_bytes:
                        o._bytes(s.read(ad, sz))
                    o._data_prefix(sz)
                    o._add("" "")
                    o._imm(off, sz, True, print_data=False, force_dont_print_data=True)
                    o._new_line()
                    ad += sz

                elif ty == MEM_ASCII:
                    prefetch_after_branch = False
                    o._label_and_address(ad)
                    o.set_line(ad)
                    sz = self.mem.get_size(ad)
                    buf = self.binary.get_string(ad, sz)

                    if buf is not None:
                        if ctx.gctx.print_bytes:
                            o._bytes(s.read(ad, sz))

                        # Split the string into multi lines

                        splitted = buf.split(""\n"")

                        j = 0
                        for i, st in enumerate(splitted):
                            if i > 0 and len(st) != 0:
                                o._new_line()
                                o.set_line(ad + j)
                                o._address(ad + j)

                            ibs = 0
                            bs = 65
                            while ibs < len(st):
                                if ibs > 0:
                                    o._new_line()
                                    o.set_line(ad + j)
                                    o._address(ad + j)

                                blk = st[ibs:ibs + bs]

                                if i < len(splitted) - 1 and ibs + bs >= len(st):
                                    o._string('""' + blk + '\\n""')
                                    j += len(blk) + 1
                                else:
                                    o._string('""' + blk + '""')
                                    j += len(blk)

                                ibs += bs

                    o._add("", 0"")
                    o._new_line()
                    ad += sz

                elif ty == MEM_ARRAY:
                    prefetch_after_branch = False
                    o._label_and_address(ad)

                    array_info = self.mem.mm[ad]
                    total_size = array_info[0]
                    entry_type = array_info[2]
                    entry_size = self.mem.get_size_from_type(entry_type)

                    n = int(total_size / entry_size)

                    o.set_line(ad)
                    o._data_prefix(entry_size)

                    k = 0
                    while k < total_size:
                        if o.curr_index > 70:
                            o._new_line()
                            o.set_line(ad)
                            o._address(ad)
                            o._data_prefix(entry_size)
                            l += 1

                        val = s.read_int(ad, entry_size)
                        if MEM_WOFFSET <= entry_type <= MEM_QOFFSET:
                            o._add("" "")
                            o._imm(val, entry_size, True,
                                   print_data=False, force_dont_print_data=True)
                        else:
                            o._word(val, entry_size, is_from_array=True)

                        ad += entry_size
                        k += entry_size

                        if k < total_size:
                            o._add("","")

                    o._new_line()

                else:
                    prefetch_after_branch = False
                    o._label_and_address(ad)
                    o.set_line(ad)
                    sz = self.mem.get_size_from_type(ty)
                    if ctx.gctx.print_bytes:
                        o._bytes(s.read(ad, sz))
                    o._word(s.read_int(ad, sz), sz)
                    o._new_line()
                    ad += sz

                l += 1

            s = self.binary.get_section(ad)
            if s is None:
                # Get the next section, it's not mandatory that sections
                # are consecutives !
                s = self.binary.get_next_section(ad)
                if s is None:
                    break
                o._new_line()
                ad = s.start
                if until != -1 and ad >= until:
                    break

            if (l >= lines and until == -1) or (ad >= until and until != -1):
                break

            o.curr_section = s

        if until == ad:
            if self.mem.is_code(ad) and ad in self.xrefs or \
                    s is not None and ad == s.start:
                if not o.last_2_lines_are_empty():
                    o._new_line()

        # remove the last empty line
        o.lines.pop(-1)
        o.token_lines.pop(-1)

        o.join_lines()

        return o",_2533.py,135,len(st) != 0,len(st)
https://github.com/LoRexxar/Kunlun-M/tree/master/core/detection.py,"def count_java_line(filename):
        count = {'count_code': 0, 'count_blank': 0, 'count_pound': 0}
        fi = open(filename, 'r')
        file_line = fi.readline()
        while fi.tell() != os.path.getsize(filename):
            file_line = file_line.lstrip()
            if len(file_line) == 0:
                count['count_blank'] += 1
            elif file_line.startswith('//'):
                count['count_pound'] += 1
            elif file_line.count('/*') == 1 and file_line.count('*/') == 1:
                if file_line.startswith('/*'):
                    count['count_pound'] += 1
                else:
                    count['count_code'] += 1
            elif file_line.count('/*') == 1 and file_line.count('*/') == 0:
                if file_line.startswith('/*'):
                    count['count_pound'] += 1
                    while True:
                        file_line = fi.readline()
                        if len(file_line) == 0 or file_line == ""\n"":
                            count['count_blank'] += 1
                        else:
                            count['count_pound'] += 1
                        if file_line.endswith('*/\n'):
                            break
                else:
                    count['count_code'] += 1
                    while True:
                        file_line = fi.readline()
                        if len(file_line) == 0 or file_line == ""\n"":
                            count['count_blank'] += 1
                        else:
                            count['count_code'] += 1
                        if file_line.find('*/'):
                            break
            else:
                count['count_code'] += 1
            file_line = fi.readline()
        fi.close()
        return count",_2624.py,7,len(file_line) == 0,not len(file_line)
https://github.com/LoRexxar/Kunlun-M/tree/master/core/detection.py,"def count_java_line(filename):
        count = {'count_code': 0, 'count_blank': 0, 'count_pound': 0}
        fi = open(filename, 'r')
        file_line = fi.readline()
        while fi.tell() != os.path.getsize(filename):
            file_line = file_line.lstrip()
            if len(file_line) == 0:
                count['count_blank'] += 1
            elif file_line.startswith('//'):
                count['count_pound'] += 1
            elif file_line.count('/*') == 1 and file_line.count('*/') == 1:
                if file_line.startswith('/*'):
                    count['count_pound'] += 1
                else:
                    count['count_code'] += 1
            elif file_line.count('/*') == 1 and file_line.count('*/') == 0:
                if file_line.startswith('/*'):
                    count['count_pound'] += 1
                    while True:
                        file_line = fi.readline()
                        if len(file_line) == 0 or file_line == ""\n"":
                            count['count_blank'] += 1
                        else:
                            count['count_pound'] += 1
                        if file_line.endswith('*/\n'):
                            break
                else:
                    count['count_code'] += 1
                    while True:
                        file_line = fi.readline()
                        if len(file_line) == 0 or file_line == ""\n"":
                            count['count_blank'] += 1
                        else:
                            count['count_code'] += 1
                        if file_line.find('*/'):
                            break
            else:
                count['count_code'] += 1
            file_line = fi.readline()
        fi.close()
        return count",_2624.py,16,file_line.count('*/') == 0,not file_line.count('*/')
https://github.com/LoRexxar/Kunlun-M/tree/master/core/detection.py,"def count_java_line(filename):
        count = {'count_code': 0, 'count_blank': 0, 'count_pound': 0}
        fi = open(filename, 'r')
        file_line = fi.readline()
        while fi.tell() != os.path.getsize(filename):
            file_line = file_line.lstrip()
            if len(file_line) == 0:
                count['count_blank'] += 1
            elif file_line.startswith('//'):
                count['count_pound'] += 1
            elif file_line.count('/*') == 1 and file_line.count('*/') == 1:
                if file_line.startswith('/*'):
                    count['count_pound'] += 1
                else:
                    count['count_code'] += 1
            elif file_line.count('/*') == 1 and file_line.count('*/') == 0:
                if file_line.startswith('/*'):
                    count['count_pound'] += 1
                    while True:
                        file_line = fi.readline()
                        if len(file_line) == 0 or file_line == ""\n"":
                            count['count_blank'] += 1
                        else:
                            count['count_pound'] += 1
                        if file_line.endswith('*/\n'):
                            break
                else:
                    count['count_code'] += 1
                    while True:
                        file_line = fi.readline()
                        if len(file_line) == 0 or file_line == ""\n"":
                            count['count_blank'] += 1
                        else:
                            count['count_code'] += 1
                        if file_line.find('*/'):
                            break
            else:
                count['count_code'] += 1
            file_line = fi.readline()
        fi.close()
        return count",_2624.py,21,len(file_line) == 0,not len(file_line)
https://github.com/LoRexxar/Kunlun-M/tree/master/core/detection.py,"def count_java_line(filename):
        count = {'count_code': 0, 'count_blank': 0, 'count_pound': 0}
        fi = open(filename, 'r')
        file_line = fi.readline()
        while fi.tell() != os.path.getsize(filename):
            file_line = file_line.lstrip()
            if len(file_line) == 0:
                count['count_blank'] += 1
            elif file_line.startswith('//'):
                count['count_pound'] += 1
            elif file_line.count('/*') == 1 and file_line.count('*/') == 1:
                if file_line.startswith('/*'):
                    count['count_pound'] += 1
                else:
                    count['count_code'] += 1
            elif file_line.count('/*') == 1 and file_line.count('*/') == 0:
                if file_line.startswith('/*'):
                    count['count_pound'] += 1
                    while True:
                        file_line = fi.readline()
                        if len(file_line) == 0 or file_line == ""\n"":
                            count['count_blank'] += 1
                        else:
                            count['count_pound'] += 1
                        if file_line.endswith('*/\n'):
                            break
                else:
                    count['count_code'] += 1
                    while True:
                        file_line = fi.readline()
                        if len(file_line) == 0 or file_line == ""\n"":
                            count['count_blank'] += 1
                        else:
                            count['count_code'] += 1
                        if file_line.find('*/'):
                            break
            else:
                count['count_code'] += 1
            file_line = fi.readline()
        fi.close()
        return count",_2624.py,31,len(file_line) == 0,not len(file_line)
https://github.com/d2l-ai/d2l-en/tree/master/d2l/mxnet.py,"def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):
    """"""Train a model with a GPU (defined in Chapter 6).

    Defined in :numref:`sec_utils`""""""
    net.initialize(force_reinit=True, ctx=device, init=init.Xavier())
    loss = gluon.loss.SoftmaxCrossEntropyLoss()
    trainer = gluon.Trainer(net.collect_params(),
                            'sgd', {'learning_rate': lr})
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
                            legend=['train loss', 'train acc', 'test acc'])
    timer, num_batches = d2l.Timer(), len(train_iter)
    for epoch in range(num_epochs):
        # Sum of training loss, sum of training accuracy, no. of examples
        metric = d2l.Accumulator(3)
        for i, (X, y) in enumerate(train_iter):
            timer.start()
            # Here is the major difference from `d2l.train_epoch_ch3`
            X, y = X.as_in_ctx(device), y.as_in_ctx(device)
            with autograd.record():
                y_hat = net(X)
                l = loss(y_hat, y)
            l.backward()
            trainer.step(X.shape[0])
            metric.add(l.sum(), d2l.accuracy(y_hat, y), X.shape[0])
            timer.stop()
            train_l = metric[0] / metric[2]
            train_acc = metric[1] / metric[2]
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (train_l, train_acc, None))
        test_acc = evaluate_accuracy_gpu(net, test_iter)
        animator.add(epoch + 1, (None, None, test_acc))
    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '
          f'test acc {test_acc:.3f}')
    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '
          f'on {str(device)}')",_2636.py,28,(i + 1) % (num_batches // 5) == 0,not (i + 1) % (num_batches // 5)
https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/core.py,"def __str__(self):
        if hasattr(self, 'name'):
            name = str(self.name)
        else:
            name = ''
        if hasattr(self, 'empty') and self.empty:
            return ''
        if self.address == '' and name == '':
            return 'EMAIL NOT DEFINED'
        if self.address == '' and name != '':
            return name
        if docassemble.base.functions.this_thread.evaluation_context == 'docx':
            return str(self.address)
        if name == '' and self.address != '':
            return '[' + str(self.address) + '](mailto:' + str(self.address) + ')'
        return '[' + str(name) + '](mailto:' + str(self.address) + ')'",_2688.py,8,self.address == '',not self.address
https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/core.py,"def __str__(self):
        if hasattr(self, 'name'):
            name = str(self.name)
        else:
            name = ''
        if hasattr(self, 'empty') and self.empty:
            return ''
        if self.address == '' and name == '':
            return 'EMAIL NOT DEFINED'
        if self.address == '' and name != '':
            return name
        if docassemble.base.functions.this_thread.evaluation_context == 'docx':
            return str(self.address)
        if name == '' and self.address != '':
            return '[' + str(self.address) + '](mailto:' + str(self.address) + ')'
        return '[' + str(name) + '](mailto:' + str(self.address) + ')'",_2688.py,8,name == '',not name
https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/core.py,"def __str__(self):
        if hasattr(self, 'name'):
            name = str(self.name)
        else:
            name = ''
        if hasattr(self, 'empty') and self.empty:
            return ''
        if self.address == '' and name == '':
            return 'EMAIL NOT DEFINED'
        if self.address == '' and name != '':
            return name
        if docassemble.base.functions.this_thread.evaluation_context == 'docx':
            return str(self.address)
        if name == '' and self.address != '':
            return '[' + str(self.address) + '](mailto:' + str(self.address) + ')'
        return '[' + str(name) + '](mailto:' + str(self.address) + ')'",_2688.py,10,self.address == '',not self.address
https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/core.py,"def __str__(self):
        if hasattr(self, 'name'):
            name = str(self.name)
        else:
            name = ''
        if hasattr(self, 'empty') and self.empty:
            return ''
        if self.address == '' and name == '':
            return 'EMAIL NOT DEFINED'
        if self.address == '' and name != '':
            return name
        if docassemble.base.functions.this_thread.evaluation_context == 'docx':
            return str(self.address)
        if name == '' and self.address != '':
            return '[' + str(self.address) + '](mailto:' + str(self.address) + ')'
        return '[' + str(name) + '](mailto:' + str(self.address) + ')'",_2688.py,10,name != '',name
https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/core.py,"def __str__(self):
        if hasattr(self, 'name'):
            name = str(self.name)
        else:
            name = ''
        if hasattr(self, 'empty') and self.empty:
            return ''
        if self.address == '' and name == '':
            return 'EMAIL NOT DEFINED'
        if self.address == '' and name != '':
            return name
        if docassemble.base.functions.this_thread.evaluation_context == 'docx':
            return str(self.address)
        if name == '' and self.address != '':
            return '[' + str(self.address) + '](mailto:' + str(self.address) + ')'
        return '[' + str(name) + '](mailto:' + str(self.address) + ')'",_2688.py,14,name == '',not name
https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/core.py,"def __str__(self):
        if hasattr(self, 'name'):
            name = str(self.name)
        else:
            name = ''
        if hasattr(self, 'empty') and self.empty:
            return ''
        if self.address == '' and name == '':
            return 'EMAIL NOT DEFINED'
        if self.address == '' and name != '':
            return name
        if docassemble.base.functions.this_thread.evaluation_context == 'docx':
            return str(self.address)
        if name == '' and self.address != '':
            return '[' + str(self.address) + '](mailto:' + str(self.address) + ')'
        return '[' + str(name) + '](mailto:' + str(self.address) + ')'",_2688.py,14,self.address != '',self.address
https://github.com/osmr/imgclsmob/tree/master/tensorflow2/tf2cv/models/igcv3.py,"def get_igcv3(width_scale,
              model_name=None,
              pretrained=False,
              root=os.path.join(""~"", "".tensorflow"", ""models""),
              **kwargs):
    """"""
    Create IGCV3-D model with specific parameters.

    Parameters:
    ----------
    width_scale : float
        Scale factor for width of layers.
    model_name : str or None, default None
        Model name for loading pretrained model.
    pretrained : bool, default False
        Whether to load the pretrained weights for model.
    root : str, default '~/.tensorflow/models'
        Location for keeping the model parameters.
    """"""

    init_block_channels = 32
    final_block_channels = 1280
    layers = [1, 4, 6, 8, 6, 6, 1]
    downsample = [0, 1, 1, 1, 0, 1, 0]
    channels_per_layers = [16, 24, 32, 64, 96, 160, 320]

    from functools import reduce
    channels = reduce(lambda x, y: x + [[y[0]] * y[1]] if y[2] != 0 else x[:-1] + [x[-1] + [y[0]] * y[1]],
                      zip(channels_per_layers, layers, downsample), [[]])

    if width_scale != 1.0:
        def make_even(x):
            return x if (x % 2 == 0) else x + 1
        channels = [[make_even(int(cij * width_scale)) for cij in ci] for ci in channels]
        init_block_channels = make_even(int(init_block_channels * width_scale))
        if width_scale > 1.0:
            final_block_channels = make_even(int(final_block_channels * width_scale))

    net = IGCV3(
        channels=channels,
        init_block_channels=init_block_channels,
        final_block_channels=final_block_channels,
        **kwargs)

    if pretrained:
        if (model_name is None) or (not model_name):
            raise ValueError(""Parameter `model_name` should be properly initialized for loading pretrained model."")
        from .model_store import get_model_file
        in_channels = kwargs[""in_channels""] if (""in_channels"" in kwargs) else 3
        input_shape = (1,) + (in_channels,) + net.in_size if net.data_format == ""channels_first"" else\
            (1,) + net.in_size + (in_channels,)
        net.build(input_shape=input_shape)
        net.load_weights(
            filepath=get_model_file(
                model_name=model_name,
                local_model_store_dir_path=root))

    return net",_2689.py,28,y[2] != 0,y[2]
https://github.com/osmr/imgclsmob/tree/master/tensorflow2/tf2cv/models/igcv3.py,"def get_igcv3(width_scale,
              model_name=None,
              pretrained=False,
              root=os.path.join(""~"", "".tensorflow"", ""models""),
              **kwargs):
    """"""
    Create IGCV3-D model with specific parameters.

    Parameters:
    ----------
    width_scale : float
        Scale factor for width of layers.
    model_name : str or None, default None
        Model name for loading pretrained model.
    pretrained : bool, default False
        Whether to load the pretrained weights for model.
    root : str, default '~/.tensorflow/models'
        Location for keeping the model parameters.
    """"""

    init_block_channels = 32
    final_block_channels = 1280
    layers = [1, 4, 6, 8, 6, 6, 1]
    downsample = [0, 1, 1, 1, 0, 1, 0]
    channels_per_layers = [16, 24, 32, 64, 96, 160, 320]

    from functools import reduce
    channels = reduce(lambda x, y: x + [[y[0]] * y[1]] if y[2] != 0 else x[:-1] + [x[-1] + [y[0]] * y[1]],
                      zip(channels_per_layers, layers, downsample), [[]])

    if width_scale != 1.0:
        def make_even(x):
            return x if (x % 2 == 0) else x + 1
        channels = [[make_even(int(cij * width_scale)) for cij in ci] for ci in channels]
        init_block_channels = make_even(int(init_block_channels * width_scale))
        if width_scale > 1.0:
            final_block_channels = make_even(int(final_block_channels * width_scale))

    net = IGCV3(
        channels=channels,
        init_block_channels=init_block_channels,
        final_block_channels=final_block_channels,
        **kwargs)

    if pretrained:
        if (model_name is None) or (not model_name):
            raise ValueError(""Parameter `model_name` should be properly initialized for loading pretrained model."")
        from .model_store import get_model_file
        in_channels = kwargs[""in_channels""] if (""in_channels"" in kwargs) else 3
        input_shape = (1,) + (in_channels,) + net.in_size if net.data_format == ""channels_first"" else\
            (1,) + net.in_size + (in_channels,)
        net.build(input_shape=input_shape)
        net.load_weights(
            filepath=get_model_file(
                model_name=model_name,
                local_model_store_dir_path=root))

    return net",_2689.py,33,x % 2 == 0,not x % 2
https://github.com/google/jax/tree/master/jax/experimental/jax2tf/jax2tf.py,"def _reduce(*operands: TfVal,
            computation: Callable,
            jaxpr: core.Jaxpr,
            consts:  Sequence[Any],
            dimensions: Sequence[int],
            _in_avals: Sequence[core.ShapedArray],
            _out_aval: core.ShapedArray) -> Sequence[TfVal]:
  del computation
  assert not consts
  assert len(operands) % 2 == 0
  # operands: op1, op2, ..., init_val1, init_val2, ...
  # reducer takes op1[i], op2[i], ..., init_val1, init_val2, ...
  nr_operands = len(operands) // 2
  init_vals = operands[nr_operands:]
  operands = operands[0:nr_operands]

  reducer_arg_spec = tuple([tf.TensorSpec((), op.dtype) for op in init_vals] * 2)

  def reducer_computation(*args: TfVal) -> TfVal:
    closed_jaxpr = core.ClosedJaxpr(jaxpr, consts)
    res = _interpret_jaxpr(closed_jaxpr, *args, extra_name_stack=None)
    return res

  xla_reducer_computation = (
      tf.function(reducer_computation,
                  autograph=False).get_concrete_function(*reducer_arg_spec))

  outs = tfxla.variadic_reduce(operands, init_vals,
                               dimensions_to_reduce=dimensions,
                               reducer=xla_reducer_computation)
  if _WRAP_JAX_JIT_WITH_TF_FUNCTION:
    outs = tuple(tf.stop_gradient(out) for out in outs)  # See #7839
  return outs",_2690.py,10,len(operands) % 2 == 0,not len(operands) % 2
https://github.com/esdalmaijer/PyGaze/tree/master/pygaze/_eyetracker/libtobiilegacy.py,"def doCalibration(self,calibrationPoints):
        
        """"""Performs a calibration; displaying points and the calibration
        menu and keyboard input are handled by PyGaze routines, calibration
        is handled by Tobii routines
        
        arguments
        calibrationPoints    --    a list of (x,y) typles, specifying the
                        coordinates for the calibration points
                        (coordinates should be in PyGaze notation,
                        where (0,0) is the topleft and coordinates
                        are specified in pixels, e.g. (1024,768))
        
        keyword arguments
        None
        
        returns
        None or retval    --    returns None if no tracker is connected
                        returns retval when a tracker is connected;
                        retval can be one of three string values:
                            'accept'
                            'retry'
                            'abort'
        """"""
        
        # immediately return when no eyetracker is connected
        if self.eyetracker is None:
            return
        
        # set some properties
        self.points = calibrationPoints
        self.point_index = -1
        
        # visuals
        img = Image.new('RGB',self.disp.dispsize)
        draw = ImageDraw.Draw(img)
        
        self.calin = {'colour':(0,0,0), 'pos':(int(self.disp.dispsize[0]/2),int(self.disp.dispsize[1]/2)), 'r':2}
        self.calout = {'colour':(128,255,128), 'pos':(int(self.disp.dispsize[0]/2),int(self.disp.dispsize[1]/2)), 'r':64}
        self.calresult = {'img':img}
        self.calresultmsg = {'text':"""",'pos':(int(self.disp.dispsize[0]/2),int(self.disp.dispsize[1]/4))}
        
        # start calibration
        self.initcalibration_completed = False
        print(""StartCalibration"")
        self.eyetracker.StartCalibration(lambda error, r: self.on_calib_start(error, r))
        while not self.initcalibration_completed:
            pass
        
        # draw central target
        self.screen.clear()
        self.screen.draw_circle(colour=self.calout['colour'], pos=self.calout['pos'], r=self.calout['r'], fill=False)
        self.screen.draw_circle(colour=self.calin['colour'], pos=self.calin['pos'], r=self.calin['r'], fill=True)
        self.disp.fill(self.screen)
        self.disp.show()
        # wait for start command
        self.kb.get_key(keylist=['space'],timeout=None)
        
        # run through all points
        for self.point_index in range(len(self.points)):
            # create tobii.eye_tracking_io.types 2D point
            px, py = self.points[self.point_index]
            p = Point2D()
            p.x, p.y = float(px)/self.disp.dispsize[0], float(py)/self.disp.dispsize[1]
            # recalculate to psycho coordinates
            self.calin['pos'] = (int(px),int(py))
            self.calout['pos'] = (int(px),int(py))

            # show target while decreasing its size for 1.5 seconds
            t0 = clock.get_time()
            currentTime = (clock.get_time() - t0) / 1000.0
            while currentTime < 1.5:
                # reduce size of the outer ring, as time passes
                self.calout['r'] = int(40*(1.5-(currentTime))+4)
                # check for input (should this even be here?)
                self.kb.get_key(keylist=None, timeout=1)
                # draw calibration point
                self.screen.clear()
                self.screen.draw_circle(colour=self.calout['colour'], pos=self.calout['pos'], r=self.calout['r'], fill=False)
                self.screen.draw_circle(colour=self.calin['colour'], pos=self.calin['pos'], r=self.calin['r'], fill=True)
                self.disp.fill(self.screen)
                self.disp.show()
                # get time
                currentTime = (clock.get_time() - t0) / 1000.0
            
            # wait for point calibration to succeed
            self.add_point_completed = False
            self.eyetracker.AddCalibrationPoint(p, lambda error, r: self.on_add_completed(error, r))
            while not self.add_point_completed:
                # TODO: why would you continuously show the same stuff and poll the keyboard without using the input?
#                psychopy.event.getKeys()
#                self.calout.draw()
#                self.calin.draw()
#                win.flip()
                pass
         
        # wait for calibration to be complete
        self.computeCalibration_completed = False
        self.computeCalibration_succeeded = False
        self.eyetracker.ComputeCalibration(lambda error, r: self.on_calib_compute(error, r))
        while not self.computeCalibration_completed:
            pass
        self.eyetracker.StopCalibration(None)

        # reset display (same seems to be done below: what's the use?)
        self.disp.show()
        
        # get calibration info
        self.getcalibration_completed = False
        self.calib = self.eyetracker.GetCalibration(lambda error, calib: self.on_calib_response(error, calib))
        while not self.getcalibration_completed:
            pass
        
        # fill screen with half-gray
        self.screen.clear(colour=(128,128,128))
        
        # show calibration info
        if not self.computeCalibration_succeeded:
            # computeCalibration failed.
            self.calresultmsg['text'] = 'Not enough data was collected (Retry:r/Abort:ESC)'
            
        elif self.calib == None:
            # no calibration data
            self.calresultmsg['text'] = 'No calibration data (Retry:r/Abort:ESC)'
            
        else:
            # show the calibration accuracy
            points = {}
            for data in self.calib.plot_data:
                points[data.true_point] = {'left':data.left, 'right':data.right}
            
            if len(points) == 0:
                self.calresultmsg['text'] = 'No ture calibration data (Retry:r/Abort:ESC)'
            
            else:
                for p,d in points.iteritems():
                    if d['left'].status == 1:
                        self.screen.draw_line(colour=(255,0,0), spos=(p.x*self.disp.dispsize[0],p.y*self.disp.dispsize[1]), epos=(d['left'].map_point.x*self.disp.dispsize[0],d['left'].map_point.y*self.disp.dispsize[1]), pw=3)
                    if d['right'].status == 1:
                        self.screen.draw_line(colour=(0,255,0), spos=(p.x*self.disp.dispsize[0],p.y*self.disp.dispsize[1]), epos=(d['right'].map_point.x*self.disp.dispsize[0],d['right'].map_point.y*self.disp.dispsize[1]), pw=3)
                    self.screen.draw_ellipse(colour=(0,0,0), x=p.x*self.disp.dispsize[0]-10, y=p.y*self.disp.dispsize[1]-10, w=20, h=20, pw=3, fill=False)

                self.calresultmsg['text'] = 'Accept calibration results (Accept:a/Retry:r/Abort:ESC)'
        
        # original approach (Sogo): draw an image, then show that image via PscyhoPy
        self.calresult['img'] = img
        
        # alternative approach (Dalmaijer): use PyGaze drawing operations on self.screen, then present self.screen
        self.screen.draw_text(text=self.calresultmsg['text'],pos=self.calresultmsg['pos'])
        self.disp.fill(self.screen)
        self.disp.show()
        
        # wait for keyboard input
        key, presstime = self.kb.get_key(keylist=['a','r','escape'], timeout=None)
        if key == 'a':
            retval = 'accept'
        elif key == 'r':
            retval = 'retry'
        elif key == 'escape':
            retval = 'abort'
        
        return retval",_2728.py,122,self.calib == None,not self.calib
https://github.com/esdalmaijer/PyGaze/tree/master/pygaze/_eyetracker/libtobiilegacy.py,"def doCalibration(self,calibrationPoints):
        
        """"""Performs a calibration; displaying points and the calibration
        menu and keyboard input are handled by PyGaze routines, calibration
        is handled by Tobii routines
        
        arguments
        calibrationPoints    --    a list of (x,y) typles, specifying the
                        coordinates for the calibration points
                        (coordinates should be in PyGaze notation,
                        where (0,0) is the topleft and coordinates
                        are specified in pixels, e.g. (1024,768))
        
        keyword arguments
        None
        
        returns
        None or retval    --    returns None if no tracker is connected
                        returns retval when a tracker is connected;
                        retval can be one of three string values:
                            'accept'
                            'retry'
                            'abort'
        """"""
        
        # immediately return when no eyetracker is connected
        if self.eyetracker is None:
            return
        
        # set some properties
        self.points = calibrationPoints
        self.point_index = -1
        
        # visuals
        img = Image.new('RGB',self.disp.dispsize)
        draw = ImageDraw.Draw(img)
        
        self.calin = {'colour':(0,0,0), 'pos':(int(self.disp.dispsize[0]/2),int(self.disp.dispsize[1]/2)), 'r':2}
        self.calout = {'colour':(128,255,128), 'pos':(int(self.disp.dispsize[0]/2),int(self.disp.dispsize[1]/2)), 'r':64}
        self.calresult = {'img':img}
        self.calresultmsg = {'text':"""",'pos':(int(self.disp.dispsize[0]/2),int(self.disp.dispsize[1]/4))}
        
        # start calibration
        self.initcalibration_completed = False
        print(""StartCalibration"")
        self.eyetracker.StartCalibration(lambda error, r: self.on_calib_start(error, r))
        while not self.initcalibration_completed:
            pass
        
        # draw central target
        self.screen.clear()
        self.screen.draw_circle(colour=self.calout['colour'], pos=self.calout['pos'], r=self.calout['r'], fill=False)
        self.screen.draw_circle(colour=self.calin['colour'], pos=self.calin['pos'], r=self.calin['r'], fill=True)
        self.disp.fill(self.screen)
        self.disp.show()
        # wait for start command
        self.kb.get_key(keylist=['space'],timeout=None)
        
        # run through all points
        for self.point_index in range(len(self.points)):
            # create tobii.eye_tracking_io.types 2D point
            px, py = self.points[self.point_index]
            p = Point2D()
            p.x, p.y = float(px)/self.disp.dispsize[0], float(py)/self.disp.dispsize[1]
            # recalculate to psycho coordinates
            self.calin['pos'] = (int(px),int(py))
            self.calout['pos'] = (int(px),int(py))

            # show target while decreasing its size for 1.5 seconds
            t0 = clock.get_time()
            currentTime = (clock.get_time() - t0) / 1000.0
            while currentTime < 1.5:
                # reduce size of the outer ring, as time passes
                self.calout['r'] = int(40*(1.5-(currentTime))+4)
                # check for input (should this even be here?)
                self.kb.get_key(keylist=None, timeout=1)
                # draw calibration point
                self.screen.clear()
                self.screen.draw_circle(colour=self.calout['colour'], pos=self.calout['pos'], r=self.calout['r'], fill=False)
                self.screen.draw_circle(colour=self.calin['colour'], pos=self.calin['pos'], r=self.calin['r'], fill=True)
                self.disp.fill(self.screen)
                self.disp.show()
                # get time
                currentTime = (clock.get_time() - t0) / 1000.0
            
            # wait for point calibration to succeed
            self.add_point_completed = False
            self.eyetracker.AddCalibrationPoint(p, lambda error, r: self.on_add_completed(error, r))
            while not self.add_point_completed:
                # TODO: why would you continuously show the same stuff and poll the keyboard without using the input?
#                psychopy.event.getKeys()
#                self.calout.draw()
#                self.calin.draw()
#                win.flip()
                pass
         
        # wait for calibration to be complete
        self.computeCalibration_completed = False
        self.computeCalibration_succeeded = False
        self.eyetracker.ComputeCalibration(lambda error, r: self.on_calib_compute(error, r))
        while not self.computeCalibration_completed:
            pass
        self.eyetracker.StopCalibration(None)

        # reset display (same seems to be done below: what's the use?)
        self.disp.show()
        
        # get calibration info
        self.getcalibration_completed = False
        self.calib = self.eyetracker.GetCalibration(lambda error, calib: self.on_calib_response(error, calib))
        while not self.getcalibration_completed:
            pass
        
        # fill screen with half-gray
        self.screen.clear(colour=(128,128,128))
        
        # show calibration info
        if not self.computeCalibration_succeeded:
            # computeCalibration failed.
            self.calresultmsg['text'] = 'Not enough data was collected (Retry:r/Abort:ESC)'
            
        elif self.calib == None:
            # no calibration data
            self.calresultmsg['text'] = 'No calibration data (Retry:r/Abort:ESC)'
            
        else:
            # show the calibration accuracy
            points = {}
            for data in self.calib.plot_data:
                points[data.true_point] = {'left':data.left, 'right':data.right}
            
            if len(points) == 0:
                self.calresultmsg['text'] = 'No ture calibration data (Retry:r/Abort:ESC)'
            
            else:
                for p,d in points.iteritems():
                    if d['left'].status == 1:
                        self.screen.draw_line(colour=(255,0,0), spos=(p.x*self.disp.dispsize[0],p.y*self.disp.dispsize[1]), epos=(d['left'].map_point.x*self.disp.dispsize[0],d['left'].map_point.y*self.disp.dispsize[1]), pw=3)
                    if d['right'].status == 1:
                        self.screen.draw_line(colour=(0,255,0), spos=(p.x*self.disp.dispsize[0],p.y*self.disp.dispsize[1]), epos=(d['right'].map_point.x*self.disp.dispsize[0],d['right'].map_point.y*self.disp.dispsize[1]), pw=3)
                    self.screen.draw_ellipse(colour=(0,0,0), x=p.x*self.disp.dispsize[0]-10, y=p.y*self.disp.dispsize[1]-10, w=20, h=20, pw=3, fill=False)

                self.calresultmsg['text'] = 'Accept calibration results (Accept:a/Retry:r/Abort:ESC)'
        
        # original approach (Sogo): draw an image, then show that image via PscyhoPy
        self.calresult['img'] = img
        
        # alternative approach (Dalmaijer): use PyGaze drawing operations on self.screen, then present self.screen
        self.screen.draw_text(text=self.calresultmsg['text'],pos=self.calresultmsg['pos'])
        self.disp.fill(self.screen)
        self.disp.show()
        
        # wait for keyboard input
        key, presstime = self.kb.get_key(keylist=['a','r','escape'], timeout=None)
        if key == 'a':
            retval = 'accept'
        elif key == 'r':
            retval = 'retry'
        elif key == 'escape':
            retval = 'abort'
        
        return retval",_2728.py,132,len(points) == 0,not len(points)
https://github.com/qiniu/python-sdk/tree/master/test/unit/test_discovery_v1.py,"def test_configuration_serialization(self):
        """"""
        Test serialization/deserialization for Configuration
        """"""

        # Construct dict forms of any model objects needed in order to build this model.

        font_setting_model = {} # FontSetting
        font_setting_model['level'] = 38
        font_setting_model['min_size'] = 38
        font_setting_model['max_size'] = 38
        font_setting_model['bold'] = True
        font_setting_model['italic'] = True
        font_setting_model['name'] = 'testString'

        pdf_heading_detection_model = {} # PdfHeadingDetection
        pdf_heading_detection_model['fonts'] = [font_setting_model]

        pdf_settings_model = {} # PdfSettings
        pdf_settings_model['heading'] = pdf_heading_detection_model

        word_style_model = {} # WordStyle
        word_style_model['level'] = 38
        word_style_model['names'] = ['testString']

        word_heading_detection_model = {} # WordHeadingDetection
        word_heading_detection_model['fonts'] = [font_setting_model]
        word_heading_detection_model['styles'] = [word_style_model]

        word_settings_model = {} # WordSettings
        word_settings_model['heading'] = word_heading_detection_model

        x_path_patterns_model = {} # XPathPatterns
        x_path_patterns_model['xpaths'] = ['testString']

        html_settings_model = {} # HtmlSettings
        html_settings_model['exclude_tags_completely'] = ['testString']
        html_settings_model['exclude_tags_keep_content'] = ['span']
        html_settings_model['keep_content'] = x_path_patterns_model
        html_settings_model['exclude_content'] = x_path_patterns_model
        html_settings_model['keep_tag_attributes'] = ['testString']
        html_settings_model['exclude_tag_attributes'] = ['testString']

        segment_settings_model = {} # SegmentSettings
        segment_settings_model['enabled'] = True
        segment_settings_model['selector_tags'] = ['h1', 'h2']
        segment_settings_model['annotated_fields'] = ['custom-field-1', 'custom-field-2']

        normalization_operation_model = {} # NormalizationOperation
        normalization_operation_model['operation'] = 'move'
        normalization_operation_model['source_field'] = 'extracted_metadata.title'
        normalization_operation_model['destination_field'] = 'metadata.title'

        conversions_model = {} # Conversions
        conversions_model['pdf'] = pdf_settings_model
        conversions_model['word'] = word_settings_model
        conversions_model['html'] = html_settings_model
        conversions_model['segment'] = segment_settings_model
        conversions_model['json_normalizations'] = [normalization_operation_model]
        conversions_model['image_text_recognition'] = True

        nlu_enrichment_keywords_model = {} # NluEnrichmentKeywords
        nlu_enrichment_keywords_model['sentiment'] = True
        nlu_enrichment_keywords_model['emotion'] = False
        nlu_enrichment_keywords_model['limit'] = 50

        nlu_enrichment_entities_model = {} # NluEnrichmentEntities
        nlu_enrichment_entities_model['sentiment'] = True
        nlu_enrichment_entities_model['emotion'] = False
        nlu_enrichment_entities_model['limit'] = 50
        nlu_enrichment_entities_model['mentions'] = True
        nlu_enrichment_entities_model['mention_types'] = True
        nlu_enrichment_entities_model['sentence_locations'] = True
        nlu_enrichment_entities_model['model'] = 'WKS-model-id'

        nlu_enrichment_sentiment_model = {} # NluEnrichmentSentiment
        nlu_enrichment_sentiment_model['document'] = True
        nlu_enrichment_sentiment_model['targets'] = ['IBM', 'Watson']

        nlu_enrichment_emotion_model = {} # NluEnrichmentEmotion
        nlu_enrichment_emotion_model['document'] = True
        nlu_enrichment_emotion_model['targets'] = ['IBM', 'Watson']

        nlu_enrichment_semantic_roles_model = {} # NluEnrichmentSemanticRoles
        nlu_enrichment_semantic_roles_model['entities'] = True
        nlu_enrichment_semantic_roles_model['keywords'] = True
        nlu_enrichment_semantic_roles_model['limit'] = 50

        nlu_enrichment_relations_model = {} # NluEnrichmentRelations
        nlu_enrichment_relations_model['model'] = 'WKS-model-id'

        nlu_enrichment_concepts_model = {} # NluEnrichmentConcepts
        nlu_enrichment_concepts_model['limit'] = 8

        nlu_enrichment_features_model = {} # NluEnrichmentFeatures
        nlu_enrichment_features_model['keywords'] = nlu_enrichment_keywords_model
        nlu_enrichment_features_model['entities'] = nlu_enrichment_entities_model
        nlu_enrichment_features_model['sentiment'] = nlu_enrichment_sentiment_model
        nlu_enrichment_features_model['emotion'] = nlu_enrichment_emotion_model
        nlu_enrichment_features_model['categories'] = {}
        nlu_enrichment_features_model['semantic_roles'] = nlu_enrichment_semantic_roles_model
        nlu_enrichment_features_model['relations'] = nlu_enrichment_relations_model
        nlu_enrichment_features_model['concepts'] = nlu_enrichment_concepts_model

        enrichment_options_model = {} # EnrichmentOptions
        enrichment_options_model['features'] = nlu_enrichment_features_model
        enrichment_options_model['language'] = 'ar'
        enrichment_options_model['model'] = 'testString'

        enrichment_model = {} # Enrichment
        enrichment_model['description'] = 'testString'
        enrichment_model['destination_field'] = 'enriched_title'
        enrichment_model['source_field'] = 'title'
        enrichment_model['overwrite'] = False
        enrichment_model['enrichment'] = 'natural_language_understanding'
        enrichment_model['ignore_downstream_errors'] = False
        enrichment_model['options'] = enrichment_options_model

        source_schedule_model = {} # SourceSchedule
        source_schedule_model['enabled'] = True
        source_schedule_model['time_zone'] = 'America/New_York'
        source_schedule_model['frequency'] = 'weekly'

        source_options_folder_model = {} # SourceOptionsFolder
        source_options_folder_model['owner_user_id'] = 'testString'
        source_options_folder_model['folder_id'] = 'testString'
        source_options_folder_model['limit'] = 38

        source_options_object_model = {} # SourceOptionsObject
        source_options_object_model['name'] = 'testString'
        source_options_object_model['limit'] = 38

        source_options_site_coll_model = {} # SourceOptionsSiteColl
        source_options_site_coll_model['site_collection_path'] = '/sites/TestSiteA'
        source_options_site_coll_model['limit'] = 10

        source_options_web_crawl_model = {} # SourceOptionsWebCrawl
        source_options_web_crawl_model['url'] = 'testString'
        source_options_web_crawl_model['limit_to_starting_hosts'] = True
        source_options_web_crawl_model['crawl_speed'] = 'normal'
        source_options_web_crawl_model['allow_untrusted_certificate'] = False
        source_options_web_crawl_model['maximum_hops'] = 38
        source_options_web_crawl_model['request_timeout'] = 38
        source_options_web_crawl_model['override_robots_txt'] = False
        source_options_web_crawl_model['blacklist'] = ['testString']

        source_options_buckets_model = {} # SourceOptionsBuckets
        source_options_buckets_model['name'] = 'testString'
        source_options_buckets_model['limit'] = 38

        source_options_model = {} # SourceOptions
        source_options_model['folders'] = [source_options_folder_model]
        source_options_model['objects'] = [source_options_object_model]
        source_options_model['site_collections'] = [source_options_site_coll_model]
        source_options_model['urls'] = [source_options_web_crawl_model]
        source_options_model['buckets'] = [source_options_buckets_model]
        source_options_model['crawl_all_buckets'] = True

        source_model = {} # Source
        source_model['type'] = 'salesforce'
        source_model['credential_id'] = '00ad0000-0000-11e8-ba89-0ed5f00f718b'
        source_model['schedule'] = source_schedule_model
        source_model['options'] = source_options_model

        # Construct a json representation of a Configuration model
        configuration_model_json = {}
        configuration_model_json['configuration_id'] = 'testString'
        configuration_model_json['name'] = 'testString'
        configuration_model_json['created'] = ""2019-01-01T12:00:00Z""
        configuration_model_json['updated'] = ""2019-01-01T12:00:00Z""
        configuration_model_json['description'] = 'testString'
        configuration_model_json['conversions'] = conversions_model
        configuration_model_json['enrichments'] = [enrichment_model]
        configuration_model_json['normalizations'] = [normalization_operation_model]
        configuration_model_json['source'] = source_model

        # Construct a model instance of Configuration by calling from_dict on the json representation
        configuration_model = Configuration.from_dict(configuration_model_json)
        assert configuration_model != False

        # Construct a model instance of Configuration by calling from_dict on the json representation
        configuration_model_dict = Configuration.from_dict(configuration_model_json).__dict__
        configuration_model2 = Configuration(**configuration_model_dict)

        # Verify the model instances are equivalent
        assert configuration_model == configuration_model2

        # Convert model instance back to dict and verify no loss of data
        configuration_model_json2 = configuration_model.to_dict()
        assert configuration_model_json2 == configuration_model_json",_2762.py,179,configuration_model != False,configuration_model
https://github.com/MarshalX/yandex-music-api/tree/master/tests/test_link.py,"def test_de_list_none(self, client):
        assert Link.de_list({}, client) == []",_2829.py,2,"Link.de_list({}, client) == []","not Link.de_list({}, client)"
https://github.com/mclarkk/lifxlan/tree/master/lifxlan/lifxlan.py,"def get_device_by_name(self, name):
        device = None
        all_devices = self.get_devices()
        for d in all_devices:
            if d.get_label() == name:
                device = d
        if device == None:               # didn't find it?
            self.discover_devices()      # update list in case it is out of date
            all_devices = self.get_devices()
            for d in all_devices:            # and try again
                if d.get_label() == name:
                    device = d
        return device",_2880.py,7,device == None,not device
https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/tree/master/util/image_pool.py,"def query(self, images):
        """"""Return an image from the pool.

        Parameters:
            images: the latest generated images from the generator

        Returns images from the buffer.

        By 50/100, the buffer will return input images.
        By 50/100, the buffer will return images previously stored in the buffer,
        and insert the current images to the buffer.
        """"""
        if self.pool_size == 0:  # if the buffer size is 0, do nothing
            return images
        return_images = []
        for image in images:
            image = torch.unsqueeze(image.data, 0)
            if self.num_imgs < self.pool_size:   # if the buffer is not full; keep inserting current images to the buffer
                self.num_imgs = self.num_imgs + 1
                self.images.append(image)
                return_images.append(image)
            else:
                p = random.uniform(0, 1)
                if p > 0.5:  # by 50% chance, the buffer will return a previously stored image, and insert the current image into the buffer
                    random_id = random.randint(0, self.pool_size - 1)  # randint is inclusive
                    tmp = self.images[random_id].clone()
                    self.images[random_id] = image
                    return_images.append(tmp)
                else:       # by another 50% chance, the buffer will return the current image
                    return_images.append(image)
        return_images = torch.cat(return_images, 0)   # collect all the images and return
        return return_images",_2901.py,13,self.pool_size == 0,not self.pool_size