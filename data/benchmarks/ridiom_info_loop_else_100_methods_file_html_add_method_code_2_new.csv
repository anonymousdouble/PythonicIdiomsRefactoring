file_html,method_content,file_name,lineno,old_code,new_code,
https://github.com/ansible/galaxy/tree/master/lib/galaxy/tool_shed/metadata/metadata_generator.py,"def generate_metadata_for_changeset_revision(self):
        """"""
        Generate metadata for a repository using its files on disk.  To generate metadata
        for changeset revisions older than the repository tip, the repository will have been
        cloned to a temporary location and updated to a specified changeset revision to access
        that changeset revision's disk files, so the value of self.repository_files_dir will not
        always be self.repository.repo_path( self.app ) (it could be an absolute path to a temporary
        directory containing a clone).  If it is an absolute path, the value of self.relative_install_dir
        must contain repository.repo_path( self.app ).

        The value of self.persist will be True when the installed repository contains a valid
        tool_data_table_conf.xml.sample file, in which case the entries should ultimately be
        persisted to the file referred to by self.app.config.shed_tool_data_table_config.
        """"""
        if self.shed_config_dict is None:
            self.shed_config_dict = {}
        if self.updating_installed_repository:
            # Keep the original tool shed repository metadata if setting metadata on a repository
            # installed into a local Galaxy instance for which we have pulled updates.
            original_repository_metadata = self.repository.metadata_
        else:
            original_repository_metadata = None
        readme_file_names = _get_readme_file_names(str(self.repository.name))
        if self.app.name == 'galaxy':
            # Shed related tool panel configs are only relevant to Galaxy.
            metadata_dict = {'shed_config_filename': self.shed_config_dict.get('config_filename')}
        else:
            metadata_dict = {}
        readme_files = []
        invalid_tool_configs = []
        if self.resetting_all_metadata_on_repository:
            if not self.relative_install_dir:
                raise Exception(""The value of self.repository.repo_path must be set when resetting all metadata on a repository."")
            # Keep track of the location where the repository is temporarily cloned so that we can
            # strip the path when setting metadata.  The value of self.repository_files_dir is the
            # full path to the temporary directory to which self.repository was cloned.
            work_dir = self.repository_files_dir
            files_dir = self.repository_files_dir
            # Since we're working from a temporary directory, we can safely copy sample files included
            # in the repository to the repository root.
        else:
            # Use a temporary working directory to copy all sample files.
            work_dir = tempfile.mkdtemp(prefix=""tmp-toolshed-gmfcr"")
            # All other files are on disk in the repository's repo_path, which is the value of
            # self.relative_install_dir.
            files_dir = self.relative_install_dir
            if self.shed_config_dict.get('tool_path'):
                files_dir = os.path.join(self.shed_config_dict['tool_path'], files_dir)
        # Create ValidationContext to load and validate tools, data tables and datatypes
        with ValidationContext.from_app(app=self.app, work_dir=work_dir) as validation_context:
            tv = ToolValidator(validation_context)
            # Handle proprietary datatypes, if any.
            datatypes_config = get_config_from_disk(suc.DATATYPES_CONFIG_FILENAME, files_dir)
            if datatypes_config:
                metadata_dict = self.generate_datatypes_metadata(tv,
                                                                 files_dir,
                                                                 datatypes_config,
                                                                 metadata_dict)
            # Get the relative path to all sample files included in the repository for storage in
            # the repository's metadata.
            sample_file_metadata_paths, sample_file_copy_paths = \
                self.get_sample_files_from_disk(repository_files_dir=files_dir,
                                                tool_path=self.shed_config_dict.get('tool_path'),
                                                relative_install_dir=self.relative_install_dir)
            if sample_file_metadata_paths:
                metadata_dict['sample_files'] = sample_file_metadata_paths
            # Copy all sample files included in the repository to a single directory location so we
            # can load tools that depend on them.
            data_table_conf_xml_sample_files = []
            for sample_file in sample_file_copy_paths:
                tool_util.copy_sample_file(self.app, sample_file, dest_path=work_dir)
                # If the list of sample files includes a tool_data_table_conf.xml.sample file, load
                # its table elements into memory.
                relative_path, filename = os.path.split(sample_file)
                if filename == 'tool_data_table_conf.xml.sample':
                    data_table_conf_xml_sample_files.append(sample_file)

            for data_table_conf_xml_sample_file in data_table_conf_xml_sample_files:
                # We create a new ToolDataTableManager to avoid adding entries to the app-wide
                # tool data tables. This is only used for checking that the data table is valid.
                new_table_elems, error_message = \
                    validation_context.tool_data_tables.add_new_entries_from_config_file(config_filename=data_table_conf_xml_sample_file,
                                                                                         tool_data_path=work_dir,
                                                                                         shed_tool_data_table_config=work_dir,
                                                                                         persist=False)
                if error_message:
                    self.invalid_file_tups.append((filename, error_message))
            for root, dirs, files in os.walk(files_dir):
                if root.find('.hg') < 0 and root.find('hgrc') < 0:
                    if '.hg' in dirs:
                        dirs.remove('.hg')
                    for name in files:
                        # See if we have a repository dependencies defined.
                        if name == REPOSITORY_DEPENDENCY_DEFINITION_FILENAME:
                            path_to_repository_dependencies_config = os.path.join(root, name)
                            metadata_dict, error_message = \
                                self.generate_repository_dependency_metadata(path_to_repository_dependencies_config,
                                                                             metadata_dict)
                            if error_message:
                                self.invalid_file_tups.append((name, error_message))
                        # See if we have one or more READ_ME files.
                        elif name.lower() in readme_file_names:
                            relative_path_to_readme = self.get_relative_path_to_repository_file(root,
                                                                                                name,
                                                                                                self.relative_install_dir,
                                                                                                work_dir,
                                                                                                self.shed_config_dict)
                            readme_files.append(relative_path_to_readme)
                        # See if we have a tool config.
                        elif looks_like_a_tool(os.path.join(root, name), invalid_names=self.NOT_TOOL_CONFIGS):
                            full_path = str(os.path.abspath(os.path.join(root, name)))  # why the str, seems very odd
                            element_tree, error_message = parse_xml(full_path)
                            if element_tree is None:
                                is_tool = False
                            else:
                                element_tree_root = element_tree.getroot()
                                is_tool = element_tree_root.tag == 'tool'
                            if is_tool:
                                tool, valid, error_message = \
                                    tv.load_tool_from_config(self.app.security.encode_id(self.repository.id),
                                                             full_path)
                                if tool is None:
                                    if not valid:
                                        invalid_tool_configs.append(name)
                                        self.invalid_file_tups.append((name, error_message))
                                else:
                                    invalid_files_and_errors_tups = \
                                        tv.check_tool_input_params(files_dir,
                                                                   name,
                                                                   tool,
                                                                   sample_file_copy_paths)
                                    can_set_metadata = True
                                    for tup in invalid_files_and_errors_tups:
                                        if name in tup:
                                            can_set_metadata = False
                                            invalid_tool_configs.append(name)
                                            break
                                    if can_set_metadata:
                                        relative_path_to_tool_config = \
                                            self.get_relative_path_to_repository_file(root,
                                                                                      name,
                                                                                      self.relative_install_dir,
                                                                                      work_dir,
                                                                                      self.shed_config_dict)
                                        metadata_dict = self.generate_tool_metadata(relative_path_to_tool_config,
                                                                                    tool,
                                                                                    metadata_dict)
                                    else:
                                        for tup in invalid_files_and_errors_tups:
                                            self.invalid_file_tups.append(tup)
                        # Find all exported workflows.
                        elif name.endswith('.ga'):
                            relative_path = os.path.join(root, name)
                            if os.path.getsize(os.path.abspath(relative_path)) > 0:
                                fp = open(relative_path, 'rb')
                                workflow_text = fp.read()
                                fp.close()
                                if workflow_text:
                                    valid_exported_galaxy_workflow = True
                                    try:
                                        exported_workflow_dict = json.loads(workflow_text)
                                    except Exception:
                                        log.exception(""Skipping file %s since it does not seem to be a valid exported Galaxy workflow"",
                                                      str(relative_path))
                                        valid_exported_galaxy_workflow = False
                                if valid_exported_galaxy_workflow and \
                                    'a_galaxy_workflow' in exported_workflow_dict and \
                                        exported_workflow_dict['a_galaxy_workflow'] == 'true':
                                    metadata_dict = self.generate_workflow_metadata(relative_path,
                                                                                    exported_workflow_dict,
                                                                                    metadata_dict)
        # Handle any data manager entries
        data_manager_config = get_config_from_disk(suc.REPOSITORY_DATA_MANAGER_CONFIG_FILENAME, files_dir)
        metadata_dict = self.generate_data_manager_metadata(files_dir,
                                                            data_manager_config,
                                                            metadata_dict,
                                                            shed_config_dict=self.shed_config_dict)

        if readme_files:
            metadata_dict['readme_files'] = readme_files
        # This step must be done after metadata for tools has been defined.
        tool_dependencies_config = get_config_from_disk(TOOL_DEPENDENCY_DEFINITION_FILENAME, files_dir)
        if tool_dependencies_config:
            metadata_dict, error_message = \
                self.generate_tool_dependency_metadata(tool_dependencies_config,
                                                       metadata_dict,
                                                       original_repository_metadata=original_repository_metadata)
            if error_message:
                self.invalid_file_tups.append((TOOL_DEPENDENCY_DEFINITION_FILENAME, error_message))
        if invalid_tool_configs:
            metadata_dict['invalid_tools'] = invalid_tool_configs
        self.metadata_dict = metadata_dict
        remove_dir(work_dir)",_101116.py,133,"for tup in invalid_files_and_errors_tups:
    if name in tup:
        can_set_metadata = False
        invalid_tool_configs.append(name)
        break","if can_set_metadata:
    relative_path_to_tool_config = self.get_relative_path_to_repository_file(root, name, self.relative_install_dir, work_dir, self.shed_config_dict)
    metadata_dict = self.generate_tool_metadata(relative_path_to_tool_config, tool, metadata_dict)
else:
    for tup in invalid_files_and_errors_tups:
        self.invalid_file_tups.append(tup)","for tup in invalid_files_and_errors_tups:
    if name in tup:
        invalid_tool_configs.append(name)
        for tup in invalid_files_and_errors_tups:
            self.invalid_file_tups.append(tup)
        break
else:
    relative_path_to_tool_config = self.get_relative_path_to_repository_file(root, name, self.relative_install_dir, work_dir, self.shed_config_dict)
    metadata_dict = self.generate_tool_metadata(relative_path_to_tool_config, tool, metadata_dict)"
https://github.com/tribe29/checkmk/tree/master/cmk/gui/plugins/userdb/ldap_connector.py,"def _group_and_user_base_dn(self):
        user_dn = ldap.dn.str2dn(self._get_user_dn())
        group_dn = ldap.dn.str2dn(self.get_group_dn())

        common_len = min(len(user_dn), len(group_dn))
        user_dn, group_dn = user_dn[-common_len:], group_dn[-common_len:]

        base_dn = None
        for i in range(common_len):
            if user_dn[i:] == group_dn[i:]:
                base_dn = user_dn[i:]
                break

        if base_dn is None:
            raise MKLDAPException(
                _(
                    ""Unable to synchronize nested groups (Found no common base DN for user base ""
                    'DN ""%s"" and group base DN ""%s"")'
                )
                % (self._get_user_dn(), self.get_group_dn())
            )

        return ldap.dn.dn2str(base_dn)",_10158.py,9,"for i in range(common_len):
    if user_dn[i:] == group_dn[i:]:
        base_dn = user_dn[i:]
        break","if base_dn is None:
    raise MKLDAPException(_('Unable to synchronize nested groups (Found no common base DN for user base DN ""%s"" and group base DN ""%s"")') % (self._get_user_dn(), self.get_group_dn()))","for i in range(common_len):
    if user_dn[i:] == group_dn[i:]:
        base_dn = user_dn[i:]
        break
else:
    raise MKLDAPException(_('Unable to synchronize nested groups (Found no common base DN for user base DN ""%s"" and group base DN ""%s"")') % (self._get_user_dn(), self.get_group_dn()))"
https://github.com/veusz/veusz/tree/master/veusz/document/commandinterface.py,"def DatasetPlugin(self, pluginname, fields, datasetnames={}):
        """"""Use a dataset plugin.

        pluginname: name of plugin to use
        fields: dict of input values to plugin
        datasetnames: dict mapping old names to new names of datasets
        if they are renamed. The new name None means dataset is deleted.""""""

        # lookup plugin (urgh)
        plugin = None
        for pkls in plugins.datasetpluginregistry:
            if pkls.name == pluginname:
                plugin = pkls()
                break
        if plugin is None:
            raise RuntimeError(""Cannot find dataset plugin '%s'"" % pluginname)

        # do the work
        op = operations.OperationDatasetPlugin(
            plugin, fields, datasetnames=datasetnames)
        outdatasets = self.document.applyOperation(op)

        if self.verbose:
            print(_(
                ""Used dataset plugin %s to make datasets %s"") % (
                    pluginname, ', '.join(outdatasets))
            )",_102526.py,11,"for pkls in plugins.datasetpluginregistry:
    if pkls.name == pluginname:
        plugin = pkls()
        break","if plugin is None:
    raise RuntimeError(""Cannot find dataset plugin '%s'"" % pluginname)","for pkls in plugins.datasetpluginregistry:
    if pkls.name == pluginname:
        plugin = pkls()
        break
else:
    raise RuntimeError(""Cannot find dataset plugin '%s'"" % pluginname)"
https://github.com/PaddlePaddle/PaddleSeg/tree/master/contrib/PanopticDeepLab/utils/evaluation/instance.py,"def update(self, preds, gts, ignore_mask=None):
        """"""
        compute y_true and y_score in this image.
        preds (list): tuple list [(label, confidence, mask), ...].
        gts (list): tuple list [(label, mask), ...].
        ignore_mask (np.ndarray): Mask to ignore.
        """"""

        pred_instances, gt_instances = self.get_instances(
            preds, gts, ignore_mask=ignore_mask)

        for i in range(self.num_classes):
            if i not in self.thing_list:
                continue
            for oi, oth in enumerate(self.overlaps):
                cur_true = np.ones((len(gt_instances[i])))
                cur_score = np.ones(len(gt_instances[i])) * (-float(""inf""))
                cur_match = np.zeros(len(gt_instances[i]), dtype=np.bool)
                for gti, gt_instance in enumerate(gt_instances[i]):
                    found_match = False
                    for pred_instance in gt_instance['matched_pred']:
                        overlap = float(pred_instance['intersection']) / (
                            gt_instance['pixel_count'] + pred_instance[
                                'pixel_count'] - pred_instance['intersection'])
                        if overlap > oth:
                            confidence = pred_instance['confidence']

                            # if we already has a prediction for this groundtruth
                            # the prediction with the lower score is automatically a false positive
                            if cur_match[gti]:
                                max_score = max(cur_score[gti], confidence)
                                min_score = min(cur_score[gti], confidence)
                                cur_score = max_score
                                # append false positive
                                cur_true = np.append(cur_true, 0)
                                cur_score = np.append(cur_score, min_score)
                                cur_match = np.append(cur_match, True)
                            # otherwise set score
                            else:
                                found_match = True
                                cur_match[gti] = True
                                cur_score[gti] = confidence

                    if not found_match:
                        self.hard_fns[i][oi] += 1
                # remove not-matched ground truth instances
                cur_true = cur_true[cur_match == True]
                cur_score = cur_score[cur_match == True]

                # collect not-matched predictions as false positive
                for pred_instance in pred_instances[i]:
                    found_gt = False
                    for gt_instance in pred_instance['matched_gt']:
                        overlap = float(gt_instance['intersection']) / (
                            gt_instance['pixel_count'] + pred_instance[
                                'pixel_count'] - gt_instance['intersection'])
                        if overlap > oth:
                            found_gt = True
                            break
                    if not found_gt:
                        proportion_ignore = 0
                        if ignore_mask is not None:
                            nb_ignore_pixels = pred_instance[
                                'void_intersection']
                            proportion_ignore = float(
                                nb_ignore_pixels) / pred_instance['pixel_count']
                        if proportion_ignore <= oth:
                            cur_true = np.append(cur_true, 0)
                            cur_score = np.append(cur_score,
                                                  pred_instance['confidence'])
                self.y_true[i][oi] = np.append(self.y_true[i][oi], cur_true)
                self.y_score[i][oi] = np.append(self.y_score[i][oi], cur_score)",_103349.py,53,"for gt_instance in pred_instance['matched_gt']:
    overlap = float(gt_instance['intersection']) / (gt_instance['pixel_count'] + pred_instance['pixel_count'] - gt_instance['intersection'])
    if overlap > oth:
        found_gt = True
        break","if not found_gt:
    proportion_ignore = 0
    if ignore_mask is not None:
        nb_ignore_pixels = pred_instance['void_intersection']
        proportion_ignore = float(nb_ignore_pixels) / pred_instance['pixel_count']
    if proportion_ignore <= oth:
        cur_true = np.append(cur_true, 0)
        cur_score = np.append(cur_score, pred_instance['confidence'])","for gt_instance in pred_instance['matched_gt']:
    overlap = float(gt_instance['intersection']) / (gt_instance['pixel_count'] + pred_instance['pixel_count'] - gt_instance['intersection'])
    if overlap > oth:
        break
else:
    proportion_ignore = 0
    if ignore_mask is not None:
        nb_ignore_pixels = pred_instance['void_intersection']
        proportion_ignore = float(nb_ignore_pixels) / pred_instance['pixel_count']
    if proportion_ignore <= oth:
        cur_true = np.append(cur_true, 0)
        cur_score = np.append(cur_score, pred_instance['confidence'])"
https://github.com/angr/angrop/tree/master/angrop/chain_builder/reg_setter.py,"def _filter_gadgets(self, gadgets):
        """"""
        filter gadgets having the same effect
        """"""
        gadgets = set(gadgets)
        skip = set({})
        while True:
            to_remove = set({})
            for g in gadgets-skip:
                to_remove.update({x for x in gadgets-{g} if self._strictly_better(g, x)})
                if to_remove:
                    break
                skip.add(g)
            if not to_remove:
                break
            gadgets -= to_remove
        return gadgets",_10342.py,9,"for g in gadgets - skip:
    to_remove.update({x for x in gadgets - {g} if self._strictly_better(g, x)})
    if to_remove:
        break
    skip.add(g)","if not to_remove:
    break","for g in gadgets - skip:
    to_remove.update({x for x in gadgets - {g} if self._strictly_better(g, x)})
    if to_remove:
        break
    skip.add(g)
else:
    break"
https://github.com/forseti-security/forseti-security/tree/master/google/cloud/forseti/services/dao.py,"def define_model(model_name, dbengine, model_seed):
    """"""Defines table classes which point to the corresponding model.

        This means, for each model being accessed this function needs to
        be called in order to generate a full set of table definitions.

        Models are name spaced via a random model seed such that multiple
        models can exist within the same database. In order to implement
        the name spacing in an abstract way.

    Args:
        model_name (str): model handle
        dbengine (object): db engine
        model_seed (str): seed to get etag

    Returns:
        tuple: (sessionmaker, ModelAccess)
    """"""

    base = declarative_base()

    denormed_group_in_group = '{}_group_in_group'.format(model_name)
    bindings_tablename = '{}_bindings'.format(model_name)
    roles_tablename = '{}_roles'.format(model_name)
    permissions_tablename = '{}_permissions'.format(model_name)
    members_tablename = '{}_members'.format(model_name)
    resources_tablename = '{}_resources'.format(model_name)

    role_permissions = Table('{}_role_permissions'.format(model_name),
                             base.metadata,
                             Column(
                                 'roles_name', ForeignKey(
                                     '{}.name'.format(roles_tablename)),
                                 primary_key=True),
                             Column(
                                 'permissions_name', ForeignKey(
                                     '{}.name'.format(permissions_tablename)),
                                 primary_key=True), )

    binding_members = Table('{}_binding_members'.format(model_name),
                            base.metadata,
                            Column(
                                'bindings_id', ForeignKey(
                                    '{}.id'.format(bindings_tablename)),
                                primary_key=True),
                            Column(
                                'members_name', ForeignKey(
                                    '{}.name'.format(members_tablename)),
                                primary_key=True), )

    group_members = Table(
        '{}_group_members'.format(model_name),
        base.metadata,
        Column('group_name',
               ForeignKey('{}.name'.format(members_tablename)),
               primary_key=True),
        Column('members_name',
               ForeignKey('{}.name'.format(members_tablename)),
               primary_key=True),
    )

    groups_settings = Table(
        '{}_groups_settings'.format(model_name),
        base.metadata,
        Column('group_name',
               ForeignKey('{}.name'.format(members_tablename)),
               primary_key=True),
        Column('settings',
               Text(16777215)),
    )

    def get_string_by_dialect(db_dialect, column_size):
        """"""Get Sqlalchemy String by dialect.
        Sqlite doesn't support collation type, need to define different
        column types for different database engine.

        This is used to make MySQL column case sensitive by adding
        an encoding type.
        Args:
            db_dialect (str): The db dialect.
            column_size (int): The size of the column.

        Returns:
            String: Sqlalchemy String.
        """"""
        if db_dialect.lower() == 'sqlite':
            return String(column_size)
        return String(column_size, collation='utf8mb4_bin')

    class Resource(base):
        """"""Row entry for a GCP resource.""""""
        __tablename__ = resources_tablename

        cai_resource_name = Column(String(4096))
        cai_resource_type = Column(String(512))
        full_name = Column(String(2048), nullable=False)
        type_name = Column(get_string_by_dialect(dbengine.dialect.name, 700),
                           primary_key=True)
        parent_type_name = Column(
            get_string_by_dialect(dbengine.dialect.name, 700),
            ForeignKey('{}.type_name'.format(resources_tablename)))
        name = Column(String(512), nullable=False)
        type = Column(String(128), nullable=False)
        policy_update_counter = Column(Integer, default=0)
        display_name = Column(String(256), default='')
        email = Column(String(256), default='')
        data = Column(Text(16777215))

        parent = relationship('Resource', remote_side=[type_name])
        bindings = relationship('Binding', back_populates='resource')

        def increment_update_counter(self):
            """"""Increments counter for this object's db updates.
            """"""
            self.policy_update_counter += 1

        def get_etag(self):
            """"""Return the etag for this resource.

            Returns:
                str: etag to avoid race condition when set policy
            """"""
            serialized_ctr = struct.pack('>I', self.policy_update_counter)
            msg = binascii.hexlify(serialized_ctr)
            msg += self.full_name.encode()
            seed = (model_seed if isinstance(model_seed, bytes)
                    else model_seed.encode())
            return hmac.new(seed, msg).hexdigest()

        def __repr__(self):
            """"""String representation.

            Returns:
                str: Resource represented as
                    (full_name='{}', name='{}' type='{}')
            """"""
            return '<Resource(full_name={}, name={} type={})>'.format(
                self.full_name, self.name, self.type)

    Resource.children = relationship(
        'Resource', order_by=Resource.full_name, back_populates='parent')

    class Binding(base):
        """"""Row for a binding between resource, roles and members.""""""

        __tablename__ = bindings_tablename
        id = Column(Integer, Sequence('{}_id_seq'.format(bindings_tablename)),
                    primary_key=True)
        resource_type_name = Column(
            get_string_by_dialect(dbengine.dialect.name, 700),
            ForeignKey('{}.type_name'.format(resources_tablename)))

        role_name = Column(get_string_by_dialect(dbengine.dialect.name, 256),
                           ForeignKey('{}.name'.format(roles_tablename)))

        resource = relationship('Resource', remote_side=[resource_type_name])
        role = relationship('Role', remote_side=[role_name])

        members = relationship('Member',
                               secondary=binding_members,
                               back_populates='bindings')

        def __repr__(self):
            """"""String Representation

            Returns:
                str: Binding represented as
                    (id='{}', role='{}', resource='{}' members='{}')
            """"""
            fmt_s = '<Binding(id={}, role={}, resource={} members={})>'
            return fmt_s.format(
                self.id,
                self.role_name,
                self.resource_type_name,
                self.members)

    class Member(base):
        """"""Row entry for a policy member.""""""

        __tablename__ = members_tablename
        name = Column(String(256), primary_key=True)
        type = Column(String(64))
        member_name = Column(String(256))

        parents = relationship(
            'Member',
            secondary=group_members,
            primaryjoin=name == group_members.c.members_name,
            secondaryjoin=name == group_members.c.group_name)

        children = relationship(
            'Member',
            secondary=group_members,
            primaryjoin=name == group_members.c.group_name,
            secondaryjoin=name == group_members.c.members_name)

        bindings = relationship('Binding',
                                secondary=binding_members,
                                back_populates='members')

        def __repr__(self):
            """"""String representation.

            Returns:
                str: Member represented as (name='{}', type='{}')
            """"""
            return '<Member(name={}, type={})>'.format(
                self.name, self.type)

    class GroupInGroup(base):
        """"""Row for a group-in-group membership.""""""

        __tablename__ = denormed_group_in_group
        parent = Column(String(256), primary_key=True)
        member = Column(String(256), primary_key=True)

        def __repr__(self):
            """"""String representation.

            Returns:
                str: GroupInGroup represented as (parent='{}', member='{}')
            """"""
            return '<GroupInGroup(parent={}, member={})>'.format(
                self.parent,
                self.member)

    class Role(base):
        """"""Row entry for an IAM role.""""""

        __tablename__ = roles_tablename
        name = Column(get_string_by_dialect(dbengine.dialect.name, 256),
                      primary_key=True)
        title = Column(String(128), default='')
        stage = Column(String(128), default='')
        description = Column(String(1024), default='')
        custom = Column(Boolean, default=False)
        permissions = relationship('Permission',
                                   secondary=role_permissions,
                                   back_populates='roles')

        def __repr__(self):
            """"""String Representation

            Returns:
                str: Role represented by name
            """"""
            return '<Role(name=%s)>' % self.name

    class Permission(base):
        """"""Row entry for an IAM permission.""""""

        __tablename__ = permissions_tablename
        name = Column(get_string_by_dialect(dbengine.dialect.name, 256),
                      primary_key=True)
        roles = relationship('Role',
                             secondary=role_permissions,
                             back_populates='permissions')

        def __repr__(self):
            """"""String Representation

            Returns:
                str: Permission represented by name
            """"""
            return '<Permission(name=%s)>' % self.name

    # pylint: disable=too-many-public-methods
    class ModelAccess(object):
        """"""Data model facade, implement main API against database.""""""
        TBL_GROUP_IN_GROUP = GroupInGroup
        TBL_GROUPS_SETTINGS = groups_settings
        TBL_BINDING = Binding
        TBL_MEMBER = Member
        TBL_PERMISSION = Permission
        TBL_ROLE = Role
        TBL_RESOURCE = Resource
        TBL_MEMBERSHIP = group_members

        # Set of member binding types that expand like groups.
        GROUP_TYPES = {'group',
                       'projecteditor',
                       'projectowner',
                       'projectviewer'}

        # Members that represent all users
        ALL_USER_MEMBERS = ['allusers', 'allauthenticatedusers']

        @classmethod
        def delete_all(cls, engine):
            """"""Delete all data from the model.

            Args:
                engine (object): database engine
            """"""

            LOGGER.info('Deleting all data from the model.')
            role_permissions.drop(engine)
            binding_members.drop(engine)
            group_members.drop(engine)
            groups_settings.drop(engine)

            Binding.__table__.drop(engine)
            Permission.__table__.drop(engine)
            GroupInGroup.__table__.drop(engine)

            Role.__table__.drop(engine)
            Member.__table__.drop(engine)
            Resource.__table__.drop(engine)

        @classmethod
        def denorm_group_in_group(cls, session):
            """"""Denormalize group-in-group relation.

            This method will fill the GroupInGroup table with
            (parent, member) if parent is an ancestor of member,
            whenever adding or removing a new group or group-group
            relationship, this method should be called to re-denormalize

            Args:
                session (object): Database session to use.

            Returns:
                int: Number of iterations.

            Raises:
                Exception: dernomalize fail
            """"""

            tbl1 = aliased(GroupInGroup.__table__, name='alias1')
            tbl2 = aliased(GroupInGroup.__table__, name='alias2')
            tbl3 = aliased(GroupInGroup.__table__, name='alias3')

            if get_sql_dialect(session) != 'sqlite':
                # Lock tables for denormalization
                # including aliases 1-3
                locked_tables = [
                    '`{}`'.format(GroupInGroup.__tablename__),
                    '`{}` as {}'.format(
                        GroupInGroup.__tablename__,
                        tbl1.name),
                    '`{}` as {}'.format(
                        GroupInGroup.__tablename__,
                        tbl2.name),
                    '`{}` as {}'.format(
                        GroupInGroup.__tablename__,
                        tbl3.name),
                    '`{}`'.format(group_members.name)]
                lock_stmts = ['{} WRITE'.format(tbl) for tbl in locked_tables]
                query = 'LOCK TABLES {}'.format(', '.join(lock_stmts))
                session.execute(query)
            try:
                # Remove all existing rows in the denormalization
                session.execute(GroupInGroup.__table__.delete())

                # Select member relation into GroupInGroup
                qry = (GroupInGroup.__table__.insert().from_select(
                    ['parent', 'member'], group_members.select().where(
                        group_members.c.group_name.startswith('group/')
                    ).where(
                        group_members.c.members_name.startswith('group/')
                    )
                ))

                session.execute(qry)

                iterations = 0
                rows_affected = True
                while rows_affected:
                    # Join membership on its own to find transitive
                    expansion = tbl1.join(tbl2, tbl1.c.member == tbl2.c.parent)

                    # Left outjoin to find the entries that
                    # are already in the table to prevent
                    # inserting already existing entries
                    expansion = expansion.outerjoin(
                        tbl3,
                        and_(tbl1.c.parent == tbl3.c.parent,
                             tbl2.c.member == tbl3.c.member))

                    # Select only such elements that are not
                    # already in the table, indicated as NULL
                    # values through the outer-left-join
                    stmt = (
                        select([tbl1.c.parent,
                                tbl2.c.member])
                        .select_from(expansion)
                        # pylint: disable=singleton-comparison
                        .where(tbl3.c.parent == None)
                        .distinct()
                    )

                    # Execute the query and insert into the table
                    qry = (GroupInGroup.__table__
                           .insert()
                           .from_select(['parent', 'member'], stmt))

                    rows_affected = bool(session.execute(qry).rowcount)
                    iterations += 1
            except Exception as e:
                LOGGER.exception(e)
                session.rollback()
                raise
            finally:
                if get_sql_dialect(session) != 'sqlite':
                    session.execute('UNLOCK TABLES')
                session.commit()
            return iterations

        @classmethod
        def expand_special_members(cls, session):
            """"""Create dynamic groups for project(Editor|Owner|Viewer).

            Should be called after IAM bindings are added to the model.

            Args:
                session (object): Database session to use.
            """"""
            member_type_map = {
                'projecteditor': 'roles/editor',
                'projectowner': 'roles/owner',
                'projectviewer': 'roles/viewer'}
            for parent_member in cls.list_group_members(
                    session, '', member_types=list(member_type_map.keys())):
                member_type, project_id = parent_member.split('/')
                role = member_type_map[member_type]
                try:
                    iam_policy = cls.get_iam_policy(
                        session,
                        'project/{}'.format(project_id),
                        roles=[role])
                    LOGGER.info('iam_policy: %s', iam_policy)
                except NoResultFound:
                    LOGGER.warning('Found a non-existent project, or project '
                                   'outside of the organization, in an IAM '
                                   'binding: %s', parent_member)
                    continue
                members = iam_policy.get('bindings', {}).get(role, [])
                expanded_members = cls.expand_members(session, members)
                for member in expanded_members:
                    stmt = cls.TBL_MEMBERSHIP.insert(
                        {'group_name': parent_member,
                         'members_name': member.name})
                    session.execute(stmt)
                    if member.type == 'group' and member.name in members:
                        session.add(cls.TBL_GROUP_IN_GROUP(
                            parent=parent_member,
                            member=member.name))
            session.commit()

        @classmethod
        def explain_granted(cls, session, member_name, resource_type_name,
                            role, permission):
            """"""Provide info about how the member has access to the resource.

            For example, member m1 can access resource r1 with permission p
            it might be granted by binding (r2, rol, g1),
            r1 is a child resource in a project or folder r2,
            role rol contains permission p,
            m1 is a member in group g1.
            This method list bindings that grant the access, member relation
            and resource hierarchy

            Args:
                session (object): Database session.
                member_name (str): name of the member
                resource_type_name (str): type_name of the resource
                role (str): role to query
                permission (str): permission to query

            Returns:
                tuples: (bindings, member_graph, resource_type_names) bindings,
                    the bindings to grant the access member_graph, the graph to
                    have member included in the binding esource_type_names, the
                    resource tree

            Raises:
                Exception: not granted
            """"""
            members, member_graph = cls.reverse_expand_members(
                session, [member_name], request_graph=True)
            member_names = [m.name for m in members]
            resource_type_names = [r.type_name for r in
                                   cls.find_resource_path(session,
                                                          resource_type_name)]

            if role:
                roles = set([role])
                qry = session.query(Binding, Member).join(
                    binding_members).join(Member)
            else:
                roles = [r.name for r in
                         cls.get_roles_by_permission_names(
                             session,
                             [permission])]
                qry = session.query(Binding, Member)
                qry = qry.join(binding_members).join(Member)
                qry = qry.join(Role).join(role_permissions).join(Permission)

            qry = qry.filter(Binding.role_name.in_(roles))
            qry = qry.filter(Member.name.in_(member_names))
            qry = qry.filter(
                Binding.resource_type_name.in_(resource_type_names))
            result = qry.all()
            if not result:
                error_message = 'Grant not found: ({},{},{})'.format(
                    member_name,
                    resource_type_name,
                    role if role is not None else permission)
                LOGGER.error(error_message)
                raise Exception(error_message)
            else:
                bindings = [(b.resource_type_name, b.role_name, m.name)
                            for b, m in result]
                return bindings, member_graph, resource_type_names

        @classmethod
        def scanner_iter(cls, session, resource_type,
                         parent_type_name=None, stream_results=True):
            """"""Iterate over all resources with the specified type.

            Args:
                session (object): Database session.
                resource_type (str): type of the resource to scan
                parent_type_name (str): type_name of the parent resource
                stream_results (bool): Enable streaming in the query.

            Yields:
                Resource: resource that match the query.
            """"""
            query = (
                session.query(Resource)
                .filter(Resource.type == resource_type)
                .options(joinedload(Resource.parent))
                .enable_eagerloads(True))

            if parent_type_name:
                query = query.filter(
                    Resource.parent_type_name == parent_type_name)

            if stream_results:
                results = query.yield_per(PER_YIELD)
            else:
                results = page_query(query)

            for row in results:
                yield row

        @classmethod
        def scanner_fetch_groups_settings(cls, session, only_iam_groups):
            """"""Fetch Groups Settings.

            Args:
                session (object): Database session.
                only_iam_groups (bool): boolean indicating whether we want to
                only fetch groups settings for which there is at least 1 iam
                policy.

            Yields:
                Resource: resource that match the query
            """"""
            if only_iam_groups:
                query = (session.query(groups_settings)
                         .join(Member).join(binding_members)
                         .distinct().enable_eagerloads(True))
            else:
                query = (session.query(groups_settings).enable_eagerloads(True))
            for resource in query.yield_per(PER_YIELD):
                yield resource

        @classmethod
        def explain_denied(cls, session, member_name, resource_type_names,
                           permission_names, role_names):
            """"""Explain why an access is denied

            Provide information how to grant access to a member if such
            access is denied with current IAM policies.
            For example, member m1 cannot access resource r1 with permission
            p, this method shows the bindings with rol that covered the
            desired permission on the resource r1 and its ancestors.
            If adding this member to any of these bindings, such access
            can be granted. An overgranting level is also provided

            Args:
                session (object): Database session.
                member_name (str): name of the member
                resource_type_names (list): list of type_names of resources
                permission_names (list): list of permissions
                role_names (list): list of roles

            Returns:
                list: list of tuples,
                    (overgranting,[(role_name,member_name,resource_name)])

            Raises:
                Exception: No roles covering requested permission set,
                    Not possible
            """"""

            if not role_names:
                role_names = [r.name for r in
                              cls.get_roles_by_permission_names(
                                  session,
                                  permission_names)]
                if not role_names:
                    error_message = 'No roles covering requested permission set'
                    LOGGER.error(error_message)
                    raise Exception(error_message)

            resource_hierarchy = (
                cls.resource_ancestors(session,
                                       resource_type_names))

            def find_binding_candidates(resource_hierarchy):
                """"""Find the root node in the ancestors.

                    From there, walk down the resource tree and add
                    every node until a node has more than one child.
                    This is the set of nodes which grants access to
                    at least all of the resources requested.
                    There is always a chain with a single node root.

                Args:
                    resource_hierarchy (dict): graph of the resource hierarchy

                Returns:
                    list: candidates to add to bindings that potentially grant
                        access
                """"""

                root = None
                for parent in resource_hierarchy.keys():
                    is_root = True
                    for children in resource_hierarchy.values():
                        if parent in children:
                            is_root = False
                            break
                    if is_root:
                        root = parent
                chain = [root]
                cur = root
                while len(resource_hierarchy[cur]) == 1:
                    cur = next(iter(resource_hierarchy[cur]))
                    chain.append(cur)
                return chain

            bind_res_candidates = find_binding_candidates(
                resource_hierarchy)

            bindings = (
                session.query(Binding, Member)
                .join(binding_members)
                .join(Member)
                .join(Role)
                .filter(Binding.resource_type_name.in_(
                    bind_res_candidates))
                .filter(Role.name.in_(role_names))
                .filter(or_(Member.type == 'group',
                            Member.name == member_name))
                .filter(and_((binding_members.c.bindings_id ==
                              Binding.id),
                             (binding_members.c.members_name ==
                              Member.name)))
                .filter(Role.name == Binding.role_name)
                .all())

            strategies = []
            for resource in bind_res_candidates:
                for role_name in role_names:
                    overgranting = (len(bind_res_candidates) -
                                    bind_res_candidates.index(resource) -
                                    1)
                    strategies.append(
                        (overgranting, [
                            (role, member_name, resource)
                            for role in [role_name]]))
            if bindings:
                for binding, member in bindings:
                    overgranting = (len(bind_res_candidates) - 1 -
                                    bind_res_candidates.index(
                                        binding.resource_type_name))
                    strategies.append(
                        (overgranting, [
                            (binding.role_name,
                             member.name,
                             binding.resource_type_name)]))

            return strategies

        @classmethod
        def query_access_by_member(cls, session, member_name, permission_names,
                                   expand_resources=False,
                                   reverse_expand_members=True):
            """"""Return the set of resources the member has access to.

            By default, this method expand group_member relation,
            so the result includes all resources can be accessed by the
            groups that the member is in.
            By default, this method does not expand resource hierarchy,
            so the result does not include a resource if such resource does
            not have a direct binding to allow access.

            Args:
                session (object): Database session.
                member_name (str): name of the member
                permission_names (list): list of names of permissions to query
                expand_resources (bool): whether to expand resources
                reverse_expand_members (bool): whether to expand members

            Returns:
                list: list of access tuples, (""role_name"", ""resource_type_name"")
            """"""

            if reverse_expand_members:
                member_names = [m.name for m in
                                cls.reverse_expand_members(session,
                                                           [member_name],
                                                           False)]
            else:
                member_names = [member_name]

            roles = cls.get_roles_by_permission_names(
                session, permission_names)

            qry = (
                session.query(Binding)
                .join(binding_members)
                .join(Member)
                .filter(Binding.role_name.in_([r.name for r in roles]))
                .filter(Member.name.in_(member_names))
            )

            bindings = qry.yield_per(1024)
            if not expand_resources:
                return [(binding.role_name,
                         [binding.resource_type_name]) for binding in bindings]

            r_type_names = [binding.resource_type_name for binding in bindings]
            expansion = cls.expand_resources_by_type_names(
                session,
                r_type_names)

            res_exp = {k.type_name: [v.type_name for v in values]
                       for k, values in expansion.items()}

            return [(binding.role_name,
                     res_exp[binding.resource_type_name])
                    for binding in bindings]

        @classmethod
        def query_access_by_permission(cls,
                                       session,
                                       role_name=None,
                                       permission_name=None,
                                       expand_groups=False,
                                       expand_resources=False):
            """"""Query access via the specified permission

            Return all the (Principal, Resource) combinations allowing
            satisfying access via the specified permission.
            By default, the group relation and resource hierarchy will not be
            expanded, so the results will only contains direct bindings
            filtered by permission. But the relations can be expanded

            Args:
                session (object): Database session.
                role_name (str): Role name to query for
                permission_name (str): Permission name to query for.
                expand_groups (bool): Whether or not to expand groups.
                expand_resources (bool): Whether or not to expand resources.

            Yields:
                obejct: A generator of access tuples.

            Raises:
                ValueError: If neither role nor permission is set.
            """"""

            if role_name:
                role_names = [role_name]
            elif permission_name:
                role_names = [p.name for p in
                              cls.get_roles_by_permission_names(
                                  session,
                                  [permission_name])]
            else:
                error_message = 'Either role or permission must be set'
                LOGGER.error(error_message)
                raise ValueError(error_message)

            if expand_resources:
                expanded_resources = aliased(Resource)
                qry = (
                    session.query(expanded_resources, Binding, Member)
                    .filter(binding_members.c.bindings_id == Binding.id)
                    .filter(binding_members.c.members_name == Member.name)
                    .filter(expanded_resources.full_name.startswith(
                        Resource.full_name))
                    .filter((Resource.type_name ==
                             Binding.resource_type_name))
                    .filter(Binding.role_name.in_(role_names))
                    .order_by(expanded_resources.name.asc(),
                              Binding.role_name.asc())
                )
            else:
                qry = (
                    session.query(Resource, Binding, Member)
                    .filter(binding_members.c.bindings_id == Binding.id)
                    .filter(binding_members.c.members_name == Member.name)
                    .filter((Resource.type_name ==
                             Binding.resource_type_name))
                    .filter(Binding.role_name.in_(role_names))
                    .order_by(Resource.name.asc(), Binding.role_name.asc())
                )

            if expand_groups:
                to_expand = set([m.name for _, _, m in
                                 qry.yield_per(PER_YIELD)])
                expansion = cls.expand_members_map(session, to_expand,
                                                   show_group_members=False,
                                                   member_contain_self=True)

            qry = qry.distinct()

            cur_resource = None
            cur_role = None
            cur_members = set()
            for resource, binding, member in qry.yield_per(PER_YIELD):
                if cur_resource != resource.type_name:
                    if cur_resource is not None:
                        yield cur_role, cur_resource, cur_members
                    cur_resource = resource.type_name
                    cur_role = binding.role_name
                    cur_members = set()
                if expand_groups:
                    for member_name in expansion[member.name]:
                        cur_members.add(member_name)
                else:
                    cur_members.add(member.name)
            if cur_resource is not None:
                yield cur_role, cur_resource, cur_members

        @classmethod
        def query_access_by_resource(cls, session, resource_type_name,
                                     permission_names, expand_groups=False):
            """"""Query access by resource

            Return members who have access to the given resource.
            The resource hierarchy will always be expanded, so even if the
            current resource does not have that binding, if its ancestors
            have the binding, the access will be shown
            By default, the group relationship will not be expanded

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource to query
                permission_names (list): list of strs, names of the permissions
                    to query
                expand_groups (bool): whether to expand groups

            Returns:
                dict: role_member_mapping, <""role_name"", ""member_names"">
            """"""

            roles = cls.get_roles_by_permission_names(
                session, permission_names)
            resources = cls.find_resource_path(session, resource_type_name)

            res = (session.query(Binding, Member)
                   .filter(
                       Binding.role_name.in_([r.name for r in roles]),
                       Binding.resource_type_name.in_(
                           [r.type_name for r in resources]))
                   .join(binding_members).join(Member))

            role_member_mapping = collections.defaultdict(set)
            for binding, member in res:
                role_member_mapping[binding.role_name].add(member.name)

            if expand_groups:
                for role in role_member_mapping:
                    role_member_mapping[role] = (
                        [m.name for m in cls.expand_members(
                            session,
                            role_member_mapping[role])])

            return role_member_mapping

        @classmethod
        def query_permissions_by_roles(cls, session, role_names, role_prefixes,
                                       _=1024):
            """"""Resolve permissions for the role.

            Args:
                session (object): db session
                role_names (list): list of strs, names of the roles
                role_prefixes (list): list of strs, prefixes of the roles
                _ (int): place occupation

            Returns:
                list: list of (Role, Permission)

            Raises:
                Exception: No roles or role prefixes specified
            """"""

            if not role_names and not role_prefixes:
                error_message = 'No roles or role prefixes specified'
                LOGGER.error(error_message)
                raise Exception(error_message)
            qry = session.query(Role, Permission).join(
                role_permissions).join(Permission)
            if role_names:
                qry = qry.filter(Role.name.in_(role_names))
            if role_prefixes:
                qry = qry.filter(
                    or_(*[Role.name.startswith(prefix)
                          for prefix in role_prefixes]))
            return qry.all()

        @classmethod
        def set_iam_policy(cls,
                           session,
                           resource_type_name,
                           policy,
                           update_members=False):
            """"""Set IAM policy

            Sets an IAM policy for the resource, check the etag when setting
            new policy and reassign new etag.
            Check etag to avoid race condition

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource
                policy (dict): the policy to set on the resource
                update_members (bool): If true, then add new members to Member
                    table. This must be set when the call to set_iam_policy
                    happens outside of the model InventoryImporter class. Tests
                    or users that manually add an IAM policy need to mark this
                    as true to ensure the model remains consistent.

            Raises:
                Exception: Etag doesn't match
            """"""

            LOGGER.info('Setting IAM policy, resource_type_name = %s, policy'
                        ' = %s, session = %s',
                        resource_type_name, policy, session)
            old_policy = cls.get_iam_policy(session, resource_type_name)
            if policy['etag'] != old_policy['etag']:
                error_message = 'Etags distinct, stored={}, provided={}'.format(
                    old_policy['etag'], policy['etag'])
                LOGGER.error(error_message)
                raise Exception(error_message)

            old_policy = old_policy['bindings']
            policy = policy['bindings']

            def filter_etag(policy):
                """"""Filter etag key/value out of policy map.

                Args:
                    policy (dict): the policy to filter

                Returns:
                    dict: policy without etag, <""bindings"":[<role, members>]>

                Raises:
                """"""

                return {k: v for k, v in policy.items() if k != 'etag'}

            def calculate_diff(policy, old_policy):
                """"""Calculate the grant/revoke difference between policies.
                   The diff = policy['bindings'] - old_policy['bindings']

                Args:
                    policy (dict): the new policy in dict format
                    old_policy (dict): the old policy in dict format

                Returns:
                    dict: <role, members> diff of bindings
                """"""

                diff = collections.defaultdict(list)
                for role, members in filter_etag(policy).items():
                    if role in old_policy:
                        for member in members:
                            if member not in old_policy[role]:
                                diff[role].append(member)
                    else:
                        diff[role] = members
                return diff

            grants = calculate_diff(policy, old_policy)
            revocations = calculate_diff(old_policy, policy)

            for role, members in revocations.items():
                bindings = (
                    session.query(Binding)
                    .filter((Binding.resource_type_name ==
                             resource_type_name))
                    .filter(Binding.role_name == role)
                    .join(binding_members).join(Member)
                    .filter(Member.name.in_(members)).all())

                for binding in bindings:
                    session.delete(binding)

            for role, members in grants.items():
                inserted = False
                existing_bindings = (
                    session.query(Binding)
                    .filter((Binding.resource_type_name ==
                             resource_type_name))
                    .filter(Binding.role_name == role)
                    .all())

                if update_members:
                    for member in members:
                        if not cls.get_member(session, member):
                            try:
                                # This is the default case, e.g. 'group/foobar'
                                m_type, name = member.split('/', 1)
                            except ValueError:
                                # Special groups like 'allUsers'
                                m_type, name = member, member
                            session.add(cls.TBL_MEMBER(
                                name=member,
                                type=m_type,
                                member_name=name))

                for binding in existing_bindings:
                    if binding.role_name == role:
                        inserted = True
                        for member in members:
                            binding.members.append(
                                session.query(Member).filter(
                                    Member.name == member).one())
                if not inserted:
                    binding = Binding(
                        resource_type_name=resource_type_name,
                        role=session.query(Role).filter(
                            Role.name == role).one())
                    binding.members = session.query(Member).filter(
                        Member.name.in_(members)).all()
                    session.add(binding)
            resource = session.query(Resource).filter(
                Resource.type_name == resource_type_name).one()
            resource.increment_update_counter()
            session.commit()

        @classmethod
        def get_iam_policy(cls, session, resource_type_name, roles=None):
            """"""Return the IAM policy for a resource.

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource to query
                roles (list): An optional list of roles to limit the results to

            Returns:
                dict: the IAM policy
            """"""

            resource = session.query(Resource).filter(
                Resource.type_name == resource_type_name).one()
            policy = {'etag': resource.get_etag(),
                      'bindings': {},
                      'resource': resource.type_name}
            bindings = session.query(Binding).filter(
                Binding.resource_type_name == resource_type_name)
            if roles:
                bindings = bindings.filter(Binding.role_name.in_(roles))
            for binding in bindings.all():
                role = binding.role_name
                members = [m.name for m in binding.members]
                policy['bindings'][role] = members
            return policy

        @classmethod
        def check_iam_policy(cls, session, resource_type_name, permission_name,
                             member_name):
            """"""Check access according to the resource IAM policy.

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource to check
                permission_name (str): name of the permission to check
                member_name (str): name of the member to check

            Returns:
                bool: whether such access is allowed

            Raises:
                Exception: member or resource not found
            """"""

            member_names = [m.name for m in
                            cls.reverse_expand_members(
                                session,
                                [member_name])]
            resource_type_names = [r.type_name for r in cls.find_resource_path(
                session,
                resource_type_name)]

            if not member_names:
                error_message = 'Member not found: {}'.format(member_name)
                LOGGER.error(error_message)
                raise Exception(error_message)
            if not resource_type_names:
                error_message = 'Resource not found: {}'.format(
                    resource_type_name)
                LOGGER.error(error_message)
                raise Exception(error_message)

            return (session.query(Permission)
                    .filter(Permission.name == permission_name)
                    .join(role_permissions).join(Role).join(Binding)
                    .filter(Binding.resource_type_name.in_(resource_type_names))
                    .join(binding_members).join(Member)
                    .filter(Member.name.in_(member_names)).first() is not None)

        @classmethod
        def list_roles_by_prefix(cls, session, role_prefix):
            """"""Provides a list of roles matched via name prefix.

            Args:
                session (object): db session
                role_prefix (str): prefix of the role_name

            Returns:
                list: list of role_names that match the query
            """"""

            return [r.name for r in session.query(Role).filter(
                Role.name.startswith(role_prefix)).all()]

        @classmethod
        def add_role_by_name(cls, session, role_name, permission_names):
            """"""Creates a new role.

            Args:
                session (object): db session
                role_name (str): name of the role to add
                permission_names (list): list of permissions in the role
            """"""

            LOGGER.info('Creating a new role, role_name = %s, permission_names'
                        ' = %s, session = %s',
                        role_name, permission_names, session)
            permission_names = set(permission_names)
            existing_permissions = session.query(Permission).filter(
                Permission.name.in_(permission_names)).all()
            for existing_permission in existing_permissions:
                try:
                    permission_names.remove(existing_permission.name)
                except KeyError:
                    LOGGER.warning('existing_permissions.name = %s, KeyError',
                                   existing_permission.name)

            new_permissions = [Permission(name=n) for n in permission_names]
            for perm in new_permissions:
                session.add(perm)
            cls.add_role(session, role_name,
                         existing_permissions + new_permissions)
            session.commit()

        @classmethod
        def add_group_member(cls,
                             session,
                             member_type_name,
                             parent_type_names,
                             denorm=False):
            """"""Add member, optionally with parent relationship.

            Args:
                session (object): db session
                member_type_name (str): type_name of the member to add
                parent_type_names (list): type_names of the parents
                denorm (bool): whether to denorm the groupingroup table after
                    addition
            """"""

            LOGGER.info('Adding a member, member_type_name = %s,'
                        ' parent_type_names = %s, denorm = %s, session = %s',
                        member_type_name, parent_type_names, denorm, session)

            cls.add_member(session,
                           member_type_name,
                           parent_type_names,
                           denorm)
            session.commit()

        @classmethod
        def list_group_members(cls,
                               session,
                               member_name_prefix,
                               member_types=None):
            """"""Returns members filtered by prefix.

            Args:
                session (object): db session
                member_name_prefix (str): the prefix of the member_name
                member_types (list): an optional list of member types to filter
                    the results by.

            Returns:
                list: list of Members that match the query
            """"""

            qry = session.query(Member).filter(
                Member.member_name.startswith(member_name_prefix))
            if member_types:
                qry = qry.filter(Member.type.in_(member_types))
            return [m.name for m in qry.all()]

        @classmethod
        def iter_groups(cls, session):
            """"""Returns iterator of all groups in model.

            Args:
                session (object): db session

            Yields:
                Member: group in the model
            """"""

            qry = session.query(Member).filter(Member.type == 'group')
            for group in qry.yield_per(1024):
                yield group

        @classmethod
        def iter_resources_by_prefix(cls,
                                     session,
                                     full_resource_name_prefix=None,
                                     type_name_prefix=None,
                                     type_prefix=None,
                                     name_prefix=None):
            """"""Returns iterator to resources filtered by prefix.

            Args:
                session (object): db session
                full_resource_name_prefix (str): the prefix of the
                    full_resource_name
                type_name_prefix (str): the prefix of the type_name
                type_prefix (str): the prefix of the type
                name_prefix (ste): the prefix of the name

            Yields:
                Resource: that match the query

            Raises:
                Exception: No prefix given
            """"""

            if not any([arg is not None for arg in [full_resource_name_prefix,
                                                    type_name_prefix,
                                                    type_prefix,
                                                    name_prefix]]):
                error_message = 'At least one prefix must be set'
                LOGGER.error(error_message)
                raise Exception(error_message)

            qry = session.query(Resource)
            if full_resource_name_prefix:
                qry = qry.filter(Resource.full_name.startswith(
                    full_resource_name_prefix))
            if type_name_prefix:
                qry = qry.filter(Resource.type_name.startswith(
                    type_name_prefix))
            if type_prefix:
                qry = qry.filter(Resource.type.startswith(
                    type_prefix))
            if name_prefix:
                qry = qry.filter(Resource.name.startswith(
                    name_prefix))

            for resource in qry.yield_per(1024):
                yield resource

        @classmethod
        def list_resources_by_prefix(cls,
                                     session,
                                     full_resource_name_prefix=None,
                                     type_name_prefix=None,
                                     type_prefix=None,
                                     name_prefix=None):
            """"""Returns resources filtered by prefix.

            Args:
                session (object): db session
                full_resource_name_prefix (str): the prefix of the
                    full_resource_name
                type_name_prefix (str): the prefix of the type_name
                type_prefix (str): the prefix of the type
                name_prefix (ste): the prefix of the name

            Returns:
                list: list of Resources match the query

            Raises:
            """"""

            return list(
                cls.iter_resources_by_prefix(session,
                                             full_resource_name_prefix,
                                             type_name_prefix,
                                             type_prefix,
                                             name_prefix))

        @classmethod
        def add_resource_by_name(cls,
                                 session,
                                 resource_type_name,
                                 parent_type_name,
                                 no_require_parent):
            """"""Adds resource specified via full name.

            Args:
                session (object): db session
                resource_type_name (str): name of the resource
                parent_type_name (str): name of the parent resource
                no_require_parent (bool): if this resource has a parent

            Returns:
                Resource: Created new resource
            """"""

            LOGGER.info('Adding resource via full name, resource_type_name'
                        ' = %s, parent_type_name = %s, no_require_parent = %s,'
                        ' session = %s', resource_type_name,
                        parent_type_name, no_require_parent, session)
            if not no_require_parent:
                parent = session.query(Resource).filter(
                    Resource.type_name == parent_type_name).one()
            else:
                parent = None
            return cls.add_resource(session, resource_type_name, parent)

        @classmethod
        def add_resource(cls, session, resource_type_name, parent=None):
            """"""Adds resource by name.

            Args:
                session (object): db session
                resource_type_name (str): name of the resource
                parent (Resource): parent of the resource

            Returns:
                Resource: Created new resource
            """"""

            LOGGER.info('Adding resource by name, resource_type_name = %s,'
                        ' session = %s', resource_type_name, session)
            res_type, res_name = resource_type_name.split('/')
            parent_full_resource_name = (
                '' if parent is None else parent.full_name)

            full_resource_name = to_full_resource_name(
                parent_full_resource_name,
                resource_type_name)

            resource = Resource(full_name=full_resource_name,
                                type_name=resource_type_name,
                                name=res_name,
                                type=res_type,
                                parent=parent)
            session.add(resource)
            return resource

        @classmethod
        def add_role(cls, session, name, permissions=None):
            """"""Add role by name.

            Args:
                session (object): db session
                name (str): name of the role to add
                permissions (list): permissions to add in the role

            Returns:
                Role: The created role
            """"""

            LOGGER.info('Adding role, name = %s, permissions = %s,'
                        ' session = %s', name, permissions, session)
            permissions = [] if permissions is None else permissions
            role = Role(name=name, permissions=permissions)
            session.add(role)
            return role

        @classmethod
        def add_permission(cls, session, name, roles=None):
            """"""Add permission by name.

            Args:
                session (object): db session
                name (str): name of the permission
                roles (list): list od roles to add the permission

            Returns:
                Permission: The created permission
            """"""

            LOGGER.info('Adding permission, name = %s, roles = %s'
                        ' session = %s', name, roles, session)
            roles = [] if roles is None else roles
            permission = Permission(name=name, roles=roles)
            session.add(permission)
            return permission

        @classmethod
        def add_binding(cls, session, resource, role, members):
            """"""Add a binding to the model.

            Args:
                session (object): db session
                resource (str): Resource to be added in the binding
                role (str): Role to be added in the binding
                members (list): members to be added in the binding

            Returns:
                Binding: the created binding
            """"""

            LOGGER.info('Adding a binding to the model, resource = %s,'
                        ' role = %s, members = %s, session = %s',
                        resource, role, members, session)
            binding = Binding(resource=resource, role=role, members=members)
            session.add(binding)
            return binding

        @classmethod
        def add_member(cls,
                       session,
                       type_name,
                       parent_type_names=None,
                       denorm=False):
            """"""Add a member to the model.

            Args:
                session (object): db session
                type_name (str): type_name of the resource to add
                parent_type_names (list): list of parent names to add
                denorm (bool): whether to denormalize the GroupInGroup relation

            Returns:
                Member: the created member

            Raises:
                Exception: parent not found
            """"""

            LOGGER.info('Adding a member to the model, type_name = %s,'
                        ' parent_type_names = %s, denorm = %s, session = %s',
                        type_name, parent_type_names, denorm, session)
            if not parent_type_names:
                parent_type_names = []
            res_type, name = type_name.split('/', 1)
            parents = session.query(Member).filter(
                Member.name.in_(parent_type_names)).all()
            if len(parents) != len(parent_type_names):
                msg = 'Parents: {}, expected: {}'.format(
                    parents, parent_type_names)
                error_message = 'Parent not found, {}'.format(msg)
                LOGGER.error(error_message)
                raise Exception(error_message)

            member = Member(name=type_name,
                            member_name=name,
                            type=res_type,
                            parents=parents)
            session.add(member)
            session.commit()
            if denorm and res_type == 'group' and parents:
                cls.denorm_group_in_group(session)
            return member

        @classmethod
        def expand_resources_by_type_names(cls, session, res_type_names):
            """"""Expand resources by type/name format.

            Args:
                session (object): db session
                res_type_names (list): list of resources in type_names

            Returns:
                dict: mapping in the form:
                      {res_type_name: Expansion(res_type_name), ... }
            """"""

            res_key = aliased(Resource, name='res_key')
            res_values = aliased(Resource, name='res_values')

            expressions = []
            for res_type_name in res_type_names:
                expressions.append(and_(
                    res_key.type_name == res_type_name))

            res = (
                session.query(res_key, res_values)
                .filter(res_key.type_name.in_(res_type_names))
                .filter(res_values.full_name.startswith(
                    res_key.full_name))
                .yield_per(1024)
            )

            mapping = collections.defaultdict(set)
            for k, value in res:
                mapping[k].add(value)
            return mapping

        @classmethod
        def reverse_expand_members(cls, session, member_names,
                                   request_graph=False):
            """"""Expand members to their groups.

            List all groups that contains these members. Also return
            the graph if requested.

            Args:
                session (object): db session
                member_names (list): list of members to expand
                request_graph (bool): wether the parent-child graph is provided

            Returns:
                object: set if graph not requested, set and graph if requested
            """"""
            member_names.extend(cls.ALL_USER_MEMBERS)
            members = session.query(Member).filter(
                Member.name.in_(member_names)).all()
            membership_graph = collections.defaultdict(set)
            member_set = set()
            new_member_set = set()

            def add_to_sets(members, child):
                """"""Adds the members & children to the sets.

                Args:
                    members (list): list of Members to be added
                    child (Member): child to be added
                """"""

                for member in members:
                    if request_graph and child:
                        membership_graph[child.name].add(member.name)
                    if request_graph and not child:
                        if member.name not in membership_graph:
                            membership_graph[member.name] = set()
                    if member not in member_set:
                        new_member_set.add(member)
                        member_set.add(member)

            add_to_sets(members, None)
            while new_member_set:
                members_to_walk = new_member_set
                new_member_set = set()
                for member in members_to_walk:
                    add_to_sets(member.parents, member)

            if request_graph:
                return member_set, membership_graph
            return member_set

        @classmethod
        def expand_members_map(cls,
                               session,
                               member_names,
                               show_group_members=True,
                               member_contain_self=True):
            """"""Expand group membership keyed by member.

            Args:
                session (object): db session
                member_names (set): Member names to expand
                show_group_members (bool): Whether to include subgroups
                member_contain_self (bool): Whether to include a parent
                    as its own member
            Returns:
                dict: <Member, set(Children)>
            """"""

            def separate_groups(member_names):
                """"""Separate groups and other members in two lists.

                This is a helper function. groups are needed to query on
                group_in_group table

                Args:
                    member_names (list): list of members to be separated

                Returns:
                    tuples: two lists of strs containing groups and others
                """"""
                groups = []
                others = []
                for name in member_names:
                    member_type = name.split('/')[0]
                    if member_type in cls.GROUP_TYPES:
                        groups.append(name)
                    else:
                        others.append(name)
                return groups, others

            selectables = []
            group_names, other_names = separate_groups(member_names)

            t_ging = GroupInGroup.__table__
            t_members = group_members

            # This resolves groups to its transitive non-group members.
            transitive_membership = (
                select([t_ging.c.parent, t_members.c.members_name])
                .select_from(t_ging.join(t_members,
                                         (t_ging.c.member ==
                                          t_members.c.group_name)))
            ).where(t_ging.c.parent.in_(group_names))

            if not show_group_members:
                transitive_membership = transitive_membership.where(
                    not_(t_members.c.members_name.startswith('group/')))

            selectables.append(
                transitive_membership.alias('transitive_membership'))

            direct_membership = (
                select([t_members.c.group_name,
                        t_members.c.members_name])
                .where(t_members.c.group_name.in_(group_names))
            )

            if not show_group_members:
                direct_membership = direct_membership.where(
                    not_(t_members.c.members_name.startswith('group/')))

            selectables.append(
                direct_membership.alias('direct_membership'))

            if show_group_members:
                # Show groups as members of other groups
                group_in_groups = (
                    select([t_ging.c.parent,
                            t_ging.c.member]).where(
                                t_ging.c.parent.in_(group_names))
                )
                selectables.append(
                    group_in_groups.alias('group_in_groups'))

            # Union all the queries
            qry = union(*selectables)

            # Build the result dict
            result = collections.defaultdict(set)
            for parent, child in session.execute(qry):
                result[parent].add(child)
            for parent in other_names:
                result[parent] = set()

            # Add each parent as its own member
            if member_contain_self:
                for name in member_names:
                    result[name].add(name)
            return result

        @classmethod
        def expand_members(cls, session, member_names):
            """"""Expand group membership towards the members.

            Args:
                session (object): db session
                member_names (list): list of strs of member names

            Returns:
                set: expanded group members
            """"""

            members = session.query(Member).filter(
                Member.name.in_(member_names)).all()

            def is_group(member):
                """"""Returns true iff the member is a group.

                Args:
                    member (Member): member to check

                Returns:
                    bool: whether the member is a group
                """"""
                return member.type in cls.GROUP_TYPES

            group_set = set()
            non_group_set = set()
            new_group_set = set()

            def add_to_sets(members):
                """"""Adds new members to the sets.

                Args:
                    members (list): members to be added
                """"""
                for member in members:
                    if is_group(member):
                        if member not in group_set:
                            new_group_set.add(member)
                        group_set.add(member)
                    else:
                        non_group_set.add(member)

            add_to_sets(members)

            while new_group_set:
                groups_to_walk = new_group_set
                new_group_set = set()
                for group in groups_to_walk:
                    add_to_sets(group.children)

            return group_set.union(non_group_set)

        @classmethod
        def resource_ancestors(cls, session, resource_type_names):
            """"""Resolve the transitive ancestors by type/name format.

            Given a group of resource and find out all their parents.
            Then this method group the pairs with parent. Used to determine
            resource candidates to grant access in explain denied.

            Args:
                session (object): db session
                resource_type_names (list): list of strs, resources to query

            Returns:
                dict: <parent, childs> graph of the resource hierarchy
            """"""

            resource_names = resource_type_names
            resource_graph = collections.defaultdict(set)

            res_childs = aliased(Resource, name='res_childs')
            res_anc = aliased(Resource, name='resource_parent')

            resources_set = set(resource_names)
            resources_new = set(resource_names)

            for resource in resources_new:
                resource_graph[resource] = set()

            while resources_new:
                resources_new = set()
                for parent, child in (
                        session.query(res_anc, res_childs)
                        .filter(res_childs.type_name.in_(resources_set))
                        .filter(res_childs.parent_type_name ==
                                res_anc.type_name)
                        .all()):

                    if parent.type_name not in resources_set:
                        resources_new.add(parent.type_name)

                    resources_set.add(parent.type_name)
                    resources_set.add(child.type_name)

                    resource_graph[parent.type_name].add(child.type_name)

            return resource_graph

        @classmethod
        def find_resource_path(cls, session, resource_type_name):
            """"""Find resource ancestors by type/name format.

            Find all ancestors of a resource and return them in order

            Args:
                session (object): db session
                resource_type_name (str): resource to query

            Returns:
                list: list of Resources, transitive ancestors for the given
                    resource
            """"""

            qry = (
                session.query(Resource).filter(
                    Resource.type_name == resource_type_name)
            )

            resources = qry.all()
            return cls._find_resource_path(session, resources)

        @classmethod
        def _find_resource_path(cls, _, resources):
            """"""Find the list of transitive ancestors for the given resource.

            Args:
                _ (object): position holder
                resources (list): list of the resources to query

            Returns:
                list: list of Resources, transitive ancestors for the given
                    resource
            """"""

            if not resources:
                return []

            path = []
            resource = resources[0]

            path.append(resource)
            while resource.parent:
                resource = resource.parent
                path.append(resource)

            return path

        @classmethod
        def get_roles_by_permission_names(cls, session, permission_names):
            """"""Return the list of roles covering the specified permissions.

            Args:
                session (object): db session
                permission_names (list): permissions to be covered by.

            Returns:
                set: roles set that cover the permissions
            """"""

            permission_set = set(permission_names)
            qry = session.query(Permission)
            if permission_set:
                qry = qry.filter(Permission.name.in_(permission_set))
            permissions = qry.all()

            roles = set()
            for permission in permissions:
                for role in permission.roles:
                    roles.add(role)

            result_set = set()
            for role in roles:
                role_permissions = set(
                    [p.name for p in role.permissions])
                if permission_set.issubset(role_permissions):
                    result_set.add(role)

            return result_set

        @classmethod
        def get_member(cls, session, name):
            """"""Get member by name.

            Args:
                session (object): db session
                name (str): the name the member to query

            Returns:
                list: Members from the query
            """"""

            return session.query(Member).filter(Member.name == name).all()

    base.metadata.create_all(dbengine)
    return sessionmaker(bind=dbengine), ModelAccess",_10354.py,633,"for children in resource_hierarchy.values():
    if parent in children:
        is_root = False
        break","if is_root:
    root = parent","for children in resource_hierarchy.values():
    if parent in children:
        break
else:
    root = parent"
https://github.com/forseti-security/forseti-security/tree/master/google/cloud/forseti/services/dao.py,"def define_model(model_name, dbengine, model_seed):
    """"""Defines table classes which point to the corresponding model.

        This means, for each model being accessed this function needs to
        be called in order to generate a full set of table definitions.

        Models are name spaced via a random model seed such that multiple
        models can exist within the same database. In order to implement
        the name spacing in an abstract way.

    Args:
        model_name (str): model handle
        dbengine (object): db engine
        model_seed (str): seed to get etag

    Returns:
        tuple: (sessionmaker, ModelAccess)
    """"""

    base = declarative_base()

    denormed_group_in_group = '{}_group_in_group'.format(model_name)
    bindings_tablename = '{}_bindings'.format(model_name)
    roles_tablename = '{}_roles'.format(model_name)
    permissions_tablename = '{}_permissions'.format(model_name)
    members_tablename = '{}_members'.format(model_name)
    resources_tablename = '{}_resources'.format(model_name)

    role_permissions = Table('{}_role_permissions'.format(model_name),
                             base.metadata,
                             Column(
                                 'roles_name', ForeignKey(
                                     '{}.name'.format(roles_tablename)),
                                 primary_key=True),
                             Column(
                                 'permissions_name', ForeignKey(
                                     '{}.name'.format(permissions_tablename)),
                                 primary_key=True), )

    binding_members = Table('{}_binding_members'.format(model_name),
                            base.metadata,
                            Column(
                                'bindings_id', ForeignKey(
                                    '{}.id'.format(bindings_tablename)),
                                primary_key=True),
                            Column(
                                'members_name', ForeignKey(
                                    '{}.name'.format(members_tablename)),
                                primary_key=True), )

    group_members = Table(
        '{}_group_members'.format(model_name),
        base.metadata,
        Column('group_name',
               ForeignKey('{}.name'.format(members_tablename)),
               primary_key=True),
        Column('members_name',
               ForeignKey('{}.name'.format(members_tablename)),
               primary_key=True),
    )

    groups_settings = Table(
        '{}_groups_settings'.format(model_name),
        base.metadata,
        Column('group_name',
               ForeignKey('{}.name'.format(members_tablename)),
               primary_key=True),
        Column('settings',
               Text(16777215)),
    )

    def get_string_by_dialect(db_dialect, column_size):
        """"""Get Sqlalchemy String by dialect.
        Sqlite doesn't support collation type, need to define different
        column types for different database engine.

        This is used to make MySQL column case sensitive by adding
        an encoding type.
        Args:
            db_dialect (str): The db dialect.
            column_size (int): The size of the column.

        Returns:
            String: Sqlalchemy String.
        """"""
        if db_dialect.lower() == 'sqlite':
            return String(column_size)
        return String(column_size, collation='utf8mb4_bin')

    class Resource(base):
        """"""Row entry for a GCP resource.""""""
        __tablename__ = resources_tablename

        cai_resource_name = Column(String(4096))
        cai_resource_type = Column(String(512))
        full_name = Column(String(2048), nullable=False)
        type_name = Column(get_string_by_dialect(dbengine.dialect.name, 700),
                           primary_key=True)
        parent_type_name = Column(
            get_string_by_dialect(dbengine.dialect.name, 700),
            ForeignKey('{}.type_name'.format(resources_tablename)))
        name = Column(String(512), nullable=False)
        type = Column(String(128), nullable=False)
        policy_update_counter = Column(Integer, default=0)
        display_name = Column(String(256), default='')
        email = Column(String(256), default='')
        data = Column(Text(16777215))

        parent = relationship('Resource', remote_side=[type_name])
        bindings = relationship('Binding', back_populates='resource')

        def increment_update_counter(self):
            """"""Increments counter for this object's db updates.
            """"""
            self.policy_update_counter += 1

        def get_etag(self):
            """"""Return the etag for this resource.

            Returns:
                str: etag to avoid race condition when set policy
            """"""
            serialized_ctr = struct.pack('>I', self.policy_update_counter)
            msg = binascii.hexlify(serialized_ctr)
            msg += self.full_name.encode()
            seed = (model_seed if isinstance(model_seed, bytes)
                    else model_seed.encode())
            return hmac.new(seed, msg).hexdigest()

        def __repr__(self):
            """"""String representation.

            Returns:
                str: Resource represented as
                    (full_name='{}', name='{}' type='{}')
            """"""
            return '<Resource(full_name={}, name={} type={})>'.format(
                self.full_name, self.name, self.type)

    Resource.children = relationship(
        'Resource', order_by=Resource.full_name, back_populates='parent')

    class Binding(base):
        """"""Row for a binding between resource, roles and members.""""""

        __tablename__ = bindings_tablename
        id = Column(Integer, Sequence('{}_id_seq'.format(bindings_tablename)),
                    primary_key=True)
        resource_type_name = Column(
            get_string_by_dialect(dbengine.dialect.name, 700),
            ForeignKey('{}.type_name'.format(resources_tablename)))

        role_name = Column(get_string_by_dialect(dbengine.dialect.name, 256),
                           ForeignKey('{}.name'.format(roles_tablename)))

        resource = relationship('Resource', remote_side=[resource_type_name])
        role = relationship('Role', remote_side=[role_name])

        members = relationship('Member',
                               secondary=binding_members,
                               back_populates='bindings')

        def __repr__(self):
            """"""String Representation

            Returns:
                str: Binding represented as
                    (id='{}', role='{}', resource='{}' members='{}')
            """"""
            fmt_s = '<Binding(id={}, role={}, resource={} members={})>'
            return fmt_s.format(
                self.id,
                self.role_name,
                self.resource_type_name,
                self.members)

    class Member(base):
        """"""Row entry for a policy member.""""""

        __tablename__ = members_tablename
        name = Column(String(256), primary_key=True)
        type = Column(String(64))
        member_name = Column(String(256))

        parents = relationship(
            'Member',
            secondary=group_members,
            primaryjoin=name == group_members.c.members_name,
            secondaryjoin=name == group_members.c.group_name)

        children = relationship(
            'Member',
            secondary=group_members,
            primaryjoin=name == group_members.c.group_name,
            secondaryjoin=name == group_members.c.members_name)

        bindings = relationship('Binding',
                                secondary=binding_members,
                                back_populates='members')

        def __repr__(self):
            """"""String representation.

            Returns:
                str: Member represented as (name='{}', type='{}')
            """"""
            return '<Member(name={}, type={})>'.format(
                self.name, self.type)

    class GroupInGroup(base):
        """"""Row for a group-in-group membership.""""""

        __tablename__ = denormed_group_in_group
        parent = Column(String(256), primary_key=True)
        member = Column(String(256), primary_key=True)

        def __repr__(self):
            """"""String representation.

            Returns:
                str: GroupInGroup represented as (parent='{}', member='{}')
            """"""
            return '<GroupInGroup(parent={}, member={})>'.format(
                self.parent,
                self.member)

    class Role(base):
        """"""Row entry for an IAM role.""""""

        __tablename__ = roles_tablename
        name = Column(get_string_by_dialect(dbengine.dialect.name, 256),
                      primary_key=True)
        title = Column(String(128), default='')
        stage = Column(String(128), default='')
        description = Column(String(1024), default='')
        custom = Column(Boolean, default=False)
        permissions = relationship('Permission',
                                   secondary=role_permissions,
                                   back_populates='roles')

        def __repr__(self):
            """"""String Representation

            Returns:
                str: Role represented by name
            """"""
            return '<Role(name=%s)>' % self.name

    class Permission(base):
        """"""Row entry for an IAM permission.""""""

        __tablename__ = permissions_tablename
        name = Column(get_string_by_dialect(dbengine.dialect.name, 256),
                      primary_key=True)
        roles = relationship('Role',
                             secondary=role_permissions,
                             back_populates='permissions')

        def __repr__(self):
            """"""String Representation

            Returns:
                str: Permission represented by name
            """"""
            return '<Permission(name=%s)>' % self.name

    # pylint: disable=too-many-public-methods
    class ModelAccess(object):
        """"""Data model facade, implement main API against database.""""""
        TBL_GROUP_IN_GROUP = GroupInGroup
        TBL_GROUPS_SETTINGS = groups_settings
        TBL_BINDING = Binding
        TBL_MEMBER = Member
        TBL_PERMISSION = Permission
        TBL_ROLE = Role
        TBL_RESOURCE = Resource
        TBL_MEMBERSHIP = group_members

        # Set of member binding types that expand like groups.
        GROUP_TYPES = {'group',
                       'projecteditor',
                       'projectowner',
                       'projectviewer'}

        # Members that represent all users
        ALL_USER_MEMBERS = ['allusers', 'allauthenticatedusers']

        @classmethod
        def delete_all(cls, engine):
            """"""Delete all data from the model.

            Args:
                engine (object): database engine
            """"""

            LOGGER.info('Deleting all data from the model.')
            role_permissions.drop(engine)
            binding_members.drop(engine)
            group_members.drop(engine)
            groups_settings.drop(engine)

            Binding.__table__.drop(engine)
            Permission.__table__.drop(engine)
            GroupInGroup.__table__.drop(engine)

            Role.__table__.drop(engine)
            Member.__table__.drop(engine)
            Resource.__table__.drop(engine)

        @classmethod
        def denorm_group_in_group(cls, session):
            """"""Denormalize group-in-group relation.

            This method will fill the GroupInGroup table with
            (parent, member) if parent is an ancestor of member,
            whenever adding or removing a new group or group-group
            relationship, this method should be called to re-denormalize

            Args:
                session (object): Database session to use.

            Returns:
                int: Number of iterations.

            Raises:
                Exception: dernomalize fail
            """"""

            tbl1 = aliased(GroupInGroup.__table__, name='alias1')
            tbl2 = aliased(GroupInGroup.__table__, name='alias2')
            tbl3 = aliased(GroupInGroup.__table__, name='alias3')

            if get_sql_dialect(session) != 'sqlite':
                # Lock tables for denormalization
                # including aliases 1-3
                locked_tables = [
                    '`{}`'.format(GroupInGroup.__tablename__),
                    '`{}` as {}'.format(
                        GroupInGroup.__tablename__,
                        tbl1.name),
                    '`{}` as {}'.format(
                        GroupInGroup.__tablename__,
                        tbl2.name),
                    '`{}` as {}'.format(
                        GroupInGroup.__tablename__,
                        tbl3.name),
                    '`{}`'.format(group_members.name)]
                lock_stmts = ['{} WRITE'.format(tbl) for tbl in locked_tables]
                query = 'LOCK TABLES {}'.format(', '.join(lock_stmts))
                session.execute(query)
            try:
                # Remove all existing rows in the denormalization
                session.execute(GroupInGroup.__table__.delete())

                # Select member relation into GroupInGroup
                qry = (GroupInGroup.__table__.insert().from_select(
                    ['parent', 'member'], group_members.select().where(
                        group_members.c.group_name.startswith('group/')
                    ).where(
                        group_members.c.members_name.startswith('group/')
                    )
                ))

                session.execute(qry)

                iterations = 0
                rows_affected = True
                while rows_affected:
                    # Join membership on its own to find transitive
                    expansion = tbl1.join(tbl2, tbl1.c.member == tbl2.c.parent)

                    # Left outjoin to find the entries that
                    # are already in the table to prevent
                    # inserting already existing entries
                    expansion = expansion.outerjoin(
                        tbl3,
                        and_(tbl1.c.parent == tbl3.c.parent,
                             tbl2.c.member == tbl3.c.member))

                    # Select only such elements that are not
                    # already in the table, indicated as NULL
                    # values through the outer-left-join
                    stmt = (
                        select([tbl1.c.parent,
                                tbl2.c.member])
                        .select_from(expansion)
                        # pylint: disable=singleton-comparison
                        .where(tbl3.c.parent == None)
                        .distinct()
                    )

                    # Execute the query and insert into the table
                    qry = (GroupInGroup.__table__
                           .insert()
                           .from_select(['parent', 'member'], stmt))

                    rows_affected = bool(session.execute(qry).rowcount)
                    iterations += 1
            except Exception as e:
                LOGGER.exception(e)
                session.rollback()
                raise
            finally:
                if get_sql_dialect(session) != 'sqlite':
                    session.execute('UNLOCK TABLES')
                session.commit()
            return iterations

        @classmethod
        def expand_special_members(cls, session):
            """"""Create dynamic groups for project(Editor|Owner|Viewer).

            Should be called after IAM bindings are added to the model.

            Args:
                session (object): Database session to use.
            """"""
            member_type_map = {
                'projecteditor': 'roles/editor',
                'projectowner': 'roles/owner',
                'projectviewer': 'roles/viewer'}
            for parent_member in cls.list_group_members(
                    session, '', member_types=list(member_type_map.keys())):
                member_type, project_id = parent_member.split('/')
                role = member_type_map[member_type]
                try:
                    iam_policy = cls.get_iam_policy(
                        session,
                        'project/{}'.format(project_id),
                        roles=[role])
                    LOGGER.info('iam_policy: %s', iam_policy)
                except NoResultFound:
                    LOGGER.warning('Found a non-existent project, or project '
                                   'outside of the organization, in an IAM '
                                   'binding: %s', parent_member)
                    continue
                members = iam_policy.get('bindings', {}).get(role, [])
                expanded_members = cls.expand_members(session, members)
                for member in expanded_members:
                    stmt = cls.TBL_MEMBERSHIP.insert(
                        {'group_name': parent_member,
                         'members_name': member.name})
                    session.execute(stmt)
                    if member.type == 'group' and member.name in members:
                        session.add(cls.TBL_GROUP_IN_GROUP(
                            parent=parent_member,
                            member=member.name))
            session.commit()

        @classmethod
        def explain_granted(cls, session, member_name, resource_type_name,
                            role, permission):
            """"""Provide info about how the member has access to the resource.

            For example, member m1 can access resource r1 with permission p
            it might be granted by binding (r2, rol, g1),
            r1 is a child resource in a project or folder r2,
            role rol contains permission p,
            m1 is a member in group g1.
            This method list bindings that grant the access, member relation
            and resource hierarchy

            Args:
                session (object): Database session.
                member_name (str): name of the member
                resource_type_name (str): type_name of the resource
                role (str): role to query
                permission (str): permission to query

            Returns:
                tuples: (bindings, member_graph, resource_type_names) bindings,
                    the bindings to grant the access member_graph, the graph to
                    have member included in the binding esource_type_names, the
                    resource tree

            Raises:
                Exception: not granted
            """"""
            members, member_graph = cls.reverse_expand_members(
                session, [member_name], request_graph=True)
            member_names = [m.name for m in members]
            resource_type_names = [r.type_name for r in
                                   cls.find_resource_path(session,
                                                          resource_type_name)]

            if role:
                roles = set([role])
                qry = session.query(Binding, Member).join(
                    binding_members).join(Member)
            else:
                roles = [r.name for r in
                         cls.get_roles_by_permission_names(
                             session,
                             [permission])]
                qry = session.query(Binding, Member)
                qry = qry.join(binding_members).join(Member)
                qry = qry.join(Role).join(role_permissions).join(Permission)

            qry = qry.filter(Binding.role_name.in_(roles))
            qry = qry.filter(Member.name.in_(member_names))
            qry = qry.filter(
                Binding.resource_type_name.in_(resource_type_names))
            result = qry.all()
            if not result:
                error_message = 'Grant not found: ({},{},{})'.format(
                    member_name,
                    resource_type_name,
                    role if role is not None else permission)
                LOGGER.error(error_message)
                raise Exception(error_message)
            else:
                bindings = [(b.resource_type_name, b.role_name, m.name)
                            for b, m in result]
                return bindings, member_graph, resource_type_names

        @classmethod
        def scanner_iter(cls, session, resource_type,
                         parent_type_name=None, stream_results=True):
            """"""Iterate over all resources with the specified type.

            Args:
                session (object): Database session.
                resource_type (str): type of the resource to scan
                parent_type_name (str): type_name of the parent resource
                stream_results (bool): Enable streaming in the query.

            Yields:
                Resource: resource that match the query.
            """"""
            query = (
                session.query(Resource)
                .filter(Resource.type == resource_type)
                .options(joinedload(Resource.parent))
                .enable_eagerloads(True))

            if parent_type_name:
                query = query.filter(
                    Resource.parent_type_name == parent_type_name)

            if stream_results:
                results = query.yield_per(PER_YIELD)
            else:
                results = page_query(query)

            for row in results:
                yield row

        @classmethod
        def scanner_fetch_groups_settings(cls, session, only_iam_groups):
            """"""Fetch Groups Settings.

            Args:
                session (object): Database session.
                only_iam_groups (bool): boolean indicating whether we want to
                only fetch groups settings for which there is at least 1 iam
                policy.

            Yields:
                Resource: resource that match the query
            """"""
            if only_iam_groups:
                query = (session.query(groups_settings)
                         .join(Member).join(binding_members)
                         .distinct().enable_eagerloads(True))
            else:
                query = (session.query(groups_settings).enable_eagerloads(True))
            for resource in query.yield_per(PER_YIELD):
                yield resource

        @classmethod
        def explain_denied(cls, session, member_name, resource_type_names,
                           permission_names, role_names):
            """"""Explain why an access is denied

            Provide information how to grant access to a member if such
            access is denied with current IAM policies.
            For example, member m1 cannot access resource r1 with permission
            p, this method shows the bindings with rol that covered the
            desired permission on the resource r1 and its ancestors.
            If adding this member to any of these bindings, such access
            can be granted. An overgranting level is also provided

            Args:
                session (object): Database session.
                member_name (str): name of the member
                resource_type_names (list): list of type_names of resources
                permission_names (list): list of permissions
                role_names (list): list of roles

            Returns:
                list: list of tuples,
                    (overgranting,[(role_name,member_name,resource_name)])

            Raises:
                Exception: No roles covering requested permission set,
                    Not possible
            """"""

            if not role_names:
                role_names = [r.name for r in
                              cls.get_roles_by_permission_names(
                                  session,
                                  permission_names)]
                if not role_names:
                    error_message = 'No roles covering requested permission set'
                    LOGGER.error(error_message)
                    raise Exception(error_message)

            resource_hierarchy = (
                cls.resource_ancestors(session,
                                       resource_type_names))

            def find_binding_candidates(resource_hierarchy):
                """"""Find the root node in the ancestors.

                    From there, walk down the resource tree and add
                    every node until a node has more than one child.
                    This is the set of nodes which grants access to
                    at least all of the resources requested.
                    There is always a chain with a single node root.

                Args:
                    resource_hierarchy (dict): graph of the resource hierarchy

                Returns:
                    list: candidates to add to bindings that potentially grant
                        access
                """"""

                root = None
                for parent in resource_hierarchy.keys():
                    is_root = True
                    for children in resource_hierarchy.values():
                        if parent in children:
                            is_root = False
                            break
                    if is_root:
                        root = parent
                chain = [root]
                cur = root
                while len(resource_hierarchy[cur]) == 1:
                    cur = next(iter(resource_hierarchy[cur]))
                    chain.append(cur)
                return chain

            bind_res_candidates = find_binding_candidates(
                resource_hierarchy)

            bindings = (
                session.query(Binding, Member)
                .join(binding_members)
                .join(Member)
                .join(Role)
                .filter(Binding.resource_type_name.in_(
                    bind_res_candidates))
                .filter(Role.name.in_(role_names))
                .filter(or_(Member.type == 'group',
                            Member.name == member_name))
                .filter(and_((binding_members.c.bindings_id ==
                              Binding.id),
                             (binding_members.c.members_name ==
                              Member.name)))
                .filter(Role.name == Binding.role_name)
                .all())

            strategies = []
            for resource in bind_res_candidates:
                for role_name in role_names:
                    overgranting = (len(bind_res_candidates) -
                                    bind_res_candidates.index(resource) -
                                    1)
                    strategies.append(
                        (overgranting, [
                            (role, member_name, resource)
                            for role in [role_name]]))
            if bindings:
                for binding, member in bindings:
                    overgranting = (len(bind_res_candidates) - 1 -
                                    bind_res_candidates.index(
                                        binding.resource_type_name))
                    strategies.append(
                        (overgranting, [
                            (binding.role_name,
                             member.name,
                             binding.resource_type_name)]))

            return strategies

        @classmethod
        def query_access_by_member(cls, session, member_name, permission_names,
                                   expand_resources=False,
                                   reverse_expand_members=True):
            """"""Return the set of resources the member has access to.

            By default, this method expand group_member relation,
            so the result includes all resources can be accessed by the
            groups that the member is in.
            By default, this method does not expand resource hierarchy,
            so the result does not include a resource if such resource does
            not have a direct binding to allow access.

            Args:
                session (object): Database session.
                member_name (str): name of the member
                permission_names (list): list of names of permissions to query
                expand_resources (bool): whether to expand resources
                reverse_expand_members (bool): whether to expand members

            Returns:
                list: list of access tuples, (""role_name"", ""resource_type_name"")
            """"""

            if reverse_expand_members:
                member_names = [m.name for m in
                                cls.reverse_expand_members(session,
                                                           [member_name],
                                                           False)]
            else:
                member_names = [member_name]

            roles = cls.get_roles_by_permission_names(
                session, permission_names)

            qry = (
                session.query(Binding)
                .join(binding_members)
                .join(Member)
                .filter(Binding.role_name.in_([r.name for r in roles]))
                .filter(Member.name.in_(member_names))
            )

            bindings = qry.yield_per(1024)
            if not expand_resources:
                return [(binding.role_name,
                         [binding.resource_type_name]) for binding in bindings]

            r_type_names = [binding.resource_type_name for binding in bindings]
            expansion = cls.expand_resources_by_type_names(
                session,
                r_type_names)

            res_exp = {k.type_name: [v.type_name for v in values]
                       for k, values in expansion.items()}

            return [(binding.role_name,
                     res_exp[binding.resource_type_name])
                    for binding in bindings]

        @classmethod
        def query_access_by_permission(cls,
                                       session,
                                       role_name=None,
                                       permission_name=None,
                                       expand_groups=False,
                                       expand_resources=False):
            """"""Query access via the specified permission

            Return all the (Principal, Resource) combinations allowing
            satisfying access via the specified permission.
            By default, the group relation and resource hierarchy will not be
            expanded, so the results will only contains direct bindings
            filtered by permission. But the relations can be expanded

            Args:
                session (object): Database session.
                role_name (str): Role name to query for
                permission_name (str): Permission name to query for.
                expand_groups (bool): Whether or not to expand groups.
                expand_resources (bool): Whether or not to expand resources.

            Yields:
                obejct: A generator of access tuples.

            Raises:
                ValueError: If neither role nor permission is set.
            """"""

            if role_name:
                role_names = [role_name]
            elif permission_name:
                role_names = [p.name for p in
                              cls.get_roles_by_permission_names(
                                  session,
                                  [permission_name])]
            else:
                error_message = 'Either role or permission must be set'
                LOGGER.error(error_message)
                raise ValueError(error_message)

            if expand_resources:
                expanded_resources = aliased(Resource)
                qry = (
                    session.query(expanded_resources, Binding, Member)
                    .filter(binding_members.c.bindings_id == Binding.id)
                    .filter(binding_members.c.members_name == Member.name)
                    .filter(expanded_resources.full_name.startswith(
                        Resource.full_name))
                    .filter((Resource.type_name ==
                             Binding.resource_type_name))
                    .filter(Binding.role_name.in_(role_names))
                    .order_by(expanded_resources.name.asc(),
                              Binding.role_name.asc())
                )
            else:
                qry = (
                    session.query(Resource, Binding, Member)
                    .filter(binding_members.c.bindings_id == Binding.id)
                    .filter(binding_members.c.members_name == Member.name)
                    .filter((Resource.type_name ==
                             Binding.resource_type_name))
                    .filter(Binding.role_name.in_(role_names))
                    .order_by(Resource.name.asc(), Binding.role_name.asc())
                )

            if expand_groups:
                to_expand = set([m.name for _, _, m in
                                 qry.yield_per(PER_YIELD)])
                expansion = cls.expand_members_map(session, to_expand,
                                                   show_group_members=False,
                                                   member_contain_self=True)

            qry = qry.distinct()

            cur_resource = None
            cur_role = None
            cur_members = set()
            for resource, binding, member in qry.yield_per(PER_YIELD):
                if cur_resource != resource.type_name:
                    if cur_resource is not None:
                        yield cur_role, cur_resource, cur_members
                    cur_resource = resource.type_name
                    cur_role = binding.role_name
                    cur_members = set()
                if expand_groups:
                    for member_name in expansion[member.name]:
                        cur_members.add(member_name)
                else:
                    cur_members.add(member.name)
            if cur_resource is not None:
                yield cur_role, cur_resource, cur_members

        @classmethod
        def query_access_by_resource(cls, session, resource_type_name,
                                     permission_names, expand_groups=False):
            """"""Query access by resource

            Return members who have access to the given resource.
            The resource hierarchy will always be expanded, so even if the
            current resource does not have that binding, if its ancestors
            have the binding, the access will be shown
            By default, the group relationship will not be expanded

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource to query
                permission_names (list): list of strs, names of the permissions
                    to query
                expand_groups (bool): whether to expand groups

            Returns:
                dict: role_member_mapping, <""role_name"", ""member_names"">
            """"""

            roles = cls.get_roles_by_permission_names(
                session, permission_names)
            resources = cls.find_resource_path(session, resource_type_name)

            res = (session.query(Binding, Member)
                   .filter(
                       Binding.role_name.in_([r.name for r in roles]),
                       Binding.resource_type_name.in_(
                           [r.type_name for r in resources]))
                   .join(binding_members).join(Member))

            role_member_mapping = collections.defaultdict(set)
            for binding, member in res:
                role_member_mapping[binding.role_name].add(member.name)

            if expand_groups:
                for role in role_member_mapping:
                    role_member_mapping[role] = (
                        [m.name for m in cls.expand_members(
                            session,
                            role_member_mapping[role])])

            return role_member_mapping

        @classmethod
        def query_permissions_by_roles(cls, session, role_names, role_prefixes,
                                       _=1024):
            """"""Resolve permissions for the role.

            Args:
                session (object): db session
                role_names (list): list of strs, names of the roles
                role_prefixes (list): list of strs, prefixes of the roles
                _ (int): place occupation

            Returns:
                list: list of (Role, Permission)

            Raises:
                Exception: No roles or role prefixes specified
            """"""

            if not role_names and not role_prefixes:
                error_message = 'No roles or role prefixes specified'
                LOGGER.error(error_message)
                raise Exception(error_message)
            qry = session.query(Role, Permission).join(
                role_permissions).join(Permission)
            if role_names:
                qry = qry.filter(Role.name.in_(role_names))
            if role_prefixes:
                qry = qry.filter(
                    or_(*[Role.name.startswith(prefix)
                          for prefix in role_prefixes]))
            return qry.all()

        @classmethod
        def set_iam_policy(cls,
                           session,
                           resource_type_name,
                           policy,
                           update_members=False):
            """"""Set IAM policy

            Sets an IAM policy for the resource, check the etag when setting
            new policy and reassign new etag.
            Check etag to avoid race condition

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource
                policy (dict): the policy to set on the resource
                update_members (bool): If true, then add new members to Member
                    table. This must be set when the call to set_iam_policy
                    happens outside of the model InventoryImporter class. Tests
                    or users that manually add an IAM policy need to mark this
                    as true to ensure the model remains consistent.

            Raises:
                Exception: Etag doesn't match
            """"""

            LOGGER.info('Setting IAM policy, resource_type_name = %s, policy'
                        ' = %s, session = %s',
                        resource_type_name, policy, session)
            old_policy = cls.get_iam_policy(session, resource_type_name)
            if policy['etag'] != old_policy['etag']:
                error_message = 'Etags distinct, stored={}, provided={}'.format(
                    old_policy['etag'], policy['etag'])
                LOGGER.error(error_message)
                raise Exception(error_message)

            old_policy = old_policy['bindings']
            policy = policy['bindings']

            def filter_etag(policy):
                """"""Filter etag key/value out of policy map.

                Args:
                    policy (dict): the policy to filter

                Returns:
                    dict: policy without etag, <""bindings"":[<role, members>]>

                Raises:
                """"""

                return {k: v for k, v in policy.items() if k != 'etag'}

            def calculate_diff(policy, old_policy):
                """"""Calculate the grant/revoke difference between policies.
                   The diff = policy['bindings'] - old_policy['bindings']

                Args:
                    policy (dict): the new policy in dict format
                    old_policy (dict): the old policy in dict format

                Returns:
                    dict: <role, members> diff of bindings
                """"""

                diff = collections.defaultdict(list)
                for role, members in filter_etag(policy).items():
                    if role in old_policy:
                        for member in members:
                            if member not in old_policy[role]:
                                diff[role].append(member)
                    else:
                        diff[role] = members
                return diff

            grants = calculate_diff(policy, old_policy)
            revocations = calculate_diff(old_policy, policy)

            for role, members in revocations.items():
                bindings = (
                    session.query(Binding)
                    .filter((Binding.resource_type_name ==
                             resource_type_name))
                    .filter(Binding.role_name == role)
                    .join(binding_members).join(Member)
                    .filter(Member.name.in_(members)).all())

                for binding in bindings:
                    session.delete(binding)

            for role, members in grants.items():
                inserted = False
                existing_bindings = (
                    session.query(Binding)
                    .filter((Binding.resource_type_name ==
                             resource_type_name))
                    .filter(Binding.role_name == role)
                    .all())

                if update_members:
                    for member in members:
                        if not cls.get_member(session, member):
                            try:
                                # This is the default case, e.g. 'group/foobar'
                                m_type, name = member.split('/', 1)
                            except ValueError:
                                # Special groups like 'allUsers'
                                m_type, name = member, member
                            session.add(cls.TBL_MEMBER(
                                name=member,
                                type=m_type,
                                member_name=name))

                for binding in existing_bindings:
                    if binding.role_name == role:
                        inserted = True
                        for member in members:
                            binding.members.append(
                                session.query(Member).filter(
                                    Member.name == member).one())
                if not inserted:
                    binding = Binding(
                        resource_type_name=resource_type_name,
                        role=session.query(Role).filter(
                            Role.name == role).one())
                    binding.members = session.query(Member).filter(
                        Member.name.in_(members)).all()
                    session.add(binding)
            resource = session.query(Resource).filter(
                Resource.type_name == resource_type_name).one()
            resource.increment_update_counter()
            session.commit()

        @classmethod
        def get_iam_policy(cls, session, resource_type_name, roles=None):
            """"""Return the IAM policy for a resource.

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource to query
                roles (list): An optional list of roles to limit the results to

            Returns:
                dict: the IAM policy
            """"""

            resource = session.query(Resource).filter(
                Resource.type_name == resource_type_name).one()
            policy = {'etag': resource.get_etag(),
                      'bindings': {},
                      'resource': resource.type_name}
            bindings = session.query(Binding).filter(
                Binding.resource_type_name == resource_type_name)
            if roles:
                bindings = bindings.filter(Binding.role_name.in_(roles))
            for binding in bindings.all():
                role = binding.role_name
                members = [m.name for m in binding.members]
                policy['bindings'][role] = members
            return policy

        @classmethod
        def check_iam_policy(cls, session, resource_type_name, permission_name,
                             member_name):
            """"""Check access according to the resource IAM policy.

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource to check
                permission_name (str): name of the permission to check
                member_name (str): name of the member to check

            Returns:
                bool: whether such access is allowed

            Raises:
                Exception: member or resource not found
            """"""

            member_names = [m.name for m in
                            cls.reverse_expand_members(
                                session,
                                [member_name])]
            resource_type_names = [r.type_name for r in cls.find_resource_path(
                session,
                resource_type_name)]

            if not member_names:
                error_message = 'Member not found: {}'.format(member_name)
                LOGGER.error(error_message)
                raise Exception(error_message)
            if not resource_type_names:
                error_message = 'Resource not found: {}'.format(
                    resource_type_name)
                LOGGER.error(error_message)
                raise Exception(error_message)

            return (session.query(Permission)
                    .filter(Permission.name == permission_name)
                    .join(role_permissions).join(Role).join(Binding)
                    .filter(Binding.resource_type_name.in_(resource_type_names))
                    .join(binding_members).join(Member)
                    .filter(Member.name.in_(member_names)).first() is not None)

        @classmethod
        def list_roles_by_prefix(cls, session, role_prefix):
            """"""Provides a list of roles matched via name prefix.

            Args:
                session (object): db session
                role_prefix (str): prefix of the role_name

            Returns:
                list: list of role_names that match the query
            """"""

            return [r.name for r in session.query(Role).filter(
                Role.name.startswith(role_prefix)).all()]

        @classmethod
        def add_role_by_name(cls, session, role_name, permission_names):
            """"""Creates a new role.

            Args:
                session (object): db session
                role_name (str): name of the role to add
                permission_names (list): list of permissions in the role
            """"""

            LOGGER.info('Creating a new role, role_name = %s, permission_names'
                        ' = %s, session = %s',
                        role_name, permission_names, session)
            permission_names = set(permission_names)
            existing_permissions = session.query(Permission).filter(
                Permission.name.in_(permission_names)).all()
            for existing_permission in existing_permissions:
                try:
                    permission_names.remove(existing_permission.name)
                except KeyError:
                    LOGGER.warning('existing_permissions.name = %s, KeyError',
                                   existing_permission.name)

            new_permissions = [Permission(name=n) for n in permission_names]
            for perm in new_permissions:
                session.add(perm)
            cls.add_role(session, role_name,
                         existing_permissions + new_permissions)
            session.commit()

        @classmethod
        def add_group_member(cls,
                             session,
                             member_type_name,
                             parent_type_names,
                             denorm=False):
            """"""Add member, optionally with parent relationship.

            Args:
                session (object): db session
                member_type_name (str): type_name of the member to add
                parent_type_names (list): type_names of the parents
                denorm (bool): whether to denorm the groupingroup table after
                    addition
            """"""

            LOGGER.info('Adding a member, member_type_name = %s,'
                        ' parent_type_names = %s, denorm = %s, session = %s',
                        member_type_name, parent_type_names, denorm, session)

            cls.add_member(session,
                           member_type_name,
                           parent_type_names,
                           denorm)
            session.commit()

        @classmethod
        def list_group_members(cls,
                               session,
                               member_name_prefix,
                               member_types=None):
            """"""Returns members filtered by prefix.

            Args:
                session (object): db session
                member_name_prefix (str): the prefix of the member_name
                member_types (list): an optional list of member types to filter
                    the results by.

            Returns:
                list: list of Members that match the query
            """"""

            qry = session.query(Member).filter(
                Member.member_name.startswith(member_name_prefix))
            if member_types:
                qry = qry.filter(Member.type.in_(member_types))
            return [m.name for m in qry.all()]

        @classmethod
        def iter_groups(cls, session):
            """"""Returns iterator of all groups in model.

            Args:
                session (object): db session

            Yields:
                Member: group in the model
            """"""

            qry = session.query(Member).filter(Member.type == 'group')
            for group in qry.yield_per(1024):
                yield group

        @classmethod
        def iter_resources_by_prefix(cls,
                                     session,
                                     full_resource_name_prefix=None,
                                     type_name_prefix=None,
                                     type_prefix=None,
                                     name_prefix=None):
            """"""Returns iterator to resources filtered by prefix.

            Args:
                session (object): db session
                full_resource_name_prefix (str): the prefix of the
                    full_resource_name
                type_name_prefix (str): the prefix of the type_name
                type_prefix (str): the prefix of the type
                name_prefix (ste): the prefix of the name

            Yields:
                Resource: that match the query

            Raises:
                Exception: No prefix given
            """"""

            if not any([arg is not None for arg in [full_resource_name_prefix,
                                                    type_name_prefix,
                                                    type_prefix,
                                                    name_prefix]]):
                error_message = 'At least one prefix must be set'
                LOGGER.error(error_message)
                raise Exception(error_message)

            qry = session.query(Resource)
            if full_resource_name_prefix:
                qry = qry.filter(Resource.full_name.startswith(
                    full_resource_name_prefix))
            if type_name_prefix:
                qry = qry.filter(Resource.type_name.startswith(
                    type_name_prefix))
            if type_prefix:
                qry = qry.filter(Resource.type.startswith(
                    type_prefix))
            if name_prefix:
                qry = qry.filter(Resource.name.startswith(
                    name_prefix))

            for resource in qry.yield_per(1024):
                yield resource

        @classmethod
        def list_resources_by_prefix(cls,
                                     session,
                                     full_resource_name_prefix=None,
                                     type_name_prefix=None,
                                     type_prefix=None,
                                     name_prefix=None):
            """"""Returns resources filtered by prefix.

            Args:
                session (object): db session
                full_resource_name_prefix (str): the prefix of the
                    full_resource_name
                type_name_prefix (str): the prefix of the type_name
                type_prefix (str): the prefix of the type
                name_prefix (ste): the prefix of the name

            Returns:
                list: list of Resources match the query

            Raises:
            """"""

            return list(
                cls.iter_resources_by_prefix(session,
                                             full_resource_name_prefix,
                                             type_name_prefix,
                                             type_prefix,
                                             name_prefix))

        @classmethod
        def add_resource_by_name(cls,
                                 session,
                                 resource_type_name,
                                 parent_type_name,
                                 no_require_parent):
            """"""Adds resource specified via full name.

            Args:
                session (object): db session
                resource_type_name (str): name of the resource
                parent_type_name (str): name of the parent resource
                no_require_parent (bool): if this resource has a parent

            Returns:
                Resource: Created new resource
            """"""

            LOGGER.info('Adding resource via full name, resource_type_name'
                        ' = %s, parent_type_name = %s, no_require_parent = %s,'
                        ' session = %s', resource_type_name,
                        parent_type_name, no_require_parent, session)
            if not no_require_parent:
                parent = session.query(Resource).filter(
                    Resource.type_name == parent_type_name).one()
            else:
                parent = None
            return cls.add_resource(session, resource_type_name, parent)

        @classmethod
        def add_resource(cls, session, resource_type_name, parent=None):
            """"""Adds resource by name.

            Args:
                session (object): db session
                resource_type_name (str): name of the resource
                parent (Resource): parent of the resource

            Returns:
                Resource: Created new resource
            """"""

            LOGGER.info('Adding resource by name, resource_type_name = %s,'
                        ' session = %s', resource_type_name, session)
            res_type, res_name = resource_type_name.split('/')
            parent_full_resource_name = (
                '' if parent is None else parent.full_name)

            full_resource_name = to_full_resource_name(
                parent_full_resource_name,
                resource_type_name)

            resource = Resource(full_name=full_resource_name,
                                type_name=resource_type_name,
                                name=res_name,
                                type=res_type,
                                parent=parent)
            session.add(resource)
            return resource

        @classmethod
        def add_role(cls, session, name, permissions=None):
            """"""Add role by name.

            Args:
                session (object): db session
                name (str): name of the role to add
                permissions (list): permissions to add in the role

            Returns:
                Role: The created role
            """"""

            LOGGER.info('Adding role, name = %s, permissions = %s,'
                        ' session = %s', name, permissions, session)
            permissions = [] if permissions is None else permissions
            role = Role(name=name, permissions=permissions)
            session.add(role)
            return role

        @classmethod
        def add_permission(cls, session, name, roles=None):
            """"""Add permission by name.

            Args:
                session (object): db session
                name (str): name of the permission
                roles (list): list od roles to add the permission

            Returns:
                Permission: The created permission
            """"""

            LOGGER.info('Adding permission, name = %s, roles = %s'
                        ' session = %s', name, roles, session)
            roles = [] if roles is None else roles
            permission = Permission(name=name, roles=roles)
            session.add(permission)
            return permission

        @classmethod
        def add_binding(cls, session, resource, role, members):
            """"""Add a binding to the model.

            Args:
                session (object): db session
                resource (str): Resource to be added in the binding
                role (str): Role to be added in the binding
                members (list): members to be added in the binding

            Returns:
                Binding: the created binding
            """"""

            LOGGER.info('Adding a binding to the model, resource = %s,'
                        ' role = %s, members = %s, session = %s',
                        resource, role, members, session)
            binding = Binding(resource=resource, role=role, members=members)
            session.add(binding)
            return binding

        @classmethod
        def add_member(cls,
                       session,
                       type_name,
                       parent_type_names=None,
                       denorm=False):
            """"""Add a member to the model.

            Args:
                session (object): db session
                type_name (str): type_name of the resource to add
                parent_type_names (list): list of parent names to add
                denorm (bool): whether to denormalize the GroupInGroup relation

            Returns:
                Member: the created member

            Raises:
                Exception: parent not found
            """"""

            LOGGER.info('Adding a member to the model, type_name = %s,'
                        ' parent_type_names = %s, denorm = %s, session = %s',
                        type_name, parent_type_names, denorm, session)
            if not parent_type_names:
                parent_type_names = []
            res_type, name = type_name.split('/', 1)
            parents = session.query(Member).filter(
                Member.name.in_(parent_type_names)).all()
            if len(parents) != len(parent_type_names):
                msg = 'Parents: {}, expected: {}'.format(
                    parents, parent_type_names)
                error_message = 'Parent not found, {}'.format(msg)
                LOGGER.error(error_message)
                raise Exception(error_message)

            member = Member(name=type_name,
                            member_name=name,
                            type=res_type,
                            parents=parents)
            session.add(member)
            session.commit()
            if denorm and res_type == 'group' and parents:
                cls.denorm_group_in_group(session)
            return member

        @classmethod
        def expand_resources_by_type_names(cls, session, res_type_names):
            """"""Expand resources by type/name format.

            Args:
                session (object): db session
                res_type_names (list): list of resources in type_names

            Returns:
                dict: mapping in the form:
                      {res_type_name: Expansion(res_type_name), ... }
            """"""

            res_key = aliased(Resource, name='res_key')
            res_values = aliased(Resource, name='res_values')

            expressions = []
            for res_type_name in res_type_names:
                expressions.append(and_(
                    res_key.type_name == res_type_name))

            res = (
                session.query(res_key, res_values)
                .filter(res_key.type_name.in_(res_type_names))
                .filter(res_values.full_name.startswith(
                    res_key.full_name))
                .yield_per(1024)
            )

            mapping = collections.defaultdict(set)
            for k, value in res:
                mapping[k].add(value)
            return mapping

        @classmethod
        def reverse_expand_members(cls, session, member_names,
                                   request_graph=False):
            """"""Expand members to their groups.

            List all groups that contains these members. Also return
            the graph if requested.

            Args:
                session (object): db session
                member_names (list): list of members to expand
                request_graph (bool): wether the parent-child graph is provided

            Returns:
                object: set if graph not requested, set and graph if requested
            """"""
            member_names.extend(cls.ALL_USER_MEMBERS)
            members = session.query(Member).filter(
                Member.name.in_(member_names)).all()
            membership_graph = collections.defaultdict(set)
            member_set = set()
            new_member_set = set()

            def add_to_sets(members, child):
                """"""Adds the members & children to the sets.

                Args:
                    members (list): list of Members to be added
                    child (Member): child to be added
                """"""

                for member in members:
                    if request_graph and child:
                        membership_graph[child.name].add(member.name)
                    if request_graph and not child:
                        if member.name not in membership_graph:
                            membership_graph[member.name] = set()
                    if member not in member_set:
                        new_member_set.add(member)
                        member_set.add(member)

            add_to_sets(members, None)
            while new_member_set:
                members_to_walk = new_member_set
                new_member_set = set()
                for member in members_to_walk:
                    add_to_sets(member.parents, member)

            if request_graph:
                return member_set, membership_graph
            return member_set

        @classmethod
        def expand_members_map(cls,
                               session,
                               member_names,
                               show_group_members=True,
                               member_contain_self=True):
            """"""Expand group membership keyed by member.

            Args:
                session (object): db session
                member_names (set): Member names to expand
                show_group_members (bool): Whether to include subgroups
                member_contain_self (bool): Whether to include a parent
                    as its own member
            Returns:
                dict: <Member, set(Children)>
            """"""

            def separate_groups(member_names):
                """"""Separate groups and other members in two lists.

                This is a helper function. groups are needed to query on
                group_in_group table

                Args:
                    member_names (list): list of members to be separated

                Returns:
                    tuples: two lists of strs containing groups and others
                """"""
                groups = []
                others = []
                for name in member_names:
                    member_type = name.split('/')[0]
                    if member_type in cls.GROUP_TYPES:
                        groups.append(name)
                    else:
                        others.append(name)
                return groups, others

            selectables = []
            group_names, other_names = separate_groups(member_names)

            t_ging = GroupInGroup.__table__
            t_members = group_members

            # This resolves groups to its transitive non-group members.
            transitive_membership = (
                select([t_ging.c.parent, t_members.c.members_name])
                .select_from(t_ging.join(t_members,
                                         (t_ging.c.member ==
                                          t_members.c.group_name)))
            ).where(t_ging.c.parent.in_(group_names))

            if not show_group_members:
                transitive_membership = transitive_membership.where(
                    not_(t_members.c.members_name.startswith('group/')))

            selectables.append(
                transitive_membership.alias('transitive_membership'))

            direct_membership = (
                select([t_members.c.group_name,
                        t_members.c.members_name])
                .where(t_members.c.group_name.in_(group_names))
            )

            if not show_group_members:
                direct_membership = direct_membership.where(
                    not_(t_members.c.members_name.startswith('group/')))

            selectables.append(
                direct_membership.alias('direct_membership'))

            if show_group_members:
                # Show groups as members of other groups
                group_in_groups = (
                    select([t_ging.c.parent,
                            t_ging.c.member]).where(
                                t_ging.c.parent.in_(group_names))
                )
                selectables.append(
                    group_in_groups.alias('group_in_groups'))

            # Union all the queries
            qry = union(*selectables)

            # Build the result dict
            result = collections.defaultdict(set)
            for parent, child in session.execute(qry):
                result[parent].add(child)
            for parent in other_names:
                result[parent] = set()

            # Add each parent as its own member
            if member_contain_self:
                for name in member_names:
                    result[name].add(name)
            return result

        @classmethod
        def expand_members(cls, session, member_names):
            """"""Expand group membership towards the members.

            Args:
                session (object): db session
                member_names (list): list of strs of member names

            Returns:
                set: expanded group members
            """"""

            members = session.query(Member).filter(
                Member.name.in_(member_names)).all()

            def is_group(member):
                """"""Returns true iff the member is a group.

                Args:
                    member (Member): member to check

                Returns:
                    bool: whether the member is a group
                """"""
                return member.type in cls.GROUP_TYPES

            group_set = set()
            non_group_set = set()
            new_group_set = set()

            def add_to_sets(members):
                """"""Adds new members to the sets.

                Args:
                    members (list): members to be added
                """"""
                for member in members:
                    if is_group(member):
                        if member not in group_set:
                            new_group_set.add(member)
                        group_set.add(member)
                    else:
                        non_group_set.add(member)

            add_to_sets(members)

            while new_group_set:
                groups_to_walk = new_group_set
                new_group_set = set()
                for group in groups_to_walk:
                    add_to_sets(group.children)

            return group_set.union(non_group_set)

        @classmethod
        def resource_ancestors(cls, session, resource_type_names):
            """"""Resolve the transitive ancestors by type/name format.

            Given a group of resource and find out all their parents.
            Then this method group the pairs with parent. Used to determine
            resource candidates to grant access in explain denied.

            Args:
                session (object): db session
                resource_type_names (list): list of strs, resources to query

            Returns:
                dict: <parent, childs> graph of the resource hierarchy
            """"""

            resource_names = resource_type_names
            resource_graph = collections.defaultdict(set)

            res_childs = aliased(Resource, name='res_childs')
            res_anc = aliased(Resource, name='resource_parent')

            resources_set = set(resource_names)
            resources_new = set(resource_names)

            for resource in resources_new:
                resource_graph[resource] = set()

            while resources_new:
                resources_new = set()
                for parent, child in (
                        session.query(res_anc, res_childs)
                        .filter(res_childs.type_name.in_(resources_set))
                        .filter(res_childs.parent_type_name ==
                                res_anc.type_name)
                        .all()):

                    if parent.type_name not in resources_set:
                        resources_new.add(parent.type_name)

                    resources_set.add(parent.type_name)
                    resources_set.add(child.type_name)

                    resource_graph[parent.type_name].add(child.type_name)

            return resource_graph

        @classmethod
        def find_resource_path(cls, session, resource_type_name):
            """"""Find resource ancestors by type/name format.

            Find all ancestors of a resource and return them in order

            Args:
                session (object): db session
                resource_type_name (str): resource to query

            Returns:
                list: list of Resources, transitive ancestors for the given
                    resource
            """"""

            qry = (
                session.query(Resource).filter(
                    Resource.type_name == resource_type_name)
            )

            resources = qry.all()
            return cls._find_resource_path(session, resources)

        @classmethod
        def _find_resource_path(cls, _, resources):
            """"""Find the list of transitive ancestors for the given resource.

            Args:
                _ (object): position holder
                resources (list): list of the resources to query

            Returns:
                list: list of Resources, transitive ancestors for the given
                    resource
            """"""

            if not resources:
                return []

            path = []
            resource = resources[0]

            path.append(resource)
            while resource.parent:
                resource = resource.parent
                path.append(resource)

            return path

        @classmethod
        def get_roles_by_permission_names(cls, session, permission_names):
            """"""Return the list of roles covering the specified permissions.

            Args:
                session (object): db session
                permission_names (list): permissions to be covered by.

            Returns:
                set: roles set that cover the permissions
            """"""

            permission_set = set(permission_names)
            qry = session.query(Permission)
            if permission_set:
                qry = qry.filter(Permission.name.in_(permission_set))
            permissions = qry.all()

            roles = set()
            for permission in permissions:
                for role in permission.roles:
                    roles.add(role)

            result_set = set()
            for role in roles:
                role_permissions = set(
                    [p.name for p in role.permissions])
                if permission_set.issubset(role_permissions):
                    result_set.add(role)

            return result_set

        @classmethod
        def get_member(cls, session, name):
            """"""Get member by name.

            Args:
                session (object): db session
                name (str): the name the member to query

            Returns:
                list: Members from the query
            """"""

            return session.query(Member).filter(Member.name == name).all()

    base.metadata.create_all(dbengine)
    return sessionmaker(bind=dbengine), ModelAccess",_10354.py,633,"for children in resource_hierarchy.values():
    if parent in children:
        is_root = False
        break","if is_root:
    root = parent","for children in resource_hierarchy.values():
    if parent in children:
        break
else:
    root = parent"
https://github.com/forseti-security/forseti-security/tree/master/google/cloud/forseti/services/dao.py,"def define_model(model_name, dbengine, model_seed):
    """"""Defines table classes which point to the corresponding model.

        This means, for each model being accessed this function needs to
        be called in order to generate a full set of table definitions.

        Models are name spaced via a random model seed such that multiple
        models can exist within the same database. In order to implement
        the name spacing in an abstract way.

    Args:
        model_name (str): model handle
        dbengine (object): db engine
        model_seed (str): seed to get etag

    Returns:
        tuple: (sessionmaker, ModelAccess)
    """"""

    base = declarative_base()

    denormed_group_in_group = '{}_group_in_group'.format(model_name)
    bindings_tablename = '{}_bindings'.format(model_name)
    roles_tablename = '{}_roles'.format(model_name)
    permissions_tablename = '{}_permissions'.format(model_name)
    members_tablename = '{}_members'.format(model_name)
    resources_tablename = '{}_resources'.format(model_name)

    role_permissions = Table('{}_role_permissions'.format(model_name),
                             base.metadata,
                             Column(
                                 'roles_name', ForeignKey(
                                     '{}.name'.format(roles_tablename)),
                                 primary_key=True),
                             Column(
                                 'permissions_name', ForeignKey(
                                     '{}.name'.format(permissions_tablename)),
                                 primary_key=True), )

    binding_members = Table('{}_binding_members'.format(model_name),
                            base.metadata,
                            Column(
                                'bindings_id', ForeignKey(
                                    '{}.id'.format(bindings_tablename)),
                                primary_key=True),
                            Column(
                                'members_name', ForeignKey(
                                    '{}.name'.format(members_tablename)),
                                primary_key=True), )

    group_members = Table(
        '{}_group_members'.format(model_name),
        base.metadata,
        Column('group_name',
               ForeignKey('{}.name'.format(members_tablename)),
               primary_key=True),
        Column('members_name',
               ForeignKey('{}.name'.format(members_tablename)),
               primary_key=True),
    )

    groups_settings = Table(
        '{}_groups_settings'.format(model_name),
        base.metadata,
        Column('group_name',
               ForeignKey('{}.name'.format(members_tablename)),
               primary_key=True),
        Column('settings',
               Text(16777215)),
    )

    def get_string_by_dialect(db_dialect, column_size):
        """"""Get Sqlalchemy String by dialect.
        Sqlite doesn't support collation type, need to define different
        column types for different database engine.

        This is used to make MySQL column case sensitive by adding
        an encoding type.
        Args:
            db_dialect (str): The db dialect.
            column_size (int): The size of the column.

        Returns:
            String: Sqlalchemy String.
        """"""
        if db_dialect.lower() == 'sqlite':
            return String(column_size)
        return String(column_size, collation='utf8mb4_bin')

    class Resource(base):
        """"""Row entry for a GCP resource.""""""
        __tablename__ = resources_tablename

        cai_resource_name = Column(String(4096))
        cai_resource_type = Column(String(512))
        full_name = Column(String(2048), nullable=False)
        type_name = Column(get_string_by_dialect(dbengine.dialect.name, 700),
                           primary_key=True)
        parent_type_name = Column(
            get_string_by_dialect(dbengine.dialect.name, 700),
            ForeignKey('{}.type_name'.format(resources_tablename)))
        name = Column(String(512), nullable=False)
        type = Column(String(128), nullable=False)
        policy_update_counter = Column(Integer, default=0)
        display_name = Column(String(256), default='')
        email = Column(String(256), default='')
        data = Column(Text(16777215))

        parent = relationship('Resource', remote_side=[type_name])
        bindings = relationship('Binding', back_populates='resource')

        def increment_update_counter(self):
            """"""Increments counter for this object's db updates.
            """"""
            self.policy_update_counter += 1

        def get_etag(self):
            """"""Return the etag for this resource.

            Returns:
                str: etag to avoid race condition when set policy
            """"""
            serialized_ctr = struct.pack('>I', self.policy_update_counter)
            msg = binascii.hexlify(serialized_ctr)
            msg += self.full_name.encode()
            seed = (model_seed if isinstance(model_seed, bytes)
                    else model_seed.encode())
            return hmac.new(seed, msg).hexdigest()

        def __repr__(self):
            """"""String representation.

            Returns:
                str: Resource represented as
                    (full_name='{}', name='{}' type='{}')
            """"""
            return '<Resource(full_name={}, name={} type={})>'.format(
                self.full_name, self.name, self.type)

    Resource.children = relationship(
        'Resource', order_by=Resource.full_name, back_populates='parent')

    class Binding(base):
        """"""Row for a binding between resource, roles and members.""""""

        __tablename__ = bindings_tablename
        id = Column(Integer, Sequence('{}_id_seq'.format(bindings_tablename)),
                    primary_key=True)
        resource_type_name = Column(
            get_string_by_dialect(dbengine.dialect.name, 700),
            ForeignKey('{}.type_name'.format(resources_tablename)))

        role_name = Column(get_string_by_dialect(dbengine.dialect.name, 256),
                           ForeignKey('{}.name'.format(roles_tablename)))

        resource = relationship('Resource', remote_side=[resource_type_name])
        role = relationship('Role', remote_side=[role_name])

        members = relationship('Member',
                               secondary=binding_members,
                               back_populates='bindings')

        def __repr__(self):
            """"""String Representation

            Returns:
                str: Binding represented as
                    (id='{}', role='{}', resource='{}' members='{}')
            """"""
            fmt_s = '<Binding(id={}, role={}, resource={} members={})>'
            return fmt_s.format(
                self.id,
                self.role_name,
                self.resource_type_name,
                self.members)

    class Member(base):
        """"""Row entry for a policy member.""""""

        __tablename__ = members_tablename
        name = Column(String(256), primary_key=True)
        type = Column(String(64))
        member_name = Column(String(256))

        parents = relationship(
            'Member',
            secondary=group_members,
            primaryjoin=name == group_members.c.members_name,
            secondaryjoin=name == group_members.c.group_name)

        children = relationship(
            'Member',
            secondary=group_members,
            primaryjoin=name == group_members.c.group_name,
            secondaryjoin=name == group_members.c.members_name)

        bindings = relationship('Binding',
                                secondary=binding_members,
                                back_populates='members')

        def __repr__(self):
            """"""String representation.

            Returns:
                str: Member represented as (name='{}', type='{}')
            """"""
            return '<Member(name={}, type={})>'.format(
                self.name, self.type)

    class GroupInGroup(base):
        """"""Row for a group-in-group membership.""""""

        __tablename__ = denormed_group_in_group
        parent = Column(String(256), primary_key=True)
        member = Column(String(256), primary_key=True)

        def __repr__(self):
            """"""String representation.

            Returns:
                str: GroupInGroup represented as (parent='{}', member='{}')
            """"""
            return '<GroupInGroup(parent={}, member={})>'.format(
                self.parent,
                self.member)

    class Role(base):
        """"""Row entry for an IAM role.""""""

        __tablename__ = roles_tablename
        name = Column(get_string_by_dialect(dbengine.dialect.name, 256),
                      primary_key=True)
        title = Column(String(128), default='')
        stage = Column(String(128), default='')
        description = Column(String(1024), default='')
        custom = Column(Boolean, default=False)
        permissions = relationship('Permission',
                                   secondary=role_permissions,
                                   back_populates='roles')

        def __repr__(self):
            """"""String Representation

            Returns:
                str: Role represented by name
            """"""
            return '<Role(name=%s)>' % self.name

    class Permission(base):
        """"""Row entry for an IAM permission.""""""

        __tablename__ = permissions_tablename
        name = Column(get_string_by_dialect(dbengine.dialect.name, 256),
                      primary_key=True)
        roles = relationship('Role',
                             secondary=role_permissions,
                             back_populates='permissions')

        def __repr__(self):
            """"""String Representation

            Returns:
                str: Permission represented by name
            """"""
            return '<Permission(name=%s)>' % self.name

    # pylint: disable=too-many-public-methods
    class ModelAccess(object):
        """"""Data model facade, implement main API against database.""""""
        TBL_GROUP_IN_GROUP = GroupInGroup
        TBL_GROUPS_SETTINGS = groups_settings
        TBL_BINDING = Binding
        TBL_MEMBER = Member
        TBL_PERMISSION = Permission
        TBL_ROLE = Role
        TBL_RESOURCE = Resource
        TBL_MEMBERSHIP = group_members

        # Set of member binding types that expand like groups.
        GROUP_TYPES = {'group',
                       'projecteditor',
                       'projectowner',
                       'projectviewer'}

        # Members that represent all users
        ALL_USER_MEMBERS = ['allusers', 'allauthenticatedusers']

        @classmethod
        def delete_all(cls, engine):
            """"""Delete all data from the model.

            Args:
                engine (object): database engine
            """"""

            LOGGER.info('Deleting all data from the model.')
            role_permissions.drop(engine)
            binding_members.drop(engine)
            group_members.drop(engine)
            groups_settings.drop(engine)

            Binding.__table__.drop(engine)
            Permission.__table__.drop(engine)
            GroupInGroup.__table__.drop(engine)

            Role.__table__.drop(engine)
            Member.__table__.drop(engine)
            Resource.__table__.drop(engine)

        @classmethod
        def denorm_group_in_group(cls, session):
            """"""Denormalize group-in-group relation.

            This method will fill the GroupInGroup table with
            (parent, member) if parent is an ancestor of member,
            whenever adding or removing a new group or group-group
            relationship, this method should be called to re-denormalize

            Args:
                session (object): Database session to use.

            Returns:
                int: Number of iterations.

            Raises:
                Exception: dernomalize fail
            """"""

            tbl1 = aliased(GroupInGroup.__table__, name='alias1')
            tbl2 = aliased(GroupInGroup.__table__, name='alias2')
            tbl3 = aliased(GroupInGroup.__table__, name='alias3')

            if get_sql_dialect(session) != 'sqlite':
                # Lock tables for denormalization
                # including aliases 1-3
                locked_tables = [
                    '`{}`'.format(GroupInGroup.__tablename__),
                    '`{}` as {}'.format(
                        GroupInGroup.__tablename__,
                        tbl1.name),
                    '`{}` as {}'.format(
                        GroupInGroup.__tablename__,
                        tbl2.name),
                    '`{}` as {}'.format(
                        GroupInGroup.__tablename__,
                        tbl3.name),
                    '`{}`'.format(group_members.name)]
                lock_stmts = ['{} WRITE'.format(tbl) for tbl in locked_tables]
                query = 'LOCK TABLES {}'.format(', '.join(lock_stmts))
                session.execute(query)
            try:
                # Remove all existing rows in the denormalization
                session.execute(GroupInGroup.__table__.delete())

                # Select member relation into GroupInGroup
                qry = (GroupInGroup.__table__.insert().from_select(
                    ['parent', 'member'], group_members.select().where(
                        group_members.c.group_name.startswith('group/')
                    ).where(
                        group_members.c.members_name.startswith('group/')
                    )
                ))

                session.execute(qry)

                iterations = 0
                rows_affected = True
                while rows_affected:
                    # Join membership on its own to find transitive
                    expansion = tbl1.join(tbl2, tbl1.c.member == tbl2.c.parent)

                    # Left outjoin to find the entries that
                    # are already in the table to prevent
                    # inserting already existing entries
                    expansion = expansion.outerjoin(
                        tbl3,
                        and_(tbl1.c.parent == tbl3.c.parent,
                             tbl2.c.member == tbl3.c.member))

                    # Select only such elements that are not
                    # already in the table, indicated as NULL
                    # values through the outer-left-join
                    stmt = (
                        select([tbl1.c.parent,
                                tbl2.c.member])
                        .select_from(expansion)
                        # pylint: disable=singleton-comparison
                        .where(tbl3.c.parent == None)
                        .distinct()
                    )

                    # Execute the query and insert into the table
                    qry = (GroupInGroup.__table__
                           .insert()
                           .from_select(['parent', 'member'], stmt))

                    rows_affected = bool(session.execute(qry).rowcount)
                    iterations += 1
            except Exception as e:
                LOGGER.exception(e)
                session.rollback()
                raise
            finally:
                if get_sql_dialect(session) != 'sqlite':
                    session.execute('UNLOCK TABLES')
                session.commit()
            return iterations

        @classmethod
        def expand_special_members(cls, session):
            """"""Create dynamic groups for project(Editor|Owner|Viewer).

            Should be called after IAM bindings are added to the model.

            Args:
                session (object): Database session to use.
            """"""
            member_type_map = {
                'projecteditor': 'roles/editor',
                'projectowner': 'roles/owner',
                'projectviewer': 'roles/viewer'}
            for parent_member in cls.list_group_members(
                    session, '', member_types=list(member_type_map.keys())):
                member_type, project_id = parent_member.split('/')
                role = member_type_map[member_type]
                try:
                    iam_policy = cls.get_iam_policy(
                        session,
                        'project/{}'.format(project_id),
                        roles=[role])
                    LOGGER.info('iam_policy: %s', iam_policy)
                except NoResultFound:
                    LOGGER.warning('Found a non-existent project, or project '
                                   'outside of the organization, in an IAM '
                                   'binding: %s', parent_member)
                    continue
                members = iam_policy.get('bindings', {}).get(role, [])
                expanded_members = cls.expand_members(session, members)
                for member in expanded_members:
                    stmt = cls.TBL_MEMBERSHIP.insert(
                        {'group_name': parent_member,
                         'members_name': member.name})
                    session.execute(stmt)
                    if member.type == 'group' and member.name in members:
                        session.add(cls.TBL_GROUP_IN_GROUP(
                            parent=parent_member,
                            member=member.name))
            session.commit()

        @classmethod
        def explain_granted(cls, session, member_name, resource_type_name,
                            role, permission):
            """"""Provide info about how the member has access to the resource.

            For example, member m1 can access resource r1 with permission p
            it might be granted by binding (r2, rol, g1),
            r1 is a child resource in a project or folder r2,
            role rol contains permission p,
            m1 is a member in group g1.
            This method list bindings that grant the access, member relation
            and resource hierarchy

            Args:
                session (object): Database session.
                member_name (str): name of the member
                resource_type_name (str): type_name of the resource
                role (str): role to query
                permission (str): permission to query

            Returns:
                tuples: (bindings, member_graph, resource_type_names) bindings,
                    the bindings to grant the access member_graph, the graph to
                    have member included in the binding esource_type_names, the
                    resource tree

            Raises:
                Exception: not granted
            """"""
            members, member_graph = cls.reverse_expand_members(
                session, [member_name], request_graph=True)
            member_names = [m.name for m in members]
            resource_type_names = [r.type_name for r in
                                   cls.find_resource_path(session,
                                                          resource_type_name)]

            if role:
                roles = set([role])
                qry = session.query(Binding, Member).join(
                    binding_members).join(Member)
            else:
                roles = [r.name for r in
                         cls.get_roles_by_permission_names(
                             session,
                             [permission])]
                qry = session.query(Binding, Member)
                qry = qry.join(binding_members).join(Member)
                qry = qry.join(Role).join(role_permissions).join(Permission)

            qry = qry.filter(Binding.role_name.in_(roles))
            qry = qry.filter(Member.name.in_(member_names))
            qry = qry.filter(
                Binding.resource_type_name.in_(resource_type_names))
            result = qry.all()
            if not result:
                error_message = 'Grant not found: ({},{},{})'.format(
                    member_name,
                    resource_type_name,
                    role if role is not None else permission)
                LOGGER.error(error_message)
                raise Exception(error_message)
            else:
                bindings = [(b.resource_type_name, b.role_name, m.name)
                            for b, m in result]
                return bindings, member_graph, resource_type_names

        @classmethod
        def scanner_iter(cls, session, resource_type,
                         parent_type_name=None, stream_results=True):
            """"""Iterate over all resources with the specified type.

            Args:
                session (object): Database session.
                resource_type (str): type of the resource to scan
                parent_type_name (str): type_name of the parent resource
                stream_results (bool): Enable streaming in the query.

            Yields:
                Resource: resource that match the query.
            """"""
            query = (
                session.query(Resource)
                .filter(Resource.type == resource_type)
                .options(joinedload(Resource.parent))
                .enable_eagerloads(True))

            if parent_type_name:
                query = query.filter(
                    Resource.parent_type_name == parent_type_name)

            if stream_results:
                results = query.yield_per(PER_YIELD)
            else:
                results = page_query(query)

            for row in results:
                yield row

        @classmethod
        def scanner_fetch_groups_settings(cls, session, only_iam_groups):
            """"""Fetch Groups Settings.

            Args:
                session (object): Database session.
                only_iam_groups (bool): boolean indicating whether we want to
                only fetch groups settings for which there is at least 1 iam
                policy.

            Yields:
                Resource: resource that match the query
            """"""
            if only_iam_groups:
                query = (session.query(groups_settings)
                         .join(Member).join(binding_members)
                         .distinct().enable_eagerloads(True))
            else:
                query = (session.query(groups_settings).enable_eagerloads(True))
            for resource in query.yield_per(PER_YIELD):
                yield resource

        @classmethod
        def explain_denied(cls, session, member_name, resource_type_names,
                           permission_names, role_names):
            """"""Explain why an access is denied

            Provide information how to grant access to a member if such
            access is denied with current IAM policies.
            For example, member m1 cannot access resource r1 with permission
            p, this method shows the bindings with rol that covered the
            desired permission on the resource r1 and its ancestors.
            If adding this member to any of these bindings, such access
            can be granted. An overgranting level is also provided

            Args:
                session (object): Database session.
                member_name (str): name of the member
                resource_type_names (list): list of type_names of resources
                permission_names (list): list of permissions
                role_names (list): list of roles

            Returns:
                list: list of tuples,
                    (overgranting,[(role_name,member_name,resource_name)])

            Raises:
                Exception: No roles covering requested permission set,
                    Not possible
            """"""

            if not role_names:
                role_names = [r.name for r in
                              cls.get_roles_by_permission_names(
                                  session,
                                  permission_names)]
                if not role_names:
                    error_message = 'No roles covering requested permission set'
                    LOGGER.error(error_message)
                    raise Exception(error_message)

            resource_hierarchy = (
                cls.resource_ancestors(session,
                                       resource_type_names))

            def find_binding_candidates(resource_hierarchy):
                """"""Find the root node in the ancestors.

                    From there, walk down the resource tree and add
                    every node until a node has more than one child.
                    This is the set of nodes which grants access to
                    at least all of the resources requested.
                    There is always a chain with a single node root.

                Args:
                    resource_hierarchy (dict): graph of the resource hierarchy

                Returns:
                    list: candidates to add to bindings that potentially grant
                        access
                """"""

                root = None
                for parent in resource_hierarchy.keys():
                    is_root = True
                    for children in resource_hierarchy.values():
                        if parent in children:
                            is_root = False
                            break
                    if is_root:
                        root = parent
                chain = [root]
                cur = root
                while len(resource_hierarchy[cur]) == 1:
                    cur = next(iter(resource_hierarchy[cur]))
                    chain.append(cur)
                return chain

            bind_res_candidates = find_binding_candidates(
                resource_hierarchy)

            bindings = (
                session.query(Binding, Member)
                .join(binding_members)
                .join(Member)
                .join(Role)
                .filter(Binding.resource_type_name.in_(
                    bind_res_candidates))
                .filter(Role.name.in_(role_names))
                .filter(or_(Member.type == 'group',
                            Member.name == member_name))
                .filter(and_((binding_members.c.bindings_id ==
                              Binding.id),
                             (binding_members.c.members_name ==
                              Member.name)))
                .filter(Role.name == Binding.role_name)
                .all())

            strategies = []
            for resource in bind_res_candidates:
                for role_name in role_names:
                    overgranting = (len(bind_res_candidates) -
                                    bind_res_candidates.index(resource) -
                                    1)
                    strategies.append(
                        (overgranting, [
                            (role, member_name, resource)
                            for role in [role_name]]))
            if bindings:
                for binding, member in bindings:
                    overgranting = (len(bind_res_candidates) - 1 -
                                    bind_res_candidates.index(
                                        binding.resource_type_name))
                    strategies.append(
                        (overgranting, [
                            (binding.role_name,
                             member.name,
                             binding.resource_type_name)]))

            return strategies

        @classmethod
        def query_access_by_member(cls, session, member_name, permission_names,
                                   expand_resources=False,
                                   reverse_expand_members=True):
            """"""Return the set of resources the member has access to.

            By default, this method expand group_member relation,
            so the result includes all resources can be accessed by the
            groups that the member is in.
            By default, this method does not expand resource hierarchy,
            so the result does not include a resource if such resource does
            not have a direct binding to allow access.

            Args:
                session (object): Database session.
                member_name (str): name of the member
                permission_names (list): list of names of permissions to query
                expand_resources (bool): whether to expand resources
                reverse_expand_members (bool): whether to expand members

            Returns:
                list: list of access tuples, (""role_name"", ""resource_type_name"")
            """"""

            if reverse_expand_members:
                member_names = [m.name for m in
                                cls.reverse_expand_members(session,
                                                           [member_name],
                                                           False)]
            else:
                member_names = [member_name]

            roles = cls.get_roles_by_permission_names(
                session, permission_names)

            qry = (
                session.query(Binding)
                .join(binding_members)
                .join(Member)
                .filter(Binding.role_name.in_([r.name for r in roles]))
                .filter(Member.name.in_(member_names))
            )

            bindings = qry.yield_per(1024)
            if not expand_resources:
                return [(binding.role_name,
                         [binding.resource_type_name]) for binding in bindings]

            r_type_names = [binding.resource_type_name for binding in bindings]
            expansion = cls.expand_resources_by_type_names(
                session,
                r_type_names)

            res_exp = {k.type_name: [v.type_name for v in values]
                       for k, values in expansion.items()}

            return [(binding.role_name,
                     res_exp[binding.resource_type_name])
                    for binding in bindings]

        @classmethod
        def query_access_by_permission(cls,
                                       session,
                                       role_name=None,
                                       permission_name=None,
                                       expand_groups=False,
                                       expand_resources=False):
            """"""Query access via the specified permission

            Return all the (Principal, Resource) combinations allowing
            satisfying access via the specified permission.
            By default, the group relation and resource hierarchy will not be
            expanded, so the results will only contains direct bindings
            filtered by permission. But the relations can be expanded

            Args:
                session (object): Database session.
                role_name (str): Role name to query for
                permission_name (str): Permission name to query for.
                expand_groups (bool): Whether or not to expand groups.
                expand_resources (bool): Whether or not to expand resources.

            Yields:
                obejct: A generator of access tuples.

            Raises:
                ValueError: If neither role nor permission is set.
            """"""

            if role_name:
                role_names = [role_name]
            elif permission_name:
                role_names = [p.name for p in
                              cls.get_roles_by_permission_names(
                                  session,
                                  [permission_name])]
            else:
                error_message = 'Either role or permission must be set'
                LOGGER.error(error_message)
                raise ValueError(error_message)

            if expand_resources:
                expanded_resources = aliased(Resource)
                qry = (
                    session.query(expanded_resources, Binding, Member)
                    .filter(binding_members.c.bindings_id == Binding.id)
                    .filter(binding_members.c.members_name == Member.name)
                    .filter(expanded_resources.full_name.startswith(
                        Resource.full_name))
                    .filter((Resource.type_name ==
                             Binding.resource_type_name))
                    .filter(Binding.role_name.in_(role_names))
                    .order_by(expanded_resources.name.asc(),
                              Binding.role_name.asc())
                )
            else:
                qry = (
                    session.query(Resource, Binding, Member)
                    .filter(binding_members.c.bindings_id == Binding.id)
                    .filter(binding_members.c.members_name == Member.name)
                    .filter((Resource.type_name ==
                             Binding.resource_type_name))
                    .filter(Binding.role_name.in_(role_names))
                    .order_by(Resource.name.asc(), Binding.role_name.asc())
                )

            if expand_groups:
                to_expand = set([m.name for _, _, m in
                                 qry.yield_per(PER_YIELD)])
                expansion = cls.expand_members_map(session, to_expand,
                                                   show_group_members=False,
                                                   member_contain_self=True)

            qry = qry.distinct()

            cur_resource = None
            cur_role = None
            cur_members = set()
            for resource, binding, member in qry.yield_per(PER_YIELD):
                if cur_resource != resource.type_name:
                    if cur_resource is not None:
                        yield cur_role, cur_resource, cur_members
                    cur_resource = resource.type_name
                    cur_role = binding.role_name
                    cur_members = set()
                if expand_groups:
                    for member_name in expansion[member.name]:
                        cur_members.add(member_name)
                else:
                    cur_members.add(member.name)
            if cur_resource is not None:
                yield cur_role, cur_resource, cur_members

        @classmethod
        def query_access_by_resource(cls, session, resource_type_name,
                                     permission_names, expand_groups=False):
            """"""Query access by resource

            Return members who have access to the given resource.
            The resource hierarchy will always be expanded, so even if the
            current resource does not have that binding, if its ancestors
            have the binding, the access will be shown
            By default, the group relationship will not be expanded

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource to query
                permission_names (list): list of strs, names of the permissions
                    to query
                expand_groups (bool): whether to expand groups

            Returns:
                dict: role_member_mapping, <""role_name"", ""member_names"">
            """"""

            roles = cls.get_roles_by_permission_names(
                session, permission_names)
            resources = cls.find_resource_path(session, resource_type_name)

            res = (session.query(Binding, Member)
                   .filter(
                       Binding.role_name.in_([r.name for r in roles]),
                       Binding.resource_type_name.in_(
                           [r.type_name for r in resources]))
                   .join(binding_members).join(Member))

            role_member_mapping = collections.defaultdict(set)
            for binding, member in res:
                role_member_mapping[binding.role_name].add(member.name)

            if expand_groups:
                for role in role_member_mapping:
                    role_member_mapping[role] = (
                        [m.name for m in cls.expand_members(
                            session,
                            role_member_mapping[role])])

            return role_member_mapping

        @classmethod
        def query_permissions_by_roles(cls, session, role_names, role_prefixes,
                                       _=1024):
            """"""Resolve permissions for the role.

            Args:
                session (object): db session
                role_names (list): list of strs, names of the roles
                role_prefixes (list): list of strs, prefixes of the roles
                _ (int): place occupation

            Returns:
                list: list of (Role, Permission)

            Raises:
                Exception: No roles or role prefixes specified
            """"""

            if not role_names and not role_prefixes:
                error_message = 'No roles or role prefixes specified'
                LOGGER.error(error_message)
                raise Exception(error_message)
            qry = session.query(Role, Permission).join(
                role_permissions).join(Permission)
            if role_names:
                qry = qry.filter(Role.name.in_(role_names))
            if role_prefixes:
                qry = qry.filter(
                    or_(*[Role.name.startswith(prefix)
                          for prefix in role_prefixes]))
            return qry.all()

        @classmethod
        def set_iam_policy(cls,
                           session,
                           resource_type_name,
                           policy,
                           update_members=False):
            """"""Set IAM policy

            Sets an IAM policy for the resource, check the etag when setting
            new policy and reassign new etag.
            Check etag to avoid race condition

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource
                policy (dict): the policy to set on the resource
                update_members (bool): If true, then add new members to Member
                    table. This must be set when the call to set_iam_policy
                    happens outside of the model InventoryImporter class. Tests
                    or users that manually add an IAM policy need to mark this
                    as true to ensure the model remains consistent.

            Raises:
                Exception: Etag doesn't match
            """"""

            LOGGER.info('Setting IAM policy, resource_type_name = %s, policy'
                        ' = %s, session = %s',
                        resource_type_name, policy, session)
            old_policy = cls.get_iam_policy(session, resource_type_name)
            if policy['etag'] != old_policy['etag']:
                error_message = 'Etags distinct, stored={}, provided={}'.format(
                    old_policy['etag'], policy['etag'])
                LOGGER.error(error_message)
                raise Exception(error_message)

            old_policy = old_policy['bindings']
            policy = policy['bindings']

            def filter_etag(policy):
                """"""Filter etag key/value out of policy map.

                Args:
                    policy (dict): the policy to filter

                Returns:
                    dict: policy without etag, <""bindings"":[<role, members>]>

                Raises:
                """"""

                return {k: v for k, v in policy.items() if k != 'etag'}

            def calculate_diff(policy, old_policy):
                """"""Calculate the grant/revoke difference between policies.
                   The diff = policy['bindings'] - old_policy['bindings']

                Args:
                    policy (dict): the new policy in dict format
                    old_policy (dict): the old policy in dict format

                Returns:
                    dict: <role, members> diff of bindings
                """"""

                diff = collections.defaultdict(list)
                for role, members in filter_etag(policy).items():
                    if role in old_policy:
                        for member in members:
                            if member not in old_policy[role]:
                                diff[role].append(member)
                    else:
                        diff[role] = members
                return diff

            grants = calculate_diff(policy, old_policy)
            revocations = calculate_diff(old_policy, policy)

            for role, members in revocations.items():
                bindings = (
                    session.query(Binding)
                    .filter((Binding.resource_type_name ==
                             resource_type_name))
                    .filter(Binding.role_name == role)
                    .join(binding_members).join(Member)
                    .filter(Member.name.in_(members)).all())

                for binding in bindings:
                    session.delete(binding)

            for role, members in grants.items():
                inserted = False
                existing_bindings = (
                    session.query(Binding)
                    .filter((Binding.resource_type_name ==
                             resource_type_name))
                    .filter(Binding.role_name == role)
                    .all())

                if update_members:
                    for member in members:
                        if not cls.get_member(session, member):
                            try:
                                # This is the default case, e.g. 'group/foobar'
                                m_type, name = member.split('/', 1)
                            except ValueError:
                                # Special groups like 'allUsers'
                                m_type, name = member, member
                            session.add(cls.TBL_MEMBER(
                                name=member,
                                type=m_type,
                                member_name=name))

                for binding in existing_bindings:
                    if binding.role_name == role:
                        inserted = True
                        for member in members:
                            binding.members.append(
                                session.query(Member).filter(
                                    Member.name == member).one())
                if not inserted:
                    binding = Binding(
                        resource_type_name=resource_type_name,
                        role=session.query(Role).filter(
                            Role.name == role).one())
                    binding.members = session.query(Member).filter(
                        Member.name.in_(members)).all()
                    session.add(binding)
            resource = session.query(Resource).filter(
                Resource.type_name == resource_type_name).one()
            resource.increment_update_counter()
            session.commit()

        @classmethod
        def get_iam_policy(cls, session, resource_type_name, roles=None):
            """"""Return the IAM policy for a resource.

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource to query
                roles (list): An optional list of roles to limit the results to

            Returns:
                dict: the IAM policy
            """"""

            resource = session.query(Resource).filter(
                Resource.type_name == resource_type_name).one()
            policy = {'etag': resource.get_etag(),
                      'bindings': {},
                      'resource': resource.type_name}
            bindings = session.query(Binding).filter(
                Binding.resource_type_name == resource_type_name)
            if roles:
                bindings = bindings.filter(Binding.role_name.in_(roles))
            for binding in bindings.all():
                role = binding.role_name
                members = [m.name for m in binding.members]
                policy['bindings'][role] = members
            return policy

        @classmethod
        def check_iam_policy(cls, session, resource_type_name, permission_name,
                             member_name):
            """"""Check access according to the resource IAM policy.

            Args:
                session (object): db session
                resource_type_name (str): type_name of the resource to check
                permission_name (str): name of the permission to check
                member_name (str): name of the member to check

            Returns:
                bool: whether such access is allowed

            Raises:
                Exception: member or resource not found
            """"""

            member_names = [m.name for m in
                            cls.reverse_expand_members(
                                session,
                                [member_name])]
            resource_type_names = [r.type_name for r in cls.find_resource_path(
                session,
                resource_type_name)]

            if not member_names:
                error_message = 'Member not found: {}'.format(member_name)
                LOGGER.error(error_message)
                raise Exception(error_message)
            if not resource_type_names:
                error_message = 'Resource not found: {}'.format(
                    resource_type_name)
                LOGGER.error(error_message)
                raise Exception(error_message)

            return (session.query(Permission)
                    .filter(Permission.name == permission_name)
                    .join(role_permissions).join(Role).join(Binding)
                    .filter(Binding.resource_type_name.in_(resource_type_names))
                    .join(binding_members).join(Member)
                    .filter(Member.name.in_(member_names)).first() is not None)

        @classmethod
        def list_roles_by_prefix(cls, session, role_prefix):
            """"""Provides a list of roles matched via name prefix.

            Args:
                session (object): db session
                role_prefix (str): prefix of the role_name

            Returns:
                list: list of role_names that match the query
            """"""

            return [r.name for r in session.query(Role).filter(
                Role.name.startswith(role_prefix)).all()]

        @classmethod
        def add_role_by_name(cls, session, role_name, permission_names):
            """"""Creates a new role.

            Args:
                session (object): db session
                role_name (str): name of the role to add
                permission_names (list): list of permissions in the role
            """"""

            LOGGER.info('Creating a new role, role_name = %s, permission_names'
                        ' = %s, session = %s',
                        role_name, permission_names, session)
            permission_names = set(permission_names)
            existing_permissions = session.query(Permission).filter(
                Permission.name.in_(permission_names)).all()
            for existing_permission in existing_permissions:
                try:
                    permission_names.remove(existing_permission.name)
                except KeyError:
                    LOGGER.warning('existing_permissions.name = %s, KeyError',
                                   existing_permission.name)

            new_permissions = [Permission(name=n) for n in permission_names]
            for perm in new_permissions:
                session.add(perm)
            cls.add_role(session, role_name,
                         existing_permissions + new_permissions)
            session.commit()

        @classmethod
        def add_group_member(cls,
                             session,
                             member_type_name,
                             parent_type_names,
                             denorm=False):
            """"""Add member, optionally with parent relationship.

            Args:
                session (object): db session
                member_type_name (str): type_name of the member to add
                parent_type_names (list): type_names of the parents
                denorm (bool): whether to denorm the groupingroup table after
                    addition
            """"""

            LOGGER.info('Adding a member, member_type_name = %s,'
                        ' parent_type_names = %s, denorm = %s, session = %s',
                        member_type_name, parent_type_names, denorm, session)

            cls.add_member(session,
                           member_type_name,
                           parent_type_names,
                           denorm)
            session.commit()

        @classmethod
        def list_group_members(cls,
                               session,
                               member_name_prefix,
                               member_types=None):
            """"""Returns members filtered by prefix.

            Args:
                session (object): db session
                member_name_prefix (str): the prefix of the member_name
                member_types (list): an optional list of member types to filter
                    the results by.

            Returns:
                list: list of Members that match the query
            """"""

            qry = session.query(Member).filter(
                Member.member_name.startswith(member_name_prefix))
            if member_types:
                qry = qry.filter(Member.type.in_(member_types))
            return [m.name for m in qry.all()]

        @classmethod
        def iter_groups(cls, session):
            """"""Returns iterator of all groups in model.

            Args:
                session (object): db session

            Yields:
                Member: group in the model
            """"""

            qry = session.query(Member).filter(Member.type == 'group')
            for group in qry.yield_per(1024):
                yield group

        @classmethod
        def iter_resources_by_prefix(cls,
                                     session,
                                     full_resource_name_prefix=None,
                                     type_name_prefix=None,
                                     type_prefix=None,
                                     name_prefix=None):
            """"""Returns iterator to resources filtered by prefix.

            Args:
                session (object): db session
                full_resource_name_prefix (str): the prefix of the
                    full_resource_name
                type_name_prefix (str): the prefix of the type_name
                type_prefix (str): the prefix of the type
                name_prefix (ste): the prefix of the name

            Yields:
                Resource: that match the query

            Raises:
                Exception: No prefix given
            """"""

            if not any([arg is not None for arg in [full_resource_name_prefix,
                                                    type_name_prefix,
                                                    type_prefix,
                                                    name_prefix]]):
                error_message = 'At least one prefix must be set'
                LOGGER.error(error_message)
                raise Exception(error_message)

            qry = session.query(Resource)
            if full_resource_name_prefix:
                qry = qry.filter(Resource.full_name.startswith(
                    full_resource_name_prefix))
            if type_name_prefix:
                qry = qry.filter(Resource.type_name.startswith(
                    type_name_prefix))
            if type_prefix:
                qry = qry.filter(Resource.type.startswith(
                    type_prefix))
            if name_prefix:
                qry = qry.filter(Resource.name.startswith(
                    name_prefix))

            for resource in qry.yield_per(1024):
                yield resource

        @classmethod
        def list_resources_by_prefix(cls,
                                     session,
                                     full_resource_name_prefix=None,
                                     type_name_prefix=None,
                                     type_prefix=None,
                                     name_prefix=None):
            """"""Returns resources filtered by prefix.

            Args:
                session (object): db session
                full_resource_name_prefix (str): the prefix of the
                    full_resource_name
                type_name_prefix (str): the prefix of the type_name
                type_prefix (str): the prefix of the type
                name_prefix (ste): the prefix of the name

            Returns:
                list: list of Resources match the query

            Raises:
            """"""

            return list(
                cls.iter_resources_by_prefix(session,
                                             full_resource_name_prefix,
                                             type_name_prefix,
                                             type_prefix,
                                             name_prefix))

        @classmethod
        def add_resource_by_name(cls,
                                 session,
                                 resource_type_name,
                                 parent_type_name,
                                 no_require_parent):
            """"""Adds resource specified via full name.

            Args:
                session (object): db session
                resource_type_name (str): name of the resource
                parent_type_name (str): name of the parent resource
                no_require_parent (bool): if this resource has a parent

            Returns:
                Resource: Created new resource
            """"""

            LOGGER.info('Adding resource via full name, resource_type_name'
                        ' = %s, parent_type_name = %s, no_require_parent = %s,'
                        ' session = %s', resource_type_name,
                        parent_type_name, no_require_parent, session)
            if not no_require_parent:
                parent = session.query(Resource).filter(
                    Resource.type_name == parent_type_name).one()
            else:
                parent = None
            return cls.add_resource(session, resource_type_name, parent)

        @classmethod
        def add_resource(cls, session, resource_type_name, parent=None):
            """"""Adds resource by name.

            Args:
                session (object): db session
                resource_type_name (str): name of the resource
                parent (Resource): parent of the resource

            Returns:
                Resource: Created new resource
            """"""

            LOGGER.info('Adding resource by name, resource_type_name = %s,'
                        ' session = %s', resource_type_name, session)
            res_type, res_name = resource_type_name.split('/')
            parent_full_resource_name = (
                '' if parent is None else parent.full_name)

            full_resource_name = to_full_resource_name(
                parent_full_resource_name,
                resource_type_name)

            resource = Resource(full_name=full_resource_name,
                                type_name=resource_type_name,
                                name=res_name,
                                type=res_type,
                                parent=parent)
            session.add(resource)
            return resource

        @classmethod
        def add_role(cls, session, name, permissions=None):
            """"""Add role by name.

            Args:
                session (object): db session
                name (str): name of the role to add
                permissions (list): permissions to add in the role

            Returns:
                Role: The created role
            """"""

            LOGGER.info('Adding role, name = %s, permissions = %s,'
                        ' session = %s', name, permissions, session)
            permissions = [] if permissions is None else permissions
            role = Role(name=name, permissions=permissions)
            session.add(role)
            return role

        @classmethod
        def add_permission(cls, session, name, roles=None):
            """"""Add permission by name.

            Args:
                session (object): db session
                name (str): name of the permission
                roles (list): list od roles to add the permission

            Returns:
                Permission: The created permission
            """"""

            LOGGER.info('Adding permission, name = %s, roles = %s'
                        ' session = %s', name, roles, session)
            roles = [] if roles is None else roles
            permission = Permission(name=name, roles=roles)
            session.add(permission)
            return permission

        @classmethod
        def add_binding(cls, session, resource, role, members):
            """"""Add a binding to the model.

            Args:
                session (object): db session
                resource (str): Resource to be added in the binding
                role (str): Role to be added in the binding
                members (list): members to be added in the binding

            Returns:
                Binding: the created binding
            """"""

            LOGGER.info('Adding a binding to the model, resource = %s,'
                        ' role = %s, members = %s, session = %s',
                        resource, role, members, session)
            binding = Binding(resource=resource, role=role, members=members)
            session.add(binding)
            return binding

        @classmethod
        def add_member(cls,
                       session,
                       type_name,
                       parent_type_names=None,
                       denorm=False):
            """"""Add a member to the model.

            Args:
                session (object): db session
                type_name (str): type_name of the resource to add
                parent_type_names (list): list of parent names to add
                denorm (bool): whether to denormalize the GroupInGroup relation

            Returns:
                Member: the created member

            Raises:
                Exception: parent not found
            """"""

            LOGGER.info('Adding a member to the model, type_name = %s,'
                        ' parent_type_names = %s, denorm = %s, session = %s',
                        type_name, parent_type_names, denorm, session)
            if not parent_type_names:
                parent_type_names = []
            res_type, name = type_name.split('/', 1)
            parents = session.query(Member).filter(
                Member.name.in_(parent_type_names)).all()
            if len(parents) != len(parent_type_names):
                msg = 'Parents: {}, expected: {}'.format(
                    parents, parent_type_names)
                error_message = 'Parent not found, {}'.format(msg)
                LOGGER.error(error_message)
                raise Exception(error_message)

            member = Member(name=type_name,
                            member_name=name,
                            type=res_type,
                            parents=parents)
            session.add(member)
            session.commit()
            if denorm and res_type == 'group' and parents:
                cls.denorm_group_in_group(session)
            return member

        @classmethod
        def expand_resources_by_type_names(cls, session, res_type_names):
            """"""Expand resources by type/name format.

            Args:
                session (object): db session
                res_type_names (list): list of resources in type_names

            Returns:
                dict: mapping in the form:
                      {res_type_name: Expansion(res_type_name), ... }
            """"""

            res_key = aliased(Resource, name='res_key')
            res_values = aliased(Resource, name='res_values')

            expressions = []
            for res_type_name in res_type_names:
                expressions.append(and_(
                    res_key.type_name == res_type_name))

            res = (
                session.query(res_key, res_values)
                .filter(res_key.type_name.in_(res_type_names))
                .filter(res_values.full_name.startswith(
                    res_key.full_name))
                .yield_per(1024)
            )

            mapping = collections.defaultdict(set)
            for k, value in res:
                mapping[k].add(value)
            return mapping

        @classmethod
        def reverse_expand_members(cls, session, member_names,
                                   request_graph=False):
            """"""Expand members to their groups.

            List all groups that contains these members. Also return
            the graph if requested.

            Args:
                session (object): db session
                member_names (list): list of members to expand
                request_graph (bool): wether the parent-child graph is provided

            Returns:
                object: set if graph not requested, set and graph if requested
            """"""
            member_names.extend(cls.ALL_USER_MEMBERS)
            members = session.query(Member).filter(
                Member.name.in_(member_names)).all()
            membership_graph = collections.defaultdict(set)
            member_set = set()
            new_member_set = set()

            def add_to_sets(members, child):
                """"""Adds the members & children to the sets.

                Args:
                    members (list): list of Members to be added
                    child (Member): child to be added
                """"""

                for member in members:
                    if request_graph and child:
                        membership_graph[child.name].add(member.name)
                    if request_graph and not child:
                        if member.name not in membership_graph:
                            membership_graph[member.name] = set()
                    if member not in member_set:
                        new_member_set.add(member)
                        member_set.add(member)

            add_to_sets(members, None)
            while new_member_set:
                members_to_walk = new_member_set
                new_member_set = set()
                for member in members_to_walk:
                    add_to_sets(member.parents, member)

            if request_graph:
                return member_set, membership_graph
            return member_set

        @classmethod
        def expand_members_map(cls,
                               session,
                               member_names,
                               show_group_members=True,
                               member_contain_self=True):
            """"""Expand group membership keyed by member.

            Args:
                session (object): db session
                member_names (set): Member names to expand
                show_group_members (bool): Whether to include subgroups
                member_contain_self (bool): Whether to include a parent
                    as its own member
            Returns:
                dict: <Member, set(Children)>
            """"""

            def separate_groups(member_names):
                """"""Separate groups and other members in two lists.

                This is a helper function. groups are needed to query on
                group_in_group table

                Args:
                    member_names (list): list of members to be separated

                Returns:
                    tuples: two lists of strs containing groups and others
                """"""
                groups = []
                others = []
                for name in member_names:
                    member_type = name.split('/')[0]
                    if member_type in cls.GROUP_TYPES:
                        groups.append(name)
                    else:
                        others.append(name)
                return groups, others

            selectables = []
            group_names, other_names = separate_groups(member_names)

            t_ging = GroupInGroup.__table__
            t_members = group_members

            # This resolves groups to its transitive non-group members.
            transitive_membership = (
                select([t_ging.c.parent, t_members.c.members_name])
                .select_from(t_ging.join(t_members,
                                         (t_ging.c.member ==
                                          t_members.c.group_name)))
            ).where(t_ging.c.parent.in_(group_names))

            if not show_group_members:
                transitive_membership = transitive_membership.where(
                    not_(t_members.c.members_name.startswith('group/')))

            selectables.append(
                transitive_membership.alias('transitive_membership'))

            direct_membership = (
                select([t_members.c.group_name,
                        t_members.c.members_name])
                .where(t_members.c.group_name.in_(group_names))
            )

            if not show_group_members:
                direct_membership = direct_membership.where(
                    not_(t_members.c.members_name.startswith('group/')))

            selectables.append(
                direct_membership.alias('direct_membership'))

            if show_group_members:
                # Show groups as members of other groups
                group_in_groups = (
                    select([t_ging.c.parent,
                            t_ging.c.member]).where(
                                t_ging.c.parent.in_(group_names))
                )
                selectables.append(
                    group_in_groups.alias('group_in_groups'))

            # Union all the queries
            qry = union(*selectables)

            # Build the result dict
            result = collections.defaultdict(set)
            for parent, child in session.execute(qry):
                result[parent].add(child)
            for parent in other_names:
                result[parent] = set()

            # Add each parent as its own member
            if member_contain_self:
                for name in member_names:
                    result[name].add(name)
            return result

        @classmethod
        def expand_members(cls, session, member_names):
            """"""Expand group membership towards the members.

            Args:
                session (object): db session
                member_names (list): list of strs of member names

            Returns:
                set: expanded group members
            """"""

            members = session.query(Member).filter(
                Member.name.in_(member_names)).all()

            def is_group(member):
                """"""Returns true iff the member is a group.

                Args:
                    member (Member): member to check

                Returns:
                    bool: whether the member is a group
                """"""
                return member.type in cls.GROUP_TYPES

            group_set = set()
            non_group_set = set()
            new_group_set = set()

            def add_to_sets(members):
                """"""Adds new members to the sets.

                Args:
                    members (list): members to be added
                """"""
                for member in members:
                    if is_group(member):
                        if member not in group_set:
                            new_group_set.add(member)
                        group_set.add(member)
                    else:
                        non_group_set.add(member)

            add_to_sets(members)

            while new_group_set:
                groups_to_walk = new_group_set
                new_group_set = set()
                for group in groups_to_walk:
                    add_to_sets(group.children)

            return group_set.union(non_group_set)

        @classmethod
        def resource_ancestors(cls, session, resource_type_names):
            """"""Resolve the transitive ancestors by type/name format.

            Given a group of resource and find out all their parents.
            Then this method group the pairs with parent. Used to determine
            resource candidates to grant access in explain denied.

            Args:
                session (object): db session
                resource_type_names (list): list of strs, resources to query

            Returns:
                dict: <parent, childs> graph of the resource hierarchy
            """"""

            resource_names = resource_type_names
            resource_graph = collections.defaultdict(set)

            res_childs = aliased(Resource, name='res_childs')
            res_anc = aliased(Resource, name='resource_parent')

            resources_set = set(resource_names)
            resources_new = set(resource_names)

            for resource in resources_new:
                resource_graph[resource] = set()

            while resources_new:
                resources_new = set()
                for parent, child in (
                        session.query(res_anc, res_childs)
                        .filter(res_childs.type_name.in_(resources_set))
                        .filter(res_childs.parent_type_name ==
                                res_anc.type_name)
                        .all()):

                    if parent.type_name not in resources_set:
                        resources_new.add(parent.type_name)

                    resources_set.add(parent.type_name)
                    resources_set.add(child.type_name)

                    resource_graph[parent.type_name].add(child.type_name)

            return resource_graph

        @classmethod
        def find_resource_path(cls, session, resource_type_name):
            """"""Find resource ancestors by type/name format.

            Find all ancestors of a resource and return them in order

            Args:
                session (object): db session
                resource_type_name (str): resource to query

            Returns:
                list: list of Resources, transitive ancestors for the given
                    resource
            """"""

            qry = (
                session.query(Resource).filter(
                    Resource.type_name == resource_type_name)
            )

            resources = qry.all()
            return cls._find_resource_path(session, resources)

        @classmethod
        def _find_resource_path(cls, _, resources):
            """"""Find the list of transitive ancestors for the given resource.

            Args:
                _ (object): position holder
                resources (list): list of the resources to query

            Returns:
                list: list of Resources, transitive ancestors for the given
                    resource
            """"""

            if not resources:
                return []

            path = []
            resource = resources[0]

            path.append(resource)
            while resource.parent:
                resource = resource.parent
                path.append(resource)

            return path

        @classmethod
        def get_roles_by_permission_names(cls, session, permission_names):
            """"""Return the list of roles covering the specified permissions.

            Args:
                session (object): db session
                permission_names (list): permissions to be covered by.

            Returns:
                set: roles set that cover the permissions
            """"""

            permission_set = set(permission_names)
            qry = session.query(Permission)
            if permission_set:
                qry = qry.filter(Permission.name.in_(permission_set))
            permissions = qry.all()

            roles = set()
            for permission in permissions:
                for role in permission.roles:
                    roles.add(role)

            result_set = set()
            for role in roles:
                role_permissions = set(
                    [p.name for p in role.permissions])
                if permission_set.issubset(role_permissions):
                    result_set.add(role)

            return result_set

        @classmethod
        def get_member(cls, session, name):
            """"""Get member by name.

            Args:
                session (object): db session
                name (str): the name the member to query

            Returns:
                list: Members from the query
            """"""

            return session.query(Member).filter(Member.name == name).all()

    base.metadata.create_all(dbengine)
    return sessionmaker(bind=dbengine), ModelAccess",_10354.py,633,"for children in resource_hierarchy.values():
    if parent in children:
        is_root = False
        break","if is_root:
    root = parent","for children in resource_hierarchy.values():
    if parent in children:
        break
else:
    root = parent"
https://github.com/asyml/texar/tree/master/texar/tf/data/tokenizers/bert_tokenizer_utils.py,"def tokenize(self, text: str) -> List[str]:
        r""""""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
            input = ""unaffable""
            output = [""un"", ""##aff"", ""##able""]

        Args:
            text: A single token or whitespace separated tokens. This should
                have already been passed through `BasicTokenizer`.

        Returns:
            A list of wordpiece tokens.
        """"""
        output_tokens = []
        for token in whitespace_tokenize(text):
            assert token is not None
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens",_104483.py,29,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end","if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)"
https://github.com/asyml/texar/tree/master/texar/tf/data/tokenizers/bert_tokenizer_utils.py,"def tokenize(self, text: str) -> List[str]:
        r""""""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
            input = ""unaffable""
            output = [""un"", ""##aff"", ""##able""]

        Args:
            text: A single token or whitespace separated tokens. This should
                have already been passed through `BasicTokenizer`.

        Returns:
            A list of wordpiece tokens.
        """"""
        output_tokens = []
        for token in whitespace_tokenize(text):
            assert token is not None
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens",_104483.py,32,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1","if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break"
https://github.com/jookies/jasmin/tree/master/jasmin/protocols/cli/morouterm.py,"def MORouteBuild(fCallback):
    """"""Parse args and try to build a route from  one of the routes in
       jasmin.routing.Routes instance to pass it to fCallback""""""

    def parse_args_and_call_with_instance(self, *args, **kwargs):
        cmd = args[0]
        arg = args[1]

        # Empty line
        if cmd is None:
            return self.protocol.sendData()
        # Initiate a route from jasmin.routing.Routes with sessBuffer content
        if cmd == 'ok':
            # Remove route_class and route_args from self.sessBuffer before checking options
            # as these 2 options are not user-typed
            if len(self.sessBuffer) - 2 < len(self.protocol.sessionCompletitions):
                return self.protocol.sendData('You must set these options before saving: %s' % ', '.join(
                    self.protocol.sessionCompletitions))

            route = {}
            for key, value in self.sessBuffer.items():
                if key not in ['order', 'type', 'route_class', 'route_args']:
                    route[key] = value
            try:
                # Instanciate a Route
                RouteInstance = self.sessBuffer['route_class'](**route)

                # Hand the instance to fCallback
                return fCallback(self, self.sessBuffer['order'], RouteInstance)
            except Exception as e:
                return self.protocol.sendData('Error: %s' % str(e))
        else:
            # Unknown key
            ra = []
            if 'route_args' in self.sessBuffer:
                ra = self.sessBuffer['route_args']
            if cmd not in MORouteKeyMap and cmd not in ra:
                return self.protocol.sendData('Unknown Route key: %s' % cmd)

            # IF we got the type, check if it's a correct one
            if cmd == 'type':
                _type = None
                for route in MOROUTES:
                    if arg.lower() == route.lower():
                        _type = route
                        break

                if _type is None:
                    return self.protocol.sendData(
                        'Unknown MO Route type:%s, available types: %s' % (arg, ', '.join(MOROUTES)))
                elif _type == 'DefaultRoute':
                    self.sessBuffer['order'] = 0

                # Before setting a new route class, remove any previous route
                # sessBuffer keys
                if 'order' in self.sessBuffer:
                    self.sessBuffer = {'order': self.sessBuffer['order']}
                else:
                    self.sessBuffer = {}

                self.sessBuffer['type'] = _type
                # Route class name must be already imported from jasmin.routing.Routes
                # in order to get it from globals()
                self.sessBuffer['route_class'] = globals()[_type]

                # Show Route help and save Route args
                RouteClassArgs = inspect.getfullargspec(self.sessBuffer['route_class'].__init__).args
                if 'self' in RouteClassArgs:
                    # Remove 'self' from args
                    RouteClassArgs.remove('self')
                if 'rate' in RouteClassArgs:
                    # MO Routes are not rated
                    RouteClassArgs.remove('rate')
                self.sessBuffer['route_args'] = RouteClassArgs

                if len(RouteClassArgs) > 0:
                    # Update completitions
                    self.protocol.sessionCompletitions = list(MORouteKeyMap) + RouteClassArgs

                    return self.protocol.sendData(
                        '%s arguments:\n%s' % (self.sessBuffer['route_class'], ', '.join(RouteClassArgs)))
            else:
                # DefaultRoute's order is always zero
                if cmd == 'order':
                    if (arg != '0' and 'type' in self.sessBuffer
                        and self.sessBuffer['type'] == 'DefaultRoute'):
                        self.sessBuffer['order'] = 0
                        return self.protocol.sendData('Route order forced to 0 since it is a DefaultRoute')
                    elif (arg == '0' and 'type' in self.sessBuffer and
                                  self.sessBuffer['type'] != 'DefaultRoute'):
                        return self.protocol.sendData(
                            'This route order (0) is reserved for DefaultRoute only')
                    elif not arg.isdigit() or int(arg) < 0:
                        return self.protocol.sendData('Route order must be a positive integer')
                    else:
                        arg = int(arg)

                # Validate connector
                if cmd == 'connector':
                    try:
                        ctype, cid = validate_typed_connector_id(arg)
                        if ctype == 'http':
                            if cid not in self.protocol.managers['httpccm'].httpccs:
                                raise Exception('Unknown http cid: %s' % (cid))

                            # Pass ready HttpConnector instance
                            arg = self.protocol.managers['httpccm'].httpccs[cid]
                        elif ctype == 'smpps':
                            # Make instance of SmppServerSystemIdConnector
                            arg = SmppServerSystemIdConnector(cid)
                        else:
                            raise NotImplementedError(""Not implemented yet !"")
                    except Exception as e:
                        return self.protocol.sendData(str(e))

                # Validate connectors
                if cmd == 'connectors':
                    CIDs = arg.split(';')
                    if len(CIDs) == 1:
                        return self.protocol.sendData(
                            '%s option value must contain a minimum of 2 connector IDs separated with "";"".' % (cmd))

                    arg = []
                    for typed_cid in CIDs:
                        try:
                            ctype, cid = validate_typed_connector_id(typed_cid)
                            if ctype == 'http':
                                if cid not in self.protocol.managers['httpccm'].httpccs:
                                    raise Exception('Unknown http cid: %s' % (cid))

                                # Pass ready HttpConnector instance
                                arg.append(self.protocol.managers['httpccm'].httpccs[cid])
                            elif ctype == 'smpps':
                                # Make instance of SmppServerSystemIdConnector
                                arg.append(SmppServerSystemIdConnector(cid))
                            else:
                                raise NotImplementedError(""Not implemented yet !"")
                        except Exception as e:
                            return self.protocol.sendData(str(e))

                # Validate filters
                if cmd == 'filters':
                    FIDs = arg.split(';')

                    arg = []
                    for fid in FIDs:
                        if fid not in self.protocol.managers['filter'].filters:
                            return self.protocol.sendData('Unknown fid: %s' % (fid))
                        else:
                            _Filter = self.protocol.managers['filter'].filters[fid]

                            if _Filter.__class__.__name__ not in MOFILTERS:
                                return self.protocol.sendData(
                                    '%s#%s is not a valid filter for MORoute (not in MOFILTERS)' % (
                                        _Filter.__class__.__name__, fid))
                            else:
                                arg.append(_Filter)

                # Buffer key for later Route initiating
                if cmd not in ra:
                    RouteKey = MORouteKeyMap[cmd]
                else:
                    RouteKey = cmd
                self.sessBuffer[RouteKey] = arg

            return self.protocol.sendData()

    return parse_args_and_call_with_instance",_104780.py,43,"for route in MOROUTES:
    if arg.lower() == route.lower():
        _type = route
        break","if _type is None:
    return self.protocol.sendData('Unknown MO Route type:%s, available types: %s' % (arg, ', '.join(MOROUTES)))
elif _type == 'DefaultRoute':
    self.sessBuffer['order'] = 0","for route in MOROUTES:
    if arg.lower() == route.lower():
        _type = route
        if _type == 'DefaultRoute':
            self.sessBuffer['order'] = 0
        break
else:
    return self.protocol.sendData('Unknown MO Route type:%s, available types: %s' % (arg, ', '.join(MOROUTES)))"
https://github.com/jookies/jasmin/tree/master/jasmin/protocols/cli/morouterm.py,"def MORouteBuild(fCallback):
    """"""Parse args and try to build a route from  one of the routes in
       jasmin.routing.Routes instance to pass it to fCallback""""""

    def parse_args_and_call_with_instance(self, *args, **kwargs):
        cmd = args[0]
        arg = args[1]

        # Empty line
        if cmd is None:
            return self.protocol.sendData()
        # Initiate a route from jasmin.routing.Routes with sessBuffer content
        if cmd == 'ok':
            # Remove route_class and route_args from self.sessBuffer before checking options
            # as these 2 options are not user-typed
            if len(self.sessBuffer) - 2 < len(self.protocol.sessionCompletitions):
                return self.protocol.sendData('You must set these options before saving: %s' % ', '.join(
                    self.protocol.sessionCompletitions))

            route = {}
            for key, value in self.sessBuffer.items():
                if key not in ['order', 'type', 'route_class', 'route_args']:
                    route[key] = value
            try:
                # Instanciate a Route
                RouteInstance = self.sessBuffer['route_class'](**route)

                # Hand the instance to fCallback
                return fCallback(self, self.sessBuffer['order'], RouteInstance)
            except Exception as e:
                return self.protocol.sendData('Error: %s' % str(e))
        else:
            # Unknown key
            ra = []
            if 'route_args' in self.sessBuffer:
                ra = self.sessBuffer['route_args']
            if cmd not in MORouteKeyMap and cmd not in ra:
                return self.protocol.sendData('Unknown Route key: %s' % cmd)

            # IF we got the type, check if it's a correct one
            if cmd == 'type':
                _type = None
                for route in MOROUTES:
                    if arg.lower() == route.lower():
                        _type = route
                        break

                if _type is None:
                    return self.protocol.sendData(
                        'Unknown MO Route type:%s, available types: %s' % (arg, ', '.join(MOROUTES)))
                elif _type == 'DefaultRoute':
                    self.sessBuffer['order'] = 0

                # Before setting a new route class, remove any previous route
                # sessBuffer keys
                if 'order' in self.sessBuffer:
                    self.sessBuffer = {'order': self.sessBuffer['order']}
                else:
                    self.sessBuffer = {}

                self.sessBuffer['type'] = _type
                # Route class name must be already imported from jasmin.routing.Routes
                # in order to get it from globals()
                self.sessBuffer['route_class'] = globals()[_type]

                # Show Route help and save Route args
                RouteClassArgs = inspect.getfullargspec(self.sessBuffer['route_class'].__init__).args
                if 'self' in RouteClassArgs:
                    # Remove 'self' from args
                    RouteClassArgs.remove('self')
                if 'rate' in RouteClassArgs:
                    # MO Routes are not rated
                    RouteClassArgs.remove('rate')
                self.sessBuffer['route_args'] = RouteClassArgs

                if len(RouteClassArgs) > 0:
                    # Update completitions
                    self.protocol.sessionCompletitions = list(MORouteKeyMap) + RouteClassArgs

                    return self.protocol.sendData(
                        '%s arguments:\n%s' % (self.sessBuffer['route_class'], ', '.join(RouteClassArgs)))
            else:
                # DefaultRoute's order is always zero
                if cmd == 'order':
                    if (arg != '0' and 'type' in self.sessBuffer
                        and self.sessBuffer['type'] == 'DefaultRoute'):
                        self.sessBuffer['order'] = 0
                        return self.protocol.sendData('Route order forced to 0 since it is a DefaultRoute')
                    elif (arg == '0' and 'type' in self.sessBuffer and
                                  self.sessBuffer['type'] != 'DefaultRoute'):
                        return self.protocol.sendData(
                            'This route order (0) is reserved for DefaultRoute only')
                    elif not arg.isdigit() or int(arg) < 0:
                        return self.protocol.sendData('Route order must be a positive integer')
                    else:
                        arg = int(arg)

                # Validate connector
                if cmd == 'connector':
                    try:
                        ctype, cid = validate_typed_connector_id(arg)
                        if ctype == 'http':
                            if cid not in self.protocol.managers['httpccm'].httpccs:
                                raise Exception('Unknown http cid: %s' % (cid))

                            # Pass ready HttpConnector instance
                            arg = self.protocol.managers['httpccm'].httpccs[cid]
                        elif ctype == 'smpps':
                            # Make instance of SmppServerSystemIdConnector
                            arg = SmppServerSystemIdConnector(cid)
                        else:
                            raise NotImplementedError(""Not implemented yet !"")
                    except Exception as e:
                        return self.protocol.sendData(str(e))

                # Validate connectors
                if cmd == 'connectors':
                    CIDs = arg.split(';')
                    if len(CIDs) == 1:
                        return self.protocol.sendData(
                            '%s option value must contain a minimum of 2 connector IDs separated with "";"".' % (cmd))

                    arg = []
                    for typed_cid in CIDs:
                        try:
                            ctype, cid = validate_typed_connector_id(typed_cid)
                            if ctype == 'http':
                                if cid not in self.protocol.managers['httpccm'].httpccs:
                                    raise Exception('Unknown http cid: %s' % (cid))

                                # Pass ready HttpConnector instance
                                arg.append(self.protocol.managers['httpccm'].httpccs[cid])
                            elif ctype == 'smpps':
                                # Make instance of SmppServerSystemIdConnector
                                arg.append(SmppServerSystemIdConnector(cid))
                            else:
                                raise NotImplementedError(""Not implemented yet !"")
                        except Exception as e:
                            return self.protocol.sendData(str(e))

                # Validate filters
                if cmd == 'filters':
                    FIDs = arg.split(';')

                    arg = []
                    for fid in FIDs:
                        if fid not in self.protocol.managers['filter'].filters:
                            return self.protocol.sendData('Unknown fid: %s' % (fid))
                        else:
                            _Filter = self.protocol.managers['filter'].filters[fid]

                            if _Filter.__class__.__name__ not in MOFILTERS:
                                return self.protocol.sendData(
                                    '%s#%s is not a valid filter for MORoute (not in MOFILTERS)' % (
                                        _Filter.__class__.__name__, fid))
                            else:
                                arg.append(_Filter)

                # Buffer key for later Route initiating
                if cmd not in ra:
                    RouteKey = MORouteKeyMap[cmd]
                else:
                    RouteKey = cmd
                self.sessBuffer[RouteKey] = arg

            return self.protocol.sendData()

    return parse_args_and_call_with_instance",_104780.py,43,"for route in MOROUTES:
    if arg.lower() == route.lower():
        _type = route
        break","if _type is None:
    return self.protocol.sendData('Unknown MO Route type:%s, available types: %s' % (arg, ', '.join(MOROUTES)))
elif _type == 'DefaultRoute':
    self.sessBuffer['order'] = 0","for route in MOROUTES:
    if arg.lower() == route.lower():
        _type = route
        if _type == 'DefaultRoute':
            self.sessBuffer['order'] = 0
        break
else:
    return self.protocol.sendData('Unknown MO Route type:%s, available types: %s' % (arg, ', '.join(MOROUTES)))"
https://github.com/openstates/openstates-scrapers/tree/master/scrapers/ca/bills.py,"def scrape_bill_type(
        self,
        chamber,
        session,
        bill_type,
        type_abbr,
        committee_abbr_regex=get_committee_name_regex(),
    ):
        bills = (
            self.session.query(CABill)
            .filter_by(session_year=session)
            .filter_by(measure_type=type_abbr)
        )

        archive_year = int(session[0:4])
        not_archive_year = archive_year >= 2009

        for bill in bills:
            bill_session = session
            if bill.session_num != ""0"":
                bill_session += "" Special Session %s"" % bill.session_num

            bill_id = bill.short_bill_id
            if bill_id.strip() == ""SB77"" and session == ""20052006"":
                continue

            fsbill = Bill(bill_id, bill_session, title="""", chamber=chamber)
            if (bill_id.startswith(""S"") and chamber == ""lower"") or (
                bill_id.startswith(""A"") and chamber == ""upper""
            ):
                print(""!!!! BAD ID/CHAMBER PAIR !!!!"", bill)
                continue

            # Construct a fake source url
            source_url = (
                ""http://leginfo.legislature.ca.gov/faces/""
                ""billNavClient.xhtml?bill_id=%s""
            ) % bill.bill_id

            fsbill.add_source(source_url)
            fsbill.add_version_link(bill_id, source_url, media_type=""text/html"")

            title = """"
            type_ = [""bill""]
            subject = """"
            all_titles = set()
            summary = """"

            # Get digest test (aka ""summary"") from latest version.
            if bill.versions and not_archive_year:
                version = bill.versions[-1]
                nsmap = version.xml.nsmap
                xpath = ""//caml:DigestText/xhtml:p""
                els = version.xml.xpath(xpath, namespaces=nsmap)
                chunks = []
                for el in els:
                    t = etree_text_content(el)
                    t = re.sub(r""\s+"", "" "", t)
                    t = re.sub(r""\)(\S)"", lambda m: "") %s"" % m.group(1), t)
                    chunks.append(t)
                summary = ""\n\n"".join(chunks)

            for version in bill.versions:
                if not version.bill_xml:
                    continue

                version_date = self._tz.localize(version.bill_version_action_date)

                # create a version name to match the state's format
                # 02/06/17 - Enrolled
                version_date_human = version_date.strftime(""%m/%d/%y"")
                version_name = ""{} - {}"".format(
                    version_date_human, version.bill_version_action
                )

                version_base = ""https://leginfo.legislature.ca.gov/faces""

                version_url_pdf = ""{}/billPdf.xhtml?bill_id={}&version={}"".format(
                    version_base, version.bill_id, version.bill_version_id
                )

                fsbill.add_version_link(
                    version_name,
                    version_url_pdf,
                    media_type=""application/pdf"",
                    date=version_date.date(),
                )

                # CA is inconsistent in that some bills have a short title
                # that is longer, more descriptive than title.
                if bill.measure_type in (""AB"", ""SB""):
                    impact_clause = clean_title(version.title)
                    title = clean_title(version.short_title)
                else:
                    impact_clause = None
                    if len(version.title) < len(
                        version.short_title
                    ) and not version.title.lower().startswith(""an act""):
                        title = clean_title(version.short_title)
                    else:
                        title = clean_title(version.title)

                if title:
                    all_titles.add(title)

                type_ = [bill_type]

                if version.appropriation == ""Yes"":
                    type_.append(""appropriation"")

                tags = []
                if version.fiscal_committee == ""Yes"":
                    tags.append(""fiscal committee"")
                if version.local_program == ""Yes"":
                    tags.append(""local program"")
                if version.urgency == ""Yes"":
                    tags.append(""urgency"")
                if version.taxlevy == ""Yes"":
                    tags.append(""tax levy"")

                if version.subject:
                    subject = clean_title(version.subject)

            if not title:
                self.warning(""Couldn't find title for %s, skipping"" % bill_id)
                continue

            fsbill.title = title
            if summary:
                fsbill.add_abstract(summary, note=""summary"")
            fsbill.classification = type_
            fsbill.subject = [subject] if subject else []
            fsbill.extras[""impact_clause""] = impact_clause
            fsbill.extras[""tags""] = tags

            # We don't want the current title in alternate_titles
            all_titles.remove(title)

            for title in all_titles:
                fsbill.add_title(title)

            for author in version.authors:
                fsbill.add_sponsorship(
                    author.name,
                    classification=SPONSOR_TYPES[author.contribution],
                    primary=author.primary_author_flg == ""Y"",
                    entity_type=""person"",
                )
                # fsbill.sponsorships[-1]['extras'] = {'official_type': author.contribution}

            seen_actions = set()
            for action in bill.actions:
                if not action.action:
                    # NULL action text seems to be an error on CA's part,
                    # unless it has some meaning I'm missing
                    continue
                actor = action.actor or chamber
                actor = actor.strip()
                match = re.match(r""(Assembly|Senate)($| \(Floor)"", actor)
                if match:
                    actor = {""Assembly"": ""lower"", ""Senate"": ""upper""}[match.group(1)]
                elif actor.startswith(""Governor""):
                    actor = ""executive""
                else:

                    def replacer(matchobj):
                        if matchobj:
                            return {""Assembly"": ""lower"", ""Senate"": ""upper""}[
                                matchobj.group()
                            ]
                        else:
                            return matchobj.group()

                    actor = re.sub(r""^(Assembly|Senate)"", replacer, actor)

                type_ = []

                act_str = action.action
                act_str = re.sub(r""\s+"", "" "", act_str)

                attrs = self.categorizer.categorize(act_str)

                # Add in the committee strings of the related committees, if any.
                kwargs = attrs
                matched_abbrs = committee_abbr_regex.findall(action.action)

                if re.search(r""Com[s]?. on"", action.action) and not matched_abbrs:
                    msg = ""Failed to extract committee abbr from %r.""
                    self.logger.warning(msg % action.action)

                if matched_abbrs:
                    committees = []
                    for abbr in matched_abbrs:
                        try:
                            name = self.committee_abbr_to_name(chamber, abbr)
                            committees.append(name)
                        except KeyError:
                            msg = (
                                ""Mapping contains no committee name for ""
                                ""abbreviation %r. Action text was %r.""
                            )
                            args = (abbr, action.action)
                            self.warning(msg % args)

                    committees = filter(None, committees)
                    kwargs[""committees""] = committees

                    code = re.search(r""C[SXZ]\d+"", actor)
                    if code is not None:
                        code = code.group()
                        kwargs[""actor_info""] = {""committee_code"": code}
                    if not_archive_year:
                        assert len(list(committees)) == len(matched_abbrs)
                    for committee, abbr in zip(committees, matched_abbrs):
                        act_str = act_str.replace(""Coms. on "", """")
                        act_str = act_str.replace(""Com. on "" + abbr, committee)
                        act_str = act_str.replace(abbr, committee)
                        if not act_str.endswith("".""):
                            act_str = act_str + "".""

                # Determine which chamber the action originated from.
                changed = False
                for committee_chamber in [""upper"", ""lower"", ""legislature""]:
                    if actor.startswith(committee_chamber):
                        actor = committee_chamber
                        changed = True
                        break
                if not changed:
                    actor = ""legislature""

                if actor != action.actor:
                    actor_info = kwargs.get(""actor_info"", {})
                    actor_info[""details""] = action.actor
                    kwargs[""actor_info""] = actor_info

                # Add strings for related legislators, if any.
                rgx = r""(?:senator|assembly[mwp][^ .,:;]+)\s+[^ .,:;]+""
                legislators = re.findall(rgx, action.action, re.I)
                if legislators:
                    kwargs[""legislators""] = legislators

                date = action.action_date
                date = self._tz.localize(date)
                date = date.date()
                if (actor, act_str, date) in seen_actions:
                    continue

                kwargs.update(self.categorizer.categorize(act_str))

                action = fsbill.add_action(
                    act_str,
                    date.strftime(""%Y-%m-%d""),
                    chamber=actor,
                    classification=kwargs[""classification""],
                )
                for committee in kwargs.get(""committees"", []):
                    action.add_related_entity(committee, entity_type=""organization"")
                seen_actions.add((actor, act_str, date))

            source_url = (
                ""http://leginfo.legislature.ca.gov/faces/billVotesClient.xhtml?""
            )
            source_url += f""bill_id={session}{bill.session_num}{fsbill.identifier}""

            # Votes for non archived years
            if archive_year > 2009:
                for vote_num, vote in enumerate(bill.votes):
                    if vote.vote_result == ""(PASS)"":
                        result = True
                    else:
                        result = False

                    if not vote.location:
                        continue

                    full_loc = vote.location.description
                    first_part = full_loc.split("" "")[0].lower()
                    if first_part in [""asm"", ""assembly""]:
                        vote_chamber = ""lower""
                        # vote_location = ' '.join(full_loc.split(' ')[1:])
                    elif first_part.startswith(""sen""):
                        vote_chamber = ""upper""
                        # vote_location = ' '.join(full_loc.split(' ')[1:])
                    else:
                        # raise ScrapeError(""Bad location: %s"" % full_loc) # To uncomment
                        continue

                    if vote.motion:
                        motion = vote.motion.motion_text or """"
                    else:
                        motion = """"

                    if ""Third Reading"" in motion or ""3rd Reading"" in motion:
                        vtype = ""passage""
                    elif ""Do Pass"" in motion:
                        vtype = ""passage""
                    else:
                        vtype = []

                    motion = motion.strip()
                    motion = re.compile(
                        r""(\w+)( Extraordinary)? Session$"", re.IGNORECASE
                    ).sub("""", motion)
                    motion = re.compile(r""^(Senate|Assembly) "", re.IGNORECASE).sub(
                        """", motion
                    )
                    motion = re.sub(
                        r""^(SCR|SJR|SB|AB|AJR|ACR)\s?\d+ \w+\.?  "", """", motion
                    )
                    motion = re.sub(r"" \(\w+\)$"", """", motion)
                    motion = re.sub(r""(SCR|SB|AB|AJR|ACR)\s?\d+ \w+\.?$"", """", motion)
                    motion = re.sub(
                        r""(SCR|SJR|SB|AB|AJR|ACR)\s?\d+ \w+\.? "" r""Urgency Clause$"",
                        ""(Urgency Clause)"",
                        motion,
                    )
                    motion = re.sub(r""\s+"", "" "", motion)

                    if not motion:
                        self.warning(""Got blank motion on vote for %s"" % bill_id)
                        continue

                    # XXX this is responsible for all the CA 'committee' votes, not
                    # sure if that's a feature or bug, so I'm leaving it as is...
                    # vote_classification = chamber if (vote_location == 'Floor') else 'committee'
                    # org = {
                    # 'name': vote_location,
                    # 'classification': vote_classification
                    # }

                    fsvote = VoteEvent(
                        motion_text=motion,
                        start_date=self._tz.localize(vote.vote_date_time),
                        result=""pass"" if result else ""fail"",
                        classification=vtype,
                        # organization=org,
                        chamber=vote_chamber,
                        bill=fsbill,
                    )
                    fsvote.extras = {""threshold"": vote.threshold}

                    fsvote.add_source(source_url)
                    fsvote.dedupe_key = source_url + ""#"" + str(vote_num)

                    rc = {""yes"": [], ""no"": [], ""other"": []}
                    for record in vote.votes:
                        if record.vote_code == ""AYE"":
                            rc[""yes""].append(record.legislator_name)
                        elif record.vote_code.startswith(""NO""):
                            rc[""no""].append(record.legislator_name)
                        else:
                            rc[""other""].append(record.legislator_name)

                    # Handle duplicate votes
                    for key in rc:
                        rc[key] = list(set(rc[key]))

                    for key, voters in rc.items():
                        for voter in voters:
                            fsvote.vote(key, voter)
                        # Set counts by summed votes for accuracy
                        fsvote.set_count(key, len(voters))

                    yield fsvote
            if len(bill.votes) > 0 and archive_year <= 2009:
                vote_page_url = (
                    ""http://leginfo.legislature.ca.gov/faces/billVotesClient.xhtml?""
                )
                vote_page_url += (
                    f""bill_id={session}{bill.session_num}{fsbill.identifier}""
                )

                # parse the bill data page, finding the latest html text
                data = self.get(vote_page_url).content
                doc = html.fromstring(data)
                doc.make_links_absolute(vote_page_url)
                num_of_votes = len(doc.xpath(""//div[@class='status']""))
                for vote_section in range(1, num_of_votes + 1):
                    lines = doc.xpath(
                        f""//div[@class='status'][{vote_section}]//div[@class='statusRow']""
                    )
                    date, result, motion, vtype, location = """", """", """", """", """"
                    votes = {}
                    for line in lines:
                        line = line.text_content().split()
                        if line[0] == ""Date"":
                            date = line[1]
                            date = datetime.datetime.strptime(date, ""%m/%d/%y"")
                            date = self._tz.localize(date)
                        elif line[0] == ""Result"":
                            result = ""pass"" if ""PASS"" in line[1] else ""fail""
                        elif line[0] == ""Motion"":
                            motion = "" "".join(line[1:])
                        elif line[0] == ""Location"":
                            location = "" "".join(line[1:])
                        elif len(line) > 1:
                            if line[0] == ""Ayes"" and line[1] != ""Count"":
                                votes[""yes""] = line[1:]
                            elif line[0] == ""Noes"" and line[1] != ""Count"":
                                votes[""no""] = line[1:]
                            elif line[0] == ""NVR"" and line[1] != ""Count"":
                                votes[""not voting""] = line[1:]
                    # Determine chamber based on location
                    first_part = location.split("" "")[0].lower()
                    vote_chamber = """"
                    if first_part in [""asm"", ""assembly""]:
                        vote_chamber = ""lower""
                    elif first_part.startswith(""sen""):
                        vote_chamber = ""upper""

                    if ""Third Reading"" in motion or ""3rd Reading"" in motion:
                        vtype = ""passage""
                    elif ""Do Pass"" in motion:
                        vtype = ""passage""
                    else:
                        vtype = ""other""
                    if len(motion) > 0:
                        fsvote = VoteEvent(
                            motion_text=motion,
                            start_date=date,
                            result=result,
                            classification=vtype,
                            chamber=vote_chamber,
                            bill=fsbill,
                        )
                        fsvote.add_source(vote_page_url)
                        fsvote.dedupe_key = vote_page_url + ""#"" + str(vote_section)

                        for how_voted, voters in votes.items():
                            for voter in voters:
                                voter = voter.replace("","", """")
                                fsvote.vote(how_voted, voter)
                        yield fsvote

            for analysis in bill.analyses:
                analysis_date = self._tz.localize(analysis.analysis_date)
                # create an analysis name to match the state's format
                # 05/31/20- Assembly Appropriations
                analysis_date_human = analysis_date.strftime(""%m/%d/%y"")
                analysis_name = ""{}- {}"".format(
                    analysis_date_human, analysis.committee_name
                )

                analysis_base = ""https://leginfo.legislature.ca.gov/faces""

                # unfortunately this just brings you to the analysis list
                # storing analysisId and analyzingOffice for a future POST request?
                analysis_url_pdf = ""{}/billAnalysisClient.xhtml?bill_id={}&analysisId={}&analyzingOffice={}"".format(
                    analysis_base,
                    analysis.bill_id,
                    analysis.analysis_id,
                    analysis.committee_name.replace("" "", ""+""),
                )

                fsbill.add_document_link(
                    analysis_name,
                    analysis_url_pdf,
                    classification=""analysis"",
                    date=analysis_date.date(),
                    media_type=""application/pdf"",
                    on_duplicate=""ignore"",
                )

            yield fsbill
            self.session.expire_all()",_10831.py,223,"for committee_chamber in ['upper', 'lower', 'legislature']:
    if actor.startswith(committee_chamber):
        actor = committee_chamber
        changed = True
        break","if not changed:
    actor = 'legislature'","for committee_chamber in ['upper', 'lower', 'legislature']:
    if actor.startswith(committee_chamber):
        actor = committee_chamber
        break
else:
    actor = 'legislature'"
https://github.com/ansible/galaxy/tree/master/lib/galaxy/tools/data_manager/manager.py,"def remove_manager(self, manager_ids):
        if not isinstance(manager_ids, list):
            manager_ids = [manager_ids]
        for manager_id in manager_ids:
            data_manager = self.get_manager(manager_id, None)
            if data_manager is not None:
                del self.data_managers[manager_id]
                # remove tool from toolbox
                if data_manager.tool:
                    self.app.toolbox.remove_tool_by_id(data_manager.tool.id)
                # determine if any data_tables are no longer tracked
                for data_table_name in data_manager.data_tables.keys():
                    remove_data_table_tracking = True
                    for other_data_manager in self.data_managers.values():
                        if data_table_name in other_data_manager.data_tables:
                            remove_data_table_tracking = False
                            break
                    if remove_data_table_tracking and data_table_name in self.managed_data_tables:
                        del self.managed_data_tables[data_table_name]",_109721.py,14,"for other_data_manager in self.data_managers.values():
    if data_table_name in other_data_manager.data_tables:
        remove_data_table_tracking = False
        break","if remove_data_table_tracking and data_table_name in self.managed_data_tables:
    del self.managed_data_tables[data_table_name]","for other_data_manager in self.data_managers.values():
    if data_table_name in other_data_manager.data_tables:
        remove_data_table_tracking = False
        break
else: 
    if data_table_name in self.managed_data_tables:
        del self.managed_data_tables[data_table_name]"
https://github.com/douban/dpark/tree/master/dpark/broadcast.py,"def fetch_blocks(self, uuid, compressed_size):
        if uuid in self.shared_uuid_fn_dict:
            return self._get_blocks_by_filename(self.shared_uuid_fn_dict[uuid],
                                                self.shared_uuid_map_dict[uuid])
        download_sock = self.ctx.socket(zmq.REQ)
        download_sock.connect(self.download_addr)
        download_sock.send_pyobj((DATA_GET,
                                  (uuid, compressed_size)))
        res = download_sock.recv_pyobj()
        if res == DATA_GET_OK:
            return self._get_blocks_by_filename(self.shared_uuid_fn_dict[uuid],
                                                self.shared_uuid_map_dict[uuid])
        if res == DATA_GET_FAIL:
            raise RuntimeError('Data GET failed for uuid:%s' % uuid)
        while True:
            with self.download_cond:
                if uuid not in self.shared_uuid_fn_dict:
                    self.download_cond.wait()
                else:
                    break
        if uuid in self.shared_uuid_fn_dict:
            return self._get_blocks_by_filename(self.shared_uuid_fn_dict[uuid],
                                                self.shared_uuid_map_dict[uuid])
        else:
            raise RuntimeError('get blocks failed')",_112330.py,15,"while True:
    with self.download_cond:
        if uuid not in self.shared_uuid_fn_dict:
            self.download_cond.wait()
        else:
            break","if uuid in self.shared_uuid_fn_dict:
    return self._get_blocks_by_filename(self.shared_uuid_fn_dict[uuid], self.shared_uuid_map_dict[uuid])
else:
    raise RuntimeError('get blocks failed')","while True:
    with self.download_cond:
        if uuid not in self.shared_uuid_fn_dict:
            self.download_cond.wait()
        else:
            return self._get_blocks_by_filename(self.shared_uuid_fn_dict[uuid], self.shared_uuid_map_dict[uuid])
            break
else:
    raise RuntimeError('get blocks failed')"
https://github.com/huggingface/transformers/tree/master/src/transformers/models/squeezebert/tokenization_squeezebert.py,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, `input = ""unaffable""` wil return as output `[""un"", ""##aff"", ""##able""]`.

        Args:
            text: A single token or whitespace separated tokens. This should have
                already been passed through *BasicTokenizer*.

        Returns:
            A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens",_112783.py,26,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end","if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)"
https://github.com/huggingface/transformers/tree/master/src/transformers/models/squeezebert/tokenization_squeezebert.py,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, `input = ""unaffable""` wil return as output `[""un"", ""##aff"", ""##able""]`.

        Args:
            text: A single token or whitespace separated tokens. This should have
                already been passed through *BasicTokenizer*.

        Returns:
            A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens",_112783.py,29,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1","if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break"
https://github.com/indico/indico/tree/master/bin/maintenance/build-wheel.py,"def compile_catalogs():
    path = None
    # find ./xxx/translations/ with at least one subdir
    for root, dirs, files in os.walk('.'):
        segments = root.split(os.sep)
        if segments[-1] == 'translations' and len(segments) == 3 and dirs:
            path = root
            break
    if path is None:
        noop('plugin has no translations')
        return
    info('compiling translations')
    try:
        subprocess.check_output([sys.executable, 'setup.py', 'compile_catalog', '-d', path],
                                stderr=subprocess.STDOUT)
    except subprocess.CalledProcessError as exc:
        fail('compile_catalog failed', verbose_msg=exc.output)",_112931.py,4,"for (root, dirs, files) in os.walk('.'):
    segments = root.split(os.sep)
    if segments[-1] == 'translations' and len(segments) == 3 and dirs:
        path = root
        break","if path is None:
    noop('plugin has no translations')
    return","for (root, dirs, files) in os.walk('.'):
    segments = root.split(os.sep)
    if segments[-1] == 'translations' and len(segments) == 3 and dirs:
        path = root
        break
else:
    noop('plugin has no translations')
    return"
https://github.com/ThilinaRajapakse/simpletransformers/tree/master/simpletransformers/question_answering/question_answering_utils.py,"def get_best_predictions_extended(
    all_examples,
    all_features,
    all_results,
    n_best_size,
    max_answer_length,
    start_n_top,
    end_n_top,
    version_2_with_negative,
    tokenizer,
    verbose_logging,
):
    """"""XLNet write prediction logic (more complex than Bert's).
    Write final predictions to the json file and log-odds of null if needed.
    Requires utils_squad_evaluate.py
    """"""
    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name
        ""PrelimPrediction"",
        [""feature_index"", ""start_index"", ""end_index"", ""start_log_prob"", ""end_log_prob""],
    )

    _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name
        ""NbestPrediction"", [""text"", ""start_log_prob"", ""end_log_prob""]
    )

    example_index_to_features = collections.defaultdict(list)
    for feature in all_features:
        example_index_to_features[feature.example_index].append(feature)

    unique_id_to_result = {}
    for result in all_results:
        unique_id_to_result[result.unique_id] = result

    all_predictions = collections.OrderedDict()
    all_nbest_json = collections.OrderedDict()
    scores_diff_json = collections.OrderedDict()

    for (example_index, example) in enumerate(all_examples):
        features = example_index_to_features[example_index]

        prelim_predictions = []
        # keep track of the minimum score of null start+end of position 0
        score_null = 1000000  # large and positive

        for (feature_index, feature) in enumerate(features):
            result = unique_id_to_result[feature.unique_id]

            cur_null_score = result.cls_logits

            # if we could have irrelevant answers, get the min score of irrelevant
            score_null = min(score_null, cur_null_score)

            for i in range(start_n_top):
                for j in range(end_n_top):
                    start_log_prob = result.start_top_log_probs[i]
                    start_index = result.start_top_index[i]

                    j_index = i * end_n_top + j

                    end_log_prob = result.end_top_log_probs[j_index]
                    end_index = result.end_top_index[j_index]

                    # We could hypothetically create invalid predictions, e.g., predict
                    # that the start of the span is in the question. We throw out all
                    # invalid predictions.
                    if start_index >= feature.paragraph_len - 1:
                        continue
                    if end_index >= feature.paragraph_len - 1:
                        continue

                    if not feature.token_is_max_context.get(start_index, False):
                        continue
                    if end_index < start_index:
                        continue
                    length = end_index - start_index + 1
                    if length > max_answer_length:
                        continue

                    prelim_predictions.append(
                        _PrelimPrediction(
                            feature_index=feature_index,
                            start_index=start_index,
                            end_index=end_index,
                            start_log_prob=start_log_prob,
                            end_log_prob=end_log_prob,
                        )
                    )

        prelim_predictions = sorted(
            prelim_predictions,
            key=lambda x: (x.start_log_prob + x.end_log_prob),
            reverse=True,
        )

        seen_predictions = {}
        nbest = []
        for pred in prelim_predictions:
            if len(nbest) >= n_best_size:
                break
            feature = features[pred.feature_index]

            # XLNet un-tokenizer
            # Let's keep it simple for now and see if we need all this later.
            #
            # tok_start_to_orig_index = feature.tok_start_to_orig_index
            # tok_end_to_orig_index = feature.tok_end_to_orig_index
            # start_orig_pos = tok_start_to_orig_index[pred.start_index]
            # end_orig_pos = tok_end_to_orig_index[pred.end_index]
            # paragraph_text = example.paragraph_text
            # final_text = paragraph_text[start_orig_pos: end_orig_pos + 1].strip()

            # Previously used Bert untokenizer
            tok_tokens = feature.tokens[pred.start_index : (pred.end_index + 1)]
            orig_doc_start = feature.token_to_orig_map[pred.start_index]
            orig_doc_end = feature.token_to_orig_map[pred.end_index]
            orig_tokens = example.doc_tokens[orig_doc_start : (orig_doc_end + 1)]
            tok_text = tokenizer.convert_tokens_to_string(tok_tokens)

            # Clean whitespace
            tok_text = tok_text.strip()
            tok_text = "" "".join(tok_text.split())
            orig_text = "" "".join(orig_tokens)

            if isinstance(tokenizer, XLMTokenizer):
                final_text = get_final_text(tok_text, orig_text, verbose_logging)
            else:
                final_text = get_final_text(
                    tok_text, orig_text, tokenizer.do_lower_case, verbose_logging
                )

            if final_text in seen_predictions:
                continue

            seen_predictions[final_text] = True

            nbest.append(
                _NbestPrediction(
                    text=final_text,
                    start_log_prob=pred.start_log_prob,
                    end_log_prob=pred.end_log_prob,
                )
            )

        # In very rare edge cases we could have no valid predictions. So we
        # just create a nonce prediction in this case to avoid failure.
        if not nbest:
            nbest.append(
                _NbestPrediction(text="""", start_log_prob=-1e6, end_log_prob=-1e6)
            )

        total_scores = []
        best_non_null_entry = None
        for entry in nbest:
            total_scores.append(entry.start_log_prob + entry.end_log_prob)
            if not best_non_null_entry:
                best_non_null_entry = entry

        probs = _compute_softmax(total_scores)

        nbest_json = []
        for (i, entry) in enumerate(nbest):
            output = collections.OrderedDict()
            output[""text""] = entry.text
            output[""probability""] = probs[i]
            output[""start_log_prob""] = entry.start_log_prob
            output[""end_log_prob""] = entry.end_log_prob
            nbest_json.append(output)

        assert len(nbest_json) >= 1
        assert best_non_null_entry is not None

        score_diff = score_null
        scores_diff_json[example.qas_id] = score_diff
        # note(zhiliny): always predict best_non_null_entry
        # and the evaluation script will search for the best threshold
        all_predictions[example.qas_id] = best_non_null_entry.text

        all_nbest_json[example.qas_id] = nbest_json

        all_best = [
            {
                ""id"": id,
                ""answer"": [answer[""text""] for answer in answers],
                ""probability"": [answer[""probability""] for answer in answers],
            }
            for id, answers in all_nbest_json.items()
        ]
    return all_best",_113225.py,97,"for pred in prelim_predictions:
    if len(nbest) >= n_best_size:
        break
    feature = features[pred.feature_index]
    tok_tokens = feature.tokens[pred.start_index:pred.end_index + 1]
    orig_doc_start = feature.token_to_orig_map[pred.start_index]
    orig_doc_end = feature.token_to_orig_map[pred.end_index]
    orig_tokens = example.doc_tokens[orig_doc_start:orig_doc_end + 1]
    tok_text = tokenizer.convert_tokens_to_string(tok_tokens)
    tok_text = tok_text.strip()
    tok_text = ' '.join(tok_text.split())
    orig_text = ' '.join(orig_tokens)
    if isinstance(tokenizer, XLMTokenizer):
        final_text = get_final_text(tok_text, orig_text, verbose_logging)
    else:
        final_text = get_final_text(tok_text, orig_text, tokenizer.do_lower_case, verbose_logging)
    if final_text in seen_predictions:
        continue
    seen_predictions[final_text] = True
    nbest.append(_NbestPrediction(text=final_text, start_log_prob=pred.start_log_prob, end_log_prob=pred.end_log_prob))","if not nbest:
    nbest.append(_NbestPrediction(text='', start_log_prob=-1000000.0, end_log_prob=-1000000.0))","for pred in prelim_predictions:
    if len(nbest) >= n_best_size:
        break
    feature = features[pred.feature_index]
    tok_tokens = feature.tokens[pred.start_index:pred.end_index + 1]
    orig_doc_start = feature.token_to_orig_map[pred.start_index]
    orig_doc_end = feature.token_to_orig_map[pred.end_index]
    orig_tokens = example.doc_tokens[orig_doc_start:orig_doc_end + 1]
    tok_text = tokenizer.convert_tokens_to_string(tok_tokens)
    tok_text = tok_text.strip()
    tok_text = ' '.join(tok_text.split())
    orig_text = ' '.join(orig_tokens)
    if isinstance(tokenizer, XLMTokenizer):
        final_text = get_final_text(tok_text, orig_text, verbose_logging)
    else:
        final_text = get_final_text(tok_text, orig_text, tokenizer.do_lower_case, verbose_logging)
    if final_text in seen_predictions:
        continue
    seen_predictions[final_text] = True
    nbest.append(_NbestPrediction(text=final_text, start_log_prob=pred.start_log_prob, end_log_prob=pred.end_log_prob))
else:
    nbest.append(_NbestPrediction(text='', start_log_prob=-1000000.0, end_log_prob=-1000000.0))"
https://github.com/huggingface/transformers/tree/master/examples/research_projects/lxmert/extracting_data.py,"def __call__(self):
        # make writer
        if not TEST:
            writer = datasets.ArrowWriter(features=self.schema, path=self.outputfile)
        # do file generator
        for i, (img_ids, filepaths) in enumerate(self.file_generator):
            images, sizes, scales_yx = self.preprocess(filepaths)
            output_dict = self.model(
                images,
                sizes,
                scales_yx=scales_yx,
                padding=""max_detections"",
                max_detections=self.config.MAX_DETECTIONS,
                pad_value=0,
                return_tensors=""np"",
                location=""cpu"",
            )
            output_dict[""boxes""] = output_dict.pop(""normalized_boxes"")
            if not TEST:
                output_dict[""img_id""] = np.array(img_ids)
                batch = self.schema.encode_batch(output_dict)
                writer.write_batch(batch)
            if TEST:
                break
            # finalizer the writer
        if not TEST:
            num_examples, num_bytes = writer.finalize()
            print(f""Success! You wrote {num_examples} entry(s) and {num_bytes >> 20} mb"")",_113714.py,6,"for (i, (img_ids, filepaths)) in enumerate(self.file_generator):
    (images, sizes, scales_yx) = self.preprocess(filepaths)
    output_dict = self.model(images, sizes, scales_yx=scales_yx, padding='max_detections', max_detections=self.config.MAX_DETECTIONS, pad_value=0, return_tensors='np', location='cpu')
    output_dict['boxes'] = output_dict.pop('normalized_boxes')
    if not TEST:
        output_dict['img_id'] = np.array(img_ids)
        batch = self.schema.encode_batch(output_dict)
        writer.write_batch(batch)
    if TEST:
        break","if not TEST:
    (num_examples, num_bytes) = writer.finalize()
    print(f'Success! You wrote {num_examples} entry(s) and {num_bytes >> 20} mb')","for (i, (img_ids, filepaths)) in enumerate(self.file_generator):
    (images, sizes, scales_yx) = self.preprocess(filepaths)
    output_dict = self.model(images, sizes, scales_yx=scales_yx, padding='max_detections', max_detections=self.config.MAX_DETECTIONS, pad_value=0, return_tensors='np', location='cpu')
    output_dict['boxes'] = output_dict.pop('normalized_boxes')
    if not TEST:
        output_dict['img_id'] = np.array(img_ids)
        batch = self.schema.encode_batch(output_dict)
        writer.write_batch(batch)
    if TEST:
        break
else:
    (num_examples, num_bytes) = writer.finalize()
    print(f'Success! You wrote {num_examples} entry(s) and {num_bytes >> 20} mb')"
https://github.com/fastavro/fastavro/tree/master/fastavro/_write_py.py,"def write_union(encoder, datum, schema, named_schemas, fname):
    """"""A union is encoded by first writing a long value indicating the
    zero-based position within the union of the schema of its value. The value
    is then encoded per the indicated schema within the union.""""""

    best_match_index = -1
    if isinstance(datum, tuple):
        (name, datum) = datum
        for index, candidate in enumerate(schema):
            extracted_type = extract_record_type(candidate)
            if extracted_type in NAMED_TYPES:
                schema_name = candidate[""name""]
            else:
                schema_name = extracted_type
            if name == schema_name:
                best_match_index = index
                break

        if best_match_index == -1:
            field = f""on field {fname}"" if fname else """"
            msg = (
                f""provided union type name {name} not found in schema ""
                + f""{schema} {field}""
            )
            raise ValueError(msg)
        index = best_match_index
    else:
        pytype = type(datum)
        most_fields = -1

        # All of Python's floating point values are doubles, so to
        # avoid loss of precision, we should always prefer 'double'
        # if we are forced to choose between float and double.
        #
        # If 'double' comes before 'float' in the union, then we'll immediately
        # choose it, and don't need to worry. But if 'float' comes before
        # 'double', we don't want to pick it.
        #
        # So, if we ever see 'float', we skim through the rest of the options,
        # just to see if 'double' is a possibility, because we'd prefer it.
        could_be_float = False

        for index, candidate in enumerate(schema):

            if could_be_float:
                if extract_record_type(candidate) == ""double"":
                    best_match_index = index
                    break
                else:
                    # Nothing except ""double"" is even worth considering.
                    continue

            if _validate(datum, candidate, named_schemas, raise_errors=False):
                record_type = extract_record_type(candidate)
                if record_type == ""record"":
                    logical_type = extract_logical_type(candidate)
                    if logical_type:
                        prepare = LOGICAL_WRITERS.get(logical_type)
                        if prepare:
                            datum = prepare(datum, candidate)

                    candidate_fields = set(f[""name""] for f in candidate[""fields""])
                    datum_fields = set(datum)
                    fields = len(candidate_fields.intersection(datum_fields))
                    if fields > most_fields:
                        best_match_index = index
                        most_fields = fields
                elif record_type == ""float"":
                    best_match_index = index
                    # Continue in the loop, because it's possible that there's
                    # another candidate which has record type 'double'
                    could_be_float = True
                else:
                    best_match_index = index
                    break
        if best_match_index == -1:
            field = f""on field {fname}"" if fname else """"
            raise ValueError(
                f""{repr(datum)} (type {pytype}) do not match {schema} {field}""
            )
        index = best_match_index

    # write data
    # TODO: There should be a way to give just the index
    encoder.write_index(index, schema[index])
    write_data(encoder, datum, schema[index], named_schemas, fname)",_114404.py,9,"for (index, candidate) in enumerate(schema):
    extracted_type = extract_record_type(candidate)
    if extracted_type in NAMED_TYPES:
        schema_name = candidate['name']
    else:
        schema_name = extracted_type
    if name == schema_name:
        best_match_index = index
        break","if best_match_index == -1:
    field = f'on field {fname}' if fname else ''
    msg = f'provided union type name {name} not found in schema ' + f'{schema} {field}'
    raise ValueError(msg)","for (index, candidate) in enumerate(schema):
    extracted_type = extract_record_type(candidate)
    if extracted_type in NAMED_TYPES:
        schema_name = candidate['name']
    else:
        schema_name = extracted_type
    if name == schema_name:
        best_match_index = index
        break
else:
    field = f'on field {fname}' if fname else ''
    msg = f'provided union type name {name} not found in schema ' + f'{schema} {field}'
    raise ValueError(msg)"
https://github.com/fastavro/fastavro/tree/master/fastavro/_write_py.py,"def write_union(encoder, datum, schema, named_schemas, fname):
    """"""A union is encoded by first writing a long value indicating the
    zero-based position within the union of the schema of its value. The value
    is then encoded per the indicated schema within the union.""""""

    best_match_index = -1
    if isinstance(datum, tuple):
        (name, datum) = datum
        for index, candidate in enumerate(schema):
            extracted_type = extract_record_type(candidate)
            if extracted_type in NAMED_TYPES:
                schema_name = candidate[""name""]
            else:
                schema_name = extracted_type
            if name == schema_name:
                best_match_index = index
                break

        if best_match_index == -1:
            field = f""on field {fname}"" if fname else """"
            msg = (
                f""provided union type name {name} not found in schema ""
                + f""{schema} {field}""
            )
            raise ValueError(msg)
        index = best_match_index
    else:
        pytype = type(datum)
        most_fields = -1

        # All of Python's floating point values are doubles, so to
        # avoid loss of precision, we should always prefer 'double'
        # if we are forced to choose between float and double.
        #
        # If 'double' comes before 'float' in the union, then we'll immediately
        # choose it, and don't need to worry. But if 'float' comes before
        # 'double', we don't want to pick it.
        #
        # So, if we ever see 'float', we skim through the rest of the options,
        # just to see if 'double' is a possibility, because we'd prefer it.
        could_be_float = False

        for index, candidate in enumerate(schema):

            if could_be_float:
                if extract_record_type(candidate) == ""double"":
                    best_match_index = index
                    break
                else:
                    # Nothing except ""double"" is even worth considering.
                    continue

            if _validate(datum, candidate, named_schemas, raise_errors=False):
                record_type = extract_record_type(candidate)
                if record_type == ""record"":
                    logical_type = extract_logical_type(candidate)
                    if logical_type:
                        prepare = LOGICAL_WRITERS.get(logical_type)
                        if prepare:
                            datum = prepare(datum, candidate)

                    candidate_fields = set(f[""name""] for f in candidate[""fields""])
                    datum_fields = set(datum)
                    fields = len(candidate_fields.intersection(datum_fields))
                    if fields > most_fields:
                        best_match_index = index
                        most_fields = fields
                elif record_type == ""float"":
                    best_match_index = index
                    # Continue in the loop, because it's possible that there's
                    # another candidate which has record type 'double'
                    could_be_float = True
                else:
                    best_match_index = index
                    break
        if best_match_index == -1:
            field = f""on field {fname}"" if fname else """"
            raise ValueError(
                f""{repr(datum)} (type {pytype}) do not match {schema} {field}""
            )
        index = best_match_index

    # write data
    # TODO: There should be a way to give just the index
    encoder.write_index(index, schema[index])
    write_data(encoder, datum, schema[index], named_schemas, fname)",_114404.py,43,"for (index, candidate) in enumerate(schema):
    if could_be_float:
        if extract_record_type(candidate) == 'double':
            best_match_index = index
            break
        else:
            continue
    if _validate(datum, candidate, named_schemas, raise_errors=False):
        record_type = extract_record_type(candidate)
        if record_type == 'record':
            logical_type = extract_logical_type(candidate)
            if logical_type:
                prepare = LOGICAL_WRITERS.get(logical_type)
                if prepare:
                    datum = prepare(datum, candidate)
            candidate_fields = set((f['name'] for f in candidate['fields']))
            datum_fields = set(datum)
            fields = len(candidate_fields.intersection(datum_fields))
            if fields > most_fields:
                best_match_index = index
                most_fields = fields
        elif record_type == 'float':
            best_match_index = index
            could_be_float = True
        else:
            best_match_index = index
            break","if best_match_index == -1:
    field = f'on field {fname}' if fname else ''
    raise ValueError(f'{repr(datum)} (type {pytype}) do not match {schema} {field}')","for (index, candidate) in enumerate(schema):
    if could_be_float:
        if extract_record_type(candidate) == 'double':
            best_match_index = index
            break
        else:
            continue
    if _validate(datum, candidate, named_schemas, raise_errors=False):
        record_type = extract_record_type(candidate)
        if record_type == 'record':
            logical_type = extract_logical_type(candidate)
            if logical_type:
                prepare = LOGICAL_WRITERS.get(logical_type)
                if prepare:
                    datum = prepare(datum, candidate)
            candidate_fields = set((f['name'] for f in candidate['fields']))
            datum_fields = set(datum)
            fields = len(candidate_fields.intersection(datum_fields))
            if fields > most_fields:
                best_match_index = index
                most_fields = fields
        elif record_type == 'float':
            best_match_index = index
            could_be_float = True
        else:
            best_match_index = index
            break
else:
    field = f'on field {fname}' if fname else ''
    raise ValueError(f'{repr(datum)} (type {pytype}) do not match {schema} {field}')"
https://github.com/coddingtonbear/inthe.am/tree/master/inthe_am/taskmanager/tasks.py,"def process_email_message(self, message_id):
    from .models import TaskAttachment, TaskStore

    def get_secret_id_and_args(address):
        inbox_id = address[0:36]
        args = []

        arg_string = address[36:]
        for arg in re.split(r""__|\+"", arg_string):
            if not arg:
                continue
            if ""="" in arg:
                params = arg.split(""="")
                if params[0] == ""priority"":
                    params[1] = params[1].upper()
                args.append(f'{params[0]}:""{params[1]}""')
            else:
                args.append(f""+{arg}"")

        return inbox_id, args

    message = Message.objects.get(pk=message_id)
    message.read = now()
    message.save()

    store: Optional[TaskStore] = None
    additional_args = []
    # Check for matching To: addresses.
    for address in message.to_addresses:
        try:
            inbox_id, additional_args = get_secret_id_and_args(address.split(""@"")[0])

            store = TaskStore.objects.get(secret_id=inbox_id)
            break
        except (TaskStore.DoesNotExist, IndexError):
            pass

    # Check for 'Received' headers matching a known e-mail address.
    if store is None:
        email_regex = re.compile(r""([0-9a-fA-F-]{36}@inthe.am)"")
        all_received_headers = message.get_email_object().get_all(""Received"")
        for header in all_received_headers:
            matched_email = email_regex.search(header)
            if matched_email:
                address = matched_email.group(1)
                try:
                    inbox_id, additional_args = get_secret_id_and_args(
                        address.split(""@"")[0]
                    )

                    store = TaskStore.objects.get(secret_id=inbox_id)
                    break
                except (TaskStore.DoesNotExist, IndexError):
                    pass

    if store is None:
        logger.error(
            ""Could not find task store for e-mail message (ID %s) addressed "" ""to %s"",
            message.pk,
            message.to_addresses,
        )
        return

    pubsub_message: Dict[str, Any] = {
        ""username"": store.user.username,
        ""message_id"": message.pk,
        ""subject"": message.subject,
        ""accepted"": True,
    }

    allowed = False
    for address in store.email_whitelist.split(""\n""):
        if glob(message.from_address[0], address):
            allowed = True

    if not allowed:
        log_args = (
            f""Incoming task creation e-mail (ID: {message.pk}) from ""
            f""'{message.from_address[0]}' does not match email ""
            ""whitelist and was ignored.""
        )
        logger.info(*log_args)
        store.log_message(*log_args)

        pubsub_message[""accepted""] = False
        pubsub_message[""rejection_reason""] = ""passlist""
        store.publish_announcement(""incoming_mail"", pubsub_message)
        return

    if not message.subject or message.subject.lower() in [""add"", ""create"", ""new""]:
        with git_checkpoint(
            store,
            ChangeSource.SOURCETYPE_MAIL,
            ""Incoming E-mail"",
            foreign_id=message_id,
        ):
            task_args = (
                [
                    ""add"",
                    f'intheamoriginalemailsubject:""{message.subject}""',
                    f""intheamoriginalemailid:{message.pk}"",
                ]
                + additional_args
                + shlex.split(
                    message.text.split(""\n\n"")[0]  # Only use text up to the first
                    # blank line.
                )
            )
            store.client._execute_safe(*task_args)
            task = store.client.get_task(intheamoriginalemailid=message.pk)[1]
            task_id = str(task[""uuid""])

            attachment_urls_raw = task.get(""intheamattachments"")
            if not attachment_urls_raw:
                attachment_urls = []
            else:
                attachment_urls = attachment_urls_raw.split(""|"")

            for record in message.attachments.all():
                attachment = record.document
                if attachment.file.size > settings.FILE_UPLOAD_MAXIMUM_BYTES:
                    logger.info(
                        ""File %s too large (%s bytes)."",
                        attachment.file.name,
                        attachment.file.size,
                    )
                    store.log_message(
                        ""Attachments must be smaller than ""
                        f""{settings.FILE_UPLOAD_MAXIMUM_BYTES} ""
                        ""bytes to be saved to a task, but the ""
                        f""attachment {attachment.file.name} ""
                        f""received for task ID {task_id} ""
                        f""is {attachment.file.size} bytes in size ""
                        ""and was not saved as a result.""
                    )
                    attachment.delete()
                    continue

                document = TaskAttachment.objects.create(
                    store=store,
                    task_id=task_id,
                    name=record.get_filename(),
                    size=attachment.file.size,
                )
                document.document.save(
                    record.get_filename(),
                    attachment.file,
                )
                attachment_urls.append(document.document.url)
                store.client.task_annotate(
                    task, f""Attached File: {document.document.url}""
                )

            if attachment_urls:
                task[""intheamattachments""] = "" "".join(attachment_urls)
                store.client.task_update(task)

        log_args = (
            f""Added task {task_id} via e-mail {message.pk} ""
            f""from {message.from_address[0]}.""
        )
        logger.info(*log_args)
        store.log_message(*log_args)

        pubsub_message[""task_id""] = task_id
        store.publish_announcement(""incoming_mail"", pubsub_message)
    else:
        log_args = (
            f""Unable to process e-mail {message.pk} ""
            f""from {message.from_address[0]}; ""
            f""unknown subject '{message.subject}': {message.text}""
        )
        logger.info(*log_args)
        store.log_message(*log_args)

        pubsub_message[""accepted""] = False
        pubsub_message[""rejection_reason""] = ""subject""
        store.publish_announcement(""incoming_mail"", pubsub_message)",_114646.py,29,"for address in message.to_addresses:
    try:
        (inbox_id, additional_args) = get_secret_id_and_args(address.split('@')[0])
        store = TaskStore.objects.get(secret_id=inbox_id)
        break
    except (TaskStore.DoesNotExist, IndexError):
        pass","if store is None:
    email_regex = re.compile('([0-9a-fA-F-]{36}@inthe.am)')
    all_received_headers = message.get_email_object().get_all('Received')
    for header in all_received_headers:
        matched_email = email_regex.search(header)
        if matched_email:
            address = matched_email.group(1)
            try:
                (inbox_id, additional_args) = get_secret_id_and_args(address.split('@')[0])
                store = TaskStore.objects.get(secret_id=inbox_id)
                break
            except (TaskStore.DoesNotExist, IndexError):
                pass","for address in message.to_addresses:
    try:
        (inbox_id, additional_args) = get_secret_id_and_args(address.split('@')[0])
        store = TaskStore.objects.get(secret_id=inbox_id)
        break
    except (TaskStore.DoesNotExist, IndexError):
        pass
else:
    email_regex = re.compile('([0-9a-fA-F-]{36}@inthe.am)')
    all_received_headers = message.get_email_object().get_all('Received')
    for header in all_received_headers:
        matched_email = email_regex.search(header)
        if matched_email:
            address = matched_email.group(1)
            try:
                (inbox_id, additional_args) = get_secret_id_and_args(address.split('@')[0])
                store = TaskStore.objects.get(secret_id=inbox_id)
                break
            except (TaskStore.DoesNotExist, IndexError):
                pass"
https://github.com/nltk/nltk/tree/master/nltk/app/chartparser_app.py,"def view_edge(self, edge):
        level = None
        for i in range(len(self._edgelevels)):
            if edge in self._edgelevels[i]:
                level = i
                break
        if level is None:
            return
        # Try to view the new edge..
        y = (level + 1) * self._chart_level_size
        dy = self._text_height + 10
        self._chart_canvas.yview(""moveto"", 1.0)
        if self._chart_height != 0:
            self._chart_canvas.yview(""moveto"", (y - dy) / self._chart_height)",_114688.py,3,"for i in range(len(self._edgelevels)):
    if edge in self._edgelevels[i]:
        level = i
        break","if level is None:
    return","for i in range(len(self._edgelevels)):
    if edge in self._edgelevels[i]:
        level = i
        break
else:
    return"
https://github.com/gpodder/gpodder/tree/master/src/gpodder/dbusproxy.py,"def get_episodes(self, url):
        """"""Return all episodes of the podcast with the given URL""""""
        podcast = None
        for channel in self._get_podcasts():
            if channel.url == url:
                podcast = channel
                break

        if podcast is None:
            return []

        def episode_to_tuple(episode):
            title = safe_str(episode.title)
            url = safe_str(episode.url)
            description = safe_first_line(episode.description)
            filename = safe_str(episode.download_filename)
            file_type = safe_str(episode.file_type())
            is_new = (episode.state == gpodder.STATE_NORMAL and episode.is_new)
            is_downloaded = episode.was_downloaded(and_exists=True)
            is_deleted = (episode.state == gpodder.STATE_DELETED)

            return (title, url, description, filename, file_type, is_new, is_downloaded, is_deleted)

        return [episode_to_tuple(e) for e in podcast.get_all_episodes()]",_115174.py,4,"for channel in self._get_podcasts():
    if channel.url == url:
        podcast = channel
        break","if podcast is None:
    return []","for channel in self._get_podcasts():
    if channel.url == url:
        podcast = channel
        break
else:
    return []"
https://github.com/shadowsocksr-backup/shadowsocksr/tree/master/shadowsocks/udprelay.py,"def _handle_server(self):
        server = self._server_socket
        data, r_addr = server.recvfrom(BUF_SIZE)
        ogn_data = data
        if not data:
            logging.debug('UDP handle_server: data is empty')
        if self._stat_callback:
            self._stat_callback(self._listen_port, len(data))
        if self._is_local:
            frag = common.ord(data[2])
            if frag != 0:
                logging.warn('drop a message since frag is not 0')
                return
            else:
                data = data[3:]
        else:
            data = encrypt.encrypt_all(self._password, self._method, 0, data)
            # decrypt data
            if not data:
                logging.debug('UDP handle_server: data is empty after decrypt')
                return

        #logging.info(""UDP data %s"" % (binascii.hexlify(data),))
        if not self._is_local:
            data = pre_parse_header(data)

            data = self._pre_parse_udp_header(data)
            if data is None:
                return

            if type(data) is tuple:
                #(cmd, request_id, data)
                #logging.info(""UDP data %d %d %s"" % (data[0], data[1], binascii.hexlify(data[2])))
                try:
                    if data[0] == 0:
                        if len(data[2]) >= 4:
                            for i in range(64):
                                req_id = random.randint(1, 65535)
                                if req_id not in self._reqid_to_hd:
                                    break
                            if req_id in self._reqid_to_hd:
                                for i in range(64):
                                    req_id = random.randint(1, 65535)
                                    if type(self._reqid_to_hd[req_id]) is tuple:
                                        break
                            # return req id
                            self._reqid_to_hd[req_id] = (data[2][0:4], None)
                            rsp_data = self._pack_rsp_data(CMD_RSP_CONNECT, req_id, RSP_STATE_CONNECTED)
                            data_to_send = encrypt.encrypt_all(self._password, self._method, 1, rsp_data)
                            self.write_to_server_socket(data_to_send, r_addr)
                    elif data[0] == CMD_CONNECT_REMOTE:
                        if len(data[2]) > 4 and data[1] in self._reqid_to_hd:
                            # create
                            if type(self._reqid_to_hd[data[1]]) is tuple:
                                if data[2][0:4] == self._reqid_to_hd[data[1]][0]:
                                    handle = TCPRelayHandler(self, self._reqid_to_hd, self._fd_to_handlers,
                                        self._eventloop, self._server_socket,
                                        self._reqid_to_hd[data[1]][0], self._reqid_to_hd[data[1]][1],
                                        self._config, self._dns_resolver, self._is_local)
                                    self._reqid_to_hd[data[1]] = handle
                                    handle.handle_client(r_addr, CMD_CONNECT, data[1], data[2])
                                    handle.handle_client(r_addr, *data)
                                    self.update_activity(handle)
                                else:
                                    # disconnect
                                    rsp_data = self._pack_rsp_data(CMD_DISCONNECT, data[1], RSP_STATE_EMPTY)
                                    data_to_send = encrypt.encrypt_all(self._password, self._method, 1, rsp_data)
                                    self.write_to_server_socket(data_to_send, r_addr)
                            else:
                                self.update_activity(self._reqid_to_hd[data[1]])
                                self._reqid_to_hd[data[1]].handle_client(r_addr, *data)
                        else:
                            # disconnect
                            rsp_data = self._pack_rsp_data(CMD_DISCONNECT, data[1], RSP_STATE_EMPTY)
                            data_to_send = encrypt.encrypt_all(self._password, self._method, 1, rsp_data)
                            self.write_to_server_socket(data_to_send, r_addr)
                    elif data[0] > CMD_CONNECT_REMOTE and data[0] <= CMD_DISCONNECT:
                        if data[1] in self._reqid_to_hd:
                            if type(self._reqid_to_hd[data[1]]) is tuple:
                                pass
                            else:
                                self.update_activity(self._reqid_to_hd[data[1]])
                                self._reqid_to_hd[data[1]].handle_client(r_addr, *data)
                        else:
                            # disconnect
                            rsp_data = self._pack_rsp_data(CMD_DISCONNECT, data[1], RSP_STATE_EMPTY)
                            data_to_send = encrypt.encrypt_all(self._password, self._method, 1, rsp_data)
                            self.write_to_server_socket(data_to_send, r_addr)
                    return
                except Exception as e:
                    trace = traceback.format_exc()
                    logging.error(trace)
                    return

        try:
            header_result = parse_header(data)
        except:
            self._handel_protocol_error(r_addr, ogn_data)
            return

        if header_result is None:
            self._handel_protocol_error(r_addr, ogn_data)
            return
        connecttype, dest_addr, dest_port, header_length = header_result

        if self._is_local:
            server_addr, server_port = self._get_a_server()
        else:
            server_addr, server_port = dest_addr, dest_port

        addrs = self._dns_cache.get(server_addr, None)
        if addrs is None:
            addrs = socket.getaddrinfo(server_addr, server_port, 0,
                                       socket.SOCK_DGRAM, socket.SOL_UDP)
            if not addrs:
                # drop
                return
            else:
                self._dns_cache[server_addr] = addrs

        af, socktype, proto, canonname, sa = addrs[0]
        key = client_key(r_addr, af)
        client = self._cache.get(key, None)
        if not client:
            # TODO async getaddrinfo
            if self._forbidden_iplist:
                if common.to_str(sa[0]) in self._forbidden_iplist:
                    logging.debug('IP %s is in forbidden list, drop' %
                                  common.to_str(sa[0]))
                    # drop
                    return
            client = socket.socket(af, socktype, proto)
            client.setblocking(False)
            self._cache[key] = client
            self._client_fd_to_server_addr[client.fileno()] = r_addr

            self._sockets.add(client.fileno())
            self._eventloop.add(client, eventloop.POLL_IN, self)

            logging.debug('UDP port %5d sockets %d' % (self._listen_port, len(self._sockets)))

            logging.info('UDP data to %s:%d from %s:%d' %
                        (common.to_str(server_addr), server_port,
                            r_addr[0], r_addr[1]))

        if self._is_local:
            data = encrypt.encrypt_all(self._password, self._method, 1, data)
            if not data:
                return
        else:
            data = data[header_length:]
        if not data:
            return
        try:
            #logging.info('UDP handle_server sendto %s:%d %d bytes' % (common.to_str(server_addr), server_port, len(data)))
            client.sendto(data, (server_addr, server_port))
        except IOError as e:
            err = eventloop.errno_from_exception(e)
            if err in (errno.EINPROGRESS, errno.EAGAIN):
                pass
            else:
                shell.print_exception(e)",_1154.py,37,"for i in range(64):
    req_id = random.randint(1, 65535)
    if req_id not in self._reqid_to_hd:
        break","if req_id in self._reqid_to_hd:
    for i in range(64):
        req_id = random.randint(1, 65535)
        if type(self._reqid_to_hd[req_id]) is tuple:
            break","for i in range(64):
    req_id = random.randint(1, 65535)
    if req_id not in self._reqid_to_hd:
        break
else:
    for i in range(64):
        req_id = random.randint(1, 65535)
        if type(self._reqid_to_hd[req_id]) is tuple:
            break"
https://github.com/kerlomz/captcha_trainer/tree/master//app.py,"def browse_dataset(self, dataset_type: DatasetType, mode: RunMode):
        if not self.current_project:
            messagebox.showerror(
                ""Error!"", ""Please define the project name first.""
            )
            return
        filename = filedialog.askdirectory()
        if not filename:
            return
        is_sub = False
        for i, item in enumerate(os.scandir(filename)):
            if item.is_dir():
                path = item.path.replace(""\\"", ""/"")
                if self.sample_map[dataset_type][mode].size() == 0:
                    self.fetch_sample([path])
                self.sample_map[dataset_type][mode].insert(tk.END, path)
                if i > 0:
                    continue
                is_sub = True
            else:
                break
        if not is_sub:
            filename = filename.replace(""\\"", ""/"")
            if self.sample_map[dataset_type][mode].size() == 0:
                self.fetch_sample([filename])
            self.sample_map[dataset_type][mode].insert(tk.END, filename)",_116531.py,11,"for (i, item) in enumerate(os.scandir(filename)):
    if item.is_dir():
        path = item.path.replace('\\', '/')
        if self.sample_map[dataset_type][mode].size() == 0:
            self.fetch_sample([path])
        self.sample_map[dataset_type][mode].insert(tk.END, path)
        if i > 0:
            continue
        is_sub = True
    else:
        break","if not is_sub:
    filename = filename.replace('\\', '/')
    if self.sample_map[dataset_type][mode].size() == 0:
        self.fetch_sample([filename])
    self.sample_map[dataset_type][mode].insert(tk.END, filename)","for (i, item) in enumerate(os.scandir(filename)):
    if item.is_dir():
        path = item.path.replace('\\', '/')
        if self.sample_map[dataset_type][mode].size() == 0:
            self.fetch_sample([path])
        self.sample_map[dataset_type][mode].insert(tk.END, path)
        if i > 0:
            continue
    else:
        break
else:
    filename = filename.replace('\\', '/')
    if self.sample_map[dataset_type][mode].size() == 0:
        self.fetch_sample([filename])
    self.sample_map[dataset_type][mode].insert(tk.END, filename)"
https://github.com/pwr-Solaar/Solaar/tree/master/lib/solaar/cli/config.py,"def select_choice(value, choices, setting, key):
    lvalue = value.lower()
    ivalue = to_int(value)
    val = None
    for choice in choices:
        if value == str(choice):
            val = choice
            break
    if val is not None:
        value = val
    elif ivalue is not None and ivalue >= 1 and ivalue <= len(choices):
        value = choices[ivalue - 1]
    elif lvalue in ('higher', 'lower'):
        old_value = setting.read() if key is None else setting.read_key(key)
        if old_value is None:
            raise Exception(""%s: could not read current value'"" % setting.name)
        if lvalue == 'lower':
            lower_values = choices[:old_value]
            value = lower_values[-1] if lower_values else choices[:][0]
        elif lvalue == 'higher':
            higher_values = choices[old_value + 1:]
            value = higher_values[0] if higher_values else choices[:][-1]
    elif lvalue in ('highest', 'max', 'first'):
        value = choices[:][-1]
    elif lvalue in ('lowest', 'min', 'last'):
        value = choices[:][0]
    else:
        raise Exception('%s: possible values are [%s]' % (setting.name, ', '.join(str(v) for v in choices)))
    return value",_116755.py,5,"for choice in choices:
    if value == str(choice):
        val = choice
        break","if val is not None:
    value = val
elif ivalue is not None and ivalue >= 1 and (ivalue <= len(choices)):
    value = choices[ivalue - 1]
elif lvalue in ('higher', 'lower'):
    old_value = setting.read() if key is None else setting.read_key(key)
    if old_value is None:
        raise Exception(""%s: could not read current value'"" % setting.name)
    if lvalue == 'lower':
        lower_values = choices[:old_value]
        value = lower_values[-1] if lower_values else choices[:][0]
    elif lvalue == 'higher':
        higher_values = choices[old_value + 1:]
        value = higher_values[0] if higher_values else choices[:][-1]
elif lvalue in ('highest', 'max', 'first'):
    value = choices[:][-1]
elif lvalue in ('lowest', 'min', 'last'):
    value = choices[:][0]
else:
    raise Exception('%s: possible values are [%s]' % (setting.name, ', '.join((str(v) for v in choices))))","for choice in choices:
    if value == str(choice):
        val = choice
        value = val
        break
else:
    if ivalue is not None and ivalue >= 1 and (ivalue <= len(choices)):
        value = choices[ivalue - 1]
    elif lvalue in ('higher', 'lower'):
        old_value = setting.read() if key is None else setting.read_key(key)
        if old_value is None:
            raise Exception(""%s: could not read current value'"" % setting.name)
        if lvalue == 'lower':
            lower_values = choices[:old_value]
            value = lower_values[-1] if lower_values else choices[:][0]
        elif lvalue == 'higher':
            higher_values = choices[old_value + 1:]
            value = higher_values[0] if higher_values else choices[:][-1]
    elif lvalue in ('highest', 'max', 'first'):
        value = choices[:][-1]
    elif lvalue in ('lowest', 'min', 'last'):
        value = choices[:][0]
    else:
        raise Exception('%s: possible values are [%s]' % (setting.name, ', '.join((str(v) for v in choices))))"
https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/util/pathfinding/pathfinding.py,"def setup(self):
		""""""Sets up variables for execution of algorithm
		@return: bool, whether setup was successful""""""
		# support for building
		if hasattr(self.source, 'position'):
			self.source = self.source.position
		if hasattr(self.destination, 'position'):
			self.destination = self.destination.position

		if isinstance(self.path_nodes, list) or isinstance(self.path_nodes, set):
			self.path_nodes = dict.fromkeys(self.path_nodes, 1.0)

		# check if target is blocked
		target_is_blocked = True
		for coord in self.destination.tuple_iter():
			if coord not in self.blocked_coords:
				target_is_blocked = False
		if target_is_blocked:
			#self.log.debug(""FindPath: target is blocked"")
			return False

		# check if target is walkable
		if not self.make_target_walkable:
			target_is_walkable = False
			for coord in self.destination:
				if coord.to_tuple() in self.path_nodes:
					target_is_walkable = True
					break
			if not target_is_walkable:
				#self.log.debug(""FindPath: target is not walkable"")
				return False

		return True",_117238.py,25,"for coord in self.destination:
    if coord.to_tuple() in self.path_nodes:
        target_is_walkable = True
        break","if not target_is_walkable:
    return False","for coord in self.destination:
    if coord.to_tuple() in self.path_nodes:
        break
else:
    return False"
https://github.com/Alexey-T/CudaText/tree/master/app/py/sys/requests/utils.py,"def get_netrc_auth(url, raise_errors=False):
    """"""Returns the Requests tuple auth for a given url from netrc.""""""

    netrc_file = os.environ.get(""NETRC"")
    if netrc_file is not None:
        netrc_locations = (netrc_file,)
    else:
        netrc_locations = (f""~/{f}"" for f in NETRC_FILES)

    try:
        from netrc import NetrcParseError, netrc

        netrc_path = None

        for f in netrc_locations:
            try:
                loc = os.path.expanduser(f)
            except KeyError:
                # os.path.expanduser can fail when $HOME is undefined and
                # getpwuid fails. See https://bugs.python.org/issue20164 &
                # https://github.com/psf/requests/issues/1846
                return

            if os.path.exists(loc):
                netrc_path = loc
                break

        # Abort early if there isn't one.
        if netrc_path is None:
            return

        ri = urlparse(url)

        # Strip port numbers from netloc. This weird `if...encode`` dance is
        # used for Python 3.2, which doesn't support unicode literals.
        splitstr = b"":""
        if isinstance(url, str):
            splitstr = splitstr.decode(""ascii"")
        host = ri.netloc.split(splitstr)[0]

        try:
            _netrc = netrc(netrc_path).authenticators(host)
            if _netrc:
                # Return with login / password
                login_i = 0 if _netrc[0] else 1
                return (_netrc[login_i], _netrc[2])
        except (NetrcParseError, OSError):
            # If there was a parsing error or a permissions issue reading the file,
            # we'll just skip netrc auth unless explicitly asked to raise errors.
            if raise_errors:
                raise

    # App Engine hackiness.
    except (ImportError, AttributeError):
        pass",_117940.py,15,"for f in netrc_locations:
    try:
        loc = os.path.expanduser(f)
    except KeyError:
        return
    if os.path.exists(loc):
        netrc_path = loc
        break","if netrc_path is None:
    return","for f in netrc_locations:
    try:
        loc = os.path.expanduser(f)
    except KeyError:
        return
    if os.path.exists(loc):
        netrc_path = loc
        break
else:
    return"
https://github.com/trailofbits/deepstate/tree/master/bin/deepstate/executors/auxiliary/reducer.py,"def main():
  global candidateRuns, currentTest, s, passStart

  parser = argparse.ArgumentParser(description=""Intelligently reduce test case"")

  parser.add_argument(
    ""binary"", type=str, help=""Path to the test binary to run."")
  parser.add_argument(
    ""input_test"", type=str, help=""Path to test to reduce."")
  parser.add_argument(
    ""output_test"", type=str, help=""Path for reduced test."")
  parser.add_argument(
    ""--which_test"", type=str, help=""Which test to run (equivalent to --input_which_test)."", default=None)
  parser.add_argument(
    ""--criterion"", type=str, help=""String to search for in valid reduction outputs (criteria are ORed by default)."",
    default=None)
  parser.add_argument(
    ""--regexpCriterion"", type=str, help=""Regexp to search for in valid reduction outputs (criteria are ORed by default)."",
    default=None)
  parser.add_argument(
    ""--exitCriterion"", type=int, help=""Exit code for valid reductions (criteria are ORed by default)."",
    default=None)
  parser.add_argument(""--andCriteria"", action=""store_true"", help=""AND criteria instead of ORing them"")
  parser.add_argument(
    ""--cmdArgs"", type=str, help=""Command line to use in place of standard DeepState arguments, file replaces @@"")
  parser.add_argument(
    ""--candidateName"", type=str, help=""Candidate name to use in place of default"")
  parser.add_argument(
    ""--search"", action=""store_true"", help=""Allow initial test to not satisfy criterion (search for test)."",
    default=None)
  parser.add_argument(
    ""--timeout"", type=int, help=""After this amount of time (in seconds), give up on reduction (default is 20 minutes (1200s))."",
    default=1200)
  parser.add_argument(
    ""--maxByteRange"", type=int, help=""Maximum size of byte chunk to try in range removals."",
    default=16)
  parser.add_argument(
    ""--fast"", action='store_true',
    help=""Faster, less complete, reduction (no byte range removal pass)."")
  parser.add_argument(
    ""--slow"", action='store_true',
    help=""Slower, more complete, reduction (byte pattern pass)."")
  parser.add_argument(
    ""--slowest"", action='store_true',
    help=""Slowest, most complete, reduction (byte pattern pass, tries all byte ranges)."")
  parser.add_argument(
    ""--verbose"", action='store_true',
    help=""Verbose reduction."")
  parser.add_argument(
    ""--fork"", action='store_true',
    help=""Fork when running."")
  parser.add_argument(
    ""--noStructure"", action='store_true',
    help=""Don't use test structure."")
  parser.add_argument(
    ""--noStaticStructure"", action='store_true',
    help='''Don't use ""static"" test structure (e.g., parens/quotes/brackets).''')
  parser.add_argument(
    ""--noPad"", action='store_true',
    help=""Don't pad test with zeros."")

  class TimeoutException(Exception):
    pass

  args = parser.parse_args()

  maxByteRange = args.maxByteRange
  deepstate = args.binary
  test = args.input_test
  out = args.output_test
  checkString = args.criterion
  if args.regexpCriterion:
    checkRegExp = re.compile(args.regexpCriterion)
  else:
    checkRegExp = None
  whichTest = args.which_test

  start = time.time()
  candidateRuns = 0

  candidateName = "".candidate."" + str(os.getpid()) + "".test""
  if args.candidateName is not None:
    candidateName = args.candidateName

  def runCandidate(candidate):
    global candidateRuns

    candidateRuns += 1
    if (time.time() - start) > args.timeout:
      raise TimeoutException
    with open("".reducer."" + str(os.getpid()) + "".out"", 'w') as outf:
      if args.cmdArgs is None:
        cmd = [deepstate + "" --input_test_file "" +
             candidate + "" --verbose_reads""]
        if whichTest is not None:
          cmd += [""--input_which_test"", whichTest]
        if not args.fork:
          cmd += [""--no_fork""]
      else:
        cmd = [deepstate + "" "" + args.cmdArgs.replace(""@@"", candidate)]
      exitCode = subprocess.call(cmd, shell=True, stdout=outf, stderr=outf)
    result = []
    with open("".reducer."" + str(os.getpid()) + "".out"", 'rb') as inf:
      for line in inf:
        dline = line.decode(""utf-8"", ""ignore"")
        result.append(dline)
    return (result, exitCode)

  def checks(resultAndExitCode):
    (result, exitCode) = resultAndExitCode
    if (args.exitCriterion is None) and (checkRegExp is None) and (checkString is None):
      # Only apply default DeepState failure check if no other criteria were defined
      for line in result:
        if ""ERROR: Failed:"" in line:
          return True
        if ""ERROR: Crashed"" in line:
          return True

    if args.exitCriterion is not None:
      exitHolds = exitCode == args.exitCriterion
    else:
      exitHolds = args.andCriteria
    if checkRegExp is not None:
      regexpHolds = re.search(checkRegExp, ""\n"".join(result)) is not None
    else:
      regexpHolds = args.andCriteria
    if checkString is not None:
      stringHolds = checkString in ""\n"".join(result)
    else:
      stringHolds = args.andCriteria
    if args.andCriteria:
      return exitHolds and regexpHolds and stringHolds
    else:
      return exitHolds or regexpHolds or stringHolds

  def writeAndRunCandidate(test):
    with open(candidateName, 'wb') as outf:
      outf.write(test)
    r = runCandidate(candidateName)
    return r

  def augmentWithDelims(OneOfsAndLastRead, testBytes):
    if args.noStaticStructure:
      return OneOfsAndLastRead
    (OneOfs, lastRead) = OneOfsAndLastRead
    delimPairs = [
      (""{"", ""}""),
      (""("", "")""),
      (""["", ""]""),
      ("";"", "";""),
      (""{"", "";""),
      ("";"", ""}""),
      (""BEGIN"", ""\n""),
      (""\n"", ""END""),
      (""\n"", ""\n""),
      (""'"", ""'""),
      ('""', '""'),
      (""/"", ""/""),
      (""/"", ""*""),
      (""/"", ""\n""),
      ("","", "",""),
      (""("", "",""),
      ("","", "")""),
      (""<"", "">"")]
    delims = []
    for (tstart, tstop) in delimPairs:
      if tstart not in [""BEGIN"", ""END""]:
        tstartBytes = bytearray(tstart, encoding=""utf8"")
        start = tstartBytes[0]
      if tstop not in [""BEGIN"", ""END""]:
        tstopBytes = bytearray(tstop, encoding=""utf8"")
        stop = tstopBytes[0]
      for i in range(len(testBytes)):
        for j in range(len(testBytes) - 1, i, -1):
          if tstart not in [""BEGIN"", ""END""]:
            imatch = testBytes[i] == start
          else:
            if tstart == ""BEGIN"":
              imatch = (i == 0)
          if tstop not in [""BEGIN"", ""END""]:
            jmatch = testBytes[j] == stop
          else:
            jmatch = (j == len(testBytes) - 1)
          if imatch and jmatch:
            delims.append((i, j))
            delims.append((i + 1, j - 1))
    return (OneOfs + delims, lastRead)

  def structure(resultAndExitCode):
    (result, exitCode) = resultAndExitCode
    lastRead = len(currentTest) - 1
    if args.noStructure:
      return ([], lastRead)
    OneOfs = []
    currentOneOf = []
    for line in result:
      if ""STARTING OneOf CALL"" in line:
        currentOneOf.append(-1)
      elif ""Reading byte at"" in line:
        lastRead = int(line.split()[-1])
        if len(currentOneOf) > 0:
          if currentOneOf[-1] == -1:
            currentOneOf[-1] = lastRead
      elif ""FINISHED OneOf CALL"" in line:
        OneOfs.append((currentOneOf[-1], lastRead))
        currentOneOf = currentOneOf[:-1]
    return (OneOfs, lastRead)

  def rangeConversions(resultAndExitCode):
    (result, exitCode) = resultAndExitCode
    conversions = []
    startedMulti = False
    multiFirst = None
    for line in result:
      if ""Reading byte at"" in line:
        lastRead = int(line.split()[-1])
      if ""STARTING MULTI-BYTE READ"" in line:
        startedMulti = True
      if startedMulti and (multiFirst is None) and (""Reading byte at"" in line):
        multiFirst = lastRead
      if ""FINISHED MULTI-BYTE READ"" in line:
        currentMulti = (multiFirst, lastRead)
        startedMulti = False
        multiFirst = None
      if ""Converting out-of-range value"" in line:
        conversions.append((currentMulti, int(line.split()[-1])))
    return conversions

  def fixRangeConversions(test, conversions):
    if args.noStructure:
      return
    numConversions = 0
    for (pos, value) in conversions:
      if pos[1] >= len(test):
        break
      if (value < 255) and (value < test[pos[1]]):
        numConversions += 1
        for b in range(pos[0], pos[1]):
          test[b] = 0
        test[pos[1]] = value
    if numConversions > 0:
      print(""Applied"", numConversions, ""range conversions"")

  initial = runCandidate(test)
  if (not args.search) and (not checks(initial)):
    print(""STARTING TEST DOES NOT SATISFY REDUCTION CRITERION!"")
    return 1

  with open(test, 'rb') as test:
    currentTest = bytearray(test.read())
  original = bytearray(currentTest)

  print(""Original test has"", len(currentTest), ""bytes"")
  if args.slowest:
    maxByteRange = len(currentTest)

  fixRangeConversions(currentTest, rangeConversions(initial))
  r = writeAndRunCandidate(currentTest)
  assert(checks(r))

  s = structure(r)
  if (s[1] + 1) < len(currentTest):
    print(""Last byte read:"", s[1])
    print(""Shrinking to ignore unread bytes"")
    currentTest = currentTest[:s[1] + 1]
  s = augmentWithDelims(s, currentTest)

  if currentTest != original:
    print(""Writing reduced test with"", len(currentTest), ""bytes to"", out)
    with open(out, 'wb') as outf:
      outf.write(currentTest)

  initialSize = float(len(currentTest))
  iteration = 0

  def updateCurrent(newTest):
      global currentTest, s
      currentTest = newTest
      fixRangeConversions(currentTest, rangeConversions(r))
      print(""Writing reduced test with"", len(currentTest), ""bytes to"", out)
      with open(out, 'wb') as outf:
        outf.write(currentTest)
      s = augmentWithDelims(structure(r), currentTest)
      percent = 100.0 * ((initialSize - len(currentTest)) / initialSize)
      print(round(time.time()-start, 2), ""secs /"",
              candidateRuns, ""execs /"", str(round(percent, 2)) + ""% reduction"")
      print(""=""*80)
      sys.stdout.flush()

  def passInfo(passName):
      global passStart
      percent = 100.0 * ((initialSize - len(currentTest)) / initialSize)
      print(passName + "":"", ""PASS FINISHED IN"", round(time.time() - passStart, 2), ""SECONDS, RUN:"",
                round(time.time()-start, 2), ""secs /"", candidateRuns, ""execs /"", str(round(percent, 2)) + ""% reduction"")
      passStart = time.time()

  oldTest = []
  lastOneOfRemovalTest = []
  lastEdgeRemovalTest = []
  lastChunkRemovalTest = {}
  lastChunkRemovalTest[1] = []
  lastChunkRemovalTest[4] = []
  lastChunkRemovalTest[8] = []
  lastReduceAndDeleteTest = {}
  lastReduceAndDeleteTest[1] = []
  lastReduceAndDeleteTest[4] = []
  lastReduceAndDeleteTest[8] = []
  lastAllRangeTest = []
  lastOneOfSwapTest = []
  lastByteReduceTest = []
  lastPatternSearchTest = []

  passStart = time.time()
  try:
    while oldTest != currentTest:
      oldTest = bytearray(currentTest)

      iteration += 1
      percent = 100.0 * ((initialSize - len(currentTest)) / initialSize)
      print(""="" * 80)
      print(""Iteration #"" + str(iteration), round(time.time()-start, 2), ""secs /"",
              candidateRuns, ""execs /"", str(round(percent, 2)) + ""% reduction"")

      if not (args.noStructure) and (currentTest != lastOneOfRemovalTest) and (len(s[0]) != 0):
        if args.verbose:
          print(""*"" * 80 + ""\nPASS: structured deletions..."")
        changed = True
        while changed:
          changed = False
          cuts = s[0]
          for c in cuts:
            newTest = currentTest[:c[0]] + currentTest[c[1] + 1:]
            if len(newTest) == len(currentTest):
              continue # Ignore non-shrinking reductions
            r = writeAndRunCandidate(newTest)
            if checks(r):
              print(""Structured deletion reduced test to"", len(newTest), ""bytes"")
              changed = True
              updateCurrent(newTest)
              break
        lastOneOfRemovalTest = bytearray(currentTest)
        passInfo(""Structured deletion"")

      if not (args.noStructure) and (currentTest != lastEdgeRemovalTest) and (len(s[0]) != 0):
        if args.verbose:
          print(""*"" * 80 + ""\nPASS: structure edge deletions..."")
        changed = True
        while changed:
          changed = False
          cuts = s[0]
          for c in cuts:
            newTest = currentTest[:c[0]] + currentTest[c[0] + 1:c[1]] + currentTest[c[1] + 1:]
            if len(newTest) == len(currentTest):
              continue # Ignore non-shrinking reductions
            r = writeAndRunCandidate(newTest)
            if checks(r):
              print(""Structure edge deletion reduced test to"", len(newTest), ""bytes"")
              changed = True
              updateCurrent(newTest)
              break
        lastEdgeRemovalTest = bytearray(currentTest)
        passInfo(""Structured edge deletion"")

      for k in [1, 4, 8]:
        if currentTest != lastChunkRemovalTest[k]:
          if args.verbose:
            print(""*"" * 80 + ""\nPASS: trying"", k, ""byte chunk removals..."")
          changed = True
          startingPos = 0
          while changed:
            changed = False
            for b in range(startingPos, len(currentTest)):
              newTest = currentTest[:b] + currentTest[b + k:]
              r = writeAndRunCandidate(newTest)
              if checks(r):
                print(""Removed"", k, ""byte(s) @"", str(b) + "": reduced test to"", len(newTest), ""bytes"")
                changed = True
                updateCurrent(newTest)
                startingPos = b
                break
            if not changed:
              for b in range(0, startingPos):
                newTest = currentTest[:b] + currentTest[b + k:]
                r = writeAndRunCandidate(newTest)
                if checks(r):
                  print(""Removed"", k, ""byte(s) @"", str(b) + "": reduced test to"", len(newTest), ""bytes"")
                  changed = True
                  updateCurrent(newTest)
                  startingPos = b
                  break
          lastChunkRemovalTest[k] = bytearray(currentTest)
          passInfo(str(k) + ""-byte chunk removal"")

      for k in [1, 4, 8]:
        if currentTest != lastReduceAndDeleteTest[k]:
          if args.verbose:
            print(""*"" * 80 + ""\nPASS: byte reduce and delete"", str(k) + ""..."")
          changed = True
          while changed:
            changed = False
            for b in range(0, len(currentTest) - k):
              if currentTest[b] == 0:
                continue
              newTest = bytearray(currentTest)
              newTest[b] = currentTest[b] - 1
              newTest = newTest[:b + 1] + newTest[b + k + 1:]
              r = writeAndRunCandidate(newTest)
              if checks(r):
                print(""Reduced byte"", b, ""by 1 and deleted"", k, ""bytes, reducing test to"", len(newTest), ""bytes"")
                changed = True
                updateCurrent(newTest)
                break
          lastReduceAndDeleteTest[k] = bytearray(currentTest)
          passInfo(str(k) + ""-byte reduce and delete"")

      if not args.fast:
        if currentTest != lastAllRangeTest:
          if args.verbose:
            print(""*"" * 80 + ""\nPASS: trying all byte range removals..."")
          changed = True
          startingPos = 0
          while changed:
            changed = False
            for b in range(startingPos, len(currentTest)):
              if args.verbose:
                print(""Trying byte range removal from"", str(b) + ""..."")
              for v in range(b + 2, min(len(currentTest), b + maxByteRange)):
                if (v-b) in [4, 8]:
                  continue
                newTest = currentTest[:b] + currentTest[v:]
                r = writeAndRunCandidate(newTest)
                if checks(r):
                  print(""Byte range removal of bytes"", str(b) + ""-"" + str(v - 1),
                          ""reduced test to"", len(newTest), ""bytes"")
                  changed = True
                  updateCurrent(newTest)
                  startingPos = b
                  break
              if changed:
                break
            if not changed:
              for b in range(0, startingPos):
                if args.verbose:
                  print(""Trying byte range removal from"", str(b) + ""..."")
                for v in range(b + 2, min(len(currentTest), b + maxByteRange)):
                  if (v-b) in [4, 8]:
                    continue
                  newTest = currentTest[:b] + currentTest[v:]
                  r = writeAndRunCandidate(newTest)
                  if checks(r):
                    print(""Byte range removal of bytes"", str(b) + ""-"" + str(v - 1),
                            ""reduced test to"", len(newTest), ""bytes"")
                    changed = True
                    updateCurrent(newTest)
                    startingPos = b
                    break
                if changed:
                  break
          lastAllRangeTest = bytearray(currentTest)
          passInfo(""Byte range removal"")

      if (not args.noStructure) and (currentTest != lastOneOfSwapTest) and (len(s[0]) != 0):
        if args.verbose:
          print(""*"" * 80 + ""\nPASS: swapping structures..."")
        changed = True
        while changed:
          changed = False
          cuts = s[0]
          for i in range(len(cuts) - 1):
            cuti = cuts[i]
            bytesi = currentTest[cuti[0]:cuti[1] + 1]
            if args.verbose:
              print(""Trying structured swap from byte"", cuti[0], ""["" + "" "".join(map(str, bytesi)) + ""]"")
            for j in range(i + 1, len(cuts)):
              cutj = cuts[j]
              if cutj[0] > cuti[1]:
                bytesj = currentTest[cutj[0]:cutj[1] + 1]
                if (len(bytesj) > 0) and (bytesi > bytesj):
                  newTest = currentTest[:cuti[0]] + bytesj + currentTest[cuti[1] + 1:cutj[0]]
                  newTest += bytesi
                  newTest += currentTest[cutj[1] + 1:]
                  newTest = bytearray(newTest)
                  r = writeAndRunCandidate(newTest)
                  if checks(r):
                    print(""Structured swap @ byte"", cuti[0], ""["" + "" "".join(map(str, bytesi)) + ""]"", ""with"",
                            cutj[0], ""["" + "" "".join(map(str, bytesj)) + ""]"")
                    changed = True
                    updateCurrent(newTest)
                    break
              if changed:
                break
            if changed:
              break
        lastOneOfSwapTest = bytearray(currentTest)
        passInfo(""Structured swap"")

      if currentTest != lastByteReduceTest:
          if args.verbose:
              print(""*"" * 80 + ""\nPASS: byte reductions..."")
          changed = True
          startingPos = 0
          while changed:
            changed = False
            for b in range(startingPos, len(currentTest)):
              for v in range(0, currentTest[b]):
                newTest = bytearray(currentTest)
                newTest[b] = v
                r = writeAndRunCandidate(newTest)
                if checks(r):
                  print(""Reduced byte"", b, ""from"", currentTest[b], ""to"", v)
                  changed = True
                  updateCurrent(newTest)
                  startingPos = b + 1
                  break
              if changed:
                break
            if changed:
              continue
            for b in range(0, startingPos):
              for v in range(0, currentTest[b]):
                newTest = bytearray(currentTest)
                newTest[b] = v
                r = writeAndRunCandidate(newTest)
                if checks(r):
                  print(""Reduced byte"", b, ""from"", currentTest[b], ""to"", v)
                  changed = True
                  updateCurrent(newTest)
                  startingPos = b + 1
                  break
              if changed:
                break
          lastByteReduceTest = bytearray(currentTest)
          passInfo(""Byte reduce"")

      if (args.slow or args.slowest) and (oldTest == currentTest):
        if currentTest != lastPatternSearchTest:
          if args.verbose:
            print(""*"" * 80 + ""\nPASS: byte pattern search..."")
          changed = True
          while changed:
            changed = False
            for b1 in range(0, len(currentTest)-4):
              if args.verbose:
                print(""Trying byte pattern search from byte"", str(b1) + ""..."")
              for b2 in range(b1 + 2, len(currentTest) - 4):
                v1 = (currentTest[b1], currentTest[b1 + 1])
                v2 = (currentTest[b2], currentTest[b2 + 1])
                if (v1 == v2):
                  ba = bytearray(v1)
                  part1 = currentTest[:b1]
                  part2 = currentTest[b1 + 2:b2]
                  part3 = currentTest[b2 + 2:]
                  banews = []
                  banews.append(ba[0:1])
                  banews.append(ba[1:2])
                  if ba[0] > 0:
                    for v in range(0, ba[0]):
                      banews.append(bytearray([v, ba[1]]))
                    banews.append(bytearray([ba[0] - 1]))
                  if ba[1] > 0:
                    for v in range(0, ba[1]):
                      banews.append(bytearray([ba[0], v]))
                  for banew in banews:
                    newTest = part1 + banew + part2 + banew + part3
                    r = writeAndRunCandidate(newTest)
                    if checks(r):
                      print(""Byte pattern"", tuple(ba), ""at"", b1, ""and"", b2, ""changed to"", tuple(banew))
                      changed = True
                      updateCurrent(newTest)
                      break
                  if changed:
                    break
              if changed:
                break
          lastPatternSearchTest = bytearray(currentTest)
          passInfo(""Byte pattern change"")

        if oldTest == currentTest:
          print(""*"" * 80)
          print(""DONE: NO (MORE) REDUCTIONS FOUND"")
  except TimeoutException:
    print(""*"" * 80)
    print(""DONE: REDUCTION TIMED OUT AFTER"", args.timeout, ""SECONDS"")

  print(""="" * 80)
  percent = 100.0 * ((initialSize - len(currentTest)) / initialSize)
  print(""Completed"", iteration, ""iterations:"", round(time.time()-start, 2), ""secs /"",
          candidateRuns, ""execs /"", str(round(percent, 2)) + ""% reduction"")

  if not args.noPad:
    if (s[1] + 1) > len(currentTest):
      print(""Padding test with"", (s[1] + 1) - len(currentTest), ""zeroes"")
      padding = bytearray('\x00' * ((s[1] + 1) - len(currentTest)), 'utf-8')
      currentTest = currentTest + padding
  
  print(""Writing reduced test with"", len(currentTest), ""bytes to"", out)
    
  with open(out, 'wb') as outf:
    outf.write(currentTest)

  return 0",_118309.py,372,"for b in range(startingPos, len(currentTest)):
    newTest = currentTest[:b] + currentTest[b + k:]
    r = writeAndRunCandidate(newTest)
    if checks(r):
        print('Removed', k, 'byte(s) @', str(b) + ': reduced test to', len(newTest), 'bytes')
        changed = True
        updateCurrent(newTest)
        startingPos = b
        break","if not changed:
    for b in range(0, startingPos):
        newTest = currentTest[:b] + currentTest[b + k:]
        r = writeAndRunCandidate(newTest)
        if checks(r):
            print('Removed', k, 'byte(s) @', str(b) + ': reduced test to', len(newTest), 'bytes')
            changed = True
            updateCurrent(newTest)
            startingPos = b
            break","for b in range(startingPos, len(currentTest)):
    newTest = currentTest[:b] + currentTest[b + k:]
    r = writeAndRunCandidate(newTest)
    if checks(r):
        print('Removed', k, 'byte(s) @', str(b) + ': reduced test to', len(newTest), 'bytes')
        changed = True
        updateCurrent(newTest)
        startingPos = b
        break
else:
    for b in range(0, startingPos):
        newTest = currentTest[:b] + currentTest[b + k:]
        r = writeAndRunCandidate(newTest)
        if checks(r):
            print('Removed', k, 'byte(s) @', str(b) + ': reduced test to', len(newTest), 'bytes')
            changed = True
            updateCurrent(newTest)
            startingPos = b
            break"
https://github.com/trailofbits/deepstate/tree/master/bin/deepstate/executors/auxiliary/reducer.py,"def main():
  global candidateRuns, currentTest, s, passStart

  parser = argparse.ArgumentParser(description=""Intelligently reduce test case"")

  parser.add_argument(
    ""binary"", type=str, help=""Path to the test binary to run."")
  parser.add_argument(
    ""input_test"", type=str, help=""Path to test to reduce."")
  parser.add_argument(
    ""output_test"", type=str, help=""Path for reduced test."")
  parser.add_argument(
    ""--which_test"", type=str, help=""Which test to run (equivalent to --input_which_test)."", default=None)
  parser.add_argument(
    ""--criterion"", type=str, help=""String to search for in valid reduction outputs (criteria are ORed by default)."",
    default=None)
  parser.add_argument(
    ""--regexpCriterion"", type=str, help=""Regexp to search for in valid reduction outputs (criteria are ORed by default)."",
    default=None)
  parser.add_argument(
    ""--exitCriterion"", type=int, help=""Exit code for valid reductions (criteria are ORed by default)."",
    default=None)
  parser.add_argument(""--andCriteria"", action=""store_true"", help=""AND criteria instead of ORing them"")
  parser.add_argument(
    ""--cmdArgs"", type=str, help=""Command line to use in place of standard DeepState arguments, file replaces @@"")
  parser.add_argument(
    ""--candidateName"", type=str, help=""Candidate name to use in place of default"")
  parser.add_argument(
    ""--search"", action=""store_true"", help=""Allow initial test to not satisfy criterion (search for test)."",
    default=None)
  parser.add_argument(
    ""--timeout"", type=int, help=""After this amount of time (in seconds), give up on reduction (default is 20 minutes (1200s))."",
    default=1200)
  parser.add_argument(
    ""--maxByteRange"", type=int, help=""Maximum size of byte chunk to try in range removals."",
    default=16)
  parser.add_argument(
    ""--fast"", action='store_true',
    help=""Faster, less complete, reduction (no byte range removal pass)."")
  parser.add_argument(
    ""--slow"", action='store_true',
    help=""Slower, more complete, reduction (byte pattern pass)."")
  parser.add_argument(
    ""--slowest"", action='store_true',
    help=""Slowest, most complete, reduction (byte pattern pass, tries all byte ranges)."")
  parser.add_argument(
    ""--verbose"", action='store_true',
    help=""Verbose reduction."")
  parser.add_argument(
    ""--fork"", action='store_true',
    help=""Fork when running."")
  parser.add_argument(
    ""--noStructure"", action='store_true',
    help=""Don't use test structure."")
  parser.add_argument(
    ""--noStaticStructure"", action='store_true',
    help='''Don't use ""static"" test structure (e.g., parens/quotes/brackets).''')
  parser.add_argument(
    ""--noPad"", action='store_true',
    help=""Don't pad test with zeros."")

  class TimeoutException(Exception):
    pass

  args = parser.parse_args()

  maxByteRange = args.maxByteRange
  deepstate = args.binary
  test = args.input_test
  out = args.output_test
  checkString = args.criterion
  if args.regexpCriterion:
    checkRegExp = re.compile(args.regexpCriterion)
  else:
    checkRegExp = None
  whichTest = args.which_test

  start = time.time()
  candidateRuns = 0

  candidateName = "".candidate."" + str(os.getpid()) + "".test""
  if args.candidateName is not None:
    candidateName = args.candidateName

  def runCandidate(candidate):
    global candidateRuns

    candidateRuns += 1
    if (time.time() - start) > args.timeout:
      raise TimeoutException
    with open("".reducer."" + str(os.getpid()) + "".out"", 'w') as outf:
      if args.cmdArgs is None:
        cmd = [deepstate + "" --input_test_file "" +
             candidate + "" --verbose_reads""]
        if whichTest is not None:
          cmd += [""--input_which_test"", whichTest]
        if not args.fork:
          cmd += [""--no_fork""]
      else:
        cmd = [deepstate + "" "" + args.cmdArgs.replace(""@@"", candidate)]
      exitCode = subprocess.call(cmd, shell=True, stdout=outf, stderr=outf)
    result = []
    with open("".reducer."" + str(os.getpid()) + "".out"", 'rb') as inf:
      for line in inf:
        dline = line.decode(""utf-8"", ""ignore"")
        result.append(dline)
    return (result, exitCode)

  def checks(resultAndExitCode):
    (result, exitCode) = resultAndExitCode
    if (args.exitCriterion is None) and (checkRegExp is None) and (checkString is None):
      # Only apply default DeepState failure check if no other criteria were defined
      for line in result:
        if ""ERROR: Failed:"" in line:
          return True
        if ""ERROR: Crashed"" in line:
          return True

    if args.exitCriterion is not None:
      exitHolds = exitCode == args.exitCriterion
    else:
      exitHolds = args.andCriteria
    if checkRegExp is not None:
      regexpHolds = re.search(checkRegExp, ""\n"".join(result)) is not None
    else:
      regexpHolds = args.andCriteria
    if checkString is not None:
      stringHolds = checkString in ""\n"".join(result)
    else:
      stringHolds = args.andCriteria
    if args.andCriteria:
      return exitHolds and regexpHolds and stringHolds
    else:
      return exitHolds or regexpHolds or stringHolds

  def writeAndRunCandidate(test):
    with open(candidateName, 'wb') as outf:
      outf.write(test)
    r = runCandidate(candidateName)
    return r

  def augmentWithDelims(OneOfsAndLastRead, testBytes):
    if args.noStaticStructure:
      return OneOfsAndLastRead
    (OneOfs, lastRead) = OneOfsAndLastRead
    delimPairs = [
      (""{"", ""}""),
      (""("", "")""),
      (""["", ""]""),
      ("";"", "";""),
      (""{"", "";""),
      ("";"", ""}""),
      (""BEGIN"", ""\n""),
      (""\n"", ""END""),
      (""\n"", ""\n""),
      (""'"", ""'""),
      ('""', '""'),
      (""/"", ""/""),
      (""/"", ""*""),
      (""/"", ""\n""),
      ("","", "",""),
      (""("", "",""),
      ("","", "")""),
      (""<"", "">"")]
    delims = []
    for (tstart, tstop) in delimPairs:
      if tstart not in [""BEGIN"", ""END""]:
        tstartBytes = bytearray(tstart, encoding=""utf8"")
        start = tstartBytes[0]
      if tstop not in [""BEGIN"", ""END""]:
        tstopBytes = bytearray(tstop, encoding=""utf8"")
        stop = tstopBytes[0]
      for i in range(len(testBytes)):
        for j in range(len(testBytes) - 1, i, -1):
          if tstart not in [""BEGIN"", ""END""]:
            imatch = testBytes[i] == start
          else:
            if tstart == ""BEGIN"":
              imatch = (i == 0)
          if tstop not in [""BEGIN"", ""END""]:
            jmatch = testBytes[j] == stop
          else:
            jmatch = (j == len(testBytes) - 1)
          if imatch and jmatch:
            delims.append((i, j))
            delims.append((i + 1, j - 1))
    return (OneOfs + delims, lastRead)

  def structure(resultAndExitCode):
    (result, exitCode) = resultAndExitCode
    lastRead = len(currentTest) - 1
    if args.noStructure:
      return ([], lastRead)
    OneOfs = []
    currentOneOf = []
    for line in result:
      if ""STARTING OneOf CALL"" in line:
        currentOneOf.append(-1)
      elif ""Reading byte at"" in line:
        lastRead = int(line.split()[-1])
        if len(currentOneOf) > 0:
          if currentOneOf[-1] == -1:
            currentOneOf[-1] = lastRead
      elif ""FINISHED OneOf CALL"" in line:
        OneOfs.append((currentOneOf[-1], lastRead))
        currentOneOf = currentOneOf[:-1]
    return (OneOfs, lastRead)

  def rangeConversions(resultAndExitCode):
    (result, exitCode) = resultAndExitCode
    conversions = []
    startedMulti = False
    multiFirst = None
    for line in result:
      if ""Reading byte at"" in line:
        lastRead = int(line.split()[-1])
      if ""STARTING MULTI-BYTE READ"" in line:
        startedMulti = True
      if startedMulti and (multiFirst is None) and (""Reading byte at"" in line):
        multiFirst = lastRead
      if ""FINISHED MULTI-BYTE READ"" in line:
        currentMulti = (multiFirst, lastRead)
        startedMulti = False
        multiFirst = None
      if ""Converting out-of-range value"" in line:
        conversions.append((currentMulti, int(line.split()[-1])))
    return conversions

  def fixRangeConversions(test, conversions):
    if args.noStructure:
      return
    numConversions = 0
    for (pos, value) in conversions:
      if pos[1] >= len(test):
        break
      if (value < 255) and (value < test[pos[1]]):
        numConversions += 1
        for b in range(pos[0], pos[1]):
          test[b] = 0
        test[pos[1]] = value
    if numConversions > 0:
      print(""Applied"", numConversions, ""range conversions"")

  initial = runCandidate(test)
  if (not args.search) and (not checks(initial)):
    print(""STARTING TEST DOES NOT SATISFY REDUCTION CRITERION!"")
    return 1

  with open(test, 'rb') as test:
    currentTest = bytearray(test.read())
  original = bytearray(currentTest)

  print(""Original test has"", len(currentTest), ""bytes"")
  if args.slowest:
    maxByteRange = len(currentTest)

  fixRangeConversions(currentTest, rangeConversions(initial))
  r = writeAndRunCandidate(currentTest)
  assert(checks(r))

  s = structure(r)
  if (s[1] + 1) < len(currentTest):
    print(""Last byte read:"", s[1])
    print(""Shrinking to ignore unread bytes"")
    currentTest = currentTest[:s[1] + 1]
  s = augmentWithDelims(s, currentTest)

  if currentTest != original:
    print(""Writing reduced test with"", len(currentTest), ""bytes to"", out)
    with open(out, 'wb') as outf:
      outf.write(currentTest)

  initialSize = float(len(currentTest))
  iteration = 0

  def updateCurrent(newTest):
      global currentTest, s
      currentTest = newTest
      fixRangeConversions(currentTest, rangeConversions(r))
      print(""Writing reduced test with"", len(currentTest), ""bytes to"", out)
      with open(out, 'wb') as outf:
        outf.write(currentTest)
      s = augmentWithDelims(structure(r), currentTest)
      percent = 100.0 * ((initialSize - len(currentTest)) / initialSize)
      print(round(time.time()-start, 2), ""secs /"",
              candidateRuns, ""execs /"", str(round(percent, 2)) + ""% reduction"")
      print(""=""*80)
      sys.stdout.flush()

  def passInfo(passName):
      global passStart
      percent = 100.0 * ((initialSize - len(currentTest)) / initialSize)
      print(passName + "":"", ""PASS FINISHED IN"", round(time.time() - passStart, 2), ""SECONDS, RUN:"",
                round(time.time()-start, 2), ""secs /"", candidateRuns, ""execs /"", str(round(percent, 2)) + ""% reduction"")
      passStart = time.time()

  oldTest = []
  lastOneOfRemovalTest = []
  lastEdgeRemovalTest = []
  lastChunkRemovalTest = {}
  lastChunkRemovalTest[1] = []
  lastChunkRemovalTest[4] = []
  lastChunkRemovalTest[8] = []
  lastReduceAndDeleteTest = {}
  lastReduceAndDeleteTest[1] = []
  lastReduceAndDeleteTest[4] = []
  lastReduceAndDeleteTest[8] = []
  lastAllRangeTest = []
  lastOneOfSwapTest = []
  lastByteReduceTest = []
  lastPatternSearchTest = []

  passStart = time.time()
  try:
    while oldTest != currentTest:
      oldTest = bytearray(currentTest)

      iteration += 1
      percent = 100.0 * ((initialSize - len(currentTest)) / initialSize)
      print(""="" * 80)
      print(""Iteration #"" + str(iteration), round(time.time()-start, 2), ""secs /"",
              candidateRuns, ""execs /"", str(round(percent, 2)) + ""% reduction"")

      if not (args.noStructure) and (currentTest != lastOneOfRemovalTest) and (len(s[0]) != 0):
        if args.verbose:
          print(""*"" * 80 + ""\nPASS: structured deletions..."")
        changed = True
        while changed:
          changed = False
          cuts = s[0]
          for c in cuts:
            newTest = currentTest[:c[0]] + currentTest[c[1] + 1:]
            if len(newTest) == len(currentTest):
              continue # Ignore non-shrinking reductions
            r = writeAndRunCandidate(newTest)
            if checks(r):
              print(""Structured deletion reduced test to"", len(newTest), ""bytes"")
              changed = True
              updateCurrent(newTest)
              break
        lastOneOfRemovalTest = bytearray(currentTest)
        passInfo(""Structured deletion"")

      if not (args.noStructure) and (currentTest != lastEdgeRemovalTest) and (len(s[0]) != 0):
        if args.verbose:
          print(""*"" * 80 + ""\nPASS: structure edge deletions..."")
        changed = True
        while changed:
          changed = False
          cuts = s[0]
          for c in cuts:
            newTest = currentTest[:c[0]] + currentTest[c[0] + 1:c[1]] + currentTest[c[1] + 1:]
            if len(newTest) == len(currentTest):
              continue # Ignore non-shrinking reductions
            r = writeAndRunCandidate(newTest)
            if checks(r):
              print(""Structure edge deletion reduced test to"", len(newTest), ""bytes"")
              changed = True
              updateCurrent(newTest)
              break
        lastEdgeRemovalTest = bytearray(currentTest)
        passInfo(""Structured edge deletion"")

      for k in [1, 4, 8]:
        if currentTest != lastChunkRemovalTest[k]:
          if args.verbose:
            print(""*"" * 80 + ""\nPASS: trying"", k, ""byte chunk removals..."")
          changed = True
          startingPos = 0
          while changed:
            changed = False
            for b in range(startingPos, len(currentTest)):
              newTest = currentTest[:b] + currentTest[b + k:]
              r = writeAndRunCandidate(newTest)
              if checks(r):
                print(""Removed"", k, ""byte(s) @"", str(b) + "": reduced test to"", len(newTest), ""bytes"")
                changed = True
                updateCurrent(newTest)
                startingPos = b
                break
            if not changed:
              for b in range(0, startingPos):
                newTest = currentTest[:b] + currentTest[b + k:]
                r = writeAndRunCandidate(newTest)
                if checks(r):
                  print(""Removed"", k, ""byte(s) @"", str(b) + "": reduced test to"", len(newTest), ""bytes"")
                  changed = True
                  updateCurrent(newTest)
                  startingPos = b
                  break
          lastChunkRemovalTest[k] = bytearray(currentTest)
          passInfo(str(k) + ""-byte chunk removal"")

      for k in [1, 4, 8]:
        if currentTest != lastReduceAndDeleteTest[k]:
          if args.verbose:
            print(""*"" * 80 + ""\nPASS: byte reduce and delete"", str(k) + ""..."")
          changed = True
          while changed:
            changed = False
            for b in range(0, len(currentTest) - k):
              if currentTest[b] == 0:
                continue
              newTest = bytearray(currentTest)
              newTest[b] = currentTest[b] - 1
              newTest = newTest[:b + 1] + newTest[b + k + 1:]
              r = writeAndRunCandidate(newTest)
              if checks(r):
                print(""Reduced byte"", b, ""by 1 and deleted"", k, ""bytes, reducing test to"", len(newTest), ""bytes"")
                changed = True
                updateCurrent(newTest)
                break
          lastReduceAndDeleteTest[k] = bytearray(currentTest)
          passInfo(str(k) + ""-byte reduce and delete"")

      if not args.fast:
        if currentTest != lastAllRangeTest:
          if args.verbose:
            print(""*"" * 80 + ""\nPASS: trying all byte range removals..."")
          changed = True
          startingPos = 0
          while changed:
            changed = False
            for b in range(startingPos, len(currentTest)):
              if args.verbose:
                print(""Trying byte range removal from"", str(b) + ""..."")
              for v in range(b + 2, min(len(currentTest), b + maxByteRange)):
                if (v-b) in [4, 8]:
                  continue
                newTest = currentTest[:b] + currentTest[v:]
                r = writeAndRunCandidate(newTest)
                if checks(r):
                  print(""Byte range removal of bytes"", str(b) + ""-"" + str(v - 1),
                          ""reduced test to"", len(newTest), ""bytes"")
                  changed = True
                  updateCurrent(newTest)
                  startingPos = b
                  break
              if changed:
                break
            if not changed:
              for b in range(0, startingPos):
                if args.verbose:
                  print(""Trying byte range removal from"", str(b) + ""..."")
                for v in range(b + 2, min(len(currentTest), b + maxByteRange)):
                  if (v-b) in [4, 8]:
                    continue
                  newTest = currentTest[:b] + currentTest[v:]
                  r = writeAndRunCandidate(newTest)
                  if checks(r):
                    print(""Byte range removal of bytes"", str(b) + ""-"" + str(v - 1),
                            ""reduced test to"", len(newTest), ""bytes"")
                    changed = True
                    updateCurrent(newTest)
                    startingPos = b
                    break
                if changed:
                  break
          lastAllRangeTest = bytearray(currentTest)
          passInfo(""Byte range removal"")

      if (not args.noStructure) and (currentTest != lastOneOfSwapTest) and (len(s[0]) != 0):
        if args.verbose:
          print(""*"" * 80 + ""\nPASS: swapping structures..."")
        changed = True
        while changed:
          changed = False
          cuts = s[0]
          for i in range(len(cuts) - 1):
            cuti = cuts[i]
            bytesi = currentTest[cuti[0]:cuti[1] + 1]
            if args.verbose:
              print(""Trying structured swap from byte"", cuti[0], ""["" + "" "".join(map(str, bytesi)) + ""]"")
            for j in range(i + 1, len(cuts)):
              cutj = cuts[j]
              if cutj[0] > cuti[1]:
                bytesj = currentTest[cutj[0]:cutj[1] + 1]
                if (len(bytesj) > 0) and (bytesi > bytesj):
                  newTest = currentTest[:cuti[0]] + bytesj + currentTest[cuti[1] + 1:cutj[0]]
                  newTest += bytesi
                  newTest += currentTest[cutj[1] + 1:]
                  newTest = bytearray(newTest)
                  r = writeAndRunCandidate(newTest)
                  if checks(r):
                    print(""Structured swap @ byte"", cuti[0], ""["" + "" "".join(map(str, bytesi)) + ""]"", ""with"",
                            cutj[0], ""["" + "" "".join(map(str, bytesj)) + ""]"")
                    changed = True
                    updateCurrent(newTest)
                    break
              if changed:
                break
            if changed:
              break
        lastOneOfSwapTest = bytearray(currentTest)
        passInfo(""Structured swap"")

      if currentTest != lastByteReduceTest:
          if args.verbose:
              print(""*"" * 80 + ""\nPASS: byte reductions..."")
          changed = True
          startingPos = 0
          while changed:
            changed = False
            for b in range(startingPos, len(currentTest)):
              for v in range(0, currentTest[b]):
                newTest = bytearray(currentTest)
                newTest[b] = v
                r = writeAndRunCandidate(newTest)
                if checks(r):
                  print(""Reduced byte"", b, ""from"", currentTest[b], ""to"", v)
                  changed = True
                  updateCurrent(newTest)
                  startingPos = b + 1
                  break
              if changed:
                break
            if changed:
              continue
            for b in range(0, startingPos):
              for v in range(0, currentTest[b]):
                newTest = bytearray(currentTest)
                newTest[b] = v
                r = writeAndRunCandidate(newTest)
                if checks(r):
                  print(""Reduced byte"", b, ""from"", currentTest[b], ""to"", v)
                  changed = True
                  updateCurrent(newTest)
                  startingPos = b + 1
                  break
              if changed:
                break
          lastByteReduceTest = bytearray(currentTest)
          passInfo(""Byte reduce"")

      if (args.slow or args.slowest) and (oldTest == currentTest):
        if currentTest != lastPatternSearchTest:
          if args.verbose:
            print(""*"" * 80 + ""\nPASS: byte pattern search..."")
          changed = True
          while changed:
            changed = False
            for b1 in range(0, len(currentTest)-4):
              if args.verbose:
                print(""Trying byte pattern search from byte"", str(b1) + ""..."")
              for b2 in range(b1 + 2, len(currentTest) - 4):
                v1 = (currentTest[b1], currentTest[b1 + 1])
                v2 = (currentTest[b2], currentTest[b2 + 1])
                if (v1 == v2):
                  ba = bytearray(v1)
                  part1 = currentTest[:b1]
                  part2 = currentTest[b1 + 2:b2]
                  part3 = currentTest[b2 + 2:]
                  banews = []
                  banews.append(ba[0:1])
                  banews.append(ba[1:2])
                  if ba[0] > 0:
                    for v in range(0, ba[0]):
                      banews.append(bytearray([v, ba[1]]))
                    banews.append(bytearray([ba[0] - 1]))
                  if ba[1] > 0:
                    for v in range(0, ba[1]):
                      banews.append(bytearray([ba[0], v]))
                  for banew in banews:
                    newTest = part1 + banew + part2 + banew + part3
                    r = writeAndRunCandidate(newTest)
                    if checks(r):
                      print(""Byte pattern"", tuple(ba), ""at"", b1, ""and"", b2, ""changed to"", tuple(banew))
                      changed = True
                      updateCurrent(newTest)
                      break
                  if changed:
                    break
              if changed:
                break
          lastPatternSearchTest = bytearray(currentTest)
          passInfo(""Byte pattern change"")

        if oldTest == currentTest:
          print(""*"" * 80)
          print(""DONE: NO (MORE) REDUCTIONS FOUND"")
  except TimeoutException:
    print(""*"" * 80)
    print(""DONE: REDUCTION TIMED OUT AFTER"", args.timeout, ""SECONDS"")

  print(""="" * 80)
  percent = 100.0 * ((initialSize - len(currentTest)) / initialSize)
  print(""Completed"", iteration, ""iterations:"", round(time.time()-start, 2), ""secs /"",
          candidateRuns, ""execs /"", str(round(percent, 2)) + ""% reduction"")

  if not args.noPad:
    if (s[1] + 1) > len(currentTest):
      print(""Padding test with"", (s[1] + 1) - len(currentTest), ""zeroes"")
      padding = bytearray('\x00' * ((s[1] + 1) - len(currentTest)), 'utf-8')
      currentTest = currentTest + padding
  
  print(""Writing reduced test with"", len(currentTest), ""bytes to"", out)
    
  with open(out, 'wb') as outf:
    outf.write(currentTest)

  return 0",_118309.py,424,"for b in range(startingPos, len(currentTest)):
    if args.verbose:
        print('Trying byte range removal from', str(b) + '...')
    for v in range(b + 2, min(len(currentTest), b + maxByteRange)):
        if v - b in [4, 8]:
            continue
        newTest = currentTest[:b] + currentTest[v:]
        r = writeAndRunCandidate(newTest)
        if checks(r):
            print('Byte range removal of bytes', str(b) + '-' + str(v - 1), 'reduced test to', len(newTest), 'bytes')
            changed = True
            updateCurrent(newTest)
            startingPos = b
            break
    if changed:
        break","if not changed:
    for b in range(0, startingPos):
        if args.verbose:
            print('Trying byte range removal from', str(b) + '...')
        for v in range(b + 2, min(len(currentTest), b + maxByteRange)):
            if v - b in [4, 8]:
                continue
            newTest = currentTest[:b] + currentTest[v:]
            r = writeAndRunCandidate(newTest)
            if checks(r):
                print('Byte range removal of bytes', str(b) + '-' + str(v - 1), 'reduced test to', len(newTest), 'bytes')
                changed = True
                updateCurrent(newTest)
                startingPos = b
                break
        if changed:
            break","for b in range(startingPos, len(currentTest)):
    if args.verbose:
        print('Trying byte range removal from', str(b) + '...')
    for v in range(b + 2, min(len(currentTest), b + maxByteRange)):
        if v - b in [4, 8]:
            continue
        newTest = currentTest[:b] + currentTest[v:]
        r = writeAndRunCandidate(newTest)
        if checks(r):
            print('Byte range removal of bytes', str(b) + '-' + str(v - 1), 'reduced test to', len(newTest), 'bytes')
            changed = True
            updateCurrent(newTest)
            startingPos = b
            break
    if changed:
        break
else:
    for b in range(0, startingPos):
        if args.verbose:
            print('Trying byte range removal from', str(b) + '...')
        for v in range(b + 2, min(len(currentTest), b + maxByteRange)):
            if v - b in [4, 8]:
                continue
            newTest = currentTest[:b] + currentTest[v:]
            r = writeAndRunCandidate(newTest)
            if checks(r):
                print('Byte range removal of bytes', str(b) + '-' + str(v - 1), 'reduced test to', len(newTest), 'bytes')
                changed = True
                updateCurrent(newTest)
                startingPos = b
                break
        if changed:
            break"
https://github.com/Chia-Network/chia-blockchain/tree/master/chia/plotting/manager.py,"def _refresh_task(self):
        while self._refreshing_enabled:

            while not self.needs_refresh() and self._refreshing_enabled:
                time.sleep(1)

            if not self._refreshing_enabled:
                return

            plot_filenames: Dict[Path, List[Path]] = get_plot_filenames(self.root_path)
            plot_directories: Set[Path] = set(plot_filenames.keys())
            plot_paths: List[Path] = []
            for paths in plot_filenames.values():
                plot_paths += paths

            total_result: PlotRefreshResult = PlotRefreshResult()
            total_size = len(plot_paths)

            self._refresh_callback(PlotRefreshEvents.started, PlotRefreshResult(remaining=total_size))

            # First drop all plots we have in plot_filename_paths but not longer in the filesystem or set in config
            def plot_removed(test_path: Path):
                return not test_path.exists() or test_path.parent not in plot_directories

            filenames_to_remove: List[str] = []
            for plot_filename, paths_entry in self.plot_filename_paths.items():
                loaded_path, duplicated_paths = paths_entry
                loaded_plot = Path(loaded_path) / Path(plot_filename)
                if plot_removed(loaded_plot):
                    filenames_to_remove.append(plot_filename)
                    if loaded_plot in self.plots:
                        del self.plots[loaded_plot]
                    total_result.removed += 1
                    # No need to check the duplicates here since we drop the whole entry
                    continue

                paths_to_remove: List[str] = []
                for path in duplicated_paths:
                    if plot_removed(Path(path) / Path(plot_filename)):
                        paths_to_remove.append(path)
                        total_result.removed += 1
                for path in paths_to_remove:
                    duplicated_paths.remove(path)

            for filename in filenames_to_remove:
                del self.plot_filename_paths[filename]

            for remaining, batch in list_to_batches(plot_paths, self.refresh_parameter.batch_size):
                batch_result: PlotRefreshResult = self.refresh_batch(batch, plot_directories)
                if not self._refreshing_enabled:
                    self.log.debug(""refresh_plots: Aborted"")
                    break
                # Set the remaining files since `refresh_batch()` doesn't know them but we want to report it
                batch_result.remaining = remaining
                total_result.loaded += batch_result.loaded
                total_result.processed += batch_result.processed
                total_result.duration += batch_result.duration

                self._refresh_callback(PlotRefreshEvents.batch_processed, batch_result)
                if remaining == 0:
                    break
                batch_sleep = self.refresh_parameter.batch_sleep_milliseconds
                self.log.debug(f""refresh_plots: Sleep {batch_sleep} milliseconds"")
                time.sleep(float(batch_sleep) / 1000.0)

            if self._refreshing_enabled:
                self._refresh_callback(PlotRefreshEvents.done, total_result)

            # Cleanup unused cache
            available_ids = set([plot_info.prover.get_id() for plot_info in self.plots.values()])
            invalid_cache_keys = [plot_id for plot_id in self.cache.keys() if plot_id not in available_ids]
            self.cache.remove(invalid_cache_keys)
            self.log.debug(f""_refresh_task: cached entries removed: {len(invalid_cache_keys)}"")

            if self.cache.changed():
                self.cache.save()

            self.last_refresh_time = time.time()

            self.log.debug(
                f""_refresh_task: total_result.loaded {total_result.loaded}, ""
                f""total_result.removed {total_result.removed}, ""
                f""total_duration {total_result.duration:.2f} seconds""
            )",_119141.py,48,"for (remaining, batch) in list_to_batches(plot_paths, self.refresh_parameter.batch_size):
    batch_result: PlotRefreshResult = self.refresh_batch(batch, plot_directories)
    if not self._refreshing_enabled:
        self.log.debug('refresh_plots: Aborted')
        break
    batch_result.remaining = remaining
    total_result.loaded += batch_result.loaded
    total_result.processed += batch_result.processed
    total_result.duration += batch_result.duration
    self._refresh_callback(PlotRefreshEvents.batch_processed, batch_result)
    if remaining == 0:
        break
    batch_sleep = self.refresh_parameter.batch_sleep_milliseconds
    self.log.debug(f'refresh_plots: Sleep {batch_sleep} milliseconds')
    time.sleep(float(batch_sleep) / 1000.0)","if self._refreshing_enabled:
    self._refresh_callback(PlotRefreshEvents.done, total_result)","for (remaining, batch) in list_to_batches(plot_paths, self.refresh_parameter.batch_size):
    batch_result: PlotRefreshResult = self.refresh_batch(batch, plot_directories)
    if not self._refreshing_enabled:
        self.log.debug('refresh_plots: Aborted')
        break
    batch_result.remaining = remaining
    total_result.loaded += batch_result.loaded
    total_result.processed += batch_result.processed
    total_result.duration += batch_result.duration
    self._refresh_callback(PlotRefreshEvents.batch_processed, batch_result)
    if remaining == 0:
        break
    batch_sleep = self.refresh_parameter.batch_sleep_milliseconds
    self.log.debug(f'refresh_plots: Sleep {batch_sleep} milliseconds')
    time.sleep(float(batch_sleep) / 1000.0)
else:
    self._refresh_callback(PlotRefreshEvents.done, total_result)"
https://github.com/pychess/pychess/tree/master/lib/pychess/perspectives/games/annotationPanel.py,"def tag_event_handler(self, tag, widget, event, iter):
        """"""
        The method handles the event specific to a tag, which is further processed
        by the button event of the main widget.
        """"""
        if (event.type == Gdk.EventType.BUTTON_PRESS) and (tag.get_property(""name"") == ""remove-variation""):
            offset = iter.get_offset()
            node = None
            for n in self.nodelist:
                if offset >= n[""start""] and offset < n[""end""]:
                    node = n
                    break
            if node is None:
                return
            self.variation_to_remove = node
        return False",_121248.py,9,"for n in self.nodelist:
    if offset >= n['start'] and offset < n['end']:
        node = n
        break","if node is None:
    return","for n in self.nodelist:
    if offset >= n['start'] and offset < n['end']:
        node = n
        break
else:
    return"
https://github.com/corelan/mona/tree/master//mona.py,"def procUpdate(args):
			""""""
			Function to update mona and optionally windbglib to the latest version
			
			Arguments : none
			
			Returns : new version of mona/windbglib (if available)
			""""""

			updateproto = ""https""

			#debugger version	
			imversion = __IMM__
			#url
			dbg.setStatusBar(""Running update process..."")
			dbg.updateLog()
			updateurl = ""https://github.com/corelan/mona/raw/master/mona.py""
			
			currentversion,currentrevision = getVersionInfo(inspect.stack()[0][1])
			u = """"
			try:
				u = urllib.urlretrieve(updateurl)
				newversion,newrevision = getVersionInfo(u[0])
				if newversion != """" and newrevision != """":
					dbg.log(""[+] Version compare :"")
					dbg.log(""    Current Version : %s, Current Revision : %s"" % (currentversion,currentrevision))
					dbg.log(""    Latest Version : %s, Latest Revision : %s"" % (newversion,newrevision))
				else:
					dbg.log(""[-] Unable to check latest version (corrupted file ?), try again later"",highlight=1)
					return
			except:
				dbg.log(""[-] Unable to check latest version (download error). Try again later"",highlight=1)
				dbg.log(""    Meanwhile, please check/confirm that you're running a recent version of python 2.7 (2.7.14 or higher)"", highlight=1)
				return
			#check versions
			doupdate = False
			if newversion != """" and newrevision != """":
				if currentversion != newversion:
					doupdate = True
				else:
					if int(currentrevision) < int(newrevision):
						doupdate = True
				
			if doupdate:
				dbg.log(""[+] New version available"",highlight=1)
				dbg.log(""    Updating to %s r%s"" % (newversion,newrevision),highlight=1)
				try:
					shutil.copyfile(u[0],inspect.stack()[0][1])
					dbg.log(""    Done"")					
				except:
					dbg.log(""    ** Unable to update mona.py"",highlight=1)
				currentversion,currentrevision = getVersionInfo(inspect.stack()[0][1])
				dbg.log(""[+] Current version : %s r%s"" % (currentversion,currentrevision))
			else:
				dbg.log(""[+] You are running the latest version"")

			# update windbglib if needed
			if __DEBUGGERAPP__ == ""WinDBG"":
				dbg.log(""[+] Locating windbglib path"")
				paths = sys.path
				filefound = False
				libfile = """"
				for ppath in paths:
					libfile = ppath + ""\\windbglib.py""
					if os.path.isfile(libfile):
						filefound=True
						break
				if not filefound:
					dbg.log(""    ** Unable to find windbglib.py ! **"")
				else:
					dbg.log(""[+] Checking if %s needs an update..."" % libfile)
					updateurl = ""https://github.com/corelan/windbglib/raw/master/windbglib.py""

					currentversion,currentrevision = getVersionInfo(libfile)
					u = """"
					try:
						u = urllib.urlretrieve(updateurl)
						newversion,newrevision = getVersionInfo(u[0])
						if newversion != """" and newrevision != """":
							dbg.log(""[+] Version compare :"")
							dbg.log(""    Current Version : %s, Current Revision : %s"" % (currentversion,currentrevision))
							dbg.log(""    Latest Version : %s, Latest Revision : %s"" % (newversion,newrevision))
						else:
							dbg.log(""[-] Unable to check latest version (corrupted file ?), try again later"",highlight=1)
							return
					except:
						dbg.log(""[-] Unable to check latest version (download error). Try again later"",highlight=1)
						dbg.log(""    Meanwhile, please check/confirm that you're running a recent version of python 2.7 (2.7.14 or higher)"", highlight=1)
						return

					#check versions
					doupdate = False
					if newversion != """" and newrevision != """":
						if currentversion != newversion:
							doupdate = True
						else:
							if int(currentrevision) < int(newrevision):
								doupdate = True
						
					if doupdate:
						dbg.log(""[+] New version available"",highlight=1)
						dbg.log(""    Updating to %s r%s"" % (newversion,newrevision),highlight=1) 
						try:
							shutil.copyfile(u[0],libfile)
							dbg.log(""    Done"")					
						except:
							dbg.log(""    ** Unable to update windbglib.py"",highlight=1)
						currentversion,currentrevision = getVersionInfo(libfile)
						dbg.log(""[+] Current version : %s r%s"" % (currentversion,currentrevision))
					else:
						dbg.log(""[+] You are running the latest version"")

			dbg.setStatusBar(""Done."")
			return",_121686.py,63,"for ppath in paths:
    libfile = ppath + '\\windbglib.py'
    if os.path.isfile(libfile):
        filefound = True
        break","if not filefound:
    dbg.log('    ** Unable to find windbglib.py ! **')
else:
    dbg.log('[+] Checking if %s needs an update...' % libfile)
    updateurl = 'https://github.com/corelan/windbglib/raw/master/windbglib.py'
    (currentversion, currentrevision) = getVersionInfo(libfile)
    u = ''
    try:
        u = urllib.urlretrieve(updateurl)
        (newversion, newrevision) = getVersionInfo(u[0])
        if newversion != '' and newrevision != '':
            dbg.log('[+] Version compare :')
            dbg.log('    Current Version : %s, Current Revision : %s' % (currentversion, currentrevision))
            dbg.log('    Latest Version : %s, Latest Revision : %s' % (newversion, newrevision))
        else:
            dbg.log('[-] Unable to check latest version (corrupted file ?), try again later', highlight=1)
            return
    except:
        dbg.log('[-] Unable to check latest version (download error). Try again later', highlight=1)
        dbg.log(""    Meanwhile, please check/confirm that you're running a recent version of python 2.7 (2.7.14 or higher)"", highlight=1)
        return
    doupdate = False
    if newversion != '' and newrevision != '':
        if currentversion != newversion:
            doupdate = True
        elif int(currentrevision) < int(newrevision):
            doupdate = True
    if doupdate:
        dbg.log('[+] New version available', highlight=1)
        dbg.log('    Updating to %s r%s' % (newversion, newrevision), highlight=1)
        try:
            shutil.copyfile(u[0], libfile)
            dbg.log('    Done')
        except:
            dbg.log('    ** Unable to update windbglib.py', highlight=1)
        (currentversion, currentrevision) = getVersionInfo(libfile)
        dbg.log('[+] Current version : %s r%s' % (currentversion, currentrevision))
    else:
        dbg.log('[+] You are running the latest version')","for ppath in paths:
    libfile = ppath + '\\windbglib.py'
    if os.path.isfile(libfile):
        dbg.log('[+] Checking if %s needs an update...' % libfile)
        updateurl = 'https://github.com/corelan/windbglib/raw/master/windbglib.py'
        (currentversion, currentrevision) = getVersionInfo(libfile)
        u = ''
        try:
            u = urllib.urlretrieve(updateurl)
            (newversion, newrevision) = getVersionInfo(u[0])
            if newversion != '' and newrevision != '':
                dbg.log('[+] Version compare :')
                dbg.log('    Current Version : %s, Current Revision : %s' % (currentversion, currentrevision))
                dbg.log('    Latest Version : %s, Latest Revision : %s' % (newversion, newrevision))
            else:
                dbg.log('[-] Unable to check latest version (corrupted file ?), try again later', highlight=1)
                return
        except:
            dbg.log('[-] Unable to check latest version (download error). Try again later', highlight=1)
            dbg.log(""    Meanwhile, please check/confirm that you're running a recent version of python 2.7 (2.7.14 or higher)"", highlight=1)
            return
        doupdate = False
        if newversion != '' and newrevision != '':
            if currentversion != newversion:
                doupdate = True
            elif int(currentrevision) < int(newrevision):
                doupdate = True
        if doupdate:
            dbg.log('[+] New version available', highlight=1)
            dbg.log('    Updating to %s r%s' % (newversion, newrevision), highlight=1)
            try:
                shutil.copyfile(u[0], libfile)
                dbg.log('    Done')
            except:
                dbg.log('    ** Unable to update windbglib.py', highlight=1)
            (currentversion, currentrevision) = getVersionInfo(libfile)
            dbg.log('[+] Current version : %s r%s' % (currentversion, currentrevision))
        else:
            dbg.log('[+] You are running the latest version')
        break
else:
    dbg.log('    ** Unable to find windbglib.py ! **')"
https://github.com/DLLXW/data-science-competition/tree/master/else/--AI+z/code/uer/utils/tokenizer.py,"def tokenize(self, text):
    """"""Tokenizes a piece of text into its word pieces.
    This uses a greedy longest-match-first algorithm to perform tokenization
    using the given vocabulary.
    For example:
      input = ""unaffable""
      output = [""un"", ""##aff"", ""##able""]
    Args:
      text: A single token or whitespace separated tokens. This should have
        already been passed through `BasicTokenizer.
    Returns:
      A list of wordpiece tokens.
    """"""

    text = convert_to_unicode(text)

    output_tokens = []
    for token in whitespace_tokenize(text):
      chars = list(token)
      if len(chars) > self.max_input_chars_per_word:
        output_tokens.append(self.unk_token)
        continue

      is_bad = False
      start = 0
      sub_tokens = []
      while start < len(chars):
        end = len(chars)
        cur_substr = None
        while start < end:
          substr = """".join(chars[start:end])
          if start > 0:
            substr = ""##"" + six.ensure_str(substr)
          if substr in self.vocab:
            cur_substr = substr
            break
          end -= 1
        if cur_substr is None:
          is_bad = True
          break
        sub_tokens.append(cur_substr)
        start = end

      if is_bad:
        output_tokens.append(self.unk_token)
      else:
        output_tokens.extend(sub_tokens)
    return output_tokens",_12602.py,27,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + six.ensure_str(substr)
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end","if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + six.ensure_str(substr)
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)"
https://github.com/DLLXW/data-science-competition/tree/master/else/--AI+z/code/uer/utils/tokenizer.py,"def tokenize(self, text):
    """"""Tokenizes a piece of text into its word pieces.
    This uses a greedy longest-match-first algorithm to perform tokenization
    using the given vocabulary.
    For example:
      input = ""unaffable""
      output = [""un"", ""##aff"", ""##able""]
    Args:
      text: A single token or whitespace separated tokens. This should have
        already been passed through `BasicTokenizer.
    Returns:
      A list of wordpiece tokens.
    """"""

    text = convert_to_unicode(text)

    output_tokens = []
    for token in whitespace_tokenize(text):
      chars = list(token)
      if len(chars) > self.max_input_chars_per_word:
        output_tokens.append(self.unk_token)
        continue

      is_bad = False
      start = 0
      sub_tokens = []
      while start < len(chars):
        end = len(chars)
        cur_substr = None
        while start < end:
          substr = """".join(chars[start:end])
          if start > 0:
            substr = ""##"" + six.ensure_str(substr)
          if substr in self.vocab:
            cur_substr = substr
            break
          end -= 1
        if cur_substr is None:
          is_bad = True
          break
        sub_tokens.append(cur_substr)
        start = end

      if is_bad:
        output_tokens.append(self.unk_token)
      else:
        output_tokens.extend(sub_tokens)
    return output_tokens",_12602.py,30,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + six.ensure_str(substr)
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1","if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + six.ensure_str(substr)
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break"
https://github.com/mesosphere/marathon-lb/tree/master//utils.py,"def _assign_new_service_port(self, app, task_port):
        assert self.can_assign

        if self.max_ports <= len(self.ports_by_app):
            logger.warning(""Service ports are exhausted"")
            return None

        # We don't want to be searching forever, so limit the number of times
        # we clash to the number of remaining ports.
        ports = self.ports_by_app.values()
        port = None
        for i in range(MAX_CLASHES):
            hash_str = ""%s-%s-%s"" % (app['id'], task_port, i)
            hash_val = hashlib.sha1(hash_str.encode(""utf-8"")).hexdigest()
            hash_int = int(hash_val[:8], 16)
            trial_port = self.min_port + (hash_int % self.max_ports)
            if trial_port not in ports:
                port = trial_port
                break
        if port is None:
            for port in range(self.min_port, self.max_port + 1):
                if port not in ports:
                    break

        # We must have assigned a unique port by now since we know there were
        # some available.
        assert port and port not in ports, port

        logger.debug(""Assigned new port: %d"", port)
        return port",_13680.py,12,"for i in range(MAX_CLASHES):
    hash_str = '%s-%s-%s' % (app['id'], task_port, i)
    hash_val = hashlib.sha1(hash_str.encode('utf-8')).hexdigest()
    hash_int = int(hash_val[:8], 16)
    trial_port = self.min_port + hash_int % self.max_ports
    if trial_port not in ports:
        port = trial_port
        break","if port is None:
    for port in range(self.min_port, self.max_port + 1):
        if port not in ports:
            break","for i in range(MAX_CLASHES):
    hash_str = '%s-%s-%s' % (app['id'], task_port, i)
    hash_val = hashlib.sha1(hash_str.encode('utf-8')).hexdigest()
    hash_int = int(hash_val[:8], 16)
    trial_port = self.min_port + hash_int % self.max_ports
    if trial_port not in ports:
        port = trial_port
        break
else:
    for port in range(self.min_port, self.max_port + 1):
        if port not in ports:
            break"
https://github.com/EssayKillerBrain/EssayKiller_V2/tree/master/LanguageNetwork/GPT2/dataset/tokenization/tokenization.py,"def tokenize(self, text):
    """"""Tokenizes a piece of text into its word pieces.

    This uses a greedy longest-match-first algorithm to perform tokenization
    using the given vocabulary.

    For example:
      input = ""unaffable""
      output = [""un"", ""##aff"", ""##able""]

    Args:
      text: A single token or whitespace separated tokens. This should have
        already been passed through `BasicTokenizer.

    Returns:
      A list of wordpiece tokens.
    """"""

    text = convert_to_unicode(text)

    output_tokens = []
    for token in whitespace_tokenize(text):
      chars = list(token)
      if len(chars) > self.max_input_chars_per_word:
        output_tokens.append(self.unk_token)
        continue

      is_bad = False
      start = 0
      sub_tokens = []
      while start < len(chars):
        end = len(chars)
        cur_substr = None
        while start < end:
          substr = """".join(chars[start:end])
          if start > 0:
            substr = ""##"" + substr
          if substr in self.vocab:
            cur_substr = substr
            break
          end -= 1
        if cur_substr is None:
          is_bad = True
          break
        sub_tokens.append(cur_substr)
        start = end

      if is_bad:
        output_tokens.append(self.unk_token)
      else:
        output_tokens.extend(sub_tokens)
    return output_tokens",_13770.py,31,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end","if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)"
https://github.com/EssayKillerBrain/EssayKiller_V2/tree/master/LanguageNetwork/GPT2/dataset/tokenization/tokenization.py,"def tokenize(self, text):
    """"""Tokenizes a piece of text into its word pieces.

    This uses a greedy longest-match-first algorithm to perform tokenization
    using the given vocabulary.

    For example:
      input = ""unaffable""
      output = [""un"", ""##aff"", ""##able""]

    Args:
      text: A single token or whitespace separated tokens. This should have
        already been passed through `BasicTokenizer.

    Returns:
      A list of wordpiece tokens.
    """"""

    text = convert_to_unicode(text)

    output_tokens = []
    for token in whitespace_tokenize(text):
      chars = list(token)
      if len(chars) > self.max_input_chars_per_word:
        output_tokens.append(self.unk_token)
        continue

      is_bad = False
      start = 0
      sub_tokens = []
      while start < len(chars):
        end = len(chars)
        cur_substr = None
        while start < end:
          substr = """".join(chars[start:end])
          if start > 0:
            substr = ""##"" + substr
          if substr in self.vocab:
            cur_substr = substr
            break
          end -= 1
        if cur_substr is None:
          is_bad = True
          break
        sub_tokens.append(cur_substr)
        start = end

      if is_bad:
        output_tokens.append(self.unk_token)
      else:
        output_tokens.extend(sub_tokens)
    return output_tokens",_13770.py,34,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1","if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break"
https://github.com/shuup/shuup/tree/master/shuup/admin/modules/media/views.py,"def handle_get_folders(self, data):
        shop = get_shop(self.request)

        users_owned_folders = Folder.objects.filter(media_folder__owners=self.user)
        root_folders = None

        # If the user has a root folder and not permission to view all folders
        if len(users_owned_folders) > 0 and not has_permission(self.user, ""media.view-all""):
            all_accessed_folders = list(
                Folder._tree_manager.filter(_get_folder_query_filter(shop, self.user)).order_by(
                    users_owned_folders.first()._mptt_meta.level_attr
                )
            )
            get_media_folder = cached_load(""SHUUP_GET_MEDIA_FOLDER_FROM_FOLDER"")

            # We will need to change the tree ordering of folders that the user owns,
            # so the owning folders shows up under the root folder.
            # This is because if admin gives view access to a folder that has a lower level (closer to the root),
            # then the folder the user owns. It would be stacked under the folder that the admin has given access to,
            # insted of under the root folder.

            ordered_folders = []

            for index, folder in enumerate(all_accessed_folders):
                media_folder = get_media_folder(folder)
                if self.user in media_folder.owners.all():
                    setattr(all_accessed_folders[index], folder._mptt_meta.level_attr, 0)
                    ordered_folders.insert(0, all_accessed_folders[index])
                else:
                    in_path = False
                    for folder_on_path in folder.logical_path:
                        if folder_on_path in all_accessed_folders:
                            ordered_folders.append(folder)
                            in_path = True
                            break

                    if not in_path:
                        setattr(folder, folder._mptt_meta.level_attr, 0)
                        ordered_folders.insert(0, folder)

            root_folders = get_cached_trees(ordered_folders)
        else:
            # Everything is shown under the fake root folder that is actually not a real folder
            root_folders = get_cached_trees(Folder._tree_manager.filter(_get_folder_query_filter(shop, self.user)))

        return JsonResponse({""rootFolder"": filer_folder_to_json_dict(None, root_folders, self.user)})",_14381.py,31,"for folder_on_path in folder.logical_path:
    if folder_on_path in all_accessed_folders:
        ordered_folders.append(folder)
        in_path = True
        break","if not in_path:
    setattr(folder, folder._mptt_meta.level_attr, 0)
    ordered_folders.insert(0, folder)","for folder_on_path in folder.logical_path:
    if folder_on_path in all_accessed_folders:
        ordered_folders.append(folder)
        break
else:
    setattr(folder, folder._mptt_meta.level_attr, 0)
    ordered_folders.insert(0, folder)"
https://github.com/microsoft/maro/tree/master/maro/simulator/core.py,"def _init_business_engine(self) -> None:
        """"""Initialize business engine object.

        NOTE:
        1. For built-in scenarios, they will always under ""maro/simulator/scenarios"" folder.
        2. For external scenarios, the business engine instance is built with the loaded business engine class.
        """"""
        max_tick = self._start_tick + self._durations

        if self._business_engine_cls is not None:
            business_class = self._business_engine_cls
        else:
            # Combine the business engine import path.
            business_class_path = f""maro.simulator.scenarios.{self._scenario}.business_engine""

            # Load the module to find business engine for that scenario.
            business_module = import_module(business_class_path)

            business_class = None

            for _, obj in getmembers(business_module, isclass):
                if issubclass(obj, AbsBusinessEngine) and obj != AbsBusinessEngine:
                    # We find it.
                    business_class = obj

                    break

            if business_class is None:
                raise BusinessEngineNotFoundError()

        self._business_engine: AbsBusinessEngine = business_class(
            event_buffer=self._event_buffer,
            topology=self._topology,
            start_tick=self._start_tick,
            max_tick=max_tick,
            snapshot_resolution=self._snapshot_resolution,
            max_snapshots=self._max_snapshots,
            additional_options=self._additional_options,
        )",_14815.py,21,"for (_, obj) in getmembers(business_module, isclass):
    if issubclass(obj, AbsBusinessEngine) and obj != AbsBusinessEngine:
        business_class = obj
        break","if business_class is None:
    raise BusinessEngineNotFoundError()","for (_, obj) in getmembers(business_module, isclass):
    if issubclass(obj, AbsBusinessEngine) and obj != AbsBusinessEngine:
        break
else:
    raise BusinessEngineNotFoundError()"
https://github.com/bunkerity/bunkerized-nginx/tree/master/autoconf/src/Config.py,"def __wait_docker(self, instances) :
		all_healthy = False
		i = 0
		while i < 120 :
			one_not_healthy = False
			for instance in instances :
				instance.reload()
				if instance.attrs[""State""][""Health""][""Status""] != ""healthy"" :
					one_not_healthy = True
					break
			if not one_not_healthy :
				all_healthy = True
				break
			time.sleep(1)
			i += 1
		return all_healthy",_14954.py,6,"for instance in instances:
    instance.reload()
    if instance.attrs['State']['Health']['Status'] != 'healthy':
        one_not_healthy = True
        break","if not one_not_healthy:
    all_healthy = True
    break","for instance in instances:
    instance.reload()
    if instance.attrs['State']['Health']['Status'] != 'healthy':
        break
else:
    all_healthy = True
    break"
https://github.com/skywind3000/ECDICT/tree/master//stardict.py,"def load (self, filename, encoding = None):
        content = open(filename, 'rb').read()
        if content[:3] == b'\xef\xbb\xbf':
            content = content[3:].decode('utf-8', 'ignore')
        elif encoding is not None:
            text = content.decode(encoding, 'ignore')
        else:
            text = None
            match = ['utf-8', sys.getdefaultencoding(), 'ascii']
            for encoding in match + ['gbk', 'latin1']:
                try:
                    text = content.decode(encoding)
                    break
                except:
                    pass
            if text is None:
                text = content.decode('utf-8', 'ignore')
        number = 0
        for line in text.split('\n'):
            number += 1
            line = line.strip('\r\n ')
            if (not line) or (line[:1] == ';'):
                continue
            pos = line.find('->')
            if not pos:
                continue
            stem = line[:pos].strip()
            p1 = stem.find('/')
            frq = 0
            if p1 >= 0:
                frq = int(stem[p1 + 1:].strip())
                stem = stem[:p1].strip()
            if not stem:
                continue
            if frq > 0:
                self._frqs[stem] = frq
            for word in line[pos + 2:].strip().split(','):
                p1 = word.find('/')
                if p1 >= 0:
                    word = word[:p1].strip()
                if not word:
                    continue
                self.add(stem, word.strip())
        return True",_15127.py,10,"for encoding in match + ['gbk', 'latin1']:
    try:
        text = content.decode(encoding)
        break
    except:
        pass","if text is None:
    text = content.decode('utf-8', 'ignore')","for encoding in match + ['gbk', 'latin1']:
    try:
        break
    except:
        pass
else:
    text = content.decode('utf-8', 'ignore')"
https://github.com/mmin18/LayoutCast/tree/master/ide/IDEAPlugin/out/production/IDEAPlugin/cast.py,"def __deps_list_gradle(list, project):
    str = ''
    with open(os.path.join(project, 'build.gradle'), 'r') as f:
        str = f.read()
    str = remove_comments(str)
    ideps = []
    # for depends in re.findall(r'dependencies\s*\{.*?\}', str, re.DOTALL | re.MULTILINE):
    for m in re.finditer(r'dependencies\s*\{', str):
        depends = balanced_braces(str[m.start():])
        for proj in re.findall(r'''compile project\(.*['""]:(.+)['""].*\)''', depends):
            ideps.append(proj.replace(':', '/'))
    if len(ideps) == 0:
        return

    path = project
    for i in range(1, 3):
        path = os.path.abspath(os.path.join(path, os.path.pardir))
        b = True
        deps = []
        for idep in ideps:
            dep = os.path.join(path, idep)
            if not os.path.isdir(dep):
                b = False
                break
            deps.append(dep)
        if b:
            for dep in deps:
                __deps_list_gradle(list, dep)
                if not dep in list:
                    list.append(dep)
            break",_18006.py,20,"for idep in ideps:
    dep = os.path.join(path, idep)
    if not os.path.isdir(dep):
        b = False
        break
    deps.append(dep)","if b:
    for dep in deps:
        __deps_list_gradle(list, dep)
        if not dep in list:
            list.append(dep)
    break","for idep in ideps:
    dep = os.path.join(path, idep)
    if not os.path.isdir(dep):
        break
    deps.append(dep)
else:
    for dep in deps:
        __deps_list_gradle(list, dep)
        if not dep in list:
            list.append(dep)
    break"
https://github.com/bitbrute/evillimiter/tree/master/evillimiter/menus/parser.py,"def parse(self, command):
        """"""
        Parses a given list of arguments
        """"""
        names = [x.name for x in (self._flag_commands + self._parameter_commands)]
        result_dict = dict.fromkeys(names, None)

        # indicates whether or not to skip the next command argument
        skip_next = False

        for i, arg in enumerate(command):
            if skip_next:
                skip_next = False
                continue

            if i == 0:
                # check if the first argument is a subparser
                for sp in self._subparsers:
                    if sp.identifier == arg:
                        # if subparser present, parse arguments there
                        result = sp.subparser.parse(command[(i + 1):])
                        if result is not None and sp.handler is not None:
                            # call the subparser's handler if available
                            sp.handler(result)

                        return result
            
            # indicates whether or not the argument has been processed
            is_arg_processed = False

            for cmd in self._flag_commands:
                if cmd.identifier == arg:
                    if cmd.type == CommandParser.CommandType.FLAG_COMMAND:
                        # if its a flag, set the flag to true
                        result_dict[cmd.name] = True
                        is_arg_processed = True
                        break
                    elif cmd.type == CommandParser.CommandType.PARAMETERIZED_FLAG_COMMAND:
                        if (len(command) - 1) < (i + 1):
                            # no more command arguments to process
                            IO.error('parameter for flag {}{}{} is missing'.format(IO.Fore.LIGHTYELLOW_EX, cmd.name, IO.Style.RESET_ALL))
                            return

                        # if parameterized flag, set value to next argument
                        value = command[i + 1]
                        result_dict[cmd.name] = value

                        # skip the next argument (already processed)
                        skip_next = True

                        is_arg_processed = True
                        break
            
            if not is_arg_processed:
                for cmd in self._parameter_commands:
                    # parameter command, since a flag could not be found
                    if result_dict[cmd.name] is None:
                        # set parameter value
                        result_dict[cmd.name] = arg
                        is_arg_processed = True
                        break

            if not is_arg_processed:
                IO.error('{}{}{} is an unknown command.'.format(IO.Fore.LIGHTYELLOW_EX, arg, IO.Style.RESET_ALL))
                return

        # check if there are any parameters missing
        for cmd in self._parameter_commands:
            if result_dict[cmd.name] is None:
                IO.error('parameter {}{}{} is missing'.format(IO.Fore.LIGHTYELLOW_EX, cmd.name, IO.Style.RESET_ALL))
                return

        # set unspecified flags to False instead of None
        for cmd in self._flag_commands:
            if cmd.type == CommandParser.CommandType.FLAG_COMMAND:
                if result_dict[cmd.name] is None:
                    result_dict[cmd.name] = False

        result_tuple = collections.namedtuple('ParseResult', sorted(result_dict))
        return result_tuple(**result_dict)",_1978.py,31,"for cmd in self._flag_commands:
    if cmd.identifier == arg:
        if cmd.type == CommandParser.CommandType.FLAG_COMMAND:
            result_dict[cmd.name] = True
            is_arg_processed = True
            break
        elif cmd.type == CommandParser.CommandType.PARAMETERIZED_FLAG_COMMAND:
            if len(command) - 1 < i + 1:
                IO.error('parameter for flag {}{}{} is missing'.format(IO.Fore.LIGHTYELLOW_EX, cmd.name, IO.Style.RESET_ALL))
                return
            value = command[i + 1]
            result_dict[cmd.name] = value
            skip_next = True
            is_arg_processed = True
            break","if not is_arg_processed:
    for cmd in self._parameter_commands:
        if result_dict[cmd.name] is None:
            result_dict[cmd.name] = arg
            is_arg_processed = True
            break","for cmd in self._flag_commands:
    if cmd.identifier == arg:
        if cmd.type == CommandParser.CommandType.FLAG_COMMAND:
            result_dict[cmd.name] = True
            is_arg_processed = True
            break
        elif cmd.type == CommandParser.CommandType.PARAMETERIZED_FLAG_COMMAND:
            if len(command) - 1 < i + 1:
                IO.error('parameter for flag {}{}{} is missing'.format(IO.Fore.LIGHTYELLOW_EX, cmd.name, IO.Style.RESET_ALL))
                return
            value = command[i + 1]
            result_dict[cmd.name] = value
            skip_next = True
            is_arg_processed = True
            break
else:
    for cmd in self._parameter_commands:
        if result_dict[cmd.name] is None:
            result_dict[cmd.name] = arg
            is_arg_processed = True
            break"
https://github.com/sobhe/hazm/tree/master/hazm/DadeganReader.py,"def chunked_trees(self):
		""""""    .

		Examples:
			>>> from hazm.Chunker import tree2brackets
			>>> tree2brackets(next(dadegan.chunked_trees()))
			'[  NP] [ PP] [    NP] [ PP] [  NP] [ _ VP] .'

		Yields:
			(str):    .
		""""""		
		for tree in self.trees():
			chunks = []
			for node in word_nodes(tree):
				n = node['address']
				item = (node['word'], node['mtag'])
				appended = False
				if node['ctag'] in {'PREP', 'POSTP'}:
					for d in node_deps(node):
						label = 'PP'
						if node['ctag'] == 'POSTP':
							label = 'POSTP'
						if d == n - 1 and type(chunks[-1]) == Tree and chunks[-1].label() == label:
							chunks[-1].append(item)
							appended = True
					if node['head'] == n - 1 and len(chunks) > 0 and type(chunks[-1]) == Tree and chunks[
						-1].label() == label:
						chunks[-1].append(item)
						appended = True
					if not appended:
						chunks.append(Tree(label, [item]))
				elif node['ctag'] in {'PUNC', 'CONJ', 'SUBR', 'PART'}:
					if item[0] in {""'"", '""', '(', ')', '{', '}', '[', ']', '-', '#', '', ''} and len(chunks) > 0 and type(chunks[-1]) == Tree:
						for l in chunks[-1].leaves():
							if l[1] == item[1]:
								chunks[-1].append(item)
								appended = True
								break
					if appended is not True:
						chunks.append(item)
				elif node['ctag'] in {'N', 'PREM', 'ADJ', 'PR', 'ADR', 'PRENUM', 'IDEN', 'POSNUM', 'SADV'}:
					if node['rel'] in {'MOZ', 'NPOSTMOD'}:
						if len(chunks) > 0:
							if type(chunks[-1]) == Tree:
								j = n - len(chunks[-1].leaves())
								chunks[-1].append(item)
							else:
								j = n - 1
								treeNode = Tree('NP', [chunks.pop(), item])
								chunks.append(treeNode)
							while j > node['head']:
								leaves = chunks.pop().leaves()
								if len(chunks) < 1:
									chunks.append(Tree('NP', leaves))
									j -= 1
								elif type(chunks[-1]) == Tree:
									j -= len(chunks[-1])
									for l in leaves:
										chunks[-1].append(l)
								else:
									leaves.insert(0, chunks.pop())
									chunks.append(Tree('NP', leaves))
									j -= 1
							continue
					elif node['rel'] == 'POSDEP' and tree.nodes[node['head']]['rel'] in {'NCONJ', 'AJCONJ'}:
						conj = tree.nodes[node['head']]
						if tree.nodes[conj['head']]['rel'] in {'MOZ', 'NPOSTMOD', 'AJCONJ', 'POSDEP'}:
							label = 'NP'
							leaves = [item]
							j = n - 1
							while j >= conj['head']:
								if type(chunks[-1]) is Tree:
									j -= len(chunks[-1].leaves())
									label = chunks[-1].label()
									leaves = chunks.pop().leaves() + leaves
								else:
									leaves.insert(0, chunks.pop())
									j -= 1
							chunks.append(Tree(label, leaves))
							appended = True
					elif node['head'] == n - 1 and len(chunks) > 0 and type(chunks[-1]) == Tree and not chunks[
						-1].label() == 'PP':
						chunks[-1].append(item)
						appended = True
					elif node['rel'] == 'AJCONJ' and tree.nodes[node['head']]['rel'] in {'NPOSTMOD', 'AJCONJ'}:
						np_nodes = [item]
						label = 'ADJP'
						i = n - node['head']
						while i > 0:
							if type(chunks[-1]) == Tree:
								label = chunks[-1].label()
								leaves = chunks.pop().leaves()
								i -= len(leaves)
								np_nodes = leaves + np_nodes
							else:
								i -= 1
								np_nodes.insert(0, chunks.pop())
						chunks.append(Tree(label, np_nodes))
						appended = True
					elif node['ctag'] == 'ADJ' and node['rel'] == 'POSDEP' and tree.nodes[node['head']]['ctag'] != 'CONJ':
						np_nodes = [item]
						i = n - node['head']
						while i > 0:
							label = 'ADJP'
							if type(chunks[-1]) == Tree:
								label = chunks[-1].label()
								leaves = chunks.pop().leaves()
								i -= len(leaves)
								np_nodes = leaves + np_nodes
							else:
								i -= 1
								np_nodes.insert(0, chunks.pop())
						chunks.append(Tree(label, np_nodes))
						appended = True
					for d in node_deps(node):
						if d == n - 1 and type(chunks[-1]) == Tree and chunks[
							-1].label() != 'PP' and appended is not True:
							label = chunks[-1].label()
							if node['rel'] == 'ADV':
								label = 'ADVP'
							elif label in {'ADJP', 'ADVP'}:
								if node['ctag'] == 'N':
									label = 'NP'
								elif node['ctag'] == 'ADJ':
									label = 'ADJP'
							leaves = chunks.pop().leaves()
							leaves.append(item)
							chunks.append(Tree(label, leaves))
							appended = True
						elif tree.nodes[d]['rel'] == 'NPREMOD' and appended is not True:
							np_nodes = [item]
							i = n - d
							while i > 0:
								if type(chunks[-1]) == Tree:
									leaves = chunks.pop().leaves()
									i -= len(leaves)
									np_nodes = leaves + np_nodes
								else:
									i -= 1
									np_nodes.insert(0, chunks.pop())
							chunks.append(Tree('NP', np_nodes))
							appended = True
					if not appended:
						label = 'NP'
						if node['ctag'] == 'ADJ':
							label = 'ADJP'
						elif node['rel'] == 'ADV':
							label = 'ADVP'
						chunks.append(Tree(label, [item]))
				elif node['ctag'] in {'V'}:
					appended = False
					for d in node_deps(node):
						if d == n - 1 and type(chunks[-1]) == Tree and tree.nodes[d]['rel'] in {'NVE', 'ENC'} and appended is not True:
							leaves = chunks.pop().leaves()
							leaves.append(item)
							chunks.append(Tree('VP', leaves))
							appended = True
						elif tree.nodes[d]['rel'] in {'VPRT', 'NVE'}:
							vp_nodes = [item]
							i = n - d
							while i > 0:
								if type(chunks[-1]) == Tree:
									leaves = chunks.pop().leaves()
									i -= len(leaves)
									vp_nodes = leaves + vp_nodes
								else:
									i -= 1
									vp_nodes.insert(0, chunks.pop())
							chunks.append(Tree('VP', vp_nodes))
							appended = True
							break
					if not appended:
						chunks.append(Tree('VP', [item]))
				elif node['ctag'] in {'PSUS'}:
					if node['rel'] == 'ADV':
						chunks.append(Tree('ADVP', [item]))
					else:
						chunks.append(Tree('VP', [item]))
				elif node['ctag'] in {'ADV', 'SADV'}:
					appended = False
					for d in node_deps(node):
						if d == n - 1 and type(chunks[-1]) == Tree:
							leaves = chunks.pop().leaves()
							leaves.append(item)
							chunks.append(Tree('ADVP', leaves))
							appended = True
					if not appended:
						chunks.append(Tree('ADVP', [item]))

			yield Tree('S', chunks)",_21331.py,152,"for d in node_deps(node):
    if d == n - 1 and type(chunks[-1]) == Tree and (tree.nodes[d]['rel'] in {'NVE', 'ENC'}) and (appended is not True):
        leaves = chunks.pop().leaves()
        leaves.append(item)
        chunks.append(Tree('VP', leaves))
        appended = True
    elif tree.nodes[d]['rel'] in {'VPRT', 'NVE'}:
        vp_nodes = [item]
        i = n - d
        while i > 0:
            if type(chunks[-1]) == Tree:
                leaves = chunks.pop().leaves()
                i -= len(leaves)
                vp_nodes = leaves + vp_nodes
            else:
                i -= 1
                vp_nodes.insert(0, chunks.pop())
        chunks.append(Tree('VP', vp_nodes))
        appended = True
        break","if not appended:
    chunks.append(Tree('VP', [item]))","for d in node_deps(node):
    if d == n - 1 and type(chunks[-1]) == Tree and (tree.nodes[d]['rel'] in {'NVE', 'ENC'}) and (appended is not True):
        leaves = chunks.pop().leaves()
        leaves.append(item)
        chunks.append(Tree('VP', leaves))
        appended = True
    elif tree.nodes[d]['rel'] in {'VPRT', 'NVE'}:
        vp_nodes = [item]
        i = n - d
        while i > 0:
            if type(chunks[-1]) == Tree:
                leaves = chunks.pop().leaves()
                i -= len(leaves)
                vp_nodes = leaves + vp_nodes
            else:
                i -= 1
                vp_nodes.insert(0, chunks.pop())
        chunks.append(Tree('VP', vp_nodes))
        appended = True
        break
else:
    chunks.append(Tree('VP', [item]))"
https://github.com/mementum/backtrader/tree/master/backtrader/feeds/pandafeed.py,"def __init__(self):
        super(PandasData, self).__init__()

        # these ""colnames"" can be strings or numeric types
        colnames = list(self.p.dataname.columns.values)
        if self.p.datetime is None:
            # datetime is expected as index col and hence not returned
            pass

        # try to autodetect if all columns are numeric
        cstrings = filter(lambda x: isinstance(x, string_types), colnames)
        colsnumeric = not len(list(cstrings))

        # Where each datafield find its value
        self._colmapping = dict()

        # Build the column mappings to internal fields in advance
        for datafield in self.getlinealiases():
            defmapping = getattr(self.params, datafield)

            if isinstance(defmapping, integer_types) and defmapping < 0:
                # autodetection requested
                for colname in colnames:
                    if isinstance(colname, string_types):
                        if self.p.nocase:
                            found = datafield.lower() == colname.lower()
                        else:
                            found = datafield == colname

                        if found:
                            self._colmapping[datafield] = colname
                            break

                if datafield not in self._colmapping:
                    # autodetection requested and not found
                    self._colmapping[datafield] = None
                    continue
            else:
                # all other cases -- used given index
                self._colmapping[datafield] = defmapping",_24264.py,23,"for colname in colnames:
    if isinstance(colname, string_types):
        if self.p.nocase:
            found = datafield.lower() == colname.lower()
        else:
            found = datafield == colname
        if found:
            self._colmapping[datafield] = colname
            break","if datafield not in self._colmapping:
    self._colmapping[datafield] = None
    continue","for colname in colnames:
    if isinstance(colname, string_types):
        if self.p.nocase:
            found = datafield.lower() == colname.lower()
        else:
            found = datafield == colname
        if found:
            self._colmapping[datafield] = colname
            break
else:
    self._colmapping[datafield] = None
    continue"
https://github.com/cisco/mindmeld/tree/master/mindmeld/components/nlp.py,"def process_query(
        self, query, allowed_nlp_classes=None, dynamic_resource=None, verbose=False
    ):
        """"""Processes the given query using the full hierarchy of natural language processing models \
        trained for this application.

        Args:
            query (Query, or tuple): The user input query, or a list of the n-best transcripts \
                query objects.
            allowed_nlp_classes (dict, optional): A dictionary of the intent section of the \
                NLP hierarchy that is selected for NLP analysis. An example: ``{'close_door': {}}``
                where close_door is the intent. The intent belongs to the smart_home domain. \
                If allowed_nlp_classes is None, we use the normal model predict functionality.
            dynamic_resource (dict, optional): A dynamic resource to aid NLP inference.
            verbose (bool, optional): If True, returns class probabilities along with class \
                prediction.

        Returns:
            (ProcessedQuery): A processed query object that contains the prediction results from \
                applying the full hierarchy of natural language processing models to the input \
                query.
        """"""
        self._check_ready()

        if isinstance(query, (list, tuple)):
            top_query = query[0]
        else:
            top_query = query

        intent_proba = None
        if len(self.intents) > 1:
            # Check if the user has specified allowed intents
            if not allowed_nlp_classes:
                if verbose:
                    intent_proba = self.intent_classifier.predict_proba(
                        top_query, dynamic_resource=dynamic_resource
                    )
                    intent = intent_proba[0][0]
                else:
                    intent = self.intent_classifier.predict(
                        top_query, dynamic_resource=dynamic_resource
                    )
            else:
                if len(allowed_nlp_classes) == 1:
                    intent = list(allowed_nlp_classes.keys())[0]
                    if verbose:
                        intent_proba = [(intent, 1.0)]
                else:
                    sorted_intents = self.intent_classifier.predict_proba(
                        top_query, dynamic_resource=dynamic_resource
                    )
                    intent = None
                    if verbose:
                        intent_proba = sorted_intents
                    for ordered_intent, _ in sorted_intents:
                        if ordered_intent in allowed_nlp_classes.keys():
                            intent = ordered_intent
                            break

                    if not intent:
                        raise AllowedNlpClassesKeyError(
                            ""Could not find user inputted intent in NLP hierarchy""
                        )
        else:
            intent = list(self.intents.keys())[0]
            if verbose:
                intent_proba = [(intent, 1.0)]

        if allowed_nlp_classes and intent in allowed_nlp_classes:
            allowed_nlp_classes = allowed_nlp_classes[intent]
        else:
            allowed_nlp_classes = None

        processed_query = self.intents[intent].process_query(
            query, allowed_nlp_classes=allowed_nlp_classes,
            dynamic_resource=dynamic_resource, verbose=verbose
        )
        processed_query.intent = intent
        if intent_proba:
            intent_scores = dict(intent_proba)
            scores = processed_query.confidence or {}
            scores[""intents""] = intent_scores
            processed_query.confidence = scores
        return processed_query",_25320.py,55,"for (ordered_intent, _) in sorted_intents:
    if ordered_intent in allowed_nlp_classes.keys():
        intent = ordered_intent
        break","if not intent:
    raise AllowedNlpClassesKeyError('Could not find user inputted intent in NLP hierarchy')","for (ordered_intent, _) in sorted_intents:
    if ordered_intent in allowed_nlp_classes.keys():
        break
else:
    raise AllowedNlpClassesKeyError('Could not find user inputted intent in NLP hierarchy')"
https://github.com/csujedihy/lc-all-solutions/tree/master/411.minimum-unique-word-abbreviation/minimum-unique-word-abbreviation.py,"def minAbbreviation(self, target, dictionary):
    """"""
    :type target: str
    :type dictionary: List[str]
    :rtype: str
    """"""

    def dfs(w, start, res):
      res.append(w)
      for i in range(start, len(w)):
        for l in reversed(range(1, len(w) - i + 1)):
          dfs(w[:i] + [str(l)] + w[i + l:], i + 2, res)

    def match(src, dest):
      i = 0
      for c in src:
        if c.isdigit():
          jump = int(c)
          i += jump
        else:
          if c != dest[i]:
            return False
          i += 1
      return True

    if not dictionary:
      return str(len(target))
    wordLen = len(target)
    res = []
    dfs(list(target), 0, res)
    res.sort(key=lambda x: len(x))
    dictionary = filter(lambda s: len(s) == wordLen, dictionary)

    for w in res:
      allMiss = True
      for d in dictionary:
        if match(w, d):
          allMiss = False
          break
      if allMiss:
        return """".join(w)
    return None",_26121.py,36,"for d in dictionary:
    if match(w, d):
        allMiss = False
        break","if allMiss:
    return ''.join(w)","for d in dictionary:
    if match(w, d):
        break
else:
    return ''.join(w)"
https://github.com/quark-engine/quark-engine/tree/master/quark/core/apkinfo.py,"def get_method_bytecode(self, method_object: MethodObject) -> Set[MethodObject]:
        method_analysis = method_object.cache
        try:
            for (
                _,
                ins,
            ) in method_analysis.get_method().get_instructions_idx():
                bytecode_obj = None
                reg_list = []

                # count the number of the registers.
                length_operands = len(ins.get_operands())
                if length_operands == 0:
                    # No register, no parameter
                    bytecode_obj = BytecodeObject(
                        ins.get_name(),
                        None,
                        None,
                    )
                else:
                    index_of_parameter_starts = None
                    for i in range(length_operands - 1, -1, -1):
                        if not isinstance(ins.get_operands()[i][0], Operand):
                            index_of_parameter_starts = i
                            break

                    if index_of_parameter_starts is not None:
                        parameter = ins.get_operands()[index_of_parameter_starts]
                        parameter = (
                            parameter[2] if len(parameter) == 3 else parameter[1]
                        )

                        for i in range(index_of_parameter_starts):
                            reg_list.append(
                                ""v"" + str(ins.get_operands()[i][1]),
                            )
                    else:
                        parameter = None
                        for i in range(length_operands):
                            reg_list.append(
                                ""v"" + str(ins.get_operands()[i][1]),
                            )

                    bytecode_obj = BytecodeObject(ins.get_name(), reg_list, parameter)

                yield bytecode_obj
        except AttributeError:
            # TODO Log the rule here
            pass",_26508.py,22,"for i in range(length_operands - 1, -1, -1):
    if not isinstance(ins.get_operands()[i][0], Operand):
        index_of_parameter_starts = i
        break","if index_of_parameter_starts is not None:
    parameter = ins.get_operands()[index_of_parameter_starts]
    parameter = parameter[2] if len(parameter) == 3 else parameter[1]
    for i in range(index_of_parameter_starts):
        reg_list.append('v' + str(ins.get_operands()[i][1]))
else:
    parameter = None
    for i in range(length_operands):
        reg_list.append('v' + str(ins.get_operands()[i][1]))","for i in range(length_operands - 1, -1, -1):
    if not isinstance(ins.get_operands()[i][0], Operand):
        parameter = ins.get_operands()[index_of_parameter_starts]
        parameter = parameter[2] if len(parameter) == 3 else parameter[1]
        for i in range(index_of_parameter_starts):
            reg_list.append('v' + str(ins.get_operands()[i][1]))
        break
else:
    parameter = None
    for i in range(length_operands):
        reg_list.append('v' + str(ins.get_operands()[i][1]))"
https://github.com/burke-software/django-report-builder/tree/master/report_builder/mixins.py,"def report_to_list(self, queryset, display_fields, user=None, property_filters=[], preview=False):
        """""" Create list from a report with all data filtering.
        queryset: initial queryset to generate results
        display_fields: list of field references or DisplayField models
        user: requesting user. If left as None - there will be no permission check
        property_filters: ???
        preview: return only first 50 rows
        Returns list, message in case of issues.
        """"""
        model_class = queryset.model

        def can_change_or_view(model):
            """""" Return True iff `user` has either change or view permission
            for `model`. """"""
            if user is None:
                return True
            model_name = model._meta.model_name
            app_label = model._meta.app_label
            can_change = user.has_perm(app_label + '.change_' + model_name)
            can_view = user.has_perm(app_label + '.view_' + model_name)

            return can_change or can_view

        if not can_change_or_view(model_class):
            return [], 'Permission Denied'

        if isinstance(display_fields, list):
            # Convert list of strings to DisplayField objects.

            new_display_fields = []

            for display_field in display_fields:
                field_list = display_field.split('__')
                field = field_list[-1]
                path = '__'.join(field_list[:-1])

                if path:
                    path += '__'  # Legacy format to append a __ here.

                new_model = get_model_from_path_string(model_class, path)
                model_field = new_model._meta.get_field_by_name(field)[0]
                choices = model_field.choices
                new_display_fields.append(DisplayField(
                    path, '', field, '', '', None, None, choices, ''
                ))

            display_fields = new_display_fields

        # Build group-by field list.

        group = [df.path + df.field for df in display_fields if df.group]

        # To support group-by with multiple fields, we turn all the other
        # fields into aggregations. The default aggregation is `Max`.

        if group:
            for field in display_fields:
                if (not field.group) and (not field.aggregate):
                    field.aggregate = 'Max'

        message = """"
        objects = self.add_aggregates(queryset, display_fields)

        # Display Values

        display_field_paths = []
        property_list = {}
        custom_list = {}
        display_totals = {}

        for i, display_field in enumerate(display_fields):
            model = get_model_from_path_string(model_class, display_field.path)

            if display_field.field_type == ""Invalid"":
                continue

            if not model or can_change_or_view(model):
                display_field_key = display_field.path + display_field.field

                if display_field.field_type == ""Property"":
                    property_list[i] = display_field_key
                elif display_field.field_type == ""Custom Field"":
                    custom_list[i] = display_field_key
                elif display_field.aggregate == ""Avg"":
                    display_field_key += '__avg'
                elif display_field.aggregate == ""Max"":
                    display_field_key += '__max'
                elif display_field.aggregate == ""Min"":
                    display_field_key += '__min'
                elif display_field.aggregate == ""Count"":
                    display_field_key += '__count'
                elif display_field.aggregate == ""Sum"":
                    display_field_key += '__sum'

                if display_field.field_type not in ('Property', 'Custom Field'):
                    display_field_paths.append(display_field_key)

                if display_field.total:
                    display_totals[display_field_key] = Decimal(0)

            else:
                message += 'Error: Permission denied on access to {0}.'.format(
                    display_field.name
                )

        def increment_total(display_field_key, val):
            """""" Increment display total by `val` if given `display_field_key` in
            `display_totals`.
            """"""
            if display_field_key in display_totals:
                if isinstance(val, bool):
                    # True: 1, False: 0
                    display_totals[display_field_key] += Decimal(val)
                elif isinstance(val, Number):
                    display_totals[display_field_key] += Decimal(str(val))
                elif val:
                    display_totals[display_field_key] += Decimal(1)

        # Select pk for primary and m2m relations in order to retrieve objects
        # for adding properties to report rows. Group-by queries do not support
        # Property nor Custom Field filters.

        if not group:
            display_field_paths.insert(0, 'pk')

            m2m_relations = []
            for position, property_path in property_list.items():
                property_root = property_path.split('__')[0]
                root_class = model_class

                try:
                    property_root_class = getattr(root_class, property_root)
                except AttributeError:  # django-hstore schema compatibility
                    continue

                if type(property_root_class) == ManyToManyDescriptor:
                    display_field_paths.insert(1, '%s__pk' % property_root)
                    m2m_relations.append(property_root)

        if group:
            values = objects.values(*group)
            values = self.add_aggregates(values, display_fields)
            filtered_report_rows = [
                [row[field] for field in display_field_paths]
                for row in values
            ]
            for row in filtered_report_rows:
                for pos, field in enumerate(display_field_paths):
                    increment_total(field, row[pos])
        else:
            filtered_report_rows = []
            values_and_properties_list = []

            values_list = objects.values_list(*display_field_paths)

            for row in values_list:
                row = list(row)
                values_and_properties_list.append(row[1:])
                obj = None  # we will get this only if needed for more complex processing
                # related_objects
                remove_row = False
                # filter properties (remove rows with excluded properties)
                for property_filter in property_filters:
                    if not obj:
                        obj = model_class.objects.get(pk=row.pop(0))
                    root_relation = property_filter.path.split('__')[0]
                    if root_relation in m2m_relations:
                        pk = row[0]
                        if pk is not None:
                            # a related object exists
                            m2m_obj = getattr(obj, root_relation).get(pk=pk)
                            val = reduce(getattr, [property_filter.field], m2m_obj)
                        else:
                            val = None
                    else:
                        if property_filter.field_type == 'Custom Field':
                            for relation in property_filter.path.split('__'):
                                if hasattr(obj, root_relation):
                                    obj = getattr(obj, root_relation)
                            val = obj.get_custom_value(property_filter.field)
                        else:
                            val = reduce(getattr, (property_filter.path + property_filter.field).split('__'), obj)
                    if property_filter.filter_property(val):
                        remove_row = True
                        values_and_properties_list.pop()
                        break
                if not remove_row:
                    for i, field in enumerate(display_field_paths[1:]):
                        increment_total(field, row[i + 1])

                    for position, display_property in property_list.items():
                        if not obj:
                            obj = model_class.objects.get(pk=row.pop(0))
                        relations = display_property.split('__')
                        root_relation = relations[0]
                        if root_relation in m2m_relations:
                            pk = row.pop(0)
                            if pk is not None:
                                # a related object exists
                                m2m_obj = getattr(obj, root_relation).get(pk=pk)
                                val = reduce(getattr, relations[1:], m2m_obj)
                            else:
                                val = None
                        else:
                            # Could error if a related field doesn't exist
                            try:
                                val = reduce(getattr, relations, obj)
                            except AttributeError:
                                val = None
                        values_and_properties_list[-1].insert(position, val)
                        increment_total(display_property, val)

                    for position, display_custom in custom_list.items():
                        if not obj:
                            obj = model_class.objects.get(pk=row.pop(0))
                        val = obj.get_custom_value(display_custom)
                        values_and_properties_list[-1].insert(position, val)
                        increment_total(display_custom, val)

                    filtered_report_rows.append(values_and_properties_list[-1])

                if preview and len(filtered_report_rows) == 50:
                    break

        # Sort results if requested.

        if hasattr(display_fields, 'filter'):
            defaults = {
                None: text_type,
                datetime.date: lambda: datetime.date(datetime.MINYEAR, 1, 1),
                datetime.datetime: lambda: datetime.datetime(datetime.MINYEAR, 1, 1),
            }

            # Order sort fields in reverse order so that ascending, descending
            # sort orders work together (based on Python's stable sort). See
            # http://stackoverflow.com/questions/6666748/ for details.

            sort_fields = display_fields.filter(sort__gt=0).order_by('-sort')
            sort_values = sort_fields.values_list('position', 'sort_reverse')

            for pos, reverse in sort_values:
                column = (row[pos] for row in filtered_report_rows)
                type_col = (type(val) for val in column if val is not None)
                field_type = next(type_col, None)
                default = defaults.get(field_type, field_type)()

                filtered_report_rows = sorted(
                    filtered_report_rows,
                    key=lambda row: self.sort_helper(row[pos], default),
                    reverse=reverse,
                )

        values_and_properties_list = filtered_report_rows

        # Build mapping from display field position to choices list.

        choice_lists = {}
        for df in display_fields:
            if df.choices and hasattr(df, 'choices_dict'):
                df_choices = df.choices_dict
                # Insert blank and None as valid choices.
                df_choices[''] = ''
                df_choices[None] = ''
                choice_lists[df.position] = df_choices

        # Build mapping from display field position to format.

        display_formats = {}

        for df in display_fields:
            if hasattr(df, 'display_format') and df.display_format:
                display_formats[df.position] = df.display_format

        def formatter(value, style):
            # Convert value to Decimal to apply numeric formats.
            try:
                value = Decimal(value)
            except Exception:
                pass

            try:
                return style.string.format(value)
            except ValueError:
                return value

        # Iterate rows and convert values by choice lists and field formats.

        final_list = []

        for row in values_and_properties_list:
            row = list(row)

            for position, choice_list in choice_lists.items():
                try:
                    row[position] = text_type(choice_list[row[position]])
                except Exception:
                    row[position] = text_type(row[position])

            for pos, style in display_formats.items():
                row[pos] = formatter(row[pos], style)

            final_list.append(row)

        values_and_properties_list = final_list

        if display_totals:
            display_totals_row = []

            fields_and_properties = list(display_field_paths[0 if group else 1:])

            for position, value in property_list.items():
                fields_and_properties.insert(position, value)

            for field in fields_and_properties:
                display_totals_row.append(display_totals.get(field, ''))

            # Add formatting to display totals.

            for pos, style in display_formats.items():
                display_totals_row[pos] = formatter(display_totals_row[pos], style)

            values_and_properties_list.append(
                ['TOTALS'] + (len(fields_and_properties) - 1) * ['']
            )
            values_and_properties_list.append(display_totals_row)

        return values_and_properties_list, message",_27227.py,163,"for property_filter in property_filters:
    if not obj:
        obj = model_class.objects.get(pk=row.pop(0))
    root_relation = property_filter.path.split('__')[0]
    if root_relation in m2m_relations:
        pk = row[0]
        if pk is not None:
            m2m_obj = getattr(obj, root_relation).get(pk=pk)
            val = reduce(getattr, [property_filter.field], m2m_obj)
        else:
            val = None
    elif property_filter.field_type == 'Custom Field':
        for relation in property_filter.path.split('__'):
            if hasattr(obj, root_relation):
                obj = getattr(obj, root_relation)
        val = obj.get_custom_value(property_filter.field)
    else:
        val = reduce(getattr, (property_filter.path + property_filter.field).split('__'), obj)
    if property_filter.filter_property(val):
        remove_row = True
        values_and_properties_list.pop()
        break","if not remove_row:
    for (i, field) in enumerate(display_field_paths[1:]):
        increment_total(field, row[i + 1])
    for (position, display_property) in property_list.items():
        if not obj:
            obj = model_class.objects.get(pk=row.pop(0))
        relations = display_property.split('__')
        root_relation = relations[0]
        if root_relation in m2m_relations:
            pk = row.pop(0)
            if pk is not None:
                m2m_obj = getattr(obj, root_relation).get(pk=pk)
                val = reduce(getattr, relations[1:], m2m_obj)
            else:
                val = None
        else:
            try:
                val = reduce(getattr, relations, obj)
            except AttributeError:
                val = None
        values_and_properties_list[-1].insert(position, val)
        increment_total(display_property, val)
    for (position, display_custom) in custom_list.items():
        if not obj:
            obj = model_class.objects.get(pk=row.pop(0))
        val = obj.get_custom_value(display_custom)
        values_and_properties_list[-1].insert(position, val)
        increment_total(display_custom, val)
    filtered_report_rows.append(values_and_properties_list[-1])","for property_filter in property_filters:
    if not obj:
        obj = model_class.objects.get(pk=row.pop(0))
    root_relation = property_filter.path.split('__')[0]
    if root_relation in m2m_relations:
        pk = row[0]
        if pk is not None:
            m2m_obj = getattr(obj, root_relation).get(pk=pk)
            val = reduce(getattr, [property_filter.field], m2m_obj)
        else:
            val = None
    elif property_filter.field_type == 'Custom Field':
        for relation in property_filter.path.split('__'):
            if hasattr(obj, root_relation):
                obj = getattr(obj, root_relation)
        val = obj.get_custom_value(property_filter.field)
    else:
        val = reduce(getattr, (property_filter.path + property_filter.field).split('__'), obj)
    if property_filter.filter_property(val):
        values_and_properties_list.pop()
        break
else:
    for (i, field) in enumerate(display_field_paths[1:]):
        increment_total(field, row[i + 1])
    for (position, display_property) in property_list.items():
        if not obj:
            obj = model_class.objects.get(pk=row.pop(0))
        relations = display_property.split('__')
        root_relation = relations[0]
        if root_relation in m2m_relations:
            pk = row.pop(0)
            if pk is not None:
                m2m_obj = getattr(obj, root_relation).get(pk=pk)
                val = reduce(getattr, relations[1:], m2m_obj)
            else:
                val = None
        else:
            try:
                val = reduce(getattr, relations, obj)
            except AttributeError:
                val = None
        values_and_properties_list[-1].insert(position, val)
        increment_total(display_property, val)
    for (position, display_custom) in custom_list.items():
        if not obj:
            obj = model_class.objects.get(pk=row.pop(0))
        val = obj.get_custom_value(display_custom)
        values_and_properties_list[-1].insert(position, val)
        increment_total(display_custom, val)
    filtered_report_rows.append(values_and_properties_list[-1])"
https://github.com/pytorch/vision/tree/master/references/classification/utils.py,"def set_weight_decay(
    model: torch.nn.Module,
    weight_decay: float,
    norm_weight_decay: Optional[float] = None,
    norm_classes: Optional[List[type]] = None,
    custom_keys_weight_decay: Optional[List[Tuple[str, float]]] = None,
):
    if not norm_classes:
        norm_classes = [
            torch.nn.modules.batchnorm._BatchNorm,
            torch.nn.LayerNorm,
            torch.nn.GroupNorm,
            torch.nn.modules.instancenorm._InstanceNorm,
            torch.nn.LocalResponseNorm,
        ]
    norm_classes = tuple(norm_classes)

    params = {
        ""other"": [],
        ""norm"": [],
    }
    params_weight_decay = {
        ""other"": weight_decay,
        ""norm"": norm_weight_decay,
    }
    custom_keys = []
    if custom_keys_weight_decay is not None:
        for key, weight_decay in custom_keys_weight_decay:
            params[key] = []
            params_weight_decay[key] = weight_decay
            custom_keys.append(key)

    def _add_params(module, prefix=""""):
        for name, p in module.named_parameters(recurse=False):
            if not p.requires_grad:
                continue
            is_custom_key = False
            for key in custom_keys:
                target_name = f""{prefix}.{name}"" if prefix != """" and ""."" in key else name
                if key == target_name:
                    params[key].append(p)
                    is_custom_key = True
                    break
            if not is_custom_key:
                if norm_weight_decay is not None and isinstance(module, norm_classes):
                    params[""norm""].append(p)
                else:
                    params[""other""].append(p)

        for child_name, child_module in module.named_children():
            child_prefix = f""{prefix}.{child_name}"" if prefix != """" else child_name
            _add_params(child_module, prefix=child_prefix)

    _add_params(model)

    param_groups = []
    for key in params:
        if len(params[key]) > 0:
            param_groups.append({""params"": params[key], ""weight_decay"": params_weight_decay[key]})
    return param_groups",_29439.py,38,"for key in custom_keys:
    target_name = f'{prefix}.{name}' if prefix != '' and '.' in key else name
    if key == target_name:
        params[key].append(p)
        is_custom_key = True
        break","if not is_custom_key:
    if norm_weight_decay is not None and isinstance(module, norm_classes):
        params['norm'].append(p)
    else:
        params['other'].append(p)","for key in custom_keys:
    target_name = f'{prefix}.{name}' if prefix != '' and '.' in key else name
    if key == target_name:
        params[key].append(p)
        break
else:
    if norm_weight_decay is not None and isinstance(module, norm_classes):
        params['norm'].append(p)
    else:
        params['other'].append(p)"
https://github.com/pytorch/vision/tree/master/references/classification/utils.py,"def set_weight_decay(
    model: torch.nn.Module,
    weight_decay: float,
    norm_weight_decay: Optional[float] = None,
    norm_classes: Optional[List[type]] = None,
    custom_keys_weight_decay: Optional[List[Tuple[str, float]]] = None,
):
    if not norm_classes:
        norm_classes = [
            torch.nn.modules.batchnorm._BatchNorm,
            torch.nn.LayerNorm,
            torch.nn.GroupNorm,
            torch.nn.modules.instancenorm._InstanceNorm,
            torch.nn.LocalResponseNorm,
        ]
    norm_classes = tuple(norm_classes)

    params = {
        ""other"": [],
        ""norm"": [],
    }
    params_weight_decay = {
        ""other"": weight_decay,
        ""norm"": norm_weight_decay,
    }
    custom_keys = []
    if custom_keys_weight_decay is not None:
        for key, weight_decay in custom_keys_weight_decay:
            params[key] = []
            params_weight_decay[key] = weight_decay
            custom_keys.append(key)

    def _add_params(module, prefix=""""):
        for name, p in module.named_parameters(recurse=False):
            if not p.requires_grad:
                continue
            is_custom_key = False
            for key in custom_keys:
                target_name = f""{prefix}.{name}"" if prefix != """" and ""."" in key else name
                if key == target_name:
                    params[key].append(p)
                    is_custom_key = True
                    break
            if not is_custom_key:
                if norm_weight_decay is not None and isinstance(module, norm_classes):
                    params[""norm""].append(p)
                else:
                    params[""other""].append(p)

        for child_name, child_module in module.named_children():
            child_prefix = f""{prefix}.{child_name}"" if prefix != """" else child_name
            _add_params(child_module, prefix=child_prefix)

    _add_params(model)

    param_groups = []
    for key in params:
        if len(params[key]) > 0:
            param_groups.append({""params"": params[key], ""weight_decay"": params_weight_decay[key]})
    return param_groups",_29439.py,38,"for key in custom_keys:
    target_name = f'{prefix}.{name}' if prefix != '' and '.' in key else name
    if key == target_name:
        params[key].append(p)
        is_custom_key = True
        break","if not is_custom_key:
    if norm_weight_decay is not None and isinstance(module, norm_classes):
        params['norm'].append(p)
    else:
        params['other'].append(p)","for key in custom_keys:
    target_name = f'{prefix}.{name}' if prefix != '' and '.' in key else name
    if key == target_name:
        params[key].append(p)
        break
else:
    if norm_weight_decay is not None and isinstance(module, norm_classes):
        params['norm'].append(p)
    else:
        params['other'].append(p)"
https://github.com/crossbario/crossbar/tree/master/crossbar/common/checkconfig.py,"def check_dict_args(spec, config, msg):
    """"""
    Check the arguments of C{config} according to C{spec}.

    C{spec} is a dict, with the key mapping to the config and the value being a
    2-tuple, for which the first item being whether or not it is mandatory, and
    the second being a list of types of which the config item can be.
    """"""
    if not isinstance(config, Mapping):
        raise InvalidConfigException(""{} - invalid type for configuration item - expected dict, got {}"".format(
            msg,
            type(config).__name__))

    for k in config:
        if k not in spec:
            raise InvalidConfigException(""{} - encountered unknown attribute '{}'"".format(msg, k))
        if spec[k][1]:
            valid_type = False
            for t in spec[k][1]:
                if isinstance(config[k], t):
                    # We're special-casing Sequence here, because in
                    # general if we say a Sequence is okay, we do NOT
                    # want strings to be allowed but Python says that
                    # ""isinstance('foo', Sequence) == True""
                    if t is Sequence:
                        if not isinstance(config[k], (str, str)):
                            valid_type = True
                            break
                    else:
                        valid_type = True
                        break
            if not valid_type:
                raise InvalidConfigException(
                    ""{} - invalid type {} encountered for attribute '{}', must be one of ({})"".format(
                        msg,
                        type(config[k]).__name__, k, ', '.join([x.__name__ for x in spec[k][1]])))

    mandatory_keys = [k for k in spec if spec[k][0]]
    for k in mandatory_keys:
        if k not in config:
            raise InvalidConfigException(""{} - missing mandatory attribute '{}'"".format(msg, k))",_31287.py,19,"for t in spec[k][1]:
    if isinstance(config[k], t):
        if t is Sequence:
            if not isinstance(config[k], (str, str)):
                valid_type = True
                break
        else:
            valid_type = True
            break","if not valid_type:
    raise InvalidConfigException(""{} - invalid type {} encountered for attribute '{}', must be one of ({})"".format(msg, type(config[k]).__name__, k, ', '.join([x.__name__ for x in spec[k][1]])))","for t in spec[k][1]:
    if isinstance(config[k], t):
        if t is Sequence:
            if not isinstance(config[k], (str, str)):
                break
        else:
            break
else:
    raise InvalidConfigException(""{} - invalid type {} encountered for attribute '{}', must be one of ({})"".format(msg, type(config[k]).__name__, k, ', '.join([x.__name__ for x in spec[k][1]])))"
https://github.com/Helpsypoo/primerpython/tree/master/blender_scripts/tools/drawn_contest_world.py,"def update_creatures(
        self,
        start_time = None,
        end_time = None,
        day = None,
        is_first_day = False,
        updating_next = False, #For showing results of last day
        graph_only = False
    ):
        if start_time == None:
            raise Warning('Need start_time for update_creatures')
        if end_time == None:
            end_time = start_time + OBJECT_APPEARANCE_TIME / FRAME_RATE

        if updating_next == False:
            creatures = day.creatures #self.sim.calendar[0].creatures
        elif updating_next == True:
            creatures = day.next_creatures
        angle_inc = 2 * math.pi / len(creatures)
        radius = 1 - self.creature_scale

        if graph_only == False:
            old_creatures = []
            #new creatures
            #doing these first to avoid updating positions of old creatures so new
            #creatures can appear to come from old ones.
            for i, cre in enumerate(creatures):
                angle = i * angle_inc

                if cre.drawn_creature != None:
                    old_creatures.append(cre)
                else:
                    destination = [
                        radius * math.cos(math.pi / 2 + angle),
                        radius * math.sin(math.pi / 2 + angle),
                        self.creature_scale * CREATURE_HEIGHT_FACTOR
                    ]

                    if is_first_day == True: #day.date == 0:
                        start_loc = destination
                    else:
                        start_loc = cre.parent.drawn_creature.ref_obj.location

                    reusables = [x for x in self.reusable_creatures if x.creature.fight_chance == cre.fight_chance]
                    if len(reusables) > 0:
                        d_cre = reusables[0]
                        self.reusable_creatures.remove(d_cre)
                        d_cre.creature = cre
                        d_cre.move_to(
                            new_location = start_loc,
                            new_angle = [math.pi / 2, 0, angle],
                            #scale = self.creature_scale,
                            end_time = start_time,
                            start_frame = start_time * FRAME_RATE - 1
                        )
                        d_cre.move_to(
                            new_scale = self.creature_scale,
                            start_time = start_time,
                            end_time = end_time
                        )

                    else:
                        d_cre = DrawnCreature(
                            creature = cre,
                            location = start_loc,
                            rotation_euler = [math.pi / 2, 0, angle],
                            scale = self.creature_scale,
                            wiggle = self.wiggle
                        )
                        self.add_subbobject(d_cre)
                        d_cre.add_to_blender(
                            appear_time = start_time,
                            transition_time = (end_time - start_time) * FRAME_RATE
                            #subbobject_timing = (end_time - start_time) * FRAME_RATE
                        )

                    self.drawn_creatures.append(d_cre)
                    cre.drawn_creature = d_cre

                    if is_first_day == False:
                        d_cre.move_to(
                            new_location = destination,
                            start_time = start_time,
                            end_time = end_time
                        )

            #old creatures
            for i, cre in enumerate(creatures):
                angle = i * angle_inc

                if cre in old_creatures:
                    destination = [
                        radius * math.cos(math.pi / 2 + angle),
                        radius * math.sin(math.pi / 2 + angle),
                        self.creature_scale * CREATURE_HEIGHT_FACTOR
                    ]
                    cre.drawn_creature.move_to(
                        new_location = destination,
                        start_time = start_time,
                        end_time = end_time
                    )

            #clean up dead creatures
            new_reusable_creatures = []
            for d_cre in self.drawn_creatures:
                alive = False
                for cre in creatures:
                    if cre.drawn_creature == d_cre:
                        alive = True
                        break
                if alive == False:
                    #d_cre.disappear(disappear_time = end_time)#start_time + OBJECT_APPEARANCE_TIME / FRAME_RATE)
                    d_cre.move_to(
                        new_scale = 0,
                        start_time = start_time,
                        end_time = end_time
                    )
                    new_reusable_creatures.append(d_cre)

            #print('New reusable creatures')
            for d_cre in new_reusable_creatures:
                #print(str(d_cre.name) + ' ' + str(d_cre.creature.fight_chance))
                self.drawn_creatures.remove(d_cre)
                self.reusable_creatures.append(d_cre)


        if self.linked_graph != None:
            if updating_next == False:
                self.update_graph(
                    date = day.date,
                    start_time = start_time,
                    end_time = end_time
                )
            elif updating_next == True:
                self.update_graph(
                    date = day.date + 1,
                    start_time = start_time,
                    end_time = end_time
                )",_31318.py,107,"for cre in creatures:
    if cre.drawn_creature == d_cre:
        alive = True
        break","if alive == False:
    d_cre.move_to(new_scale=0, start_time=start_time, end_time=end_time)
    new_reusable_creatures.append(d_cre)","for cre in creatures:
    if cre.drawn_creature == d_cre:
        break
else:
    d_cre.move_to(new_scale=0, start_time=start_time, end_time=end_time)
    new_reusable_creatures.append(d_cre)"
https://github.com/Palashio/libra/tree/master/libra/data_generation/grammartree.py,"def get_value_instruction(sent):
    # Text blob part of speech identification algorithm
    blob = TextBlob(sent)
    blob.parse()

    # isolating tags of words in instruction
    tags = blob.tags

    decoded = """"

    # if an adjective is present then truth is set to True to activate the
    # correct pipeline
    truth = False
    for x in range(len(tags)):
        if ""JJ"" in tags[x]:
            truth = True
            break

    # when an adjective exists this pipeline is run
    if truth:
        try:
            for x in range(len(tags)):
                if ""JJ"" in tags[x]:
                    q = x + 1
                    decoded += sent.split()[x] + ""_""

                    # while the word after the adjective is any of these parts
                    # of speech they're added to the instruction final
                    while (""VBN"" in tags[q] or ""VBG"" in tags[q] or ""NN"" in tags[q] or ""NNS"" in tags[q] or ""RB"" in tags[
                        q] or (""NNS"" in tags[q] and ""IN"" in tags[q + 1])):
                        decoded += sent.split()[q] + ""_""
                        # if an interjection is present then you want to skip
                        # over it
                        if (""IN"" in tags[q + 1]):
                            decoded += sent.split()[q + 1] + ""_""
                            q += 2
                            continue
                        q += 1
                        if q >= len(tags):
                            break

        except BaseException:
            pass

    # if there's no adjective present you want to run this pipeline
    else:
        try:
            # you iterate through the tags and identify certain parts of speech
            for x in range(len(tags)):
                if x < len(tags) - 1:
                    if ""IN"" in tags[x + 1]:
                        decoded += sent.split()[x] + ""_""
                        decoded += sent.split()[x + 1] + ""_""
                        x = x + 2
                        continue
                # if any of these parts of speech are in the instruction then
                # you want to extract them. The parts of speech can be found:
                # https://repository.upenn.edu/cgi/viewcontent.cgi?article=1603&context=cis_reports
                if ""NN"" in tags[x] or ""NNS"" in tags[x] or ""RB"" in tags[x] or ""VBG"" in tags[x] or ""VBN"" in tags[x]:
                    decoded += sent.split()[x] + ""_""
                else:
                    continue
        except BaseException:
            print(x)
            print(tags[x])
            print(""Please try re-typing your sentence"")

    decoded = decoded[:-1]

    # If it's two words then you just choose the second word: we're assuming two words = predict apples, even if this is false in the example apples red
    # similarity identificatin will still pick up on the right column

    if len(sent.split()) == 2:
        decoded = sent.split()[1]

    return decoded",_31553.py,14,"for x in range(len(tags)):
    if 'JJ' in tags[x]:
        truth = True
        break","if truth:
    try:
        for x in range(len(tags)):
            if 'JJ' in tags[x]:
                q = x + 1
                decoded += sent.split()[x] + '_'
                while 'VBN' in tags[q] or 'VBG' in tags[q] or 'NN' in tags[q] or ('NNS' in tags[q]) or ('RB' in tags[q]) or ('NNS' in tags[q] and 'IN' in tags[q + 1]):
                    decoded += sent.split()[q] + '_'
                    if 'IN' in tags[q + 1]:
                        decoded += sent.split()[q + 1] + '_'
                        q += 2
                        continue
                    q += 1
                    if q >= len(tags):
                        break
    except BaseException:
        pass
else:
    try:
        for x in range(len(tags)):
            if x < len(tags) - 1:
                if 'IN' in tags[x + 1]:
                    decoded += sent.split()[x] + '_'
                    decoded += sent.split()[x + 1] + '_'
                    x = x + 2
                    continue
            if 'NN' in tags[x] or 'NNS' in tags[x] or 'RB' in tags[x] or ('VBG' in tags[x]) or ('VBN' in tags[x]):
                decoded += sent.split()[x] + '_'
            else:
                continue
    except BaseException:
        print(x)
        print(tags[x])
        print('Please try re-typing your sentence')","for x in range(len(tags)):
    if 'JJ' in tags[x]:
        try:
            for x in range(len(tags)):
                if 'JJ' in tags[x]:
                    q = x + 1
                    decoded += sent.split()[x] + '_'
                    while 'VBN' in tags[q] or 'VBG' in tags[q] or 'NN' in tags[q] or ('NNS' in tags[q]) or ('RB' in tags[q]) or ('NNS' in tags[q] and 'IN' in tags[q + 1]):
                        decoded += sent.split()[q] + '_'
                        if 'IN' in tags[q + 1]:
                            decoded += sent.split()[q + 1] + '_'
                            q += 2
                            continue
                        q += 1
                        if q >= len(tags):
                            break
        except BaseException:
            pass
        break
else:
    try:
        for x in range(len(tags)):
            if x < len(tags) - 1:
                if 'IN' in tags[x + 1]:
                    decoded += sent.split()[x] + '_'
                    decoded += sent.split()[x + 1] + '_'
                    x = x + 2
                    continue
            if 'NN' in tags[x] or 'NNS' in tags[x] or 'RB' in tags[x] or ('VBG' in tags[x]) or ('VBN' in tags[x]):
                decoded += sent.split()[x] + '_'
            else:
                continue
    except BaseException:
        print(x)
        print(tags[x])
        print('Please try re-typing your sentence')"
https://github.com/django-extensions/django-extensions/tree/master/django_extensions/management/commands/shell_plus.py,"def load_base_kernel_spec(self, app):
        """"""Finds and returns the base Python kernelspec to extend from.""""""
        ksm = app.kernel_spec_manager
        try_spec_names = getattr(settings, 'NOTEBOOK_KERNEL_SPEC_NAMES', [
            'python3',
            'python',
        ])

        if isinstance(try_spec_names, str):
            try_spec_names = [try_spec_names]

        ks = None
        for spec_name in try_spec_names:
            try:
                ks = ksm.get_kernel_spec(spec_name)
                break
            except Exception:
                continue
        if not ks:
            raise CommandError(""No notebook (Python) kernel specs found. Tried %r"" % try_spec_names)

        return ks",_38032.py,13,"for spec_name in try_spec_names:
    try:
        ks = ksm.get_kernel_spec(spec_name)
        break
    except Exception:
        continue","if not ks:
    raise CommandError('No notebook (Python) kernel specs found. Tried %r' % try_spec_names)","for spec_name in try_spec_names:
    try:
        ks = ksm.get_kernel_spec(spec_name)
        break
    except Exception:
        continue
else:
    raise CommandError('No notebook (Python) kernel specs found. Tried %r' % try_spec_names)"
https://github.com/kivy/kivy/tree/master/kivy/uix/behaviors/focus.py,"def _get_focus_next(self, focus_dir):
        current = self
        walk_tree = 'walk' if focus_dir == 'focus_next' else 'walk_reverse'

        while 1:
            # if we hit a focusable, walk through focus_xxx
            while getattr(current, focus_dir) is not None:
                current = getattr(current, focus_dir)
                if current is self or current is StopIteration:
                    return None  # make sure we don't loop forever
                if current.is_focusable and not current.disabled:
                    return current

            # hit unfocusable, walk widget tree
            itr = getattr(current, walk_tree)(loopback=True)
            if focus_dir == 'focus_next':
                next(itr)  # current is returned first  when walking forward
            for current in itr:
                if isinstance(current, FocusBehavior):
                    break
            # why did we stop
            if isinstance(current, FocusBehavior):
                if current is self:
                    return None
                if current.is_focusable and not current.disabled:
                    return current
            else:
                return None",_38923.py,18,"for current in itr:
    if isinstance(current, FocusBehavior):
        break","if isinstance(current, FocusBehavior):
    if current is self:
        return None
    if current.is_focusable and (not current.disabled):
        return current
else:
    return None","for current in itr:
    if isinstance(current, FocusBehavior):
        if current is self:
            return None
        if current.is_focusable and (not current.disabled):
            return current
        break
else:
    return None"
https://github.com/nlplab/brat/tree/master/server/src/annotator.py,"def _create_relation(ann_obj, projectconf, mods, origin, target, type,
                     attributes, old_type, old_target, undo_resp={}):
    attributes = _parse_attributes(attributes)

    if old_type is not None or old_target is not None:
        assert type in projectconf.get_relation_types(), (
            ('attempting to convert relation to non-relation ""%s"" ' % (target.type, )) +
            ('(legit types: %s)' % (str(projectconf.get_relation_types()), )))

        sought_target = (old_target
                         if old_target is not None else target.id)
        sought_type = (old_type
                       if old_type is not None else type)
        sought_origin = origin.id

        # We are to change the type, target, and/or attributes
        found = None
        for ann in ann_obj.get_relations():
            if (ann.arg1 == sought_origin and ann.arg2 == sought_target and
                    ann.type == sought_type):
                found = ann
                break

        if found is None:
            # TODO: better response
            Messager.error(
                '_create_relation: failed to identify target relation (type %s, target %s) (deleted?)' %
                (str(old_type), str(old_target)))
        elif found.arg2 == target.id and found.type == type:
            # no changes to type or target
            pass
        else:
            # type and/or target changed, mark.
            before = str(found)
            found.arg2 = target.id
            found.type = type
            mods.change(before, found)

        target_ann = found
    else:
        # Create a new annotation
        new_id = ann_obj.get_new_id('R')
        # TODO: do we need to support different relation arg labels
        # depending on participant types? This doesn't.
        rels = projectconf.get_relations_by_type(type)
        rel = rels[0] if rels else None
        assert rel is not None and len(rel.arg_list) == 2
        a1l, a2l = rel.arg_list
        ann = BinaryRelationAnnotation(
            new_id, type, a1l, origin.id, a2l, target.id, '\t')
        mods.addition(ann)
        ann_obj.add_annotation(ann)

        target_ann = ann

    # process attributes
    if target_ann is not None:
        _set_attributes(ann_obj, ann, attributes, mods, undo_resp)
    elif attributes is not None:
        Messager.error(
            '_create_relation: cannot set arguments: failed to identify target relation (type %s, target %s) (deleted?)' %
            (str(old_type), str(old_target)))

    return target_ann",_40755.py,18,"for ann in ann_obj.get_relations():
    if ann.arg1 == sought_origin and ann.arg2 == sought_target and (ann.type == sought_type):
        found = ann
        break","if found is None:
    Messager.error('_create_relation: failed to identify target relation (type %s, target %s) (deleted?)' % (str(old_type), str(old_target)))
elif found.arg2 == target.id and found.type == type:
    pass
else:
    before = str(found)
    found.arg2 = target.id
    found.type = type
    mods.change(before, found)","for ann in ann_obj.get_relations():
    if ann.arg1 == sought_origin and ann.arg2 == sought_target and (ann.type == sought_type):
        found = ann
        if found.arg2 == target.id and found.type == type:
            pass
        else:
            before = str(found)
            found.arg2 = target.id
            found.type = type
            mods.change(before, found)
        break
else:
    Messager.error('_create_relation: failed to identify target relation (type %s, target %s) (deleted?)' % (str(old_type), str(old_target)))"
https://github.com/freewym/espresso/tree/master/examples/roberta/wsc/wsc_utils.py,"def filter_noun_chunks(
    chunks, exclude_pronouns=False, exclude_query=None, exact_match=False
):
    if exclude_pronouns:
        chunks = [
            np
            for np in chunks
            if (np.lemma_ != ""-PRON-"" and not all(tok.pos_ == ""PRON"" for tok in np))
        ]

    if exclude_query is not None:
        excl_txt = [exclude_query.lower()]
        filtered_chunks = []
        for chunk in chunks:
            lower_chunk = chunk.text.lower()
            found = False
            for excl in excl_txt:
                if (
                    not exact_match and (lower_chunk in excl or excl in lower_chunk)
                ) or lower_chunk == excl:
                    found = True
                    break
            if not found:
                filtered_chunks.append(chunk)
        chunks = filtered_chunks

    return chunks",_40890.py,17,"for excl in excl_txt:
    if not exact_match and (lower_chunk in excl or excl in lower_chunk) or lower_chunk == excl:
        found = True
        break","if not found:
    filtered_chunks.append(chunk)","for excl in excl_txt:
    if not exact_match and (lower_chunk in excl or excl in lower_chunk) or lower_chunk == excl:
        break
else:
    filtered_chunks.append(chunk)"
https://github.com/annoviko/pyclustering/tree/master/pyclustering/gcolor/dsatur.py,"def process(self):
        """"""!
        @brief Perform graph coloring using DSATUR algorithm.
        
        @see get_colors()
        
        """"""        
        color_counter = 1;
        
        degrees = list();
        saturation_degrees = [0] * len(self.__data_pointer);
        
        self.__coloring = [0] * len(self.__data_pointer);
        uncolored_vertices = set(range(len(self.__data_pointer)));
        
        index_maximum_degree = 0;
        maximum_degree = 0;
        for index_node in range(len(self.__data_pointer)):
            # Fill degree of nodes in the input graph
            degrees.append( ( sum(self.__data_pointer[index_node]), index_node ) );
            
            # And find node with maximal degree at the same time.
            if (degrees[index_node][0] > maximum_degree):
                (maximum_degree, node_index) = degrees[index_node];
                index_maximum_degree = index_node;
            
        # Update saturation
        neighbors = self.__get_neighbors(index_maximum_degree);
        for index_neighbor in neighbors:
            saturation_degrees[index_neighbor] += 1;
        
        # Coloring the first node
        self.__coloring[index_maximum_degree] = color_counter;
        uncolored_vertices.remove(index_maximum_degree);
        
        while(len(uncolored_vertices) > 0):
            # Get maximum saturation degree
            maximum_satur_degree = -1;
            for index in uncolored_vertices:
                if (saturation_degrees[index] > maximum_satur_degree):
                    maximum_satur_degree = saturation_degrees[index];
            
            # Get list of indexes with maximum saturation degree
            indexes_maximum_satur_degree = [index for index in uncolored_vertices if saturation_degrees[index] == maximum_satur_degree];           
    
            coloring_index = indexes_maximum_satur_degree[0];
            if (len(indexes_maximum_satur_degree) > 1): # There are more then one node with maximum saturation
                # Find node with maximum degree
                maximum_degree = -1;
                for index in indexes_maximum_satur_degree:
                    (degree, node_index) = degrees[index];
                    if (degree > maximum_degree):
                        coloring_index = node_index;
                        maximum_degree = degree;
            
            # Coloring
            node_index_neighbors = self.__get_neighbors(coloring_index);
            for number_color in range(1, color_counter + 1, 1):
                if (self.__get_amount_color(node_index_neighbors, number_color) == 0):
                    self.__coloring[coloring_index] = number_color;
                    break;
                    
            # If it has not been colored then
            if (self.__coloring[coloring_index] == 0):
                color_counter += 1;     # Add new color
                self.__coloring[coloring_index] = color_counter;
            
            # Remove node from uncolored set
            uncolored_vertices.remove(coloring_index);
            
            
            # Update degree of saturation
            for index_neighbor in node_index_neighbors:
                subneighbors = self.__get_neighbors(index_neighbor);
                
                if (self.__get_amount_color(subneighbors, self.__coloring[coloring_index]) == 1):
                    saturation_degrees[index_neighbor] += 1;",_40973.py,58,"for number_color in range(1, color_counter + 1, 1):
    if self.__get_amount_color(node_index_neighbors, number_color) == 0:
        self.__coloring[coloring_index] = number_color
        break","if self.__coloring[coloring_index] == 0:
    color_counter += 1
    self.__coloring[coloring_index] = color_counter","for number_color in range(1, color_counter + 1, 1):
    if self.__get_amount_color(node_index_neighbors, number_color) == 0:
        self.__coloring[coloring_index] = number_color
        break
else:
    color_counter += 1
    self.__coloring[coloring_index] = color_counter"
https://github.com/huggingface/transformers/tree/master/src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, `input = ""unaffable""` wil return as output `[""un"", ""##aff"", ""##able""]`.

        Args:
            text: A single token or whitespace separated tokens. This should have
                already been passed through *BasicTokenizer*.

        Returns:
            A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens",_42680.py,26,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end","if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)"
https://github.com/huggingface/transformers/tree/master/src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py,"def tokenize(self, text):
        """"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.

        For example, `input = ""unaffable""` wil return as output `[""un"", ""##aff"", ""##able""]`.

        Args:
            text: A single token or whitespace separated tokens. This should have
                already been passed through *BasicTokenizer*.

        Returns:
            A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens",_42680.py,29,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1","if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break"
https://github.com/Dinnerbone/mcstatus/tree/master/mcstatus/server.py,"def status(self, tries: int = 3, **kwargs) -> BedrockStatusResponse:
        """"""Checks the status of a Minecraft Bedrock Edition server.

        :param int tries: How many times to retry if it fails.
        :param type **kwargs: Passed to a `BedrockServerStatus` instance.
        :return: Status information in a `BedrockStatusResponse` instance.
        :rtype: BedrockStatusResponse
        """"""
        exception = None

        for _ in range(tries):
            try:
                resp = BedrockServerStatus(self.host, self.port, **kwargs).read_status()
                break
            except BaseException as e:
                exception = e

        if exception:
            raise exception

        return resp",_43043.py,11,"for _ in range(tries):
    try:
        resp = BedrockServerStatus(self.host, self.port, **kwargs).read_status()
        break
    except BaseException as e:
        exception = e","if exception:
    raise exception","for _ in range(tries):
    try:
        resp = BedrockServerStatus(self.host, self.port, **kwargs).read_status()
        break
    except BaseException as e:
        exception = e
else:
    raise exception"
https://github.com/angr/angr-doc/tree/master/examples/secconquals2016_ropsynth/ropsynth.py,"def main(argv):
    basedir = os.path.dirname(os.path.abspath(argv[0]))
    os.chdir(basedir)
    rootdir = "".""
    flag = open(""flag"").read()
    # read secret in rootdir
    secret_word = open(""%s/secret"" % rootdir).read()

    stage = 1
    while stage <= 5:
        print_msg(""stage %d/5"" % stage)

        # generate gadgets
        gadgets = generate_gadgets()
        encoded_gadgets = binascii.b2a_base64(gadgets).strip()

        # send gadgets, and then receive ropchain
        signal.alarm(60)
        print_msg(encoded_gadgets)
        encoded_ropchain = sys.stdin.readline()
        signal.alarm(0)
        ropchain = binascii.a2b_base64(encoded_ropchain)
        if len(ropchain) > 4096:
            print_msg(""Invalid ROP chain"")
            break

        child = Popen(
            [
                ""./launcher.elf"",
                #rootdir,
            ],
            stdin=PIPE, stdout=PIPE, close_fds=True)
        child.stdin.write(fill(gadgets, '\xCC', 4096))
        child.stdin.write(fill(ropchain, '\xFF', 4096))
        answer_word = child.stdout.read()
        child.wait()
        print(""EXPECTED: {}"".format(repr(secret_word)), file=sys.stderr)
        print(""GOT.....: {}"".format(repr(answer_word)), file=sys.stderr)
        if secret_word != answer_word:
            print_msg(""NG"")
            break
        print_msg(""OK"")
        stage += 1

    if stage >= 6:
        print_msg(flag)",_44395.py,10,"while stage <= 5:
    print_msg('stage %d/5' % stage)
    gadgets = generate_gadgets()
    encoded_gadgets = binascii.b2a_base64(gadgets).strip()
    signal.alarm(60)
    print_msg(encoded_gadgets)
    encoded_ropchain = sys.stdin.readline()
    signal.alarm(0)
    ropchain = binascii.a2b_base64(encoded_ropchain)
    if len(ropchain) > 4096:
        print_msg('Invalid ROP chain')
        break
    child = Popen(['./launcher.elf'], stdin=PIPE, stdout=PIPE, close_fds=True)
    child.stdin.write(fill(gadgets, '', 4096))
    child.stdin.write(fill(ropchain, '', 4096))
    answer_word = child.stdout.read()
    child.wait()
    print('EXPECTED: {}'.format(repr(secret_word)), file=sys.stderr)
    print('GOT.....: {}'.format(repr(answer_word)), file=sys.stderr)
    if secret_word != answer_word:
        print_msg('NG')
        break
    print_msg('OK')
    stage += 1","if stage >= 6:
    print_msg(flag)","while stage <= 5:
    print_msg('stage %d/5' % stage)
    gadgets = generate_gadgets()
    encoded_gadgets = binascii.b2a_base64(gadgets).strip()
    signal.alarm(60)
    print_msg(encoded_gadgets)
    encoded_ropchain = sys.stdin.readline()
    signal.alarm(0)
    ropchain = binascii.a2b_base64(encoded_ropchain)
    if len(ropchain) > 4096:
        print_msg('Invalid ROP chain')
        break
    child = Popen(['./launcher.elf'], stdin=PIPE, stdout=PIPE, close_fds=True)
    child.stdin.write(fill(gadgets, '', 4096))
    child.stdin.write(fill(ropchain, '', 4096))
    answer_word = child.stdout.read()
    child.wait()
    print('EXPECTED: {}'.format(repr(secret_word)), file=sys.stderr)
    print('GOT.....: {}'.format(repr(answer_word)), file=sys.stderr)
    if secret_word != answer_word:
        print_msg('NG')
        break
    print_msg('OK')
    stage += 1
else:
    print_msg(flag)"
https://github.com/cannatag/ldap3/tree/master/ldap3/utils/dn.py,"def safe_rdn(dn, decompose=False):
    """"""Returns a list of rdn for the dn, usually there is only one rdn, but it can be more than one when the + sign is used""""""
    escaped_rdn = []
    one_more = True
    for component in parse_dn(dn, escape=True):
        if component[2] == '+' or one_more:
            if decompose:
                escaped_rdn.append((component[0], component[1]))
            else:
                escaped_rdn.append(component[0] + '=' + component[1])
            if component[2] == '+':
                one_more = True
            else:
                one_more = False
                break

    if one_more:
        raise LDAPInvalidDnError('bad dn ' + str(dn))

    return escaped_rdn",_46086.py,5,"for component in parse_dn(dn, escape=True):
    if component[2] == '+' or one_more:
        if decompose:
            escaped_rdn.append((component[0], component[1]))
        else:
            escaped_rdn.append(component[0] + '=' + component[1])
        if component[2] == '+':
            one_more = True
        else:
            one_more = False
            break","if one_more:
    raise LDAPInvalidDnError('bad dn ' + str(dn))","for component in parse_dn(dn, escape=True):
    if component[2] == '+' or one_more:
        if decompose:
            escaped_rdn.append((component[0], component[1]))
        else:
            escaped_rdn.append(component[0] + '=' + component[1])
        if component[2] == '+':
            one_more = True
        else:
            one_more = False
            break
else:
    raise LDAPInvalidDnError('bad dn ' + str(dn))"
https://github.com/kellyjonbrazil/jc/tree/master/jc/parsers/ifconfig.py,"def parser(self, source_data):
        """"""
        :param source_data:
        :return:
        """"""

        # Linux syntax
        re_linux_interface = re.compile(
            r""(?P<name>[a-zA-Z0-9:._-]+)\s+Link encap:(?P<type>\S+\s?\S+)(\s+HWaddr\s+\b""
            r""(?P<mac_addr>[0-9A-Fa-f:?]+))?"",
            re.I)
        re_linux_ipv4 = re.compile(
            r""inet addr:(?P<ipv4_addr>(?:[0-9]{1,3}\.){3}[0-9]{1,3})(\s+Bcast:""
            r""(?P<ipv4_bcast>(?:[0-9]{1,3}\.){3}[0-9]{1,3}))?\s+Mask:(?P<ipv4_mask>(?:[0-9]{1,3}\.){3}[0-9]{1,3})"",
            re.I)
        re_linux_ipv6 = re.compile(
            r""inet6 addr:\s+(?P<ipv6_addr>\S+)/(?P<ipv6_mask>[0-9]+)\s+Scope:(?P<ipv6_scope>Link|Host)"",
            re.I)
        re_linux_state = re.compile(
            r""\W+(?P<state>(?:\w+\s)+)(?:\s+)?MTU:(?P<mtu>[0-9]+)\s+Metric:(?P<metric>[0-9]+)"", re.I)
        re_linux_rx = re.compile(
            r""RX packets:(?P<rx_packets>[0-9]+)\s+errors:(?P<rx_errors>[0-9]+)\s+dropped:""
            r""(?P<rx_dropped>[0-9]+)\s+overruns:(?P<rx_overruns>[0-9]+)\s+frame:(?P<rx_frame>[0-9]+)"",
            re.I)
        re_linux_tx = re.compile(
            r""TX packets:(?P<tx_packets>[0-9]+)\s+errors:(?P<tx_errors>[0-9]+)\s+dropped:""
            r""(?P<tx_dropped>[0-9]+)\s+overruns:(?P<tx_overruns>[0-9]+)\s+carrier:(?P<tx_carrier>[0-9]+)"",
            re.I)
        re_linux_bytes = re.compile(r""\W+RX bytes:(?P<rx_bytes>\d+)\s+\(.*\)\s+TX bytes:(?P<tx_bytes>\d+)\s+\(.*\)"", re.I)
        re_linux_tx_stats = re.compile(r""collisions:(?P<tx_collisions>[0-9]+)\s+txqueuelen:[0-9]+"", re.I)
        re_linux = [re_linux_interface, re_linux_ipv4, re_linux_ipv6, re_linux_state, re_linux_rx, re_linux_tx,
                    re_linux_bytes, re_linux_tx_stats]

        # OpenBSD syntax
        re_openbsd_interface = re.compile(
            r""(?P<name>[a-zA-Z0-9:._-]+):\s+flags=(?P<flags>[0-9]+)<(?P<state>\S+)?>\s+mtu\s+(?P<mtu>[0-9]+)"",
            re.I)
        re_openbsd_ipv4 = re.compile(
            r""inet (?P<ipv4_addr>(?:[0-9]{1,3}\.){3}[0-9]{1,3})\s+netmask\s+""
            r""(?P<ipv4_mask>(?:[0-9]{1,3}\.){3}[0-9]{1,3})(\s+broadcast\s+""
            r""(?P<ipv4_bcast>(?:[0-9]{1,3}\.){3}[0-9]{1,3}))?"",
            re.I)
        re_openbsd_ipv6 = re.compile(
            r""inet6\s+(?P<ipv6_addr>\S+)\s+prefixlen\s+(?P<ipv6_mask>[0-9]+)\s+scopeid\s+(?P<ipv6_scope>\w+x\w+)<""
            r""(?:link|host)>"",
            re.I)
        re_openbsd_details = re.compile(
            r""\S+\s+(?:(?P<mac_addr>[0-9A-Fa-f:?]+)\s+)?txqueuelen\s+[0-9]+\s+\((?P<type>\S+\s?\S+)\)"", re.I)
        re_openbsd_rx = re.compile(r""RX packets (?P<rx_packets>[0-9]+)\s+bytes\s+(?P<rx_bytes>\d+)\s+.*"", re.I)
        re_openbsd_rx_stats = re.compile(
            r""RX errors (?P<rx_errors>[0-9]+)\s+dropped\s+(?P<rx_dropped>[0-9]+)\s+overruns\s+""
            r""(?P<rx_overruns>[0-9]+)\s+frame\s+(?P<rx_frame>[0-9]+)"",
            re.I)
        re_openbsd_tx = re.compile(r""TX packets (?P<tx_packets>[0-9]+)\s+bytes\s+(?P<tx_bytes>\d+)\s+.*"", re.I)
        re_openbsd_tx_stats = re.compile(
            r""TX errors (?P<tx_errors>[0-9]+)\s+dropped\s+(?P<tx_dropped>[0-9]+)\s+overruns\s+""
            r""(?P<tx_overruns>[0-9]+)\s+carrier\s+(?P<tx_carrier>[0-9]+)\s+collisions\s+(?P<tx_collisions>[0-9]+)"",
            re.I)
        re_openbsd = [re_openbsd_interface, re_openbsd_ipv4, re_openbsd_ipv6, re_openbsd_details, re_openbsd_rx,
                      re_openbsd_rx_stats, re_openbsd_tx, re_openbsd_tx_stats]

        # FreeBSD syntax
        re_freebsd_interface = re.compile(
            r""(?P<name>[a-zA-Z0-9:._-]+):\s+flags=(?P<flags>[0-9]+)<(?P<state>\S+)>\s+metric\s+""
            r""(?P<metric>[0-9]+)\s+mtu\s+(?P<mtu>[0-9]+)"",
            re.I)
        re_freebsd_ipv4 = re.compile(
            r""inet (?P<ipv4_addr>(?:[0-9]{1,3}\.){3}[0-9]{1,3})\s+netmask\s+(?P<ipv4_mask>0x\S+)(\s+broadcast\s+""
            r""(?P<ipv4_bcast>(?:[0-9]{1,3}\.){3}[0-9]{1,3}))?"",
            re.I)
        re_freebsd_ipv6 = re.compile(r""\s?inet6\s(?P<ipv6_addr>.*)(?:\%\w+\d+)\sprefixlen\s(?P<ipv6_mask>\d+)(?:\s\w+)?\sscopeid\s(?P<ipv6_scope>\w+x\w+)"", re.I)
        re_freebsd_details = re.compile(r""ether\s+(?P<mac_addr>[0-9A-Fa-f:?]+)"", re.I)
        re_freebsd = [re_freebsd_interface, re_freebsd_ipv4, re_freebsd_ipv6, re_freebsd_details]

        available_interfaces = dict()

        for pattern in [re_linux_interface, re_openbsd_interface, re_freebsd_interface]:
            network_interfaces = re.finditer(pattern, source_data)
            positions = []
            while True:
                try:
                    pos = next(network_interfaces)
                    positions.append(max(pos.start() - 1, 0))
                except StopIteration:
                    break
            if positions:
                positions.append(len(source_data))
                break

        if not positions:
            return available_interfaces

        for l, r in zip(positions, positions[1:]):
            chunk = source_data[l:r]
            _interface = dict()
            for pattern in re_linux + re_openbsd + re_freebsd:
                match = re.search(pattern, chunk.replace('\t', '\n'))
                if match:
                    details = match.groupdict()
                    for k, v in details.items():
                        if isinstance(v, str):
                            details[k] = v.strip()
                    _interface.update(details)
            if _interface is not None:
                available_interfaces[_interface['name']] = self.update_interface_details(_interface)

        return available_interfaces",_47042.py,77,"for pattern in [re_linux_interface, re_openbsd_interface, re_freebsd_interface]:
    network_interfaces = re.finditer(pattern, source_data)
    positions = []
    while True:
        try:
            pos = next(network_interfaces)
            positions.append(max(pos.start() - 1, 0))
        except StopIteration:
            break
    if positions:
        positions.append(len(source_data))
        break","if not positions:
    return available_interfaces","for pattern in [re_linux_interface, re_openbsd_interface, re_freebsd_interface]:
    network_interfaces = re.finditer(pattern, source_data)
    positions = []
    while True:
        try:
            pos = next(network_interfaces)
            positions.append(max(pos.start() - 1, 0))
        except StopIteration:
            break
    if positions:
        positions.append(len(source_data))
        break
else:
    return available_interfaces"
https://github.com/facebookresearch/ReAgent/tree/master/reagent/data/oss_data_fetcher.py,"def upload_as_parquet(df) -> Dataset:
    """"""Generate a random parquet. Fails if cannot generate a non-existent name.""""""

    # get a random tmp name and check if it exists
    sqlCtx = get_spark_session()
    success = False
    for _ in range(MAX_UPLOAD_PARQUET_TRIES):
        suffix = rand_string(length=UPLOAD_PARQUET_TMP_SUFFIX_LEN)
        rand_name = f""tmp_parquet_{suffix}""
        if not sqlCtx.catalog._jcatalog.tableExists(rand_name):
            success = True
            break
    if not success:
        raise Exception(f""Failed to find name after {MAX_UPLOAD_PARQUET_TRIES} tries."")

    # perform the write
    # pyre-fixme[61]: `rand_name` may not be initialized here.
    df.write.mode(""errorifexists"").format(""parquet"").saveAsTable(rand_name)
    # pyre-fixme[61]: `rand_name` may not be initialized here.
    parquet_url = get_table_url(rand_name)
    logger.info(f""Saved parquet to {parquet_url}"")
    return Dataset(parquet_url=parquet_url)",_47702.py,7,"for _ in range(MAX_UPLOAD_PARQUET_TRIES):
    suffix = rand_string(length=UPLOAD_PARQUET_TMP_SUFFIX_LEN)
    rand_name = f'tmp_parquet_{suffix}'
    if not sqlCtx.catalog._jcatalog.tableExists(rand_name):
        success = True
        break","if not success:
    raise Exception(f'Failed to find name after {MAX_UPLOAD_PARQUET_TRIES} tries.')","for _ in range(MAX_UPLOAD_PARQUET_TRIES):
    suffix = rand_string(length=UPLOAD_PARQUET_TMP_SUFFIX_LEN)
    rand_name = f'tmp_parquet_{suffix}'
    if not sqlCtx.catalog._jcatalog.tableExists(rand_name):
        break
else:
    raise Exception(f'Failed to find name after {MAX_UPLOAD_PARQUET_TRIES} tries.')"
https://github.com/cloud-custodian/cloud-custodian/tree/master/tools/c7n_trailcreator/c7n_trailcreator/trailcreator.py,"def format_record(r):
    """"""Format a resource creation cloud trail event for db schema
    """"""
    rinfos = resource_map.get((r['eventSource'], r['eventName']))
    if rinfos is None:
        log.warning(
            ""Could not resolve rinfo %s %s"", r['eventSource'], r['eventName'])
        return

    utype = r['userType']
    if utype == 'Root':
        uid = 'root'
    elif utype == 'SAMLUser':
        uid = r['userName']
    elif utype is None and r['invokedBy'] == 'AWS Internal':
        uid = r['userIdentity']['invokedBy']
    else:
        uid = r['userArn']

    for rinfo in rinfos:
        # todo consider lite implementation
        rid = jmespath.search(rinfo['ids'], r)
        if isinstance(rid, list):
            rid = "" ,"".join(rid)
        if rid:
            break
    if rid is None:
        log.warning(
            ""couldn't find rids account:%s region:%s service:%s api:%s"",
            r['accountId'], r['region'], r['eventSource'], r['eventName'])
        return

    return (
        r['accountId'],
        r['region'],
        r['eventTime'],
        r['eventName'],
        r['eventSource'],
        r.get('userAgent', ''),
        r['sourceIPAddress'],
        uid,
        rinfo['resource']['resource'],
        rid)",_48105.py,20,"for rinfo in rinfos:
    rid = jmespath.search(rinfo['ids'], r)
    if isinstance(rid, list):
        rid = ' ,'.join(rid)
    if rid:
        break","if rid is None:
    log.warning(""couldn't find rids account:%s region:%s service:%s api:%s"", r['accountId'], r['region'], r['eventSource'], r['eventName'])
    return","for rinfo in rinfos:
    rid = jmespath.search(rinfo['ids'], r)
    if isinstance(rid, list):
        rid = ' ,'.join(rid)
    if rid:
        break
else:
    log.warning(""couldn't find rids account:%s region:%s service:%s api:%s"", r['accountId'], r['region'], r['eventSource'], r['eventName'])
    return"
https://github.com/tensorflow/lingvo/tree/master/lingvo/core/ops/mass_op_test.py,"def FindResultFromList(result, expected_results):
  """"""Find the given result from a list of expected results.

  Args:
    result: A MassOutput tuple, from running ops.mass().
    expected_results: A list of MassOutput.  The test asserts `result` is equal
      to at least one result from `expected_results`.

  Returns:
    The index of first match found, or None for not found.

  We use this when the specific output from ops.mass() is not stable across
  different platforms. Specifically, the implementation currently uses
  std::shuffle(), which have different implementations between libc++ and
  stdlibc++.
  """"""
  for idx, expected in enumerate(expected_results):
    match = True
    for attr in MassOutput._fields:
      if not np.array_equal(getattr(result, attr), getattr(expected, attr)):
        match = False
        break
    if match:
      return idx

  tf.logging.error('Found unexpected output from op.mass that fails to match'
                   ' any expected result.')
  for attr in MassOutput._fields:
    tf.logging.info('%s = %s', attr, np.array_repr(getattr(result, attr)))
  return None",_52430.py,19,"for attr in MassOutput._fields:
    if not np.array_equal(getattr(result, attr), getattr(expected, attr)):
        match = False
        break","if match:
    return idx","for attr in MassOutput._fields:
    if not np.array_equal(getattr(result, attr), getattr(expected, attr)):
        break
else:
    return idx"
https://github.com/Cigaras/IPTV.bundle/tree/master/Contents/Code/locale_patch.py,"def initialize_locale():
    if 'Plex-Locale-Patch' in Request.Headers:
        return
    for parse_func in [parse_x_plex_language_value, parse_accept_language_value]:
        value = parse_func()
        if value:
            set_language_header(value)
            break
    if not value:
        Log('Locale Patch: language not detected. All request headers: %s' % str(Request.Headers))
    Request.Headers['Plex-Locale-Patch'] = 'y'",_52700.py,4,"for parse_func in [parse_x_plex_language_value, parse_accept_language_value]:
    value = parse_func()
    if value:
        set_language_header(value)
        break","if not value:
    Log('Locale Patch: language not detected. All request headers: %s' % str(Request.Headers))","for parse_func in [parse_x_plex_language_value, parse_accept_language_value]:
    value = parse_func()
    if value:
        set_language_header(value)
        break
else:
    Log('Locale Patch: language not detected. All request headers: %s' % str(Request.Headers))"
https://github.com/TDAmeritrade/stumpy/tree/master/stumpy/ostinato.py,"def _ostinato(Ts, m, M_Ts, _Ts, dask_client=None, device_id=None, mp_func=stump):
    """"""
    Find the consensus motif amongst a list of time series

    Parameters
    ----------
    Ts : list
        A list of time series for which to find the consensus motif

    m : int
        Window size

    M_Ts : list
        A list of rolling window means for each time series in `Ts`

    _Ts : list
        A list of rolling window standard deviations for each time series in `Ts`

    dask_client : client, default None
        A Dask Distributed client that is connected to a Dask scheduler and
        Dask workers. Setting up a Dask distributed cluster is beyond the
        scope of this library. Please refer to the Dask Distributed
        documentation.

    device_id : int or list, default None
        The (GPU) device number to use. The default value is `0`. A list of
        valid device ids (int) may also be provided for parallel GPU-STUMP
        computation. A list of all valid device ids can be obtained by
        executing `[device.id for device in numba.cuda.list_devices()]`.

    mp_func : object, default stump
        Specify a custom matrix profile function to use for computing matrix profiles

    Returns
    -------
    bsf_radius : float
        The (best-so-far) Radius of the consensus motif

    bsf_Ts_idx : int
        The time series index in `Ts` which contains the consensus motif

    bsf_subseq_idx : int
        The subsequence index within time series `Ts[bsf_Ts_idx]` the contains the
        consensus motif

    Notes
    -----
    `DOI: 10.1109/ICDM.2019.00140 \
    <https://www.cs.ucr.edu/~eamonn/consensus_Motif_ICDM_Long_version.pdf>`__

    See Table 2

    The ostinato algorithm proposed in the paper finds the best radius
    in `Ts`. Intuitively, the radius is the minimum distance of a
    subsequence to encompass at least one nearest neighbor subsequence
    from all other time series. The best radius in `Ts` is the minimum
    radius amongst all radii. Some data sets might contain multiple
    subsequences which have the same optimal radius.
    The greedy Ostinato algorithm only finds one of them, which might
    not be the most central motif. The most central motif amongst the
    subsequences with the best radius is the one with the smallest mean
    distance to nearest neighbors in all other time series. To find this
    central motif it is necessary to search the subsequences with the
    best radius via `stumpy.ostinato._get_central_motif`
    """"""
    bsf_radius = np.inf
    bsf_Ts_idx = 0
    bsf_subseq_idx = 0

    partial_mp_func = core._get_partial_mp_func(
        mp_func, dask_client=dask_client, device_id=device_id
    )

    k = len(Ts)
    for j in range(k):
        if j < (k - 1):
            h = j + 1
        else:
            h = 0

        mp = partial_mp_func(Ts[j], m, Ts[h], ignore_trivial=False)
        si = np.argsort(mp[:, 0])
        for q in si:
            radius = mp[q, 0]
            if radius >= bsf_radius:
                break
            for i in range(k):
                if i != j and i != h:
                    QT = core.sliding_dot_product(Ts[j][q : q + m], Ts[i])
                    radius = np.max(
                        (
                            radius,
                            np.min(
                                core._mass(
                                    Ts[j][q : q + m],
                                    Ts[i],
                                    QT,
                                    M_Ts[j][q],
                                    _Ts[j][q],
                                    M_Ts[i],
                                    _Ts[i],
                                )
                            ),
                        )
                    )
                    if radius >= bsf_radius:
                        break
            if radius < bsf_radius:
                bsf_radius, bsf_Ts_idx, bsf_subseq_idx = radius, j, q

    return bsf_radius, bsf_Ts_idx, bsf_subseq_idx",_53370.py,87,"for i in range(k):
    if i != j and i != h:
        QT = core.sliding_dot_product(Ts[j][q:q + m], Ts[i])
        radius = np.max((radius, np.min(core._mass(Ts[j][q:q + m], Ts[i], QT, M_Ts[j][q], _Ts[j][q], M_Ts[i], _Ts[i]))))
        if radius >= bsf_radius:
            break","if radius < bsf_radius:
    (bsf_radius, bsf_Ts_idx, bsf_subseq_idx) = (radius, j, q)","for i in range(k):
    if i != j and i != h:
        QT = core.sliding_dot_product(Ts[j][q:q + m], Ts[i])
        radius = np.max((radius, np.min(core._mass(Ts[j][q:q + m], Ts[i], QT, M_Ts[j][q], _Ts[j][q], M_Ts[i], _Ts[i]))))
        if radius >= bsf_radius:
            break
else:
    (bsf_radius, bsf_Ts_idx, bsf_subseq_idx) = (radius, j, q)"
https://github.com/confluentinc/confluent-kafka-python/tree/master/src/confluent_kafka/schema_registry/protobuf.py,"def _create_index_array(msg_desc):
    """"""
    Creates an index array specifying the location of msg_desc in
    the referenced FileDescriptor.

    Args:
        msg_desc (MessageDescriptor): Protobuf MessageDescriptor

    Returns:
        list of int: Protobuf MessageDescriptor index array.

    Raises:
        ValueError: If the message descriptor is malformed.
    """"""

    msg_idx = deque()

    # Walk the nested MessageDescriptor tree up to the root.
    current = msg_desc
    found = False
    while current.containing_type is not None:
        previous = current
        current = previous.containing_type
        # find child's position
        for idx, node in enumerate(current.nested_types):
            if node == previous:
                msg_idx.appendleft(idx)
                found = True
                break
        if not found:
            raise ValueError(""Nested MessageDescriptor not found"")

    # Add the index of the root MessageDescriptor in the FileDescriptor.
    found = False
    for idx, msg_type_name in enumerate(msg_desc.file.message_types_by_name):
        if msg_type_name == current.name:
            msg_idx.appendleft(idx)
            found = True
            break
    if not found:
        raise ValueError(""MessageDescriptor not found in file"")

    return list(msg_idx)",_53392.py,35,"for (idx, msg_type_name) in enumerate(msg_desc.file.message_types_by_name):
    if msg_type_name == current.name:
        msg_idx.appendleft(idx)
        found = True
        break","if not found:
    raise ValueError('MessageDescriptor not found in file')","for (idx, msg_type_name) in enumerate(msg_desc.file.message_types_by_name):
    if msg_type_name == current.name:
        msg_idx.appendleft(idx)
        found = True
        break
else:
    raise ValueError('MessageDescriptor not found in file')"
https://github.com/confluentinc/confluent-kafka-python/tree/master/src/confluent_kafka/schema_registry/protobuf.py,"def _create_index_array(msg_desc):
    """"""
    Creates an index array specifying the location of msg_desc in
    the referenced FileDescriptor.

    Args:
        msg_desc (MessageDescriptor): Protobuf MessageDescriptor

    Returns:
        list of int: Protobuf MessageDescriptor index array.

    Raises:
        ValueError: If the message descriptor is malformed.
    """"""

    msg_idx = deque()

    # Walk the nested MessageDescriptor tree up to the root.
    current = msg_desc
    found = False
    while current.containing_type is not None:
        previous = current
        current = previous.containing_type
        # find child's position
        for idx, node in enumerate(current.nested_types):
            if node == previous:
                msg_idx.appendleft(idx)
                found = True
                break
        if not found:
            raise ValueError(""Nested MessageDescriptor not found"")

    # Add the index of the root MessageDescriptor in the FileDescriptor.
    found = False
    for idx, msg_type_name in enumerate(msg_desc.file.message_types_by_name):
        if msg_type_name == current.name:
            msg_idx.appendleft(idx)
            found = True
            break
    if not found:
        raise ValueError(""MessageDescriptor not found in file"")

    return list(msg_idx)",_53392.py,25,"for (idx, node) in enumerate(current.nested_types):
    if node == previous:
        msg_idx.appendleft(idx)
        found = True
        break","if not found:
    raise ValueError('Nested MessageDescriptor not found')","for (idx, node) in enumerate(current.nested_types):
    if node == previous:
        msg_idx.appendleft(idx)
        break
else:
    raise ValueError('Nested MessageDescriptor not found')"
https://github.com/ansible/ansible-modules-core/tree/master/cloud/amazon/ec2_key.py,"def main():
    argument_spec = ec2_argument_spec()
    argument_spec.update(dict(
            name=dict(required=True),
            key_material=dict(required=False),
            state = dict(default='present', choices=['present', 'absent']),
            wait = dict(type='bool', default=False),
            wait_timeout = dict(default=300),
        )
    )
    module = AnsibleModule(
        argument_spec=argument_spec,
        supports_check_mode=True,
    )

    if not HAS_BOTO:
        module.fail_json(msg='boto required for this module')

    name = module.params['name']
    state = module.params.get('state')
    key_material = module.params.get('key_material')
    wait = module.params.get('wait')
    wait_timeout = int(module.params.get('wait_timeout'))

    changed = False

    ec2 = ec2_connect(module)

    # find the key if present
    key = ec2.get_key_pair(name)

    # Ensure requested key is absent
    if state == 'absent':
        if key:
            '''found a match, delete it'''
            if not module.check_mode:
                try:
                    key.delete()
                    if wait:
                        start = time.time()
                        action_complete = False
                        while (time.time() - start) < wait_timeout:
                            if not ec2.get_key_pair(name):
                                action_complete = True
                                break
                            time.sleep(1)
                        if not action_complete:
                            module.fail_json(msg=""timed out while waiting for the key to be removed"")
                except Exception as e:
                    module.fail_json(msg=""Unable to delete key pair '%s' - %s"" % (key, e))
            key = None
            changed = True

    # Ensure requested key is present
    elif state == 'present':
        if key:
            # existing key found
            if key_material:
                # EC2's fingerprints are non-trivial to generate, so push this key 
                # to a temporary name and make ec2 calculate the fingerprint for us.
                #
                # http://blog.jbrowne.com/?p=23
                # https://forums.aws.amazon.com/thread.jspa?messageID=352828

                # find an unused name
                test = 'empty'
                while test:
                    randomchars = [random.choice(string.ascii_letters + string.digits) for x in range(0,10)]
                    tmpkeyname = ""ansible-"" + ''.join(randomchars)
                    test = ec2.get_key_pair(tmpkeyname)

                # create tmp key
                tmpkey = ec2.import_key_pair(tmpkeyname, key_material)
                # get tmp key fingerprint
                tmpfingerprint = tmpkey.fingerprint
                # delete tmp key
                tmpkey.delete()

                if key.fingerprint != tmpfingerprint:
                    if not module.check_mode:
                        key.delete()
                        key = ec2.import_key_pair(name, key_material)    

                        if wait:
                            start = time.time()
                            action_complete = False
                            while (time.time() - start) < wait_timeout:
                                if ec2.get_key_pair(name):
                                    action_complete = True
                                    break
                                time.sleep(1)
                            if not action_complete:
                                module.fail_json(msg=""timed out while waiting for the key to be re-created"")

                    changed = True
            pass

        # if the key doesn't exist, create it now
        else:
            '''no match found, create it'''
            if not module.check_mode:
                if key_material:
                    '''We are providing the key, need to import'''
                    key = ec2.import_key_pair(name, key_material)
                else:
                    '''
                    No material provided, let AWS handle the key creation and 
                    retrieve the private key
                    '''
                    key = ec2.create_key_pair(name)

                if wait:
                    start = time.time()
                    action_complete = False
                    while (time.time() - start) < wait_timeout:
                        if ec2.get_key_pair(name):
                            action_complete = True
                            break
                        time.sleep(1)
                    if not action_complete:
                        module.fail_json(msg=""timed out while waiting for the key to be created"")

            changed = True

    if key:
        data = {
            'name': key.name,
            'fingerprint': key.fingerprint
        }
        if key.material:
            data.update({'private_key': key.material})

        module.exit_json(changed=changed, key=data)
    else:
        module.exit_json(changed=changed, key=None)",_55433.py,42,"while time.time() - start < wait_timeout:
    if not ec2.get_key_pair(name):
        action_complete = True
        break
    time.sleep(1)","if not action_complete:
    module.fail_json(msg='timed out while waiting for the key to be removed')","while time.time() - start < wait_timeout:
    if not ec2.get_key_pair(name):
        break
    time.sleep(1)
else:
    module.fail_json(msg='timed out while waiting for the key to be removed')"
https://github.com/ansible/ansible-modules-core/tree/master/cloud/amazon/ec2_key.py,"def main():
    argument_spec = ec2_argument_spec()
    argument_spec.update(dict(
            name=dict(required=True),
            key_material=dict(required=False),
            state = dict(default='present', choices=['present', 'absent']),
            wait = dict(type='bool', default=False),
            wait_timeout = dict(default=300),
        )
    )
    module = AnsibleModule(
        argument_spec=argument_spec,
        supports_check_mode=True,
    )

    if not HAS_BOTO:
        module.fail_json(msg='boto required for this module')

    name = module.params['name']
    state = module.params.get('state')
    key_material = module.params.get('key_material')
    wait = module.params.get('wait')
    wait_timeout = int(module.params.get('wait_timeout'))

    changed = False

    ec2 = ec2_connect(module)

    # find the key if present
    key = ec2.get_key_pair(name)

    # Ensure requested key is absent
    if state == 'absent':
        if key:
            '''found a match, delete it'''
            if not module.check_mode:
                try:
                    key.delete()
                    if wait:
                        start = time.time()
                        action_complete = False
                        while (time.time() - start) < wait_timeout:
                            if not ec2.get_key_pair(name):
                                action_complete = True
                                break
                            time.sleep(1)
                        if not action_complete:
                            module.fail_json(msg=""timed out while waiting for the key to be removed"")
                except Exception as e:
                    module.fail_json(msg=""Unable to delete key pair '%s' - %s"" % (key, e))
            key = None
            changed = True

    # Ensure requested key is present
    elif state == 'present':
        if key:
            # existing key found
            if key_material:
                # EC2's fingerprints are non-trivial to generate, so push this key 
                # to a temporary name and make ec2 calculate the fingerprint for us.
                #
                # http://blog.jbrowne.com/?p=23
                # https://forums.aws.amazon.com/thread.jspa?messageID=352828

                # find an unused name
                test = 'empty'
                while test:
                    randomchars = [random.choice(string.ascii_letters + string.digits) for x in range(0,10)]
                    tmpkeyname = ""ansible-"" + ''.join(randomchars)
                    test = ec2.get_key_pair(tmpkeyname)

                # create tmp key
                tmpkey = ec2.import_key_pair(tmpkeyname, key_material)
                # get tmp key fingerprint
                tmpfingerprint = tmpkey.fingerprint
                # delete tmp key
                tmpkey.delete()

                if key.fingerprint != tmpfingerprint:
                    if not module.check_mode:
                        key.delete()
                        key = ec2.import_key_pair(name, key_material)    

                        if wait:
                            start = time.time()
                            action_complete = False
                            while (time.time() - start) < wait_timeout:
                                if ec2.get_key_pair(name):
                                    action_complete = True
                                    break
                                time.sleep(1)
                            if not action_complete:
                                module.fail_json(msg=""timed out while waiting for the key to be re-created"")

                    changed = True
            pass

        # if the key doesn't exist, create it now
        else:
            '''no match found, create it'''
            if not module.check_mode:
                if key_material:
                    '''We are providing the key, need to import'''
                    key = ec2.import_key_pair(name, key_material)
                else:
                    '''
                    No material provided, let AWS handle the key creation and 
                    retrieve the private key
                    '''
                    key = ec2.create_key_pair(name)

                if wait:
                    start = time.time()
                    action_complete = False
                    while (time.time() - start) < wait_timeout:
                        if ec2.get_key_pair(name):
                            action_complete = True
                            break
                        time.sleep(1)
                    if not action_complete:
                        module.fail_json(msg=""timed out while waiting for the key to be created"")

            changed = True

    if key:
        data = {
            'name': key.name,
            'fingerprint': key.fingerprint
        }
        if key.material:
            data.update({'private_key': key.material})

        module.exit_json(changed=changed, key=data)
    else:
        module.exit_json(changed=changed, key=None)",_55433.py,87,"while time.time() - start < wait_timeout:
    if ec2.get_key_pair(name):
        action_complete = True
        break
    time.sleep(1)","if not action_complete:
    module.fail_json(msg='timed out while waiting for the key to be re-created')","while time.time() - start < wait_timeout:
    if ec2.get_key_pair(name):
        break
    time.sleep(1)
else:
    module.fail_json(msg='timed out while waiting for the key to be re-created')"
https://github.com/ansible/ansible-modules-core/tree/master/cloud/amazon/ec2_key.py,"def main():
    argument_spec = ec2_argument_spec()
    argument_spec.update(dict(
            name=dict(required=True),
            key_material=dict(required=False),
            state = dict(default='present', choices=['present', 'absent']),
            wait = dict(type='bool', default=False),
            wait_timeout = dict(default=300),
        )
    )
    module = AnsibleModule(
        argument_spec=argument_spec,
        supports_check_mode=True,
    )

    if not HAS_BOTO:
        module.fail_json(msg='boto required for this module')

    name = module.params['name']
    state = module.params.get('state')
    key_material = module.params.get('key_material')
    wait = module.params.get('wait')
    wait_timeout = int(module.params.get('wait_timeout'))

    changed = False

    ec2 = ec2_connect(module)

    # find the key if present
    key = ec2.get_key_pair(name)

    # Ensure requested key is absent
    if state == 'absent':
        if key:
            '''found a match, delete it'''
            if not module.check_mode:
                try:
                    key.delete()
                    if wait:
                        start = time.time()
                        action_complete = False
                        while (time.time() - start) < wait_timeout:
                            if not ec2.get_key_pair(name):
                                action_complete = True
                                break
                            time.sleep(1)
                        if not action_complete:
                            module.fail_json(msg=""timed out while waiting for the key to be removed"")
                except Exception as e:
                    module.fail_json(msg=""Unable to delete key pair '%s' - %s"" % (key, e))
            key = None
            changed = True

    # Ensure requested key is present
    elif state == 'present':
        if key:
            # existing key found
            if key_material:
                # EC2's fingerprints are non-trivial to generate, so push this key 
                # to a temporary name and make ec2 calculate the fingerprint for us.
                #
                # http://blog.jbrowne.com/?p=23
                # https://forums.aws.amazon.com/thread.jspa?messageID=352828

                # find an unused name
                test = 'empty'
                while test:
                    randomchars = [random.choice(string.ascii_letters + string.digits) for x in range(0,10)]
                    tmpkeyname = ""ansible-"" + ''.join(randomchars)
                    test = ec2.get_key_pair(tmpkeyname)

                # create tmp key
                tmpkey = ec2.import_key_pair(tmpkeyname, key_material)
                # get tmp key fingerprint
                tmpfingerprint = tmpkey.fingerprint
                # delete tmp key
                tmpkey.delete()

                if key.fingerprint != tmpfingerprint:
                    if not module.check_mode:
                        key.delete()
                        key = ec2.import_key_pair(name, key_material)    

                        if wait:
                            start = time.time()
                            action_complete = False
                            while (time.time() - start) < wait_timeout:
                                if ec2.get_key_pair(name):
                                    action_complete = True
                                    break
                                time.sleep(1)
                            if not action_complete:
                                module.fail_json(msg=""timed out while waiting for the key to be re-created"")

                    changed = True
            pass

        # if the key doesn't exist, create it now
        else:
            '''no match found, create it'''
            if not module.check_mode:
                if key_material:
                    '''We are providing the key, need to import'''
                    key = ec2.import_key_pair(name, key_material)
                else:
                    '''
                    No material provided, let AWS handle the key creation and 
                    retrieve the private key
                    '''
                    key = ec2.create_key_pair(name)

                if wait:
                    start = time.time()
                    action_complete = False
                    while (time.time() - start) < wait_timeout:
                        if ec2.get_key_pair(name):
                            action_complete = True
                            break
                        time.sleep(1)
                    if not action_complete:
                        module.fail_json(msg=""timed out while waiting for the key to be created"")

            changed = True

    if key:
        data = {
            'name': key.name,
            'fingerprint': key.fingerprint
        }
        if key.material:
            data.update({'private_key': key.material})

        module.exit_json(changed=changed, key=data)
    else:
        module.exit_json(changed=changed, key=None)",_55433.py,115,"while time.time() - start < wait_timeout:
    if ec2.get_key_pair(name):
        action_complete = True
        break
    time.sleep(1)","if not action_complete:
    module.fail_json(msg='timed out while waiting for the key to be created')","while time.time() - start < wait_timeout:
    if ec2.get_key_pair(name):
        break
    time.sleep(1)
else:
    module.fail_json(msg='timed out while waiting for the key to be created')"
https://github.com/devbisme/skidl/tree/master/skidl/tools/kicad/kicad.py,"def load_sch_lib(self, filename=None, lib_search_paths_=None, lib_section=None):
    """"""
    Load the parts from a KiCad schematic library file.

    Args:
        filename: The name of the KiCad schematic library file.
    """"""

    from ...skidl import lib_suffixes
    from .. import KICAD

    # Try to open the file using allowable suffixes for the versions of KiCAD.
    suffixes = lib_suffixes[KICAD]
    base, suffix = os.path.splitext(filename)
    if suffix:
        # If an explicit file extension was given, use it instead of tool lib default extensions.
        suffixes = [suffix]
    for suffix in suffixes:
        # Allow file open failure so multiple suffixes can be tried without error messages.
        f, _ = find_and_open_file(
            filename, lib_search_paths_, suffix, allow_failure=True
        )
        if f:
            # Break from the loop once a library file is successfully opened.
            break
    if not f:
        raise FileNotFoundError(
            ""Unable to open KiCad Schematic Library File {}"".format(filename)
        )

    if suffix == "".kicad_sym"":
        _load_sch_lib_kicad_v6(self, f, filename, lib_search_paths_)
    else:
        _load_sch_lib_kicad(self, f, filename, lib_search_paths_)",_55799.py,18,"for suffix in suffixes:
    (f, _) = find_and_open_file(filename, lib_search_paths_, suffix, allow_failure=True)
    if f:
        break","if not f:
    raise FileNotFoundError('Unable to open KiCad Schematic Library File {}'.format(filename))","for suffix in suffixes:
    (f, _) = find_and_open_file(filename, lib_search_paths_, suffix, allow_failure=True)
    if f:
        break
else:
    raise FileNotFoundError('Unable to open KiCad Schematic Library File {}'.format(filename))"
https://github.com/nschloe/meshio/tree/master/src/meshio/_cli/_info.py,"def info(args):
    # read mesh data
    mesh = read(args.infile, file_format=args.input_format)
    print(mesh)

    # check if the cell arrays are consistent with the points
    is_consistent = True
    for cells in mesh.cells:
        if np.any(cells.data > mesh.points.shape[0]):
            print(""\nATTENTION: Inconsistent mesh. Cells refer to nonexistent points."")
            is_consistent = False
            break

    # check if there are redundant points
    if is_consistent:
        point_is_used = np.zeros(mesh.points.shape[0], dtype=bool)
        for cells in mesh.cells:
            point_is_used[cells.data] = True
        if np.any(~point_is_used):
            print(""ATTENTION: Some points are not part of any cell."")

    return 0",_56216.py,8,"for cells in mesh.cells:
    if np.any(cells.data > mesh.points.shape[0]):
        print('\nATTENTION: Inconsistent mesh. Cells refer to nonexistent points.')
        is_consistent = False
        break","if is_consistent:
    point_is_used = np.zeros(mesh.points.shape[0], dtype=bool)
    for cells in mesh.cells:
        point_is_used[cells.data] = True
    if np.any(~point_is_used):
        print('ATTENTION: Some points are not part of any cell.')","for cells in mesh.cells:
    if np.any(cells.data > mesh.points.shape[0]):
        print('\nATTENTION: Inconsistent mesh. Cells refer to nonexistent points.')
        break
else:
    point_is_used = np.zeros(mesh.points.shape[0], dtype=bool)
    for cells in mesh.cells:
        point_is_used[cells.data] = True
    if np.any(~point_is_used):
        print('ATTENTION: Some points are not part of any cell.')"
https://github.com/ashkamath/mdetr/tree/master/datasets/phrasecut_utils/data_transfer.py,"def polygon_in_box(polygon, box, xywh=True):
    """"""
    Output polygon is the intersect region of given polygon and box
    """"""

    def point_in_box(p, b):
        # b: xyxy
        return b[0] <= p[0] <= b[2] and b[1] <= p[1] <= b[3]

    def point_in_polygon(point, polygon):
        pb = polygon_to_box(polygon)
        pb = xywh_to_xyxy([pb])[0]
        if not point_in_box(point, pb):
            return False
        p_mask = polygons_to_mask([polygon], int(pb[0] + pb[2] + 1), int(pb[1] + pb[3] + 1))
        is_in = p_mask[point[0], point[1]]
        almost_in = np.mean(p_mask.astype(float)[point[0] - 1 : point[0] + 1, point[1] - 1 : point[1] + 1])
        return max(is_in, almost_in)

    def points_in_line(p1, p2):
        return p1[0] == p2[0] or p1[1] == p2[1]

    def points_intersect_box(p0, p1, b):
        def point_in_seg(p0, p1, pt):
            if not min(p0[0], p1[0]) <= pt[0] <= max(p0[0], p1[0]):
                return False
            if not min(p0[1], p1[1]) <= pt[1] <= max(p0[1], p1[1]):
                return False
            # if p0==pt or p1 == pt:
            #     return False
            return True

        # b: xyxy
        valid_ps = []
        if p0[1] != p1[1]:
            k = (p0[0] - p1[0]) * 1.0 / (p0[1] - p1[1])
            x1 = k * (b[1] - p0[1]) + p0[0]
            vp = [x1, b[1]]
            if point_in_box(vp, b) and point_in_seg(p0, p1, vp):
                valid_ps.append(vp)
            x2 = k * (b[3] - p0[1]) + p0[0]
            vp = [x2, b[3]]
            if point_in_box(vp, b) and point_in_seg(p0, p1, vp):
                valid_ps.append(vp)
        if p0[0] != p1[0]:
            k = (p0[1] - p1[1]) * 1.0 / (p0[0] - p1[0])
            y1 = k * (b[0] - p0[0]) + p0[1]
            vp = [b[0], y1]
            if point_in_box(vp, b) and point_in_seg(p0, p1, vp):
                valid_ps.append(vp)
            y2 = k * (b[2] - p0[0]) + p0[1]
            vp = [b[2], y2]
            if point_in_box(vp, b) and point_in_seg(p0, p1, vp):
                valid_ps.append(vp)
        if len(valid_ps) > 1:
            valid_ps.sort(key=lambda q: abs(q[0] - p0[0]))
        return valid_ps

    if xywh:
        box = xywh_to_xyxy([box])[0]

    start_i = -1
    for i, p in enumerate(polygon):
        if point_in_box(p, box):
            start_i = i
            break
    if start_i < 0:
        return None

    polygon_n = []
    out_ps = []
    p_out = None
    for p in polygon[start_i:] + polygon[: start_i + 1]:
        if point_in_box(p, box):
            if not p_out:
                polygon_n.append(p)
            else:
                inter_ps = points_intersect_box(out_ps[-1], p, box)
                if len(inter_ps) < 1:
                    print(box)
                    print(out_ps)
                    print(p)
                    print(inter_ps)
                assert len(inter_ps) >= 1
                p_in = inter_ps[0]
                if points_in_line(p_in, p_out):
                    polygon_n += [p_out, p_in]
                else:
                    to_add = []
                    has_in = False
                    for pt in [[box[0], box[1]], [box[0], box[3]], [box[2], box[1]], [box[2], box[3]]]:
                        is_in = point_in_polygon(pt, out_ps + [p_in, p_out])
                        assert 0 <= is_in <= 1
                        if is_in == 1:
                            has_in = True
                        if is_in > 0:
                            s = 0
                            if points_in_line(pt, p_out):
                                s = -1
                            if points_in_line(pt, p_in):
                                s = 1
                            to_add.append((pt, s, is_in))
                    if has_in:
                        to_add = [x for x in to_add if x[2] == 1]
                        to_add.sort(key=lambda x: x[1])
                    else:
                        to_add.sort(key=lambda x: -x[2])
                        to_add = [to_add[0]]
                    if not to_add:
                        print(""to_add"", box)
                        print(""to_add"", out_ps + [p_in, p_out])
                        print(""to_add"", to_add)
                    assert to_add
                    polygon_n += [p_out] + [x[0] for x in to_add] + [p_in]
                #     print('to_add', to_add)
                # print('p_out', p_out)
                # print('p_in', p_in)
                # print('out_ps', out_ps)
                # print('polygon_n', polygon_n)
                polygon_n.append(p)
                p_out = None
                out_ps = []
        else:
            if not p_out:
                inter_ps = points_intersect_box(polygon_n[-1], p, box)
                if len(inter_ps) < 1:
                    print(box)
                    print(polygon_n)
                    print(p)
                    print(inter_ps)
                assert len(inter_ps) >= 1
                p_out = inter_ps[-1]
                out_ps.append(p)
            else:
                inter_ps = points_intersect_box(out_ps[-1], p, box)
                if inter_ps:
                    if len(inter_ps) != 2:
                        print(box)
                        print(out_ps)
                        print(p)
                        print(inter_ps)
                    assert len(inter_ps) == 2
                    polygon_n += inter_ps
                    out_ps = [p]
                    p_out = inter_ps[-1]
                else:
                    out_ps.append(p)

    return polygon_n[:-1]",_57451.py,63,"for (i, p) in enumerate(polygon):
    if point_in_box(p, box):
        start_i = i
        break","if start_i < 0:
    return None","for (i, p) in enumerate(polygon):
    if point_in_box(p, box):
        start_i = i
        break
else:
    return None"
https://github.com/sugiany/blender_mmd_tools/tree/master/mmd_tools/core/model.py,"def jointGroupObject(self):
        if self.__joint_grp is None:
            for i in filter(lambda x: x.mmd_type == 'JOINT_GRP_OBJ', self.__root.children):
                self.__joint_grp = i
                break
            if self.__joint_grp is None:
                joints = bpy.data.objects.new(name='joints', object_data=None)
                joints.mmd_type = 'JOINT_GRP_OBJ'
                joints.parent = self.__root
                bpy.context.scene.objects.link(joints)
                self.__joint_grp = joints
        return self.__joint_grp",_57534.py,3,"for i in filter(lambda x: x.mmd_type == 'JOINT_GRP_OBJ', self.__root.children):
    self.__joint_grp = i
    break","if self.__joint_grp is None:
    joints = bpy.data.objects.new(name='joints', object_data=None)
    joints.mmd_type = 'JOINT_GRP_OBJ'
    joints.parent = self.__root
    bpy.context.scene.objects.link(joints)
    self.__joint_grp = joints","for i in filter(lambda x: x.mmd_type == 'JOINT_GRP_OBJ', self.__root.children):
    self.__joint_grp = i
    break
else:
    joints = bpy.data.objects.new(name='joints', object_data=None)
    joints.mmd_type = 'JOINT_GRP_OBJ'
    joints.parent = self.__root
    bpy.context.scene.objects.link(joints)
    self.__joint_grp = joints"
https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/kawaii_player.py,"def dataReady(self, p):
        global new_epn, epn, opt, site
        global cache_empty, buffering_mplayer
        global artist_name_mplayer, server
        global pause_indicator
        global mpv_indicator
        global sub_id, audio_id, current_playing_file_path, desktop_session
        
        try:
            a = str(p.readAllStandardOutput(), 'utf-8').strip()
            #logger.debug('\n-->{0}<--\n'.format(a))
            if self.player_val in ['vlc', 'cvlc'] and ""status change:"" in a:
                self.mpvplayer_val.write(bytes(""get_length"", ""utf-8""))
            if self.player_val in ['vlc', 'cvlc'] and ""main playlist debug:"" in a:
                pls_item_search = re.search(""main playlist debug: using item (?P<row>\d+)\n"", a)
                if pls_item_search:
                    pls_item_number = int(pls_item_search.group(""row""))
                    if pls_item_number in range(0, self.list2.count()):
                        self.cur_row = pls_item_number
                        self.list2.setCurrentRow(self.cur_row)
                        current_item = self.list2.currentItem(self.cur_row)
                        text = current_item.text()
                        self.progressEpn.setFormat((text))
                        MainWindow.setWindowTitle(text)
                        self.epn_name_in_list = text
            elif 'volume' in a:
                logger.debug(a)
            elif 'Video' in a:
                logger.debug(a)
            elif 'Audio' in a:
                logger.info(a)
            if self.custom_mpv_input_conf and self.player_val.lower() == 'mpv':
                if 'set property: fullscreen' in a.lower():
                    logger.debug(a)
                    if (self.tab_5.width() >= screen_width
                            and self.tab_5.height() >= screen_height):
                        self.tab_5.player_fs()
                    else:
                        self.tab_5.player_fs(mode='fs')
                    a = 'FULLSCREEN_TOGGLE'
                elif 'playlist-next' in a.lower():
                    self.mpvNextEpnList()
                elif 'playlist-prev' in a.lower():
                    self.mpvPrevEpnList()
                elif 'set property: pause' in a.lower():
                    if self.mpv_custom_pause:
                        self.mpv_custom_pause = False
                        self.player_play_pause_status('play')
                    else:
                        self.mpv_custom_pause = True
                        self.player_play_pause_status('pause')
                elif 'Exiting... (Quit)' in a:
                    self.playerStop(msg='already quit')
                elif 'load_sub: load_external_subtitle' in a.lower():
                    if not self.acquire_subtitle_lock:
                        self.acquire_subtitle_lock = True
                        self.tab_5.load_external_sub()
                elif 'stop_cmd: stop_after_current_file' in a.lower():
                    self.quit_really = 'yes'
                    txt_osd = '\n show-text ""Stop After Current File"" \n'
                    self.mpvplayer_val.write(bytes(txt_osd, 'utf-8'))
                    logger.debug(a)
                elif 'set property: video-aspect=' in a.lower():
                    logger.debug(a)
                    aspect_val = a.split('video-aspect=')[1].split(' ')[0]
                    for asp_ratio in self.mpvplayer_aspect:
                        if aspect_val == self.mpvplayer_aspect[asp_ratio]:
                            self.mpvplayer_aspect_cycle = int(asp_ratio)
                            logger.info('SET ASPECT = {0}::{1}'.format(self.mpvplayer_aspect_cycle, aspect_val))
                            a = 'ASPECT CHANGED'
                            if aspect_val == '-1':
                                show_text_val = 'Original Aspect'
                            elif aspect_val == '0':
                                show_text_val = 'Aspect Ratio Disabled'
                            else:
                                show_text_val = aspect_val
                            txt_osd = '\n show-text ""{0}"" \n'.format(show_text_val)
                            self.mpvplayer_val.write(bytes(txt_osd, 'utf-8'))
                            break
                if a and 'AV:' not in a and 'A:' not in a:
                    #logger.debug('-->{0}<--'.format(a))
                    pass
            if (self.player_val.lower() == 'mpv' and self.mplayerLength
                    and (""EOF code: 1"" in a
                    or ""HTTP error 403 Forbidden"" in a 
                    or self.progress_counter > self.mplayerLength + 2
                    or 'finished playback, success (reason 0)' in a)):
                if not self.eof_reached and not self.player_setLoop_var:
                    self.eof_reached = True
                    self.eof_lock = True
                logger.debug('-- start --')
                logger.debug(a)
                logger.debug('{0}::{1}'.format(self.mplayerLength, self.progress_counter))
                logger.debug('-- end --')
            if 'icy info:' in a.lower() or 'icy-title:' in a.lower():
                if 'icy info:' in a.lower():
                    song_title = re.search(""'[^']*"", a)
                    self.epn_name_in_list = song_title.group().replace(""'"", '')
                else:
                    song_title = re.search(""icy-title:[^\n]*"", a)
                    self.epn_name_in_list = song_title.group().replace('icy-title:', '')
                print(self.epn_name_in_list, '--radio--song--')
                self.mplayerLength = 1
                self.epn_name_in_list = self.epn_name_in_list.strip()
                server._emitMeta('internet-radio#'+self.epn_name_in_list, site, self.epn_arr_list)
        except Exception as err:
            logger.error(err)
            a = """"
        try:
            if self.player_val.lower() == ""mpv"":
                if ""AUDIO_ID"" in a or ""AUDIO_KEY_ID"" in a:
                    new_arr = a.split('\n')
                    a_id = None
                    for i in new_arr:
                        if i.startswith('AUDIO_ID') or i.startswith('AUDIO_KEY_ID'):
                            a_id = i.split('=')[-1]
                            break
                    if a_id is not None:
                        audio_s = (re.search('[(][0-9]+[)]', a_id))
                        if audio_s:
                            audio_id = audio_s.group()
                            audio_id = re.sub('\)|\(', '', audio_id).strip()
                        else:
                            audio_id=""auto""
                        self.audio_track.setText(""A:""+str(a_id[:8]))
                        if 'AUDIO_KEY_ID' in a:
                            self.change_sid_aid_video(aid=audio_id)
                    else:
                        logger.error('error getting proper a_id')
                elif ""SUB_ID"" in a or ""SUB_KEY_ID"" in a:
                    tsid = sub_id
                    new_arr = a.split('\n')
                    s_id = None
                    for i in new_arr:
                        if i.startswith('SUB_ID') or i.startswith('SUB_KEY_ID'):
                            s_id = i.split('=')[-1]
                            break
                    if s_id is not None:
                        sub_s = (re.search('[(][^)]*', s_id))
                        if sub_s:
                            sub_id = (sub_s.group()).replace('(', '')
                        else:
                            sub_id = ""no""
                        if tsid == 'auto' and sub_id == 'no':
                            val_id = 'auto'
                            sub_id = 'auto'
                            self.mpvplayer_val.write(b'\n cycle sub \n')
                        else:
                            val_id = str(s_id[:8])
                        self.subtitle_track.setText(""Sub:""+val_id)
                        if 'SUB_KEY_ID' in a:
                            self.change_sid_aid_video(sid=sub_id)
                    else:
                        logger.error('error getting proper sub id')
                elif 'volume-print=' in a:
                    if 'ao-volume-print=' in a:
                        self.volume_type = 'ao-volume'
                    else:
                        self.volume_type = 'volume'
                    player_vol = re.search('volume-print=[0-9]+', a)
                    print(player_vol)
                    if player_vol:
                        player_vol = player_vol.group()
                        self.player_volume = player_vol.split('=')[1]
                        self.change_sid_aid_video(vol=self.player_volume)
                        logger.debug('set volume={0}'.format(self.player_volume))
                        if self.player_volume.isnumeric():
                            if self.volume_type == 'ao-volume':
                                msg_str = 'AO Volume'
                            else:
                                msg_str = 'Volume'
                            msg = '\n show-text ""{}: {}%"" \n'.format(msg_str, self.player_volume)
                            self.mpvplayer_val.write(bytes(msg, 'utf-8'))
                elif ""LENGTH_SECONDS="" in a:
                    mpl = re.search('LENGTH_SECONDS=[0-9]+[:][0-9]+[:][0-9]+', a)
                    if not mpl:
                        mpl = re.search('LENGTH_SECONDS=[0-9]+', a)
                    logger.debug('{} {}'.format(mpl, a))
                    if mpl:
                        mpl = mpl.group()
                        logger.debug(mpl)
                        if ':' in mpl:
                            n = mpl.split('=')[1]
                            o = n.split(':')
                            self.mplayerLength = int(o[0])*3600+int(o[1])*60+int(o[2])
                            self.mpv_playback_duration = n
                        else:
                            self.mplayerLength = int(mpl.split('=')[1])
                        self.slider.setRange(0, int(self.mplayerLength))
                elif (""AV:"" in a or ""A:"" in a or ""V:"" in a) and not self.eof_reached:
                    if not self.mpv_start:
                        self.mpv_start = True
                        try:
                            msg = 'Playing: {0}'.format(self.epn_name_in_list.replace('#', '', 1))
                            if self.epn_name_in_list:
                                MainWindow.setWindowTitle(self.epn_name_in_list)
                            if not self.float_window.isHidden():
                                self.float_window.setWindowTitle(self.epn_name_in_list)
                            msg = bytes('\n show-text ""{0}"" 2000 \n'.format(msg), 'utf-8')
                            self.mpvplayer_val.write(msg)
                        except Exception as err:
                            logger.error(err)
                        self.mplayer_finished_counter = 0
                        if (MainWindow.isFullScreen() and site != ""Music""
                                and self.list2.isHidden() and self.tab_6.isHidden()
                                and self.tab_2.isHidden()):
                            self.gridLayout.setSpacing(0)
                            if self.frame_timer.isActive():
                                self.frame_timer.stop()
                            if self.tab_6.isHidden():
                                self.frame_timer.start(5000)
                        self.subMplayer()
                    if (""Buffering"" in a and not mpv_indicator 
                            and (site != ""Local"" or site != ""Music"" 
                            or site != ""Video"")):
                        cache_empty = ""yes""
                        mpv_indicator.append(""cache empty"") 
                        print(""buffering"")
                        self.mpvplayer_val.write(b'\n set pause yes \n')
                        self.player_play_pause.setText(self.player_buttons['play'])
                        if not pause_indicator:
                            pause_indicator.append('Pause')
                        if MainWindow.isFullScreen() and self.layout_mode != ""Music"":
                            self.gridLayout.setSpacing(0)
                            self.frame1.show()
                    timearr = re.findall(""[0-9][0-9]+:[0-9][0-9]+:[0-9][0-9]+"", a)
                    percomp = re.search(""[(]*[0-9]*\%[)]*"", a)
                    if timearr:
                        val1 = timearr[0].split(':')
                        if val1:
                            val = int(val1[0])*3600+int(val1[1])*60+int(val1[2])
                        else:
                            val = 0
                        if len(timearr) == 1:
                            end_time = '00:00:00'
                            if self.mplayerLength > 1:
                                txt = self.progressEpn.text()
                                timearr = re.findall(""[0-9][0-9]+:[0-9][0-9]+:[0-9][0-9]+"", txt)
                                if timearr and len(timearr) == 2:
                                    end_time = timearr[1]
                                elif self.mpv_playback_duration:
                                    end_time = self.mpv_playback_duration
                            out = timearr[0] + ' / ' + end_time
                        else:
                            out = timearr[0] + ' / ' + timearr[1]
                        per_comp = '(0%)'
                        if percomp:
                            per_comp = percomp.group()
                            if not per_comp.endswith(')'):
                                per_comp = per_comp + ')'
                        else:
                            txt = self.progressEpn.text()
                            percomp = re.search(""[(]*[0-9]*\%[)]*"", txt)
                            if percomp:
                                per_comp = percomp.group()
                            elif self.mplayerLength > 1:
                                per_comp = '('+str(int(100*val/self.mplayerLength))+'%)'
                                
                        out = out + ' ' + per_comp
                            
                        cache_exists = False
                        if ""Cache:"" in a:
                            self.cache_mpv_indicator = True
                            cache_int = 0
                            n = re.findall(""Cache:[^+]*"", a)
                            if 's' in n[0]:
                                cache_val = re.search(""[0-9][^s]*"", n[0]).group()
                            else:
                                cache_val = self.cache_mpv_counter
                            
                            try:
                                cache_int = int(cache_val)
                            except Exception as err_val:
                                print(err_val)
                                cache_int = 0
                            
                            if cache_int >= 119:
                                cache_int = 119
                            elif cache_int >=9 and cache_int < 12:
                                cache_int = 10
                            if cache_int < 10:
                                cache_val = '0'+str(cache_int)
                            else:
                                cache_val = str(cache_int)
                            cache_exists = True
                            self.cache_mpv_counter = cache_val
                        else:
                            cache_val = '00'
                            cache_int = 0
                        try:
                            new_cache_val = cache_int
                        except Exception as e:
                            print(e, '--cache-val-error--')
                            new_cache_val = 0
                        if self.cache_mpv_indicator:
                            out = out +""  Cache:""+str(self.cache_mpv_counter)+'s'
                        if ""Paused"" in a and not mpv_indicator:
                            out = ""(Paused) ""+out
                            if self.custom_mpv_input_conf:
                                self.mpv_custom_pause = True
                        elif ""Paused"" in a and mpv_indicator:
                            out = ""(Paused Caching..) ""+out
                            if self.custom_mpv_input_conf:
                                self.mpv_custom_pause = True
                        
                        
                        if not self.mplayerLength:
                            if self.mpv_cnt > 4:
                                m = re.findall('[/][^(]*', out)
                                try:
                                    n = re.sub(' |[/]', '', m[0])
                                except Exception as err_msg:
                                    print(err_msg)
                                    n = '00:00:00'
                                print(n)
                                o = n.split(':')
                                self.mplayerLength = int(o[0])*3600+int(o[1])*60+int(o[2])
                                print(self.mplayerLength, ""--mpvlength"", a)
                                if self.mplayerLength == 0:
                                    if self.mpv_length_find_attempt >= 4:
                                        self.mplayerLength = 1
                                        self.mpv_length_find_attempt = 0
                                        logger.warning('No Suitable length detected')
                                        msg = '\n print-text ""LENGTH_SECONDS=${duration}"" \n'
                                        self.mpvplayer_val.write(bytes(msg, 'utf-8'))
                                    else:
                                        self.mpv_cnt = 0
                                        self.mpv_length_find_attempt += 1
                                        logger.warning(self.mpv_length_find_attempt)
                                        msg = '\n print-text ""LENGTH_SECONDS=${duration}"" \n'
                                        self.mpvplayer_val.write(bytes(msg, 'utf-8'))
                                self.mpv_playback_duration = n
                                self.progressEpn.setMaximum(int(self.mplayerLength))
                                self.slider.setRange(0, int(self.mplayerLength))
                                self.mpv_cnt = 0
                            logger.debug(self.mplayerLength)
                            self.mpv_cnt = self.mpv_cnt + 1
                            if (MainWindow.isFullScreen() and site != 'Music'
                                    and self.list2.isHidden() and self.tab_6.isHidden()
                                    and self.tab_2.isHidden()):
                                if not self.gapless_playback:
                                    self.gridLayout.setSpacing(0)
                                    self.frame1.show()
                                if self.frame_timer.isActive():
                                    self.frame_timer.stop()
                                if self.tab_6.isHidden():
                                    self.frame_timer.start(2000)
                        out1 = out
                        self.progressEpn.setFormat((out1))
                        if self.mplayerLength == 1:
                            val = 0
                            self.slider.setValue(0)
                        else:
                            self.slider.setValue(val)
                        if self.progress_counter == self.mplayerLength:
                            self.progress_counter += 1
                            logger.debug(self.progress_counter)
                        else:
                            self.progress_counter = val
                            if (self.gapless_network_stream and not self.queue_url_list
                                    and site in ['Music', 'PlayLists', 'None', 'NONE']): 
                                if self.progress_counter > int(self.mplayerLength/2):
                                    if (self.cur_row + 1) < self.list2.count():
                                        item_index = self.cur_row + 1
                                    else:
                                        item_index = 0
                                    if self.tmp_pls_file_dict.get(item_index) is False and self.list2.count() > 1:
                                        self.start_gapless_stream_process(item_index)
                        if not self.new_tray_widget.isHidden():
                            self.new_tray_widget.update_signal.emit(out, val)
                        if cache_empty == 'yes':
                            try:
                                if new_cache_val > self.cache_pause_seconds:
                                    cache_empty = 'no'
                                    self.mpvplayer_val.write(b'\n set pause no \n')
                                    self.player_play_pause.setText(self.player_buttons['pause'])
                                    if mpv_indicator:
                                        mpv_indicator.pop()
                                    if pause_indicator:
                                        pause_indicator.pop()
                                    if self.frame_timer.isActive():
                                        self.frame_timer.stop()
                                    self.frame_timer.start(100)
                            except Exception as err_val:
                                print(err_val, '--mpv--cache-error--')
                elif (""VO:"" in a or ""AO:"" in a or 'Stream opened successfully' in a) and not self.mplayerLength:
                    self.cache_mpv_indicator = False
                    self.cache_mpv_counter = '00'
                    self.mpv_playback_duration = 0
                    t = ""Loading: ""+self.epn_name_in_list+"" (Please Wait)""
                    self.progressEpn.setFormat((t))
                    self.eof_reached = False
                    self.eof_lock = False
                    QtCore.QTimer.singleShot(1000, partial(self.set_sub_audio_text, 'aid'))
                    QtCore.QTimer.singleShot(1500, partial(self.set_sub_audio_text, 'sid'))
                    if OSNAME == 'nt':
                        sub_spacing = self.subtitle_dict.get('sub-spacing')
                        if sub_spacing:
                            cmd = 'set sub-spacing {}'.format(sub_spacing)
                            QtCore.QTimer.singleShot(
                                2000, partial(self.mpv_execute_command, cmd, self.cur_row)
                            )
                    if self.gapless_network_stream:
                        if self.append_audio_start:
                            if self.append_audio_gapless:
                                if self.append_counter > 0:
                                    self.append_counter = 0
                                    self.mpv_execute_command('set aid 2', self.cur_row, timer=1000)
                elif (self.eof_reached and self.eof_lock 
                        and not self.epn_wait_thread.isRunning()):
                    if self.gapless_network_stream:
                        if self.append_audio_start:
                            if self.append_audio_gapless:
                                if self.append_counter > 0:
                                    self.mpv_execute_command('set aid 2', self.cur_row, timer=2000)
                                else:
                                    self.mpv_execute_command('set aid 2', self.cur_row)
                            else:
                                self.mpv_execute_command('set aid 1', self.cur_row)
                                self.append_counter += 1
                    self.eof_lock = False
                    self.eof_reached = False
                    if ""EOF code: 1"" in a:
                        reason_end = 'EOF code: 1'
                    else:
                        reason_end = 'length of file equals progress counter'
                    if self.final_playing_url in self.history_dict_obj:
                        param_avail = False
                        if self.video_parameters:
                            if self.final_playing_url == self.video_parameters[0]:
                                asp = self.video_parameters[-1]
                                vol = self.video_parameters[-2]
                                param_avail = True
                        if not param_avail:        
                            asp = self.mpvplayer_aspect.get(str(self.mpvplayer_aspect_cycle))
                            vol = self.player_volume
                        self.history_dict_obj.update(
                            {   self.final_playing_url:[
                                    0, time.time(), sub_id, audio_id,
                                    0, vol, asp
                                ]
                            }
                            )
                        logger.debug(self.video_parameters)
                    self.cache_mpv_indicator = False
                    self.cache_mpv_counter = '00'
                    self.mpv_playback_duration = 0
                    logger.debug('\ntrack no. {0} ended due to reason={1}\n::{2}'.format(self.cur_row, reason_end, a))
                    logger.debug('{0}::{1}'.format(self.mplayerLength, self.progress_counter))
                    queue_item = None
                    if self.queue_url_list:
                        queue_item = self.queue_url_list[0]
                    elif not self.queue_url_list and self.playlist_queue_used:
                        self.playlist_queue_used = False
                        pls_file = self.tmp_pls_file
                        if OSNAME == 'nt':
                            pls_file = 'file:///{}'.format(self.tmp_pls_file.replace('\\', '/'))
                        cmd = 'loadlist ""{}""'.format(pls_file)
                        self.mpv_execute_command(cmd, self.cur_row)
                        self.cur_row = -1
                    if self.player_setLoop_var:
                        pass
                    elif queue_item is None or isinstance(queue_item, tuple):
                        if self.list2.count() == 0:
                            return 0
                        if self.cur_row == self.list2.count() - 1:
                            self.cur_row = 0
                            if site == ""Music"" and not self.playerPlaylist_setLoop_var:
                                r1 = self.list1.currentRow()
                                it1 = self.list1.item(r1)
                                if it1:
                                    if r1 < self.list1.count():
                                        r2 = r1+1
                                    else:
                                        r2 = 0
                                    self.list1.setCurrentRow(r2)
                                    self.listfound()
                        else:
                            self.cur_row += 1
                        self.list2.setCurrentRow(self.cur_row)
                        logger.debug('\ncurR={0}\n'.format(self.cur_row))
                    self.mplayerLength = 0
                    self.total_file_size = 0
                    if self.mpv_start:
                        self.mpv_start = False
                    if ""HTTP error 403 Forbidden"" in a:
                        print(a)
                        self.quit_really = ""yes""
                    if self.playlist_continue and self.quit_really == ""no"" and not self.epn_wait_thread.isRunning():
                        if self.tab_5.isHidden() and thumbnail_indicator:
                            length_1 = self.list2.count()
                            q3=""self.label_epn_""+str(length_1+self.thumbnail_label_number[0])+"".setText(self.epn_name_in_list)""
                            exec(q3)
                            q3=""self.label_epn_""+str(length_1+self.thumbnail_label_number[0])+"".setAlignment(QtCore.Qt.AlignCenter)""
                            exec(q3)
                        if site in [""Video"", ""Music"", ""PlayLists"", ""None"", ""MyServer""]:
                            if queue_item is None or isinstance(queue_item, tuple):
                                move_ahead = True
                                if self.gapless_network_stream:
                                    if self.tmp_pls_file_dict.get(self.cur_row):
                                        if self.tmp_pls_file_lines[self.cur_row].startswith('http'):
                                            move_ahead = False
                                            self.final_playing_url = self.tmp_pls_file_lines[self.cur_row]
                                            tname = self.epn_arr_list[self.cur_row]
                                            if tname.startswith('#'):
                                                tname = tname.replace('#', '', 1)
                                            if '\t' in tname:
                                                self.epn_name_in_list = tname.split('\t')[0]
                                            else:
                                                self.epn_name_in_list = tname
                                            MainWindow.setWindowTitle(self.epn_name_in_list)
                                            self.float_window.setWindowTitle(self.epn_name_in_list)
                                            server._emitMeta(""Next"", site, self.epn_arr_list)
                                            if self.cur_row == 0 and 'master_abs_path=' in self.final_playing_url:
                                                move_ahead = True
                                            if self.pc_to_pc_casting == 'slave' and 'master_abs_path=' in self.final_playing_url:
                                                self.check_and_start_getsub_method()
                                        self.tmp_pls_file_dict.update({self.cur_row:False})
                                if move_ahead:
                                    self.localGetInList(eofcode='end')
                            else:
                                self.getQueueInList(eofcode='end')
                        else:
                            if queue_item is None or isinstance(queue_item, tuple):
                                self.getNextInList(eofcode='end')
                            else:
                                self.getQueueInList(eofcode='end')
                    elif (self.quit_really == ""yes"" or not self.playlist_continue): 
                        self.player_stop.clicked_emit()
                        self.list2.setFocus()
            elif self.player_val.lower() == ""mplayer"":
                if ""PAUSE"" in a:
                    if buffering_mplayer != 'yes':
                        self.player_play_pause.setText(self.player_buttons['play'])
                    if MainWindow.isFullScreen() and self.layout_mode != ""Music"":
                        self.gridLayout.setSpacing(0)
                        self.frame1.show()
                        if (buffering_mplayer == ""yes""):
                            if self.frame_timer.isActive:
                                self.frame_timer.stop()
                            self.frame_timer.start(10000)
                if ""Cache empty"" in a:
                    cache_empty = ""yes""
                    
                if ""ID_VIDEO_BITRATE"" in a:
                    try:
                        a0 = re.findall('ID_VIDEO_BITRATE=[^\n]*', a)
                        print(a0[0], '--videobit')
                        a1 = a0[0].replace('ID_VIDEO_BITRATE=', '')
                        self.id_video_bitrate=int(a1)
                    except:
                        self.id_video_bitrate = 0
                    
                if ""ID_AUDIO_BITRATE"" in a:
                    try:
                        a0 = re.findall('ID_AUDIO_BITRATE=[^\n]*', a)
                        print(a0[0], '--audiobit')
                        a1 = a0[0].replace('ID_AUDIO_BITRATE=', '')
                        self.id_audio_bitrate=int(a1)
                    except:
                        self.id_audio_bitrate=0
                if ""ANS_switch_audio"" in a:
                    print(a)
                    audio_id = a.split('=')[-1]
                    
                    print(""audio_id=""+audio_id)
                    self.audio_track.setText(""A:""+str(audio_id))
                if ""ANS_sub"" in a:
                    sub_id = a.split('=')[-1]
                    
                    print(""sub_id=""+sub_id)
                    self.subtitle_track.setText(""Sub:""+str(sub_id))
                
                if ""ID_LENGTH"" in a and not self.mplayerLength:
                    t = re.findall('ID_LENGTH=[0-9][^.]*', a)
                    self.mplayerLength = re.sub('ID_LENGTH=', '', t[0])
                    print(self.mplayerLength)
                    self.mplayerLength = int(self.mplayerLength) *1000
                    self.slider.setRange(0, int(self.mplayerLength))
                    self.total_file_size = int(((self.id_audio_bitrate+self.id_video_bitrate)*self.mplayerLength)/(8*1024*1024*1000))
                    print(self.total_file_size, ' MB')
                if (""A:"" in a) or (""PAUSE"" in a):
                    if not self.mpv_start:
                        self.mpv_start = True
                        try:
                            if self.tab_5.mplayer_aspect_msg:
                                aspect_val = self.mpvplayer_aspect.get(str(self.mpvplayer_aspect_cycle))
                                if aspect_val == '-1':
                                    show_text_val = 'Original Aspect'
                                elif aspect_val == '0':
                                    show_text_val = 'Aspect Ratio Disabled'
                                else:
                                    show_text_val = aspect_val
                                txt_osd = '\n osd_show_text ""{0}"" 2000\n'.format(show_text_val)
                                self.mpvplayer_val.write(bytes(txt_osd, 'utf-8'))
                                self.tab_5.mplayer_aspect_msg = False
                            else:
                                npn = '""'+""Playing: ""+self.epn_name_in_list.replace('#', '', 1)+'""'
                                npn1 = bytes('\n'+'osd_show_text '+str(npn)+' 4000'+'\n', 'utf-8')
                                MainWindow.setWindowTitle(self.epn_name_in_list)
                                if not self.float_window.isHidden():
                                    self.float_window.setWindowTitle(self.epn_name_in_list)
                                self.mpvplayer_val.write(npn1)
                            self.mplayer_finished_counter = 0
                        except:
                            pass
                        if (MainWindow.isFullScreen() and self.layout_mode != ""Music""
                                and self.list2.isHidden() and self.tab_2.isHidden()
                                and self.tab_6.isHidden()):
                            self.gridLayout.setSpacing(0)
                            if not self.frame1.isHidden():
                                self.frame1.hide()
                            if self.frame_timer.isActive():
                                self.frame_timer.stop()
                            self.frame_timer.start(1000)
                        self.subMplayer()
                    if ""PAUSE"" in a:
                        print(a, 'Pause A')
                        c = None
                        c_int = 0
                        if ""%"" in a:
                            m = a.split(' ')
                            print(m)
                            if m:
                                try:
                                    c = m[-1]
                                    if len(c) > 3:
                                        c = ""0%""
                                    c_int = int(c.replace('%', '')) 
                                except Exception as e:
                                    print(e, '--percent cache error--')
                        try:
                            t = str(self.progressEpn.text())
                            if c and c_int:
                                t = re.sub('Cache:[0-9]*%', '', t)
                            t = t.strip()
                            if '(Paused) ' in t:
                                t = t.replace('(Paused) ', '')
                            if '(Paused Caching..Wait) ' in t:
                                t = t.replace('(Paused Caching..Wait) ', '')
                        except:
                            t = """"
                        if buffering_mplayer == ""yes"" or self.mplayer_pause_buffer:
                            print('buffering mplayer')
                            if 'Cache:' not in t:
                                out = ""(Paused Caching..Wait) ""+t+' Cache:'+c
                            else:
                                out = ""(Paused Caching..Wait) ""+t
                            if ((not self.mplayer_timer.isActive()) 
                                    and (not self.video_local_stream) and c_int > 0):
                                self.mplayer_timer.start(1000)
                            elif ((not self.mplayer_timer.isActive()) 
                                    and (self.video_local_stream) and c_int > 5):
                                self.mplayer_timer.start(1000)
                        else:
                            if c_int and c:
                                out = ""(Paused) ""+t+' Cache:'+c
                            else:
                                out = ""(Paused) ""+t
                            
                            if ((not self.mplayer_timer.isActive()) 
                                    and (self.video_local_stream) and c_int > 5):
                                self.mplayer_timer.start(1000)
                    else:
                        if ""%"" in a:
                            m = a.split(' ')
                            try:
                                c = m[-2]
                            except:
                                c = ""0%""
                        else:
                            c = ""0%""
                    
                        t = re.findall('A:[^.]*', a)
                        l = re.sub('A:[^0-9]*', '', t[0])
                        l =int(l)*1000
                        
                        if self.mplayerLength == 1:
                            l = 0
                            self.slider.setValue(0)
                        else:
                            self.slider.setValue(l)
                        
                        if self.progress_counter == self.mplayerLength:
                            self.progress_counter += 1
                            logger.debug(self.progress_counter)
                        else:
                            self.progress_counter = l
                        
                        if site == ""Music"":
                            out_time = str(datetime.timedelta(milliseconds=int(l))) + "" / "" + str(datetime.timedelta(milliseconds=int(self.mplayerLength)))
                            
                            out = out_time + "" [""+self.epn_name_in_list+'('+artist_name_mplayer+')' +""]""
                        else:
                            out_time = str(datetime.timedelta(milliseconds=int(l))) + "" / "" + str(datetime.timedelta(milliseconds=int(self.mplayerLength)))
                            
                            out = out_time + "" [""+self.epn_name_in_list+""]"" +' Cache:'+c
                            
                        if not self.new_tray_widget.isHidden():
                            self.new_tray_widget.update_signal.emit(out_time, int(l))
                        if self.video_local_stream:
                            if c == '0%' and not self.mplayer_pause_buffer and not self.mplayer_nop_error_pause:
                                self.mpvplayer_val.write(b'\n pause \n')
                                self.mplayer_pause_buffer = True
                    if ((cache_empty == ""yes"" ) 
                            and (site != ""Local"" or site != ""Music"" or site != ""Video"")):
                        print('---nop--error--pausing---')
                        if not self.mplayer_pause_buffer:
                            self.mpvplayer_val.write(b'\n pause \n')
                            cache_empty = ""no""
                            buffering_mplayer = ""yes""
                    elif (('nop_streaming_read_error' in a) 
                            and (site != ""Local"" or site != ""Music"" or site != ""Video"")):
                        print('---nop--error--pausing---')
                        if not self.mplayer_pause_buffer:
                            self.mpvplayer_val.write(b'\n pause \n')
                            cache_empty = ""no""
                            buffering_mplayer = ""yes""
                            self.mplayer_nop_error_pause = True
                    if self.total_seek != 0:
                        r = ""Seeking ""+str(self.total_seek)+'s'
                        self.progressEpn.setFormat((r))
                    else:
                        self.progressEpn.setFormat((out))
                if 'http' in a:
                    t = ""Loading: ""+self.epn_name_in_list+"" (Please Wait)""
                    self.progressEpn.setFormat((t))
                    if MainWindow.isFullScreen() and self.layout_mode != ""Music"":
                        self.gridLayout.setSpacing(0)
                        self.frame1.show()
                        if self.frame_timer.isActive():
                            self.frame_timer.stop()
                        self.frame_timer.start(1000)
                if (""EOF code: 1"" in a or ""HTTP error 403 Forbidden"" in a):
                    self.mplayerLength = 0
                    self.total_file_size = 0
                    self.mpv_start = False
                    if self.final_playing_url in self.history_dict_obj:
                        asp = self.mpvplayer_aspect.get(str(self.mpvplayer_aspect_cycle))
                        self.history_dict_obj.update(
                                {
                                    self.final_playing_url:[
                                        0, time.time(), sub_id, audio_id,
                                        0, self.player_volume, asp
                                        ]
                                }
                            )
                    if self.player_setLoop_var:
                        t2 = bytes('\n'+""loadfile ""+(current_playing_file_path)+"" replace""+'\n', 'utf-8')
                        self.mpvplayer_val.write(t2)
                        return 0
                    else:
                        if not self.queue_url_list:
                            if self.list2.count() == 0:
                                return 0
                            if self.cur_row == self.list2.count() - 1:
                                self.cur_row = 0
                                if site == ""Music"" and not self.playerPlaylist_setLoop_var:
                                    r1 = self.list1.currentRow()
                                    it1 = self.list1.item(r1)
                                    if it1:
                                        if r1 < self.list1.count():
                                            r2 = r1+1
                                        else:
                                            r2 = 0
                                        self.list1.setCurrentRow(r2)
                                        self.listfound()
                            else:
                                self.cur_row += 1
                            self.list2.setCurrentRow(self.cur_row)
                        
                    if ""HTTP error 403 Forbidden"" in a:
                        print(a)
                        self.quit_really = ""yes""
                    if self.quit_really == ""no"" and not self.epn_wait_thread.isRunning():
                        if site in [""Video"", ""Music"", ""PlayLists"", ""None"", ""MyServer""]:
                            if self.queue_url_list:
                                if isinstance(self.queue_url_list[0], tuple):
                                    self.localGetInList(eofcode='end')
                                else:
                                    self.getQueueInList(eofcode='end')
                            else:
                                self.localGetInList(eofcode='end')
                        else:
                            if self.queue_url_list:
                                if isinstance(self.queue_url_list[0], tuple):
                                    self.getNextInList(eofcode='end')
                                else:
                                    self.getQueueInList(eofcode='end')
                            else:
                                self.getNextInList(eofcode='end')
                        if self.tab_5.isHidden() and thumbnail_indicator:
                            length_1 = self.list2.count()
                            q3=""self.label_epn_""+str(length_1+self.thumbnail_label_number[0])+"".setText((self.epn_name_in_list))""
                            exec (q3)
                            q3=""self.label_epn_""+str(length_1+self.thumbnail_label_number[0])+"".setAlignment(QtCore.Qt.AlignCenter)""
                            exec(q3)
                            QtWidgets.QApplication.processEvents()
                    elif self.quit_really == ""yes"":
                        self.player_stop.clicked_emit() 
                        self.list2.setFocus()
        except Exception as err:
            logger.error('{0}::dataready-exception'.format(err))",_58214.py,114,"for i in new_arr:
    if i.startswith('AUDIO_ID') or i.startswith('AUDIO_KEY_ID'):
        a_id = i.split('=')[-1]
        break","if a_id is not None:
    audio_s = re.search('[(][0-9]+[)]', a_id)
    if audio_s:
        audio_id = audio_s.group()
        audio_id = re.sub('\\)|\\(', '', audio_id).strip()
    else:
        audio_id = 'auto'
    self.audio_track.setText('A:' + str(a_id[:8]))
    if 'AUDIO_KEY_ID' in a:
        self.change_sid_aid_video(aid=audio_id)
else:
    logger.error('error getting proper a_id')","for i in new_arr:
    if i.startswith('AUDIO_ID') or i.startswith('AUDIO_KEY_ID'):
        a_id = i.split('=')[-1]
        audio_s = re.search('[(][0-9]+[)]', a_id)
        if audio_s:
            audio_id = audio_s.group()
            audio_id = re.sub('\\)|\\(', '', audio_id).strip()
        else:
            audio_id = 'auto'
        self.audio_track.setText('A:' + str(a_id[:8]))
        if 'AUDIO_KEY_ID' in a:
            self.change_sid_aid_video(aid=audio_id)
        break
else:
    logger.error('error getting proper a_id')"
https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/kawaii_player.py,"def dataReady(self, p):
        global new_epn, epn, opt, site
        global cache_empty, buffering_mplayer
        global artist_name_mplayer, server
        global pause_indicator
        global mpv_indicator
        global sub_id, audio_id, current_playing_file_path, desktop_session
        
        try:
            a = str(p.readAllStandardOutput(), 'utf-8').strip()
            #logger.debug('\n-->{0}<--\n'.format(a))
            if self.player_val in ['vlc', 'cvlc'] and ""status change:"" in a:
                self.mpvplayer_val.write(bytes(""get_length"", ""utf-8""))
            if self.player_val in ['vlc', 'cvlc'] and ""main playlist debug:"" in a:
                pls_item_search = re.search(""main playlist debug: using item (?P<row>\d+)\n"", a)
                if pls_item_search:
                    pls_item_number = int(pls_item_search.group(""row""))
                    if pls_item_number in range(0, self.list2.count()):
                        self.cur_row = pls_item_number
                        self.list2.setCurrentRow(self.cur_row)
                        current_item = self.list2.currentItem(self.cur_row)
                        text = current_item.text()
                        self.progressEpn.setFormat((text))
                        MainWindow.setWindowTitle(text)
                        self.epn_name_in_list = text
            elif 'volume' in a:
                logger.debug(a)
            elif 'Video' in a:
                logger.debug(a)
            elif 'Audio' in a:
                logger.info(a)
            if self.custom_mpv_input_conf and self.player_val.lower() == 'mpv':
                if 'set property: fullscreen' in a.lower():
                    logger.debug(a)
                    if (self.tab_5.width() >= screen_width
                            and self.tab_5.height() >= screen_height):
                        self.tab_5.player_fs()
                    else:
                        self.tab_5.player_fs(mode='fs')
                    a = 'FULLSCREEN_TOGGLE'
                elif 'playlist-next' in a.lower():
                    self.mpvNextEpnList()
                elif 'playlist-prev' in a.lower():
                    self.mpvPrevEpnList()
                elif 'set property: pause' in a.lower():
                    if self.mpv_custom_pause:
                        self.mpv_custom_pause = False
                        self.player_play_pause_status('play')
                    else:
                        self.mpv_custom_pause = True
                        self.player_play_pause_status('pause')
                elif 'Exiting... (Quit)' in a:
                    self.playerStop(msg='already quit')
                elif 'load_sub: load_external_subtitle' in a.lower():
                    if not self.acquire_subtitle_lock:
                        self.acquire_subtitle_lock = True
                        self.tab_5.load_external_sub()
                elif 'stop_cmd: stop_after_current_file' in a.lower():
                    self.quit_really = 'yes'
                    txt_osd = '\n show-text ""Stop After Current File"" \n'
                    self.mpvplayer_val.write(bytes(txt_osd, 'utf-8'))
                    logger.debug(a)
                elif 'set property: video-aspect=' in a.lower():
                    logger.debug(a)
                    aspect_val = a.split('video-aspect=')[1].split(' ')[0]
                    for asp_ratio in self.mpvplayer_aspect:
                        if aspect_val == self.mpvplayer_aspect[asp_ratio]:
                            self.mpvplayer_aspect_cycle = int(asp_ratio)
                            logger.info('SET ASPECT = {0}::{1}'.format(self.mpvplayer_aspect_cycle, aspect_val))
                            a = 'ASPECT CHANGED'
                            if aspect_val == '-1':
                                show_text_val = 'Original Aspect'
                            elif aspect_val == '0':
                                show_text_val = 'Aspect Ratio Disabled'
                            else:
                                show_text_val = aspect_val
                            txt_osd = '\n show-text ""{0}"" \n'.format(show_text_val)
                            self.mpvplayer_val.write(bytes(txt_osd, 'utf-8'))
                            break
                if a and 'AV:' not in a and 'A:' not in a:
                    #logger.debug('-->{0}<--'.format(a))
                    pass
            if (self.player_val.lower() == 'mpv' and self.mplayerLength
                    and (""EOF code: 1"" in a
                    or ""HTTP error 403 Forbidden"" in a 
                    or self.progress_counter > self.mplayerLength + 2
                    or 'finished playback, success (reason 0)' in a)):
                if not self.eof_reached and not self.player_setLoop_var:
                    self.eof_reached = True
                    self.eof_lock = True
                logger.debug('-- start --')
                logger.debug(a)
                logger.debug('{0}::{1}'.format(self.mplayerLength, self.progress_counter))
                logger.debug('-- end --')
            if 'icy info:' in a.lower() or 'icy-title:' in a.lower():
                if 'icy info:' in a.lower():
                    song_title = re.search(""'[^']*"", a)
                    self.epn_name_in_list = song_title.group().replace(""'"", '')
                else:
                    song_title = re.search(""icy-title:[^\n]*"", a)
                    self.epn_name_in_list = song_title.group().replace('icy-title:', '')
                print(self.epn_name_in_list, '--radio--song--')
                self.mplayerLength = 1
                self.epn_name_in_list = self.epn_name_in_list.strip()
                server._emitMeta('internet-radio#'+self.epn_name_in_list, site, self.epn_arr_list)
        except Exception as err:
            logger.error(err)
            a = """"
        try:
            if self.player_val.lower() == ""mpv"":
                if ""AUDIO_ID"" in a or ""AUDIO_KEY_ID"" in a:
                    new_arr = a.split('\n')
                    a_id = None
                    for i in new_arr:
                        if i.startswith('AUDIO_ID') or i.startswith('AUDIO_KEY_ID'):
                            a_id = i.split('=')[-1]
                            break
                    if a_id is not None:
                        audio_s = (re.search('[(][0-9]+[)]', a_id))
                        if audio_s:
                            audio_id = audio_s.group()
                            audio_id = re.sub('\)|\(', '', audio_id).strip()
                        else:
                            audio_id=""auto""
                        self.audio_track.setText(""A:""+str(a_id[:8]))
                        if 'AUDIO_KEY_ID' in a:
                            self.change_sid_aid_video(aid=audio_id)
                    else:
                        logger.error('error getting proper a_id')
                elif ""SUB_ID"" in a or ""SUB_KEY_ID"" in a:
                    tsid = sub_id
                    new_arr = a.split('\n')
                    s_id = None
                    for i in new_arr:
                        if i.startswith('SUB_ID') or i.startswith('SUB_KEY_ID'):
                            s_id = i.split('=')[-1]
                            break
                    if s_id is not None:
                        sub_s = (re.search('[(][^)]*', s_id))
                        if sub_s:
                            sub_id = (sub_s.group()).replace('(', '')
                        else:
                            sub_id = ""no""
                        if tsid == 'auto' and sub_id == 'no':
                            val_id = 'auto'
                            sub_id = 'auto'
                            self.mpvplayer_val.write(b'\n cycle sub \n')
                        else:
                            val_id = str(s_id[:8])
                        self.subtitle_track.setText(""Sub:""+val_id)
                        if 'SUB_KEY_ID' in a:
                            self.change_sid_aid_video(sid=sub_id)
                    else:
                        logger.error('error getting proper sub id')
                elif 'volume-print=' in a:
                    if 'ao-volume-print=' in a:
                        self.volume_type = 'ao-volume'
                    else:
                        self.volume_type = 'volume'
                    player_vol = re.search('volume-print=[0-9]+', a)
                    print(player_vol)
                    if player_vol:
                        player_vol = player_vol.group()
                        self.player_volume = player_vol.split('=')[1]
                        self.change_sid_aid_video(vol=self.player_volume)
                        logger.debug('set volume={0}'.format(self.player_volume))
                        if self.player_volume.isnumeric():
                            if self.volume_type == 'ao-volume':
                                msg_str = 'AO Volume'
                            else:
                                msg_str = 'Volume'
                            msg = '\n show-text ""{}: {}%"" \n'.format(msg_str, self.player_volume)
                            self.mpvplayer_val.write(bytes(msg, 'utf-8'))
                elif ""LENGTH_SECONDS="" in a:
                    mpl = re.search('LENGTH_SECONDS=[0-9]+[:][0-9]+[:][0-9]+', a)
                    if not mpl:
                        mpl = re.search('LENGTH_SECONDS=[0-9]+', a)
                    logger.debug('{} {}'.format(mpl, a))
                    if mpl:
                        mpl = mpl.group()
                        logger.debug(mpl)
                        if ':' in mpl:
                            n = mpl.split('=')[1]
                            o = n.split(':')
                            self.mplayerLength = int(o[0])*3600+int(o[1])*60+int(o[2])
                            self.mpv_playback_duration = n
                        else:
                            self.mplayerLength = int(mpl.split('=')[1])
                        self.slider.setRange(0, int(self.mplayerLength))
                elif (""AV:"" in a or ""A:"" in a or ""V:"" in a) and not self.eof_reached:
                    if not self.mpv_start:
                        self.mpv_start = True
                        try:
                            msg = 'Playing: {0}'.format(self.epn_name_in_list.replace('#', '', 1))
                            if self.epn_name_in_list:
                                MainWindow.setWindowTitle(self.epn_name_in_list)
                            if not self.float_window.isHidden():
                                self.float_window.setWindowTitle(self.epn_name_in_list)
                            msg = bytes('\n show-text ""{0}"" 2000 \n'.format(msg), 'utf-8')
                            self.mpvplayer_val.write(msg)
                        except Exception as err:
                            logger.error(err)
                        self.mplayer_finished_counter = 0
                        if (MainWindow.isFullScreen() and site != ""Music""
                                and self.list2.isHidden() and self.tab_6.isHidden()
                                and self.tab_2.isHidden()):
                            self.gridLayout.setSpacing(0)
                            if self.frame_timer.isActive():
                                self.frame_timer.stop()
                            if self.tab_6.isHidden():
                                self.frame_timer.start(5000)
                        self.subMplayer()
                    if (""Buffering"" in a and not mpv_indicator 
                            and (site != ""Local"" or site != ""Music"" 
                            or site != ""Video"")):
                        cache_empty = ""yes""
                        mpv_indicator.append(""cache empty"") 
                        print(""buffering"")
                        self.mpvplayer_val.write(b'\n set pause yes \n')
                        self.player_play_pause.setText(self.player_buttons['play'])
                        if not pause_indicator:
                            pause_indicator.append('Pause')
                        if MainWindow.isFullScreen() and self.layout_mode != ""Music"":
                            self.gridLayout.setSpacing(0)
                            self.frame1.show()
                    timearr = re.findall(""[0-9][0-9]+:[0-9][0-9]+:[0-9][0-9]+"", a)
                    percomp = re.search(""[(]*[0-9]*\%[)]*"", a)
                    if timearr:
                        val1 = timearr[0].split(':')
                        if val1:
                            val = int(val1[0])*3600+int(val1[1])*60+int(val1[2])
                        else:
                            val = 0
                        if len(timearr) == 1:
                            end_time = '00:00:00'
                            if self.mplayerLength > 1:
                                txt = self.progressEpn.text()
                                timearr = re.findall(""[0-9][0-9]+:[0-9][0-9]+:[0-9][0-9]+"", txt)
                                if timearr and len(timearr) == 2:
                                    end_time = timearr[1]
                                elif self.mpv_playback_duration:
                                    end_time = self.mpv_playback_duration
                            out = timearr[0] + ' / ' + end_time
                        else:
                            out = timearr[0] + ' / ' + timearr[1]
                        per_comp = '(0%)'
                        if percomp:
                            per_comp = percomp.group()
                            if not per_comp.endswith(')'):
                                per_comp = per_comp + ')'
                        else:
                            txt = self.progressEpn.text()
                            percomp = re.search(""[(]*[0-9]*\%[)]*"", txt)
                            if percomp:
                                per_comp = percomp.group()
                            elif self.mplayerLength > 1:
                                per_comp = '('+str(int(100*val/self.mplayerLength))+'%)'
                                
                        out = out + ' ' + per_comp
                            
                        cache_exists = False
                        if ""Cache:"" in a:
                            self.cache_mpv_indicator = True
                            cache_int = 0
                            n = re.findall(""Cache:[^+]*"", a)
                            if 's' in n[0]:
                                cache_val = re.search(""[0-9][^s]*"", n[0]).group()
                            else:
                                cache_val = self.cache_mpv_counter
                            
                            try:
                                cache_int = int(cache_val)
                            except Exception as err_val:
                                print(err_val)
                                cache_int = 0
                            
                            if cache_int >= 119:
                                cache_int = 119
                            elif cache_int >=9 and cache_int < 12:
                                cache_int = 10
                            if cache_int < 10:
                                cache_val = '0'+str(cache_int)
                            else:
                                cache_val = str(cache_int)
                            cache_exists = True
                            self.cache_mpv_counter = cache_val
                        else:
                            cache_val = '00'
                            cache_int = 0
                        try:
                            new_cache_val = cache_int
                        except Exception as e:
                            print(e, '--cache-val-error--')
                            new_cache_val = 0
                        if self.cache_mpv_indicator:
                            out = out +""  Cache:""+str(self.cache_mpv_counter)+'s'
                        if ""Paused"" in a and not mpv_indicator:
                            out = ""(Paused) ""+out
                            if self.custom_mpv_input_conf:
                                self.mpv_custom_pause = True
                        elif ""Paused"" in a and mpv_indicator:
                            out = ""(Paused Caching..) ""+out
                            if self.custom_mpv_input_conf:
                                self.mpv_custom_pause = True
                        
                        
                        if not self.mplayerLength:
                            if self.mpv_cnt > 4:
                                m = re.findall('[/][^(]*', out)
                                try:
                                    n = re.sub(' |[/]', '', m[0])
                                except Exception as err_msg:
                                    print(err_msg)
                                    n = '00:00:00'
                                print(n)
                                o = n.split(':')
                                self.mplayerLength = int(o[0])*3600+int(o[1])*60+int(o[2])
                                print(self.mplayerLength, ""--mpvlength"", a)
                                if self.mplayerLength == 0:
                                    if self.mpv_length_find_attempt >= 4:
                                        self.mplayerLength = 1
                                        self.mpv_length_find_attempt = 0
                                        logger.warning('No Suitable length detected')
                                        msg = '\n print-text ""LENGTH_SECONDS=${duration}"" \n'
                                        self.mpvplayer_val.write(bytes(msg, 'utf-8'))
                                    else:
                                        self.mpv_cnt = 0
                                        self.mpv_length_find_attempt += 1
                                        logger.warning(self.mpv_length_find_attempt)
                                        msg = '\n print-text ""LENGTH_SECONDS=${duration}"" \n'
                                        self.mpvplayer_val.write(bytes(msg, 'utf-8'))
                                self.mpv_playback_duration = n
                                self.progressEpn.setMaximum(int(self.mplayerLength))
                                self.slider.setRange(0, int(self.mplayerLength))
                                self.mpv_cnt = 0
                            logger.debug(self.mplayerLength)
                            self.mpv_cnt = self.mpv_cnt + 1
                            if (MainWindow.isFullScreen() and site != 'Music'
                                    and self.list2.isHidden() and self.tab_6.isHidden()
                                    and self.tab_2.isHidden()):
                                if not self.gapless_playback:
                                    self.gridLayout.setSpacing(0)
                                    self.frame1.show()
                                if self.frame_timer.isActive():
                                    self.frame_timer.stop()
                                if self.tab_6.isHidden():
                                    self.frame_timer.start(2000)
                        out1 = out
                        self.progressEpn.setFormat((out1))
                        if self.mplayerLength == 1:
                            val = 0
                            self.slider.setValue(0)
                        else:
                            self.slider.setValue(val)
                        if self.progress_counter == self.mplayerLength:
                            self.progress_counter += 1
                            logger.debug(self.progress_counter)
                        else:
                            self.progress_counter = val
                            if (self.gapless_network_stream and not self.queue_url_list
                                    and site in ['Music', 'PlayLists', 'None', 'NONE']): 
                                if self.progress_counter > int(self.mplayerLength/2):
                                    if (self.cur_row + 1) < self.list2.count():
                                        item_index = self.cur_row + 1
                                    else:
                                        item_index = 0
                                    if self.tmp_pls_file_dict.get(item_index) is False and self.list2.count() > 1:
                                        self.start_gapless_stream_process(item_index)
                        if not self.new_tray_widget.isHidden():
                            self.new_tray_widget.update_signal.emit(out, val)
                        if cache_empty == 'yes':
                            try:
                                if new_cache_val > self.cache_pause_seconds:
                                    cache_empty = 'no'
                                    self.mpvplayer_val.write(b'\n set pause no \n')
                                    self.player_play_pause.setText(self.player_buttons['pause'])
                                    if mpv_indicator:
                                        mpv_indicator.pop()
                                    if pause_indicator:
                                        pause_indicator.pop()
                                    if self.frame_timer.isActive():
                                        self.frame_timer.stop()
                                    self.frame_timer.start(100)
                            except Exception as err_val:
                                print(err_val, '--mpv--cache-error--')
                elif (""VO:"" in a or ""AO:"" in a or 'Stream opened successfully' in a) and not self.mplayerLength:
                    self.cache_mpv_indicator = False
                    self.cache_mpv_counter = '00'
                    self.mpv_playback_duration = 0
                    t = ""Loading: ""+self.epn_name_in_list+"" (Please Wait)""
                    self.progressEpn.setFormat((t))
                    self.eof_reached = False
                    self.eof_lock = False
                    QtCore.QTimer.singleShot(1000, partial(self.set_sub_audio_text, 'aid'))
                    QtCore.QTimer.singleShot(1500, partial(self.set_sub_audio_text, 'sid'))
                    if OSNAME == 'nt':
                        sub_spacing = self.subtitle_dict.get('sub-spacing')
                        if sub_spacing:
                            cmd = 'set sub-spacing {}'.format(sub_spacing)
                            QtCore.QTimer.singleShot(
                                2000, partial(self.mpv_execute_command, cmd, self.cur_row)
                            )
                    if self.gapless_network_stream:
                        if self.append_audio_start:
                            if self.append_audio_gapless:
                                if self.append_counter > 0:
                                    self.append_counter = 0
                                    self.mpv_execute_command('set aid 2', self.cur_row, timer=1000)
                elif (self.eof_reached and self.eof_lock 
                        and not self.epn_wait_thread.isRunning()):
                    if self.gapless_network_stream:
                        if self.append_audio_start:
                            if self.append_audio_gapless:
                                if self.append_counter > 0:
                                    self.mpv_execute_command('set aid 2', self.cur_row, timer=2000)
                                else:
                                    self.mpv_execute_command('set aid 2', self.cur_row)
                            else:
                                self.mpv_execute_command('set aid 1', self.cur_row)
                                self.append_counter += 1
                    self.eof_lock = False
                    self.eof_reached = False
                    if ""EOF code: 1"" in a:
                        reason_end = 'EOF code: 1'
                    else:
                        reason_end = 'length of file equals progress counter'
                    if self.final_playing_url in self.history_dict_obj:
                        param_avail = False
                        if self.video_parameters:
                            if self.final_playing_url == self.video_parameters[0]:
                                asp = self.video_parameters[-1]
                                vol = self.video_parameters[-2]
                                param_avail = True
                        if not param_avail:        
                            asp = self.mpvplayer_aspect.get(str(self.mpvplayer_aspect_cycle))
                            vol = self.player_volume
                        self.history_dict_obj.update(
                            {   self.final_playing_url:[
                                    0, time.time(), sub_id, audio_id,
                                    0, vol, asp
                                ]
                            }
                            )
                        logger.debug(self.video_parameters)
                    self.cache_mpv_indicator = False
                    self.cache_mpv_counter = '00'
                    self.mpv_playback_duration = 0
                    logger.debug('\ntrack no. {0} ended due to reason={1}\n::{2}'.format(self.cur_row, reason_end, a))
                    logger.debug('{0}::{1}'.format(self.mplayerLength, self.progress_counter))
                    queue_item = None
                    if self.queue_url_list:
                        queue_item = self.queue_url_list[0]
                    elif not self.queue_url_list and self.playlist_queue_used:
                        self.playlist_queue_used = False
                        pls_file = self.tmp_pls_file
                        if OSNAME == 'nt':
                            pls_file = 'file:///{}'.format(self.tmp_pls_file.replace('\\', '/'))
                        cmd = 'loadlist ""{}""'.format(pls_file)
                        self.mpv_execute_command(cmd, self.cur_row)
                        self.cur_row = -1
                    if self.player_setLoop_var:
                        pass
                    elif queue_item is None or isinstance(queue_item, tuple):
                        if self.list2.count() == 0:
                            return 0
                        if self.cur_row == self.list2.count() - 1:
                            self.cur_row = 0
                            if site == ""Music"" and not self.playerPlaylist_setLoop_var:
                                r1 = self.list1.currentRow()
                                it1 = self.list1.item(r1)
                                if it1:
                                    if r1 < self.list1.count():
                                        r2 = r1+1
                                    else:
                                        r2 = 0
                                    self.list1.setCurrentRow(r2)
                                    self.listfound()
                        else:
                            self.cur_row += 1
                        self.list2.setCurrentRow(self.cur_row)
                        logger.debug('\ncurR={0}\n'.format(self.cur_row))
                    self.mplayerLength = 0
                    self.total_file_size = 0
                    if self.mpv_start:
                        self.mpv_start = False
                    if ""HTTP error 403 Forbidden"" in a:
                        print(a)
                        self.quit_really = ""yes""
                    if self.playlist_continue and self.quit_really == ""no"" and not self.epn_wait_thread.isRunning():
                        if self.tab_5.isHidden() and thumbnail_indicator:
                            length_1 = self.list2.count()
                            q3=""self.label_epn_""+str(length_1+self.thumbnail_label_number[0])+"".setText(self.epn_name_in_list)""
                            exec(q3)
                            q3=""self.label_epn_""+str(length_1+self.thumbnail_label_number[0])+"".setAlignment(QtCore.Qt.AlignCenter)""
                            exec(q3)
                        if site in [""Video"", ""Music"", ""PlayLists"", ""None"", ""MyServer""]:
                            if queue_item is None or isinstance(queue_item, tuple):
                                move_ahead = True
                                if self.gapless_network_stream:
                                    if self.tmp_pls_file_dict.get(self.cur_row):
                                        if self.tmp_pls_file_lines[self.cur_row].startswith('http'):
                                            move_ahead = False
                                            self.final_playing_url = self.tmp_pls_file_lines[self.cur_row]
                                            tname = self.epn_arr_list[self.cur_row]
                                            if tname.startswith('#'):
                                                tname = tname.replace('#', '', 1)
                                            if '\t' in tname:
                                                self.epn_name_in_list = tname.split('\t')[0]
                                            else:
                                                self.epn_name_in_list = tname
                                            MainWindow.setWindowTitle(self.epn_name_in_list)
                                            self.float_window.setWindowTitle(self.epn_name_in_list)
                                            server._emitMeta(""Next"", site, self.epn_arr_list)
                                            if self.cur_row == 0 and 'master_abs_path=' in self.final_playing_url:
                                                move_ahead = True
                                            if self.pc_to_pc_casting == 'slave' and 'master_abs_path=' in self.final_playing_url:
                                                self.check_and_start_getsub_method()
                                        self.tmp_pls_file_dict.update({self.cur_row:False})
                                if move_ahead:
                                    self.localGetInList(eofcode='end')
                            else:
                                self.getQueueInList(eofcode='end')
                        else:
                            if queue_item is None or isinstance(queue_item, tuple):
                                self.getNextInList(eofcode='end')
                            else:
                                self.getQueueInList(eofcode='end')
                    elif (self.quit_really == ""yes"" or not self.playlist_continue): 
                        self.player_stop.clicked_emit()
                        self.list2.setFocus()
            elif self.player_val.lower() == ""mplayer"":
                if ""PAUSE"" in a:
                    if buffering_mplayer != 'yes':
                        self.player_play_pause.setText(self.player_buttons['play'])
                    if MainWindow.isFullScreen() and self.layout_mode != ""Music"":
                        self.gridLayout.setSpacing(0)
                        self.frame1.show()
                        if (buffering_mplayer == ""yes""):
                            if self.frame_timer.isActive:
                                self.frame_timer.stop()
                            self.frame_timer.start(10000)
                if ""Cache empty"" in a:
                    cache_empty = ""yes""
                    
                if ""ID_VIDEO_BITRATE"" in a:
                    try:
                        a0 = re.findall('ID_VIDEO_BITRATE=[^\n]*', a)
                        print(a0[0], '--videobit')
                        a1 = a0[0].replace('ID_VIDEO_BITRATE=', '')
                        self.id_video_bitrate=int(a1)
                    except:
                        self.id_video_bitrate = 0
                    
                if ""ID_AUDIO_BITRATE"" in a:
                    try:
                        a0 = re.findall('ID_AUDIO_BITRATE=[^\n]*', a)
                        print(a0[0], '--audiobit')
                        a1 = a0[0].replace('ID_AUDIO_BITRATE=', '')
                        self.id_audio_bitrate=int(a1)
                    except:
                        self.id_audio_bitrate=0
                if ""ANS_switch_audio"" in a:
                    print(a)
                    audio_id = a.split('=')[-1]
                    
                    print(""audio_id=""+audio_id)
                    self.audio_track.setText(""A:""+str(audio_id))
                if ""ANS_sub"" in a:
                    sub_id = a.split('=')[-1]
                    
                    print(""sub_id=""+sub_id)
                    self.subtitle_track.setText(""Sub:""+str(sub_id))
                
                if ""ID_LENGTH"" in a and not self.mplayerLength:
                    t = re.findall('ID_LENGTH=[0-9][^.]*', a)
                    self.mplayerLength = re.sub('ID_LENGTH=', '', t[0])
                    print(self.mplayerLength)
                    self.mplayerLength = int(self.mplayerLength) *1000
                    self.slider.setRange(0, int(self.mplayerLength))
                    self.total_file_size = int(((self.id_audio_bitrate+self.id_video_bitrate)*self.mplayerLength)/(8*1024*1024*1000))
                    print(self.total_file_size, ' MB')
                if (""A:"" in a) or (""PAUSE"" in a):
                    if not self.mpv_start:
                        self.mpv_start = True
                        try:
                            if self.tab_5.mplayer_aspect_msg:
                                aspect_val = self.mpvplayer_aspect.get(str(self.mpvplayer_aspect_cycle))
                                if aspect_val == '-1':
                                    show_text_val = 'Original Aspect'
                                elif aspect_val == '0':
                                    show_text_val = 'Aspect Ratio Disabled'
                                else:
                                    show_text_val = aspect_val
                                txt_osd = '\n osd_show_text ""{0}"" 2000\n'.format(show_text_val)
                                self.mpvplayer_val.write(bytes(txt_osd, 'utf-8'))
                                self.tab_5.mplayer_aspect_msg = False
                            else:
                                npn = '""'+""Playing: ""+self.epn_name_in_list.replace('#', '', 1)+'""'
                                npn1 = bytes('\n'+'osd_show_text '+str(npn)+' 4000'+'\n', 'utf-8')
                                MainWindow.setWindowTitle(self.epn_name_in_list)
                                if not self.float_window.isHidden():
                                    self.float_window.setWindowTitle(self.epn_name_in_list)
                                self.mpvplayer_val.write(npn1)
                            self.mplayer_finished_counter = 0
                        except:
                            pass
                        if (MainWindow.isFullScreen() and self.layout_mode != ""Music""
                                and self.list2.isHidden() and self.tab_2.isHidden()
                                and self.tab_6.isHidden()):
                            self.gridLayout.setSpacing(0)
                            if not self.frame1.isHidden():
                                self.frame1.hide()
                            if self.frame_timer.isActive():
                                self.frame_timer.stop()
                            self.frame_timer.start(1000)
                        self.subMplayer()
                    if ""PAUSE"" in a:
                        print(a, 'Pause A')
                        c = None
                        c_int = 0
                        if ""%"" in a:
                            m = a.split(' ')
                            print(m)
                            if m:
                                try:
                                    c = m[-1]
                                    if len(c) > 3:
                                        c = ""0%""
                                    c_int = int(c.replace('%', '')) 
                                except Exception as e:
                                    print(e, '--percent cache error--')
                        try:
                            t = str(self.progressEpn.text())
                            if c and c_int:
                                t = re.sub('Cache:[0-9]*%', '', t)
                            t = t.strip()
                            if '(Paused) ' in t:
                                t = t.replace('(Paused) ', '')
                            if '(Paused Caching..Wait) ' in t:
                                t = t.replace('(Paused Caching..Wait) ', '')
                        except:
                            t = """"
                        if buffering_mplayer == ""yes"" or self.mplayer_pause_buffer:
                            print('buffering mplayer')
                            if 'Cache:' not in t:
                                out = ""(Paused Caching..Wait) ""+t+' Cache:'+c
                            else:
                                out = ""(Paused Caching..Wait) ""+t
                            if ((not self.mplayer_timer.isActive()) 
                                    and (not self.video_local_stream) and c_int > 0):
                                self.mplayer_timer.start(1000)
                            elif ((not self.mplayer_timer.isActive()) 
                                    and (self.video_local_stream) and c_int > 5):
                                self.mplayer_timer.start(1000)
                        else:
                            if c_int and c:
                                out = ""(Paused) ""+t+' Cache:'+c
                            else:
                                out = ""(Paused) ""+t
                            
                            if ((not self.mplayer_timer.isActive()) 
                                    and (self.video_local_stream) and c_int > 5):
                                self.mplayer_timer.start(1000)
                    else:
                        if ""%"" in a:
                            m = a.split(' ')
                            try:
                                c = m[-2]
                            except:
                                c = ""0%""
                        else:
                            c = ""0%""
                    
                        t = re.findall('A:[^.]*', a)
                        l = re.sub('A:[^0-9]*', '', t[0])
                        l =int(l)*1000
                        
                        if self.mplayerLength == 1:
                            l = 0
                            self.slider.setValue(0)
                        else:
                            self.slider.setValue(l)
                        
                        if self.progress_counter == self.mplayerLength:
                            self.progress_counter += 1
                            logger.debug(self.progress_counter)
                        else:
                            self.progress_counter = l
                        
                        if site == ""Music"":
                            out_time = str(datetime.timedelta(milliseconds=int(l))) + "" / "" + str(datetime.timedelta(milliseconds=int(self.mplayerLength)))
                            
                            out = out_time + "" [""+self.epn_name_in_list+'('+artist_name_mplayer+')' +""]""
                        else:
                            out_time = str(datetime.timedelta(milliseconds=int(l))) + "" / "" + str(datetime.timedelta(milliseconds=int(self.mplayerLength)))
                            
                            out = out_time + "" [""+self.epn_name_in_list+""]"" +' Cache:'+c
                            
                        if not self.new_tray_widget.isHidden():
                            self.new_tray_widget.update_signal.emit(out_time, int(l))
                        if self.video_local_stream:
                            if c == '0%' and not self.mplayer_pause_buffer and not self.mplayer_nop_error_pause:
                                self.mpvplayer_val.write(b'\n pause \n')
                                self.mplayer_pause_buffer = True
                    if ((cache_empty == ""yes"" ) 
                            and (site != ""Local"" or site != ""Music"" or site != ""Video"")):
                        print('---nop--error--pausing---')
                        if not self.mplayer_pause_buffer:
                            self.mpvplayer_val.write(b'\n pause \n')
                            cache_empty = ""no""
                            buffering_mplayer = ""yes""
                    elif (('nop_streaming_read_error' in a) 
                            and (site != ""Local"" or site != ""Music"" or site != ""Video"")):
                        print('---nop--error--pausing---')
                        if not self.mplayer_pause_buffer:
                            self.mpvplayer_val.write(b'\n pause \n')
                            cache_empty = ""no""
                            buffering_mplayer = ""yes""
                            self.mplayer_nop_error_pause = True
                    if self.total_seek != 0:
                        r = ""Seeking ""+str(self.total_seek)+'s'
                        self.progressEpn.setFormat((r))
                    else:
                        self.progressEpn.setFormat((out))
                if 'http' in a:
                    t = ""Loading: ""+self.epn_name_in_list+"" (Please Wait)""
                    self.progressEpn.setFormat((t))
                    if MainWindow.isFullScreen() and self.layout_mode != ""Music"":
                        self.gridLayout.setSpacing(0)
                        self.frame1.show()
                        if self.frame_timer.isActive():
                            self.frame_timer.stop()
                        self.frame_timer.start(1000)
                if (""EOF code: 1"" in a or ""HTTP error 403 Forbidden"" in a):
                    self.mplayerLength = 0
                    self.total_file_size = 0
                    self.mpv_start = False
                    if self.final_playing_url in self.history_dict_obj:
                        asp = self.mpvplayer_aspect.get(str(self.mpvplayer_aspect_cycle))
                        self.history_dict_obj.update(
                                {
                                    self.final_playing_url:[
                                        0, time.time(), sub_id, audio_id,
                                        0, self.player_volume, asp
                                        ]
                                }
                            )
                    if self.player_setLoop_var:
                        t2 = bytes('\n'+""loadfile ""+(current_playing_file_path)+"" replace""+'\n', 'utf-8')
                        self.mpvplayer_val.write(t2)
                        return 0
                    else:
                        if not self.queue_url_list:
                            if self.list2.count() == 0:
                                return 0
                            if self.cur_row == self.list2.count() - 1:
                                self.cur_row = 0
                                if site == ""Music"" and not self.playerPlaylist_setLoop_var:
                                    r1 = self.list1.currentRow()
                                    it1 = self.list1.item(r1)
                                    if it1:
                                        if r1 < self.list1.count():
                                            r2 = r1+1
                                        else:
                                            r2 = 0
                                        self.list1.setCurrentRow(r2)
                                        self.listfound()
                            else:
                                self.cur_row += 1
                            self.list2.setCurrentRow(self.cur_row)
                        
                    if ""HTTP error 403 Forbidden"" in a:
                        print(a)
                        self.quit_really = ""yes""
                    if self.quit_really == ""no"" and not self.epn_wait_thread.isRunning():
                        if site in [""Video"", ""Music"", ""PlayLists"", ""None"", ""MyServer""]:
                            if self.queue_url_list:
                                if isinstance(self.queue_url_list[0], tuple):
                                    self.localGetInList(eofcode='end')
                                else:
                                    self.getQueueInList(eofcode='end')
                            else:
                                self.localGetInList(eofcode='end')
                        else:
                            if self.queue_url_list:
                                if isinstance(self.queue_url_list[0], tuple):
                                    self.getNextInList(eofcode='end')
                                else:
                                    self.getQueueInList(eofcode='end')
                            else:
                                self.getNextInList(eofcode='end')
                        if self.tab_5.isHidden() and thumbnail_indicator:
                            length_1 = self.list2.count()
                            q3=""self.label_epn_""+str(length_1+self.thumbnail_label_number[0])+"".setText((self.epn_name_in_list))""
                            exec (q3)
                            q3=""self.label_epn_""+str(length_1+self.thumbnail_label_number[0])+"".setAlignment(QtCore.Qt.AlignCenter)""
                            exec(q3)
                            QtWidgets.QApplication.processEvents()
                    elif self.quit_really == ""yes"":
                        self.player_stop.clicked_emit() 
                        self.list2.setFocus()
        except Exception as err:
            logger.error('{0}::dataready-exception'.format(err))",_58214.py,134,"for i in new_arr:
    if i.startswith('SUB_ID') or i.startswith('SUB_KEY_ID'):
        s_id = i.split('=')[-1]
        break","if s_id is not None:
    sub_s = re.search('[(][^)]*', s_id)
    if sub_s:
        sub_id = sub_s.group().replace('(', '')
    else:
        sub_id = 'no'
    if tsid == 'auto' and sub_id == 'no':
        val_id = 'auto'
        sub_id = 'auto'
        self.mpvplayer_val.write(b'\n cycle sub \n')
    else:
        val_id = str(s_id[:8])
    self.subtitle_track.setText('Sub:' + val_id)
    if 'SUB_KEY_ID' in a:
        self.change_sid_aid_video(sid=sub_id)
else:
    logger.error('error getting proper sub id')","for i in new_arr:
    if i.startswith('SUB_ID') or i.startswith('SUB_KEY_ID'):
        s_id = i.split('=')[-1]
        sub_s = re.search('[(][^)]*', s_id)
        if sub_s:
            sub_id = sub_s.group().replace('(', '')
        else:
            sub_id = 'no'
        if tsid == 'auto' and sub_id == 'no':
            val_id = 'auto'
            sub_id = 'auto'
            self.mpvplayer_val.write(b'\n cycle sub \n')
        else:
            val_id = str(s_id[:8])
        self.subtitle_track.setText('Sub:' + val_id)
        if 'SUB_KEY_ID' in a:
            self.change_sid_aid_video(sid=sub_id)
        break
else:
    logger.error('error getting proper sub id')"
https://github.com/scikit-tda/kepler-mapper/tree/master/kmapper/kmapper.py,"def map(
        self,
        lens,
        X=None,
        clusterer=None,
        cover=None,
        nerve=None,
        precomputed=False,
        remove_duplicate_nodes=False,
    ):
        """"""Apply Mapper algorithm on this projection and build a simplicial complex. Returns a dictionary with nodes and links.

        Parameters
        ----------
        lens: Numpy Array
            Lower dimensional representation of data. In general will be output of `fit_transform`.

        X: Numpy Array
            Original data or data to run clustering on. If `None`, then use `lens` as default. X can be a SciPy sparse matrix.

        clusterer: Default: DBSCAN
            Scikit-learn API compatible clustering algorithm. Must provide `fit` and `predict`.

        cover: kmapper.Cover
            Cover scheme for lens. Instance of kmapper.cover providing methods `fit` and `transform`.

        nerve: kmapper.Nerve
            Nerve builder implementing `__call__(nodes)` API

        precomputed : Boolean
            Tell Mapper whether the data that you are clustering on is a precomputed distance matrix. If set to
            `True`, the assumption is that you are also telling your `clusterer` that `metric='precomputed'` (which
            is an argument for DBSCAN among others), which
            will then cause the clusterer to expect a square distance matrix for each hypercube. `precomputed=True` will give a square matrix
            to the clusterer to fit on for each hypercube.

        remove_duplicate_nodes: Boolean
            Removes duplicate nodes before edges are determined. A node is considered to be duplicate
            if it has exactly the same set of points as another node.

        nr_cubes: Int

            .. deprecated:: 1.1.6

                define Cover explicitly in future versions

            The number of intervals/hypercubes to create. Default = 10.

        overlap_perc: Float
            .. deprecated:: 1.1.6

                define Cover explicitly in future versions

            The percentage of overlap ""between"" the intervals/hypercubes. Default = 0.1.



        Returns
        =======
        simplicial_complex : dict
            A dictionary with ""nodes"", ""links"" and ""meta"" information.

        Examples
        ========

        >>> # Default mapping.
        >>> graph = mapper.map(X_projected, X_inverse)

        >>> # Apply clustering on the projection instead of on inverse X
        >>> graph = mapper.map(X_projected)

        >>> # Use 20 cubes/intervals per projection dimension, with a 50% overlap
        >>> graph = mapper.map(X_projected, X_inverse,
        >>>                    cover=kmapper.Cover(n_cubes=20, perc_overlap=0.5))

        >>> # Use multiple different cubes/intervals per projection dimension,
        >>> # And vary the overlap
        >>> graph = mapper.map(X_projected, X_inverse,
        >>>                    cover=km.Cover(n_cubes=[10,20,5],
        >>>                                         perc_overlap=[0.1,0.2,0.5]))

        >>> # Use KMeans with 2 clusters
        >>> graph = mapper.map(X_projected, X_inverse,
        >>>     clusterer=sklearn.cluster.KMeans(2))

        >>> # Use DBSCAN with ""cosine""-distance
        >>> graph = mapper.map(X_projected, X_inverse,
        >>>     clusterer=sklearn.cluster.DBSCAN(metric=""cosine""))

        >>> # Use HDBSCAN as the clusterer
        >>> graph = mapper.map(X_projected, X_inverse,
        >>>     clusterer=hdbscan.HDBSCAN())

        >>> # Parametrize the nerve of the covering
        >>> graph = mapper.map(X_projected, X_inverse,
        >>>     nerve=km.GraphNerve(min_intersection=3))


        """"""

        start = datetime.now()

        clusterer = clusterer or cluster.DBSCAN(eps=0.5, min_samples=3)
        self.cover = cover or Cover(n_cubes=10, perc_overlap=0.1)
        nerve = nerve or GraphNerve()

        nodes = defaultdict(list)
        meta = defaultdict(list)
        graph = {}

        # If inverse image is not provided, we use the projection as the inverse image (suffer projection loss)
        if X is None:
            X = lens

        if self.verbose > 0:
            print(
                ""Mapping on data shaped %s using lens shaped %s\n""
                % (str(X.shape), str(lens.shape))
            )

        # Prefix'ing the data with an ID column
        ids = np.array([x for x in range(lens.shape[0])])
        lens = np.c_[ids, lens]
        if issparse(X):
            X = hstack([ids[np.newaxis].T, X], format=""csr"")
        else:
            X = np.c_[ids, X]

        # Cover scheme defines a list of elements
        bins = self.cover.fit(lens)

        # Algo's like K-Means, have a set number of clusters. We need this number
        # to adjust for the minimal number of samples inside an interval before
        # we consider clustering or skipping it.
        cluster_params = clusterer.get_params()

        min_cluster_samples = None
        for parameter in [""n_clusters"", ""min_cluster_size"", ""min_samples""]:
            value = cluster_params.get(parameter)
            if value and isinstance(value, int):
                min_cluster_samples = value
                break
        if not min_cluster_samples:
            min_cluster_samples = 2

        if self.verbose > 1:
            print(
                ""Minimal points in hypercube before clustering: {}"".format(
                    min_cluster_samples
                )
            )

        # Subdivide the projected data X in intervals/hypercubes with overlap
        if self.verbose > 0:
            bins = list(bins)  # extract list from generator
            total_bins = len(bins)
            print(""Creating %s hypercubes."" % total_bins)

        for i, hypercube in enumerate(self.cover.transform(lens)):

            # If at least min_cluster_samples samples inside the hypercube
            if hypercube.shape[0] >= min_cluster_samples:
                # Cluster the data point(s) in the cube, skipping the id-column
                # Note that we apply clustering on the inverse image (original data samples) that fall inside the cube.
                ids = [int(nn) for nn in hypercube[:, 0]]
                X_cube = X[ids]

                fit_data = X_cube[:, 1:]
                if precomputed:
                    fit_data = fit_data[:, ids]

                cluster_predictions = clusterer.fit_predict(fit_data)

                if self.verbose > 1:
                    print(
                        ""   > Found %s clusters in hypercube %s.""
                        % (
                            np.unique(
                                cluster_predictions[cluster_predictions > -1]
                            ).shape[0],
                            i,
                        )
                    )

                for pred in np.unique(cluster_predictions):
                    # if not predicted as noise
                    if pred != -1 and not np.isnan(pred):
                        cluster_id = ""cube{}_cluster{}"".format(i, int(pred))

                        nodes[cluster_id] = (
                            hypercube[:, 0][cluster_predictions == pred]
                            .astype(int)
                            .tolist()
                        )
            elif self.verbose > 1:
                print(""Cube_%s is empty.\n"" % (i))

        if remove_duplicate_nodes:
            nodes = self._remove_duplicate_nodes(nodes)

        links, simplices = nerve.compute(nodes)

        graph[""nodes""] = nodes
        graph[""links""] = links
        graph[""simplices""] = simplices
        graph[""meta_data""] = {
            ""projection"": self.projection if self.projection else ""custom"",
            ""n_cubes"": self.cover.n_cubes,
            ""perc_overlap"": self.cover.perc_overlap,
            ""clusterer"": str(clusterer),
            ""scaler"": str(self.scaler),
            ""nerve_min_intersection"": nerve.min_intersection
        }
        graph[""meta_nodes""] = meta

        if self.verbose > 0:
            self._summary(graph, str(datetime.now() - start))

        return graph",_60538.py,138,"for parameter in ['n_clusters', 'min_cluster_size', 'min_samples']:
    value = cluster_params.get(parameter)
    if value and isinstance(value, int):
        min_cluster_samples = value
        break","if not min_cluster_samples:
    min_cluster_samples = 2","for parameter in ['n_clusters', 'min_cluster_size', 'min_samples']:
    value = cluster_params.get(parameter)
    if value and isinstance(value, int):
        min_cluster_samples = value
        break
else:
    min_cluster_samples = 2"
https://github.com/jimmysong/programmingbitcoin/tree/master/code-ch13/op.py,"def op_if(stack, items):
    if len(stack) < 1:
        return False
    # go through and re-make the items array based on the top stack element
    true_items = []
    false_items = []
    current_array = true_items
    found = False
    num_endifs_needed = 1
    while len(items) > 0:
        item = items.pop(0)
        if item in (99, 100):
            # nested if, we have to go another endif
            num_endifs_needed += 1
            current_array.append(item)
        elif num_endifs_needed == 1 and item == 103:
            current_array = false_items
        elif item == 104:
            if num_endifs_needed == 1:
                found = True
                break
            else:
                num_endifs_needed -= 1
                current_array.append(item)
        else:
            current_array.append(item)
    if not found:
        return False
    element = stack.pop()
    if decode_num(element) == 0:
        items[:0] = false_items
    else:
        items[:0] = true_items
    return True",_61294.py,10,"while len(items) > 0:
    item = items.pop(0)
    if item in (99, 100):
        num_endifs_needed += 1
        current_array.append(item)
    elif num_endifs_needed == 1 and item == 103:
        current_array = false_items
    elif item == 104:
        if num_endifs_needed == 1:
            found = True
            break
        else:
            num_endifs_needed -= 1
            current_array.append(item)
    else:
        current_array.append(item)","if not found:
    return False","while len(items) > 0:
    item = items.pop(0)
    if item in (99, 100):
        num_endifs_needed += 1
        current_array.append(item)
    elif num_endifs_needed == 1 and item == 103:
        current_array = false_items
    elif item == 104:
        if num_endifs_needed == 1:
            break
        else:
            num_endifs_needed -= 1
            current_array.append(item)
    else:
        current_array.append(item)
else:
    return False"
https://github.com/chris-belcher/electrum-personal-server/tree/master/electrumpersonalserver/server/jsonrpc.py,"def call(self, method, params):
        currentId = self.queryId
        self.queryId += 1

        request = {""method"": method, ""params"": params, ""id"": currentId}
        #query can fail from keepalive timeout; keep retrying if it does, up
        #to a reasonable limit, then raise (failure to access blockchain
        #is a critical failure). Note that a real failure to connect (e.g.
        #wrong port) is raised in queryHTTP directly.
        response_received = False
        for i in range(100):
            response = self.queryHTTP(request)
            if response != ""CONNFAILURE"":
                response_received = True
                break
            #Failure means keepalive timed out, just make a new one
            self.conn = http.client.HTTPConnection(self.host, self.port)
        if not response_received:
            raise JsonRpcConnectionError(""Unable to connect over RPC"")
        if response[""id""] != currentId:
            raise JsonRpcConnectionError(""invalid id returned by query"")
        if response[""error""] is not None:
            raise JsonRpcError(response[""error""])
        return response[""result""]",_62271.py,11,"for i in range(100):
    response = self.queryHTTP(request)
    if response != 'CONNFAILURE':
        response_received = True
        break
    self.conn = http.client.HTTPConnection(self.host, self.port)","if not response_received:
    raise JsonRpcConnectionError('Unable to connect over RPC')","for i in range(100):
    response = self.queryHTTP(request)
    if response != 'CONNFAILURE':
        break
    self.conn = http.client.HTTPConnection(self.host, self.port)
else:
    raise JsonRpcConnectionError('Unable to connect over RPC')"
https://github.com/handcraftsman/GeneticAlgorithmsWithPython/tree/master/es/ch12/vendedor.py,"def intercambiar(genesDePadre, donanteGenes, fnObtenerAptitud):
    pares = {Par(donanteGenes[0], donanteGenes[-1]): 0}

    for i in range(len(donanteGenes) - 1):
        pares[Par(donanteGenes[i], donanteGenes[i + 1])] = 0

    genesTemporales = genesDePadre[:]
    if Par(genesDePadre[0], genesDePadre[-1]) in pares:
        # encontrar una discontinuidad
        encontr = False
        for i in range(len(genesDePadre) - 1):
            if Par(genesDePadre[i], genesDePadre[i + 1]) in pares:
                continue
            genesTemporales = genesDePadre[i + 1:] + genesDePadre[:i + 1]
            encontr = True
            break
        if not encontr:
            return None

    series = [[genesTemporales[0]]]
    for i in range(len(genesTemporales) - 1):
        if Par(genesTemporales[i], genesTemporales[i + 1]) in pares:
            series[-1].append(genesTemporales[i + 1])
            continue
        series.append([genesTemporales[i + 1]])

    aptitudInicial = fnObtenerAptitud(genesDePadre)
    cuenta = random.randint(2, 20)
    ndicesDeSerie = range(len(series))
    while cuenta > 0:
        cuenta -= 1
        for i in ndicesDeSerie:
            if len(series[i]) == 1:
                continue
            if random.randint(0, len(series)) == 0:
                series[i] = [n for n in reversed(series[i])]

        ndiceA, ndiceB = random.sample(ndicesDeSerie, 2)
        series[ndiceA], series[ndiceB] = series[ndiceB], series[ndiceA]
        genesDelNio = list(chain.from_iterable(series))
        if fnObtenerAptitud(genesDelNio) > aptitudInicial:
            return genesDelNio
    return genesDelNio",_63694.py,11,"for i in range(len(genesDePadre) - 1):
    if Par(genesDePadre[i], genesDePadre[i + 1]) in pares:
        continue
    genesTemporales = genesDePadre[i + 1:] + genesDePadre[:i + 1]
    encontr = True
    break","if not encontr:
    return None","for i in range(len(genesDePadre) - 1):
    if Par(genesDePadre[i], genesDePadre[i + 1]) in pares:
        continue
    genesTemporales = genesDePadre[i + 1:] + genesDePadre[:i + 1]
    break
else:
    return None"
https://github.com/uber-research/go-explore/tree/master/policy_based/goexplore_py/trajectory_trackers.py,"def step(self, current_cell, final_goal) -> [Any, float, bool]:
        reward = 0
        sub_goal_reached = False

        for i in range(self.trajectory_index, len(self.super_cell_trajectory)):
            if current_cell in self.super_cell_trajectory[i]:
                self.trajectory_index = i+1
                reward = 1
                sub_goal_reached = True
                break

        # We did not reach a new super-cell, continue towards the current sub-goal
        if reward == 0:
            pass
        # We reached a new super-cell, determine the next goal and return a reward
        elif self.trajectory_index < len(self.super_cell_trajectory):
            super_cell = self.super_cell_trajectory[self.trajectory_index]
            self.sub_goal = super_cell[-1]
        # We reached the final super-cell in our trajectory, set the final goal as our sub-goal and return a reward
        else:
            self.sub_goal = final_goal
        return self.sub_goal, reward, sub_goal_reached",_63960.py,5,"for i in range(self.trajectory_index, len(self.super_cell_trajectory)):
    if current_cell in self.super_cell_trajectory[i]:
        self.trajectory_index = i + 1
        reward = 1
        sub_goal_reached = True
        break","if reward == 0:
    pass
elif self.trajectory_index < len(self.super_cell_trajectory):
    super_cell = self.super_cell_trajectory[self.trajectory_index]
    self.sub_goal = super_cell[-1]
else:
    self.sub_goal = final_goal","for i in range(self.trajectory_index, len(self.super_cell_trajectory)):
    if current_cell in self.super_cell_trajectory[i]:
        self.trajectory_index = i + 1
        reward = 1
        sub_goal_reached = True
        if self.trajectory_index < len(self.super_cell_trajectory):
            super_cell = self.super_cell_trajectory[self.trajectory_index]
            self.sub_goal = super_cell[-1]
        else:
            self.sub_goal = final_goal
        break
else:
    pass"
https://github.com/ansible/ansible-modules-extras/tree/master/system/open_iscsi.py,"def main():

    # load ansible module object
    module = AnsibleModule(
        argument_spec = dict(

            # target
            portal = dict(required=False, aliases=['ip']),
            port = dict(required=False, default=3260),
            target = dict(required=False, aliases=['name', 'targetname']),
            node_auth = dict(required=False, default='CHAP'),
            node_user = dict(required=False),
            node_pass = dict(required=False),

            # actions
            login = dict(type='bool', aliases=['state']),
            auto_node_startup = dict(type='bool', aliases=['automatic']),
            discover = dict(type='bool', default=False),
            show_nodes = dict(type='bool', default=False)
        ),

        required_together=[['discover_user', 'discover_pass'],
                           ['node_user', 'node_pass']],
        supports_check_mode=True
    )

    global iscsiadm_cmd
    iscsiadm_cmd = module.get_bin_path('iscsiadm', required=True)

    # parameters
    portal = module.params['portal']
    target = module.params['target']
    port = module.params['port']
    login = module.params['login']
    automatic = module.params['auto_node_startup']
    discover = module.params['discover']
    show_nodes = module.params['show_nodes']

    check = module.check_mode

    cached = iscsi_get_cached_nodes(module, portal)

    # return json dict
    result = {}
    result['changed'] = False

    if discover:
        if portal is None:
            module.fail_json(msg = ""Need to specify at least the portal (ip) to discover"")
        elif check:
            nodes = cached
        else:
            iscsi_discover(module, portal, port)
            nodes = iscsi_get_cached_nodes(module, portal)
        if not compare_nodelists(cached, nodes):
            result['changed'] |= True
            result['cache_updated'] = True
    else:
        nodes = cached

    if login is not None or automatic is not None:
        if target is None:
            if len(nodes) > 1:
                module.fail_json(msg = ""Need to specify a target"")
            else:
                target = nodes[0]
        else:
            # check given target is in cache
            check_target = False
            for node in nodes:
                if node == target:
                    check_target = True
                    break
            if not check_target:
                module.fail_json(msg = ""Specified target not found"")

    if show_nodes:
        result['nodes'] = nodes

    if login is not None:
        loggedon = target_loggedon(module, target)
        if (login and loggedon) or (not login and not loggedon):
            result['changed'] |= False
            if login:
                result['devicenodes'] = target_device_node(module, target)
        elif not check:
            if login:
                target_login(module, target)
                # give udev some time
                time.sleep(1)
                result['devicenodes'] = target_device_node(module, target)
            else:
                target_logout(module, target)
            result['changed'] |= True
            result['connection_changed'] = True
        else:
            result['changed'] |= True
            result['connection_changed'] = True

    if automatic is not None:
        isauto = target_isauto(module, target)
        if (automatic and isauto) or (not automatic and not isauto):
            result['changed'] |= False
            result['automatic_changed'] = False
        elif not check:
            if automatic:
                target_setauto(module, target)
            else:
                target_setmanual(module, target)
            result['changed'] |= True
            result['automatic_changed'] = True
        else:
            result['changed'] |= True
            result['automatic_changed'] = True

    module.exit_json(**result)",_65351.py,70,"for node in nodes:
    if node == target:
        check_target = True
        break","if not check_target:
    module.fail_json(msg='Specified target not found')","for node in nodes:
    if node == target:
        break
else:
    module.fail_json(msg='Specified target not found')"
https://github.com/networkx/networkx/tree/master/networkx/algorithms/planarity.py,"def dfs_testing(self, v):
        """"""Test for LR partition.""""""
        # the recursion stack
        dfs_stack = [v]
        # index of next edge to handle in adjacency list of each node
        ind = defaultdict(lambda: 0)
        # boolean to indicate whether to skip the initial work for an edge
        skip_init = defaultdict(lambda: False)

        while dfs_stack:
            v = dfs_stack.pop()
            e = self.parent_edge[v]
            # to indicate whether to skip the final block after the for loop
            skip_final = False

            for w in self.ordered_adjs[v][ind[v] :]:
                ei = (v, w)

                if not skip_init[ei]:
                    self.stack_bottom[ei] = top_of_stack(self.S)

                    if ei == self.parent_edge[w]:  # tree edge
                        dfs_stack.append(v)  # revisit v after finishing w
                        dfs_stack.append(w)  # visit w next
                        skip_init[ei] = True  # don't redo this block
                        skip_final = True  # skip final work after breaking
                        break  # handle next node in dfs_stack (i.e. w)
                    else:  # back edge
                        self.lowpt_edge[ei] = ei
                        self.S.append(ConflictPair(right=Interval(ei, ei)))

                # integrate new return edges
                if self.lowpt[ei] < self.height[v]:
                    if w == self.ordered_adjs[v][0]:  # e_i has return edge
                        self.lowpt_edge[e] = self.lowpt_edge[ei]
                    else:  # add constraints of e_i
                        if not self.add_constraints(ei, e):
                            # graph is not planar
                            return False

                ind[v] += 1

            if not skip_final:
                # remove back edges returning to parent
                if e is not None:  # v isn't root
                    self.remove_back_edges(e)

        return True",_66109.py,16,"for w in self.ordered_adjs[v][ind[v]:]:
    ei = (v, w)
    if not skip_init[ei]:
        self.stack_bottom[ei] = top_of_stack(self.S)
        if ei == self.parent_edge[w]:
            dfs_stack.append(v)
            dfs_stack.append(w)
            skip_init[ei] = True
            skip_final = True
            break
        else:
            self.lowpt_edge[ei] = ei
            self.S.append(ConflictPair(right=Interval(ei, ei)))
    if self.lowpt[ei] < self.height[v]:
        if w == self.ordered_adjs[v][0]:
            self.lowpt_edge[e] = self.lowpt_edge[ei]
        elif not self.add_constraints(ei, e):
            return False
    ind[v] += 1","if not skip_final:
    if e is not None:
        self.remove_back_edges(e)","for w in self.ordered_adjs[v][ind[v]:]:
    ei = (v, w)
    if not skip_init[ei]:
        self.stack_bottom[ei] = top_of_stack(self.S)
        if ei == self.parent_edge[w]:
            dfs_stack.append(v)
            dfs_stack.append(w)
            skip_init[ei] = True
            break
        else:
            self.lowpt_edge[ei] = ei
            self.S.append(ConflictPair(right=Interval(ei, ei)))
    if self.lowpt[ei] < self.height[v]:
        if w == self.ordered_adjs[v][0]:
            self.lowpt_edge[e] = self.lowpt_edge[ei]
        elif not self.add_constraints(ei, e):
            return False
    ind[v] += 1
else:
    if e is not None:
        self.remove_back_edges(e)"
https://github.com/sphinx-doc/sphinx/tree/master/sphinx/ext/napoleon/docstring.py,"def _strip_empty(self, lines: List[str]) -> List[str]:
        if lines:
            start = -1
            for i, line in enumerate(lines):
                if line:
                    start = i
                    break
            if start == -1:
                lines = []
            end = -1
            for i in reversed(range(len(lines))):
                line = lines[i]
                if line:
                    end = i
                    break
            if start > 0 or end + 1 < len(lines):
                lines = lines[start:end + 1]
        return lines",_67575.py,4,"for (i, line) in enumerate(lines):
    if line:
        start = i
        break","if start == -1:
    lines = []","for (i, line) in enumerate(lines):
    if line:
        start = i
        break
else:
    lines = []"
https://github.com/spesmilo/electrum/tree/master/electrum/wallet.py,"def _bump_fee_through_decreasing_payment(
            self,
            *,
            tx: PartialTransaction,
            new_fee_rate: Union[int, Decimal],
    ) -> PartialTransaction:
        """"""Increase the miner fee of 'tx'.

        - keeps all inputs
        - no new inputs are added
        - decreases payment outputs (not change!). Each non-ismine output is decreased
          proportionally to their byte-size.
        """"""
        tx = copy.deepcopy(tx)
        tx.add_info_from_wallet(self)
        assert tx.get_fee() is not None
        inputs = tx.inputs()
        outputs = tx.outputs()

        # select non-ismine outputs
        s = [(idx, out) for (idx, out) in enumerate(outputs)
             if not self.is_mine(out.address)]
        s = [(idx, out) for (idx, out) in s if self._is_rbf_allowed_to_touch_tx_output(out)]
        if not s:
            raise CannotBumpFee(""Cannot find payment output"")

        del_out_idxs = set()
        tx_size = tx.estimated_size()
        cur_fee = tx.get_fee()
        # Main loop. Each iteration decreases value of all selected outputs.
        # The number of iterations is bounded by len(s) as only the final iteration
        # can *not remove* any output.
        for __ in range(len(s) + 1):
            target_fee = int(math.ceil(tx_size * new_fee_rate))
            delta_total = target_fee - cur_fee
            if delta_total <= 0:
                break
            out_size_total = sum(Transaction.estimated_output_size_for_script(out.scriptpubkey.hex())
                                 for (idx, out) in s if idx not in del_out_idxs)
            for idx, out in s:
                out_size = Transaction.estimated_output_size_for_script(out.scriptpubkey.hex())
                delta = int(math.ceil(delta_total * out_size / out_size_total))
                if out.value - delta >= self.dust_threshold():
                    new_output_value = out.value - delta
                    assert isinstance(new_output_value, int)
                    outputs[idx].value = new_output_value
                    cur_fee += delta
                else:  # remove output
                    tx_size -= out_size
                    cur_fee += out.value
                    del_out_idxs.add(idx)
        if delta_total > 0:
            raise CannotBumpFee(_('Could not find suitable outputs'))

        outputs = [out for (idx, out) in enumerate(outputs) if idx not in del_out_idxs]
        return PartialTransaction.from_io(inputs, outputs)",_71614.py,33,"for __ in range(len(s) + 1):
    target_fee = int(math.ceil(tx_size * new_fee_rate))
    delta_total = target_fee - cur_fee
    if delta_total <= 0:
        break
    out_size_total = sum((Transaction.estimated_output_size_for_script(out.scriptpubkey.hex()) for (idx, out) in s if idx not in del_out_idxs))
    for (idx, out) in s:
        out_size = Transaction.estimated_output_size_for_script(out.scriptpubkey.hex())
        delta = int(math.ceil(delta_total * out_size / out_size_total))
        if out.value - delta >= self.dust_threshold():
            new_output_value = out.value - delta
            assert isinstance(new_output_value, int)
            outputs[idx].value = new_output_value
            cur_fee += delta
        else:
            tx_size -= out_size
            cur_fee += out.value
            del_out_idxs.add(idx)","if delta_total > 0:
    raise CannotBumpFee(_('Could not find suitable outputs'))","for __ in range(len(s) + 1):
    target_fee = int(math.ceil(tx_size * new_fee_rate))
    delta_total = target_fee - cur_fee
    if delta_total <= 0:
        break
    out_size_total = sum((Transaction.estimated_output_size_for_script(out.scriptpubkey.hex()) for (idx, out) in s if idx not in del_out_idxs))
    for (idx, out) in s:
        out_size = Transaction.estimated_output_size_for_script(out.scriptpubkey.hex())
        delta = int(math.ceil(delta_total * out_size / out_size_total))
        if out.value - delta >= self.dust_threshold():
            new_output_value = out.value - delta
            assert isinstance(new_output_value, int)
            outputs[idx].value = new_output_value
            cur_fee += delta
        else:
            tx_size -= out_size
            cur_fee += out.value
            del_out_idxs.add(idx)
else:
    raise CannotBumpFee(_('Could not find suitable outputs'))"
https://github.com/waveform80/picamera/tree/master/picamera/mmalobj.py,"def __init__(self):
        super(MMALCameraInfo, self).__init__()
        if PARAM_TYPES[mmal.MMAL_PARAMETER_CAMERA_INFO] is None:
            found = False
            # try smallest structure to largest as later firmwares reject
            # older structures
            for struct in MMALCameraInfo.info_structs:
                try:
                    PARAM_TYPES[mmal.MMAL_PARAMETER_CAMERA_INFO] = struct
                    self.control.params[mmal.MMAL_PARAMETER_CAMERA_INFO]
                except PiCameraMMALError:
                    pass
                else:
                    found = True
                    break
            if not found:
                PARAM_TYPES[mmal.MMAL_PARAMETER_CAMERA_INFO] = None
                raise PiCameraMMALError(
                        mmal.MMAL_EINVAL, ""unknown camera info structure revision"")",_72398.py,7,"for struct in MMALCameraInfo.info_structs:
    try:
        PARAM_TYPES[mmal.MMAL_PARAMETER_CAMERA_INFO] = struct
        self.control.params[mmal.MMAL_PARAMETER_CAMERA_INFO]
    except PiCameraMMALError:
        pass
    else:
        found = True
        break","if not found:
    PARAM_TYPES[mmal.MMAL_PARAMETER_CAMERA_INFO] = None
    raise PiCameraMMALError(mmal.MMAL_EINVAL, 'unknown camera info structure revision')","for struct in MMALCameraInfo.info_structs:
    try:
        PARAM_TYPES[mmal.MMAL_PARAMETER_CAMERA_INFO] = struct
        self.control.params[mmal.MMAL_PARAMETER_CAMERA_INFO]
    except PiCameraMMALError:
        pass
    else:
        break
else:
    PARAM_TYPES[mmal.MMAL_PARAMETER_CAMERA_INFO] = None
    raise PiCameraMMALError(mmal.MMAL_EINVAL, 'unknown camera info structure revision')"
https://github.com/gcollazo/BrowserRefresh-Sublime/tree/master/win/pywinauto/controls/common_controls.py,"def Click(self, button = ""left"", double = False, where = ""text""):
        """"""Click on the treeview item

        where can be any one of ""text"", ""icon"", ""button""
        defaults to ""text""
        """"""

        # find the text rectangle for the item,
        point_to_click = self.Rectangle().mid_point()

        if where.lower() != ""text"":
            remote_mem = _RemoteMemoryBlock(self.tree_ctrl)

            point_to_click.x = self.Rectangle().left

            found = False
            while not found and point_to_click.x >= 0:

                hittest = win32structures.TVHITTESTINFO()
                hittest.pt = point_to_click
                hittest.hItem = self.elem

                remote_mem.Write(hittest)

                self.tree_ctrl.SendMessage(win32defines.TVM_HITTEST, 0, remote_mem)
                remote_mem.Read(hittest)

                if where.lower() == 'button' and \
                    hittest.flags == win32defines.TVHT_ONITEMBUTTON:
                    found = True
                    break

                if where.lower() == 'icon' and \
                    hittest.flags == win32defines.TVHT_ONITEMICON:
                    found = True
                    break

                point_to_click.x -= 1

            if not found:
                raise Exception(""Area ('%s') not found for this tree view item""% where)

        self.tree_ctrl.ClickInput(
            button,
            coords = (point_to_click.x, point_to_click.y),
            double = double)",_72409.py,17,"while not found and point_to_click.x >= 0:
    hittest = win32structures.TVHITTESTINFO()
    hittest.pt = point_to_click
    hittest.hItem = self.elem
    remote_mem.Write(hittest)
    self.tree_ctrl.SendMessage(win32defines.TVM_HITTEST, 0, remote_mem)
    remote_mem.Read(hittest)
    if where.lower() == 'button' and hittest.flags == win32defines.TVHT_ONITEMBUTTON:
        found = True
        break
    if where.lower() == 'icon' and hittest.flags == win32defines.TVHT_ONITEMICON:
        found = True
        break
    point_to_click.x -= 1","if not found:
    raise Exception(""Area ('%s') not found for this tree view item"" % where)","while not found and point_to_click.x >= 0:
    hittest = win32structures.TVHITTESTINFO()
    hittest.pt = point_to_click
    hittest.hItem = self.elem
    remote_mem.Write(hittest)
    self.tree_ctrl.SendMessage(win32defines.TVM_HITTEST, 0, remote_mem)
    remote_mem.Read(hittest)
    if where.lower() == 'button' and hittest.flags == win32defines.TVHT_ONITEMBUTTON:
        found = True
        break
    if where.lower() == 'icon' and hittest.flags == win32defines.TVHT_ONITEMICON:
        found = True
        break
    point_to_click.x -= 1
else:
    raise Exception(""Area ('%s') not found for this tree view item"" % where)"
https://github.com/google/clusterfuzz/tree/master/src/clusterfuzz/_internal/chrome/crash_uploader.py,"def get_crash_info_and_stacktrace(application_command_line, crash_stacktrace,
                                  gestures):
  """"""Return crash minidump location and updated crash stacktrace.""""""
  app_name_lower = environment.get_value('APP_NAME').lower()
  retry_limit = environment.get_value('FAIL_RETRIES')
  using_android = environment.is_android()
  using_chrome = 'chrome' in app_name_lower or 'chromium' in app_name_lower
  warmup_timeout = environment.get_value('WARMUP_TIMEOUT', 90)

  # Minidump generation is only applicable on Chrome application.
  # FIXME: Support minidump generation on platforms other than Android.
  if not using_chrome or not using_android:
    return None, crash_stacktrace

  # Get the crash info from stacktrace.
  crash_info = get_crash_info(crash_stacktrace)

  # If we lost the minidump file, we need to recreate it.
  # Note that because of the way crash_info is generated now, if we have a
  # non-None crash_info, we should also have its minidump path; we insert
  # the check to safeguard against possibly constructing the crash_info in
  # other ways in the future that might potentially lose the minidump path.
  if not crash_info or not crash_info.minidump_info.path:
    for _ in range(retry_limit):
      _, _, output = (
          process_handler.run_process(
              application_command_line,
              timeout=warmup_timeout,
              gestures=gestures))

      crash_info = get_crash_info(output)
      if crash_info and crash_info.minidump_info.path:
        crash_stacktrace = utils.decode_to_unicode(output)
        break

    if not crash_info or not crash_info.minidump_info.path:
      # We could not regenerate a minidump for this crash.
      logs.log('Unable to regenerate a minidump for this crash.')

  return crash_info, crash_stacktrace",_73100.py,24,"for _ in range(retry_limit):
    (_, _, output) = process_handler.run_process(application_command_line, timeout=warmup_timeout, gestures=gestures)
    crash_info = get_crash_info(output)
    if crash_info and crash_info.minidump_info.path:
        crash_stacktrace = utils.decode_to_unicode(output)
        break","if not crash_info or not crash_info.minidump_info.path:
    logs.log('Unable to regenerate a minidump for this crash.')","for _ in range(retry_limit):
    (_, _, output) = process_handler.run_process(application_command_line, timeout=warmup_timeout, gestures=gestures)
    crash_info = get_crash_info(output)
    if crash_info and crash_info.minidump_info.path:
        crash_stacktrace = utils.decode_to_unicode(output)
        break
else:
    logs.log('Unable to regenerate a minidump for this crash.')"
https://github.com/yuanxiaosc/Entity-Relation-Extraction/tree/master/bert/tokenization.py,"def tokenize(self, text):
    """"""Tokenizes a piece of text into its word pieces.

    This uses a greedy longest-match-first algorithm to perform tokenization
    using the given vocabulary.

    For example:
      input = ""unaffable""
      output = [""un"", ""##aff"", ""##able""]

    Args:
      text: A single token or whitespace separated tokens. This should have
        already been passed through `BasicTokenizer.

    Returns:
      A list of wordpiece tokens.
    """"""

    text = convert_to_unicode(text)

    output_tokens = []
    for token in whitespace_tokenize(text):
      chars = list(token)
      if len(chars) > self.max_input_chars_per_word:
        output_tokens.append(self.unk_token)
        continue

      is_bad = False
      start = 0
      sub_tokens = []
      while start < len(chars):
        end = len(chars)
        cur_substr = None
        while start < end:
          substr = """".join(chars[start:end])
          if start > 0:
            substr = ""##"" + substr
          if substr in self.vocab:
            cur_substr = substr
            break
          end -= 1
        if cur_substr is None:
          is_bad = True
          break
        sub_tokens.append(cur_substr)
        start = end

      if is_bad:
        output_tokens.append(self.unk_token)
      else:
        output_tokens.extend(sub_tokens)
    return output_tokens",_77192.py,31,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end","if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)"
https://github.com/yuanxiaosc/Entity-Relation-Extraction/tree/master/bert/tokenization.py,"def tokenize(self, text):
    """"""Tokenizes a piece of text into its word pieces.

    This uses a greedy longest-match-first algorithm to perform tokenization
    using the given vocabulary.

    For example:
      input = ""unaffable""
      output = [""un"", ""##aff"", ""##able""]

    Args:
      text: A single token or whitespace separated tokens. This should have
        already been passed through `BasicTokenizer.

    Returns:
      A list of wordpiece tokens.
    """"""

    text = convert_to_unicode(text)

    output_tokens = []
    for token in whitespace_tokenize(text):
      chars = list(token)
      if len(chars) > self.max_input_chars_per_word:
        output_tokens.append(self.unk_token)
        continue

      is_bad = False
      start = 0
      sub_tokens = []
      while start < len(chars):
        end = len(chars)
        cur_substr = None
        while start < end:
          substr = """".join(chars[start:end])
          if start > 0:
            substr = ""##"" + substr
          if substr in self.vocab:
            cur_substr = substr
            break
          end -= 1
        if cur_substr is None:
          is_bad = True
          break
        sub_tokens.append(cur_substr)
        start = end

      if is_bad:
        output_tokens.append(self.unk_token)
      else:
        output_tokens.extend(sub_tokens)
    return output_tokens",_77192.py,34,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1","if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break"
https://github.com/honeynet/droidbot/tree/master/droidbot/input_script.py,"def get_transformed_event(self, input_policy):
        device = input_policy.device
        event_dict = self.event_dict.copy()
        if 'target_view' in event_dict:
            target_view = event_dict.pop('target_view')
            target_view_selector = event_dict.pop('target_view_selector')

            state = device.get_last_known_state()
            if not state:
                state = device.get_current_state()
            if state:
                matched_view = None
                for view_dict in state.views:
                    if target_view_selector.match(view_dict):
                        matched_view = view_dict
                        break
                if matched_view is None:
                    device.logger.warning(""target_view no match: %s"" % target_view)
                else:
                    event_dict['view'] = matched_view
        if event_dict['event_type'] == 'spawn':
            event_dict['master'] = input_policy.master
        return InputEvent.from_dict(event_dict)",_78628.py,13,"for view_dict in state.views:
    if target_view_selector.match(view_dict):
        matched_view = view_dict
        break","if matched_view is None:
    device.logger.warning('target_view no match: %s' % target_view)
else:
    event_dict['view'] = matched_view","for view_dict in state.views:
    if target_view_selector.match(view_dict):
        matched_view = view_dict
        event_dict['view'] = matched_view
        break
else:
    device.logger.warning('target_view no match: %s' % target_view)"
https://github.com/openvinotoolkit/open_model_zoo/tree/master/tools/accuracy_checker/openvino/tools/accuracy_checker/metrics/detection.py,"def evaluate(self, annotations, predictions):
        true_positive = 0
        false_positive = 0

        for (annotation, prediction) in zip(annotations, predictions):
            for gt_idx in range(annotation.x_mins.size):
                gt_face = [
                    annotation.x_mins[gt_idx],
                    annotation.y_mins[gt_idx],
                    annotation.x_maxs[gt_idx],
                    annotation.y_maxs[gt_idx]
                ]
                found = False
                for i in range(prediction.scores.size):
                    dt_face = [
                        prediction.x_mins[i],
                        prediction.y_mins[i],
                        prediction.x_maxs[i],
                        prediction.y_maxs[i]
                    ]
                    iou = calc_iou(gt_face, dt_face)
                    if iou:
                        intersect_area, dt_area, gt_area = iou
                        if intersect_area / dt_area < self.overlap:
                            continue
                        if dt_area / gt_area >= self.relative_size:
                            found = True
                            break
                if found:
                    true_positive += 1
                else:
                    false_positive += 1
        accuracy = true_positive / (true_positive + false_positive)
        return accuracy",_79765.py,14,"for i in range(prediction.scores.size):
    dt_face = [prediction.x_mins[i], prediction.y_mins[i], prediction.x_maxs[i], prediction.y_maxs[i]]
    iou = calc_iou(gt_face, dt_face)
    if iou:
        (intersect_area, dt_area, gt_area) = iou
        if intersect_area / dt_area < self.overlap:
            continue
        if dt_area / gt_area >= self.relative_size:
            found = True
            break","if found:
    true_positive += 1
else:
    false_positive += 1","for i in range(prediction.scores.size):
    dt_face = [prediction.x_mins[i], prediction.y_mins[i], prediction.x_maxs[i], prediction.y_maxs[i]]
    iou = calc_iou(gt_face, dt_face)
    if iou:
        (intersect_area, dt_area, gt_area) = iou
        if intersect_area / dt_area < self.overlap:
            continue
        if dt_area / gt_area >= self.relative_size:
            true_positive += 1
            break
else:
    false_positive += 1"
https://github.com/cannatag/ldap3/tree/master/ldap3/abstract/entry.py,"def __getattr__(self, item):
        if isinstance(item, STRING_TYPES):
            if item == '_state':
                return object.__getattr__(self, item)
            item = ''.join(item.split()).lower()
            attr_found = None
            for attr in self._state.attributes.keys():
                if item == attr.lower():
                    attr_found = attr
                    break
            if not attr_found:
                for attr in self._state.attributes.aliases():
                    if item == attr.lower():
                        attr_found = attr
                        break
            if not attr_found:
                for attr in self._state.attributes.keys():
                    if item + ';binary' == attr.lower():
                        attr_found = attr
                        break
            if not attr_found:
                for attr in self._state.attributes.aliases():
                    if item + ';binary' == attr.lower():
                        attr_found = attr
                        break
            if not attr_found:
                for attr in self._state.attributes.keys():
                    if item + ';range' in attr.lower():
                        attr_found = attr
                        break
            if not attr_found:
                for attr in self._state.attributes.aliases():
                    if item + ';range' in attr.lower():
                        attr_found = attr
                        break
            if not attr_found:
                error_message = 'attribute \'%s\' not found' % item
                if log_enabled(ERROR):
                    log(ERROR, '%s for <%s>', error_message, self)
                raise LDAPCursorAttributeError(error_message)
            return self._state.attributes[attr]
        error_message = 'attribute name must be a string'
        if log_enabled(ERROR):
            log(ERROR, '%s for <%s>', error_message, self)
        raise LDAPCursorAttributeError(error_message)",_8235.py,7,"for attr in self._state.attributes.keys():
    if item == attr.lower():
        attr_found = attr
        break","if not attr_found:
    for attr in self._state.attributes.aliases():
        if item == attr.lower():
            attr_found = attr
            break","for attr in self._state.attributes.keys():
    if item == attr.lower():
        attr_found = attr
        break
else:
    for attr in self._state.attributes.aliases():
        if item == attr.lower():
            attr_found = attr
            break"
https://github.com/PaddlePaddle/PARL/tree/master/parl/core/fluid/agent.py,"def restore(self, save_path=None, program=None):
        """"""Restore previously saved parameters from save_path. 

        Args:
            save_path(str): path where parameters were previously saved.
            program(fluid.Program): program that describes the neural network structure. If None, will restore all program.

        Raises:
            Error: if save_path does not exist or can not find the specific program file in save_path.

        Example:

        .. code-block:: python

            agent = AtariAgent()
            agent.save('./model_dir')
            agent.restore('./model_dir')

        """"""
        assert save_path is not None, 'please specify `save_path` '
        if not os.path.exists(save_path):
            raise Exception(
                'can not restore from {}, directory does not exists'.format(
                    save_path))
        if os.path.isfile(save_path):
            raise Exception(
                'can not restore from {}, it is a file, not directory'.format(
                    save_path))
        all_programs = [(key, val) for (key, val) in self.__dict__.items()
                        if (isinstance(val, fluid.framework.Program)
                            or isinstance(val, fluid.compiler.CompiledProgram))
                        ]

        if program:
            filename = None
            for (name, prog) in all_programs:
                if program == prog:
                    filename = name
                    break
            if filename is None:
                raise Exception('can not find the program to restore.')
            if not os.path.isfile('{}/{}'.format(save_path, filename)):
                raise Exception('{}/{} does not exits'.format(
                    save_path, filename))
            if type(program) is fluid.compiler.CompiledProgram:
                program = program._init_program
            fluid.io.load_params(
                executor=self.fluid_executor,
                dirname=save_path,
                main_program=program,
                filename=filename)
        else:
            programs_list = [kv[0] for kv in all_programs]
            exist_files = os.listdir(save_path)
            if len(programs_list) != len(exist_files):
                raise Exception(
                    'expected to restore {} model file under directory {}: {}, but {} files are found: {}.'
                    .format(
                        len(programs_list), save_path, programs_list,
                        len(exist_files), exist_files))
            for (filename, program) in all_programs:
                if not os.path.isfile('{}/{}'.format(save_path, filename)):
                    raise Exception('{}/{} does not exits'.format(
                        save_path, filename))
                if type(program) is fluid.compiler.CompiledProgram:
                    program = program._init_program

                fluid.io.load_params(
                    executor=self.fluid_executor,
                    dirname=save_path,
                    main_program=program,
                    filename=filename)",_86526.py,36,"for (name, prog) in all_programs:
    if program == prog:
        filename = name
        break","if filename is None:
    raise Exception('can not find the program to restore.')","for (name, prog) in all_programs:
    if program == prog:
        filename = name
        break
else:
    raise Exception('can not find the program to restore.')"
https://github.com/jimmysong/programmingbitcoin/tree/master/code-ch05/op.py,"def op_notif(stack, items):
    if len(stack) < 1:
        return False
    # go through and re-make the items array based on the top stack element
    true_items = []
    false_items = []
    current_array = true_items
    found = False
    num_endifs_needed = 1
    while len(items) > 0:
        item = items.pop(0)
        if item in (99, 100):
            # nested if, we have to go another endif
            num_endifs_needed += 1
            current_array.append(item)
        elif num_endifs_needed == 1 and item == 103:
            current_array = false_items
        elif item == 104:
            if num_endifs_needed == 1:
                found = True
                break
            else:
                num_endifs_needed -= 1
                current_array.append(item)
        else:
            current_array.append(item)
    if not found:
        return False
    element = stack.pop()
    if decode_num(element) == 0:
        items[:0] = true_items
    else:
        items[:0] = false_items
    return True",_8656.py,10,"while len(items) > 0:
    item = items.pop(0)
    if item in (99, 100):
        num_endifs_needed += 1
        current_array.append(item)
    elif num_endifs_needed == 1 and item == 103:
        current_array = false_items
    elif item == 104:
        if num_endifs_needed == 1:
            found = True
            break
        else:
            num_endifs_needed -= 1
            current_array.append(item)
    else:
        current_array.append(item)","if not found:
    return False","while len(items) > 0:
    item = items.pop(0)
    if item in (99, 100):
        num_endifs_needed += 1
        current_array.append(item)
    elif num_endifs_needed == 1 and item == 103:
        current_array = false_items
    elif item == 104:
        if num_endifs_needed == 1:
            break
        else:
            num_endifs_needed -= 1
            current_array.append(item)
    else:
        current_array.append(item)
else:
    return False"
https://github.com/hangoutsbot/hangoutsbot/tree/master/hangupsbot/plugins/slackrtm/commands_hangouts.py,"def slack_disconnect(bot, event, *args):
    """"""stop syncing the current hangout with given slack team and channel

    usage: /bot slack_disconnect <teamname> <channelid>""""""

    if len(args) != 2:
        yield from bot.coro_send_message(event.conv_id, ""specify slack team and channel"")
        return

    slackname = args[0]
    slackrtm = None
    for s in _slackrtms:
        if s.name == slackname:
            slackrtm = s
            break
    if not slackrtm:
        yield from bot.coro_send_message(event.conv_id, ""there is no slack team with name **{}**, use _/bot slacks_ to list all teams"".format(slackname))
        return

    channelid = args[1]
    channelname = slackrtm.get_channelgroupname(channelid)
    if not channelname:
        yield from bot.coro_send_message(
            event.conv_id,
            ""there is no channel with name **{0}** in **{1}**, use _/bot slack_channels {1}_ to list all channels"".format(channelid, slackname) )
        return

    try:
        slackrtm.config_disconnect(channelid, event.conv.id_)
    except NotSyncingError:
        yield from bot.coro_send_message(event.conv_id, ""current hangout not previously synced with {} : {}"".format(slackname, channelname))
        return

    yield from bot.coro_send_message(event.conv_id, ""this hangout disconnected from {} : {}"".format(slackname, channelname))",_86645.py,12,"for s in _slackrtms:
    if s.name == slackname:
        slackrtm = s
        break","if not slackrtm:
    yield from bot.coro_send_message(event.conv_id, 'there is no slack team with name **{}**, use _/bot slacks_ to list all teams'.format(slackname))
    return","for s in _slackrtms:
    if s.name == slackname:
        slackrtm = s
        break
else:
    yield from bot.coro_send_message(event.conv_id, 'there is no slack team with name **{}**, use _/bot slacks_ to list all teams'.format(slackname))
    return"
https://github.com/armory3d/armory/tree/master/blender/arm/props.py,"def set_project_bundle(self, value):
    value = arm.utils.safestr(value)
    v_a = value.strip().split('.')
    if (len(value) > 0) and (not value.isdigit()) and (not value[0].isdigit()) and (len(v_a) > 1):
        check = True
        for item in v_a:
            if (item.isdigit()) or (item[0].isdigit()):
                check = False
                break
        if check:
            self['arm_project_bundle'] = value",_88045.py,6,"for item in v_a:
    if item.isdigit() or item[0].isdigit():
        check = False
        break","if check:
    self['arm_project_bundle'] = value","for item in v_a:
    if item.isdigit() or item[0].isdigit():
        break
else:
    self['arm_project_bundle'] = value"
https://github.com/rockstor/rockstor-core/tree/master/src/rockstor/system/samba.py,"def refresh_smb_config(exports):
    fh, npath = mkstemp()
    with open(SMB_CONFIG) as sfo, open(npath, ""w"") as tfo:
        rockstor_section = False
        for line in sfo.readlines():
            if re.match(RS_SHARES_HEADER, line) is not None:
                rockstor_section = True
                rockstor_smb_config(tfo, exports)
                break
            else:
                tfo.write(line)
        if rockstor_section is False:
            rockstor_smb_config(tfo, exports)
    test_parm(npath)
    shutil.move(npath, SMB_CONFIG)",_8918.py,5,"for line in sfo.readlines():
    if re.match(RS_SHARES_HEADER, line) is not None:
        rockstor_section = True
        rockstor_smb_config(tfo, exports)
        break
    else:
        tfo.write(line)","if rockstor_section is False:
    rockstor_smb_config(tfo, exports)","for line in sfo.readlines():
    if re.match(RS_SHARES_HEADER, line) is not None:
        rockstor_smb_config(tfo, exports)
        break
    else:
        tfo.write(line)
else:
    rockstor_smb_config(tfo, exports)"
https://github.com/saltstack/salt/tree/master/tests/support/pytest/mysql.py,"def mysql_container(request, salt_factories, salt_call_cli):

    try:
        docker_client = docker.from_env()
    except docker_errors.DockerException:
        pytest.skip(""Failed to get a connection to docker running on the system"")
    connectable = Container.client_connectable(docker_client)
    if connectable is not True:  # pragma: no cover
        pytest.skip(connectable)

    mysql_image = request.param

    mysql_user = ""root""
    mysql_passwd = ""password""

    combo = MySQLCombo(
        mysql_name=mysql_image.name,
        mysql_version=mysql_image.tag,
        mysql_user=mysql_user,
        mysql_passwd=mysql_passwd,
    )
    container = salt_factories.get_container(
        mysql_image.container_id,
        ""{}:{}"".format(combo.mysql_name, combo.mysql_version),
        docker_client=docker_client,
        check_ports=[combo.mysql_port],
        container_run_kwargs={
            ""ports"": {""3306/tcp"": combo.mysql_port},
            ""environment"": {
                ""MYSQL_ROOT_PASSWORD"": mysql_passwd,
                ""MYSQL_ROOT_HOST"": ""%"",
            },
        },
    )
    with container.started():
        authenticated = False
        login_attempts = 6
        while login_attempts:
            login_attempts -= 1
            # Make sure ""MYSQL"" is ready
            ret = salt_call_cli.run(
                ""docker.run"",
                name=mysql_image.container_id,
                cmd=""mysql --user=root --password=password -e 'SELECT 1'"",
            )
            authenticated = ret.exitcode == 0
            if authenticated:
                break

            time.sleep(2)

        if authenticated:
            yield combo
        else:
            pytest.fail(
                ""Failed to login into mysql server running in container(id: {})"".format(
                    mysql_image.container_id
                )
            )",_89695.py,38,"while login_attempts:
    login_attempts -= 1
    ret = salt_call_cli.run('docker.run', name=mysql_image.container_id, cmd=""mysql --user=root --password=password -e 'SELECT 1'"")
    authenticated = ret.exitcode == 0
    if authenticated:
        break
    time.sleep(2)","if authenticated:
    yield combo
else:
    pytest.fail('Failed to login into mysql server running in container(id: {})'.format(mysql_image.container_id))","while login_attempts:
    login_attempts -= 1
    ret = salt_call_cli.run('docker.run', name=mysql_image.container_id, cmd=""mysql --user=root --password=password -e 'SELECT 1'"")
    authenticated = ret.exitcode == 0
    if authenticated:
        yield combo
        break
    time.sleep(2)
else:
    pytest.fail('Failed to login into mysql server running in container(id: {})'.format(mysql_image.container_id))"
https://github.com/freeipa/freeipa/tree/master/ipaserver/plugins/aci.py,"def _aci_to_kw(ldap, a, test=False, pkey_only=False):
    """"""Convert an ACI into its equivalent keywords.

       This is used for the modify operation so we can merge the
       incoming kw and existing ACI and pass the result to
       _make_aci().
    """"""
    kw = {}
    kw['aciprefix'], kw['aciname'] = _parse_aci_name(a.name)
    if pkey_only:
        return kw
    kw['permissions'] = tuple(a.permissions)
    if 'targetattr' in a.target:
        kw['attrs'] = tuple(unicode(e)
                            for e in a.target['targetattr']['expression'])
    if 'targetfilter' in a.target:
        target = a.target['targetfilter']['expression']
        if target.startswith('(memberOf=') or target.startswith('memberOf='):
            _junk, memberof = target.split('memberOf=', 1)
            memberof = DN(memberof)
            kw['memberof'] = memberof['cn']
        else:
            kw['filter'] = unicode(target)
    if 'target' in a.target:
        target = a.target['target']['expression']
        found = False
        for k, value in _type_map.items():
            if value == target:
                kw['type'] = unicode(k)
                found = True
                break
        if not found:
            if target.startswith('('):
                kw['filter'] = unicode(target)
            else:
                # See if the target is a group. If so we set the
                # targetgroup attr, otherwise we consider it a subtree
                try:
                    targetdn = DN(target.replace('ldap:///',''))
                except ValueError as e:
                    raise errors.ValidationError(
                        name='subtree', error=_(""invalid DN (%s)"") % e)
                if targetdn.endswith(DN(api.env.container_group, api.env.basedn)):
                    kw['targetgroup'] = targetdn[0]['cn']
                else:
                    kw['subtree'] = unicode(target)

    groupdn = a.bindrule['expression']
    groupdn = groupdn.replace('ldap:///','')
    if groupdn == 'self':
        kw['selfaci'] = True
    elif groupdn == 'anyone':
        pass
    else:
        groupdn = DN(groupdn)
        if len(groupdn) and groupdn[0].attr == 'cn':
            dn = DN()
            entry = ldap.make_entry(dn)
            try:
                entry = ldap.get_entry(groupdn, ['cn'])
            except errors.NotFound:
                # FIXME, use real name here
                if test:
                    dn = DN(('cn', 'test'), api.env.container_permission,
                            api.env.basedn)
                    entry = ldap.make_entry(dn, {'cn': [u'test']})
            if api.env.container_permission in entry.dn:
                kw['permission'] = entry['cn'][0]
            else:
                if 'cn' in entry:
                    kw['group'] = entry['cn'][0]

    return kw",_91617.py,27,"for (k, value) in _type_map.items():
    if value == target:
        kw['type'] = unicode(k)
        found = True
        break","if not found:
    if target.startswith('('):
        kw['filter'] = unicode(target)
    else:
        try:
            targetdn = DN(target.replace('ldap:///', ''))
        except ValueError as e:
            raise errors.ValidationError(name='subtree', error=_('invalid DN (%s)') % e)
        if targetdn.endswith(DN(api.env.container_group, api.env.basedn)):
            kw['targetgroup'] = targetdn[0]['cn']
        else:
            kw['subtree'] = unicode(target)","for (k, value) in _type_map.items():
    if value == target:
        kw['type'] = unicode(k)
        break
else:
    if target.startswith('('):
        kw['filter'] = unicode(target)
    else:
        try:
            targetdn = DN(target.replace('ldap:///', ''))
        except ValueError as e:
            raise errors.ValidationError(name='subtree', error=_('invalid DN (%s)') % e)
        if targetdn.endswith(DN(api.env.container_group, api.env.basedn)):
            kw['targetgroup'] = targetdn[0]['cn']
        else:
            kw['subtree'] = unicode(target)"
https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/util/random_map.py,"def generate_random_map(seed, map_size, water_percent, max_island_size,
                        preferred_island_size, island_size_deviation):
	""""""
	Generates a random map.

	@param seed: random number generator seed
	@param map_size: maximum map side length
	@param water_percent: minimum percent of map covered with water
	@param max_island_size: maximum island side length
	@param preferred_island_size: mean of island side lengths
	@param island_size_deviation: deviation of island side lengths
	@return: filename of the SQLite database containing the map
	""""""
	max_island_size = min(max_island_size, map_size)
	rand = random.Random(_simplify_seed(seed))
	min_island_size = 20 # minimum chosen island side length (the real size my be smaller)
	min_island_separation = 3 + map_size // 100 # minimum distance between two islands
	max_island_side_coefficient = 4 # maximum value of island's max(side length) / min(side length)

	islands = [] # type: List[Rect]
	estimated_land = 0
	max_land_amount = map_size * map_size * (100 - water_percent) // 100

	trial_number = 0
	while trial_number < 100:
		trial_number += 1
		width = max(min_island_size, min(max_island_size, int(round(rand.gauss(preferred_island_size, island_size_deviation)))))
		side_coefficient = min(1 + abs(rand.gauss(0, 0.2)), max_island_side_coefficient)
		side_coefficient = side_coefficient if rand.randint(0, 1) else 1.0 / side_coefficient
		height = max(min_island_size, min(max_island_size, int(round(width * side_coefficient))))
		size = width * height
		if estimated_land + size > max_land_amount:
			continue

		for _ in range(13):
			x = rand.randint(0, map_size - width)
			y = rand.randint(0, map_size - height)

			rect = Rect.init_from_topleft_and_size(x, y, width, height)
			blocked = False
			for existing_island in islands:
				if existing_island.distance(rect) < min_island_separation:
					blocked = True
					break
			if not blocked:
				islands.append(rect)
				estimated_land += size
				trial_number = 0
				break

	# move some of the islands to stretch the map to the right size
	if len(islands) > 1:
		min_top = min(rect.top for rect in islands)
		rect = rand.choice([rect for rect in islands if rect.top == min_top])
		islands[islands.index(rect)] = Rect.init_from_borders(rect.left, rect.top - min_top, rect.right, rect.bottom - min_top)

		max_bottom = max(rect.bottom for rect in islands)
		rect = rand.choice([rect for rect in islands if rect.bottom == max_bottom])
		shift = map_size - max_bottom - 1
		islands[islands.index(rect)] = Rect.init_from_borders(rect.left, rect.top + shift, rect.right, rect.bottom + shift)

		min_left = min(rect.left for rect in islands)
		rect = rand.choice([rect for rect in islands if rect.left == min_left])
		islands[islands.index(rect)] = Rect.init_from_borders(rect.left - min_left, rect.top, rect.right - min_left, rect.bottom)

		max_right = max(rect.right for rect in islands)
		rect = rand.choice([rect for rect in islands if rect.right == max_right])
		shift = map_size - max_right - 1
		islands[islands.index(rect)] = Rect.init_from_borders(rect.left + shift, rect.top, rect.right + shift, rect.bottom)

	island_strings = []
	for rect in islands:
		# The bounds must be platform independent to make sure the same maps are generated on all platforms.
		island_seed = rand.randint(-2147483648, 2147483647)
		island_params = {'creation_method': 2, 'seed': island_seed, 'width': rect.width,
						 'height': rect.height, 'island_x': rect.left, 'island_y': rect.top}
		island_string = string.Template(_random_island_id_template).safe_substitute(island_params)
		island_strings.append(island_string)
	return island_strings",_91892.py,41,"for existing_island in islands:
    if existing_island.distance(rect) < min_island_separation:
        blocked = True
        break","if not blocked:
    islands.append(rect)
    estimated_land += size
    trial_number = 0
    break","for existing_island in islands:
    if existing_island.distance(rect) < min_island_separation:
        break
else:
    islands.append(rect)
    estimated_land += size
    trial_number = 0
    break"
https://github.com/nvaccess/nvda/tree/master/source/NVDAObjects/window/excel.py,"def script_openDropdown(self,gesture):
		gesture.send()
		d=None
		curTime=startTime=time.time()
		while (curTime-startTime)<=0.25:
			if scriptHandler.isScriptWaiting():
				# Prevent lag if keys are pressed rapidly
				return
			if eventHandler.isPendingEvents('gainFocus'):
				return
			d=self._getDropdown()
			if d:
				break
			api.processPendingEvents(processEventQueue=False)
			time.sleep(0.025)
			curTime=time.time()
		if not d:
			log.debugWarning(""Failed to get dropDown, giving up"")
			return
		d.parent=self
		eventHandler.queueEvent(""gainFocus"",d)",_94772.py,5,"while curTime - startTime <= 0.25:
    if scriptHandler.isScriptWaiting():
        return
    if eventHandler.isPendingEvents('gainFocus'):
        return
    d = self._getDropdown()
    if d:
        break
    api.processPendingEvents(processEventQueue=False)
    time.sleep(0.025)
    curTime = time.time()","if not d:
    log.debugWarning('Failed to get dropDown, giving up')
    return","while curTime - startTime <= 0.25:
    if scriptHandler.isScriptWaiting():
        return
    if eventHandler.isPendingEvents('gainFocus'):
        return
    d = self._getDropdown()
    if d:
        break
    api.processPendingEvents(processEventQueue=False)
    time.sleep(0.025)
    curTime = time.time()
else:
    log.debugWarning('Failed to get dropDown, giving up')
    return"
https://github.com/dagster-io/dagster/tree/master/python_modules/automation/automation/graphql/python_client/query.py,"def check():
    """"""This command checks whether backcompatability test setup is completed for the Dagster Python GraphQL Client.

    It checks if the current versions of queries in use by the GraphQL client are present in the dehydrated
    GraphQL backcompatability directory in `dagster_graphql_tests.graphql.client_backcompat.queries`

    This is useful as a reminder when new queries or added or changes are made to existing queries in
    use by the client.
    """"""
    current_queries_dict = get_queries()
    legacy_query_info = LegacyQueryHistoryInfo.get()

    query_directories_present = {
        query_name: query_name in legacy_query_info.legacy_queries
        for query_name in current_queries_dict
    }
    missing_query_history_subdirs = [
        query_name
        for (query_name, query_present) in query_directories_present.items()
        if not query_present
    ]
    if missing_query_history_subdirs:
        raise Exception(
            ""Missing some query history (sub)directories:""
            f""\n\t{missing_query_history_subdirs}""
            + f""\n\t at {legacy_query_info.directory}""
            + ""\n\t Please run `dagster-graphql-client query snapshot` on the command line ""
            + ""or manually resolve these issues""
        )
    for query_name in query_directories_present:
        query_dir = os.path.join(legacy_query_info.directory, query_name)
        query_is_present = False
        for filename in os.listdir(query_dir):
            file_path = os.path.join(query_dir, filename)
            with open(file_path, ""r"", encoding=""utf8"") as f:
                old_query = f.read()
                if are_queries_compatible(old_query, current_queries_dict[query_name]):
                    query_is_present = True
                    break
        if not query_is_present:
            raise Exception(
                f""The query dagster_graphql.client.client_queries.{query_name} ""
                + ""is not present in the backcompatability history ""
                + f""directory {legacy_query_info.directory} ""
                + ""\n\tPlease run `dagster-graphql-client query snapshot` on the command line ""
                + ""or manually resolve these issues""
            )
    click.echo(""All GraphQL Python Client backcompatability checks complete!"")",_95403.py,33,"for filename in os.listdir(query_dir):
    file_path = os.path.join(query_dir, filename)
    with open(file_path, 'r', encoding='utf8') as f:
        old_query = f.read()
        if are_queries_compatible(old_query, current_queries_dict[query_name]):
            query_is_present = True
            break","if not query_is_present:
    raise Exception(f'The query dagster_graphql.client.client_queries.{query_name} ' + 'is not present in the backcompatability history ' + f'directory {legacy_query_info.directory} ' + '\n\tPlease run `dagster-graphql-client query snapshot` on the command line ' + 'or manually resolve these issues')","for filename in os.listdir(query_dir):
    file_path = os.path.join(query_dir, filename)
    with open(file_path, 'r', encoding='utf8') as f:
        old_query = f.read()
        if are_queries_compatible(old_query, current_queries_dict[query_name]):
            break
else:
    raise Exception(f'The query dagster_graphql.client.client_queries.{query_name} ' + 'is not present in the backcompatability history ' + f'directory {legacy_query_info.directory} ' + '\n\tPlease run `dagster-graphql-client query snapshot` on the command line ' + 'or manually resolve these issues')"
https://github.com/google/mobly/tree/master/mobly/controllers/android_device_lib/sl4a_client.py,"def _retry_connect(self):
    self._adb.forward(['tcp:%d' % self.host_port, 'tcp:%d' % self.device_port])
    expiration_time = time.perf_counter() + _APP_START_WAIT_TIME
    started = False
    while time.perf_counter() < expiration_time:
      self.log.debug('Attempting to start %s.', self.app_name)
      try:
        self.connect()
        started = True
        break
      except Exception:
        self.log.debug('%s is not yet running, retrying',
                       self.app_name,
                       exc_info=True)
      time.sleep(1)
    if not started:
      raise jsonrpc_client_base.AppRestoreConnectionError(
          self._ad, '%s failed to connect for %s at host port %s, '
          'device port %s' %
          (self.app_name, self._adb.serial, self.host_port, self.device_port))",_122412.py,5,"while time.perf_counter() < expiration_time:
    self.log.debug('Attempting to start %s.', self.app_name)
    try:
        self.connect()
        started = True
        break
    except Exception:
        self.log.debug('%s is not yet running, retrying', self.app_name, exc_info=True)
    time.sleep(1)","if not started:
    raise jsonrpc_client_base.AppRestoreConnectionError(self._ad, '%s failed to connect for %s at host port %s, device port %s' % (self.app_name, self._adb.serial, self.host_port, self.device_port))","while time.perf_counter() < expiration_time:
    self.log.debug('Attempting to start %s.', self.app_name)
    try:
        self.connect()
        break
    except Exception:
        self.log.debug('%s is not yet running, retrying', self.app_name, exc_info=True)
    time.sleep(1)
else:
    raise jsonrpc_client_base.AppRestoreConnectionError(self._ad, '%s failed to connect for %s at host port %s, device port %s' % (self.app_name, self._adb.serial, self.host_port, self.device_port))"
https://github.com/microsoft/muzic/tree/master/deeprapper/tokenizations/tokenization_bert.py,"def tokenize(self, text):
        """"""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
          input = ""unaffable""
          output = [""un"", ""##aff"", ""##able""]

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer`.

        Returns:
          A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens",_123063.py,29,"while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        is_bad = True
        break
    sub_tokens.append(cur_substr)
    start = end","if is_bad:
    output_tokens.append(self.unk_token)
else:
    output_tokens.extend(sub_tokens)","while start < len(chars):
    end = len(chars)
    cur_substr = None
    while start < end:
        substr = ''.join(chars[start:end])
        if start > 0:
            substr = '##' + substr
        if substr in self.vocab:
            cur_substr = substr
            break
        end -= 1
    if cur_substr is None:
        output_tokens.append(self.unk_token)
        break
    sub_tokens.append(cur_substr)
    start = end
else:
    output_tokens.extend(sub_tokens)"
https://github.com/microsoft/muzic/tree/master/deeprapper/tokenizations/tokenization_bert.py,"def tokenize(self, text):
        """"""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
          input = ""unaffable""
          output = [""un"", ""##aff"", ""##able""]

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer`.

        Returns:
          A list of wordpiece tokens.
        """"""

        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = """".join(chars[start:end])
                    if start > 0:
                        substr = ""##"" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens",_123063.py,32,"while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1","if cur_substr is None:
    is_bad = True
    break","while start < end:
    substr = ''.join(chars[start:end])
    if start > 0:
        substr = '##' + substr
    if substr in self.vocab:
        cur_substr = substr
        break
    end -= 1
else:
    is_bad = True
    break"
https://github.com/linkedin/kafka-tools/tree/master/kafka/tools/assigner/actions/remove.py,"def process_cluster(self):
        # Make a deque for the target brokers so we can round-robin assignments
        todeque = deque(self.to_brokers)

        for broker_id in self.brokers:
            broker = self.cluster.brokers[broker_id]
            for position in broker.partitions:
                iterlist = list(broker.partitions[position])
                for partition in iterlist:
                    if partition.topic.name in self.args.exclude_topics:
                        continue

                    # Find a new replica for this partition
                    newreplica = None
                    attempts = 0
                    while attempts < len(todeque):
                        proposed = todeque.popleft()
                        todeque.append(proposed)
                        proposed_broker = self.cluster.brokers[proposed]
                        if proposed_broker not in partition.replicas:
                            newreplica = proposed_broker
                            break
                        attempts += 1

                    if newreplica is None:
                        raise NotEnoughReplicasException(""Cannot find a new broker for {0}:{1} with replica list {2}"".format(partition.topic.name,
                                                                                                                             partition.num,
                                                                                                                             partition.replicas))

                    # Replace the broker coming out with the new one
                    partition.swap_replicas(broker, newreplica)",_123791.py,16,"while attempts < len(todeque):
    proposed = todeque.popleft()
    todeque.append(proposed)
    proposed_broker = self.cluster.brokers[proposed]
    if proposed_broker not in partition.replicas:
        newreplica = proposed_broker
        break
    attempts += 1","if newreplica is None:
    raise NotEnoughReplicasException('Cannot find a new broker for {0}:{1} with replica list {2}'.format(partition.topic.name, partition.num, partition.replicas))","while attempts < len(todeque):
    proposed = todeque.popleft()
    todeque.append(proposed)
    proposed_broker = self.cluster.brokers[proposed]
    if proposed_broker not in partition.replicas:
        newreplica = proposed_broker
        break
    attempts += 1
else:
    raise NotEnoughReplicasException('Cannot find a new broker for {0}:{1} with replica list {2}'.format(partition.topic.name, partition.num, partition.replicas))"
https://github.com/salesforce/policy_sentry/tree/master/policy_sentry/shared/awsdocs.py,"def header_matches(string, table):
    """"""checks if the string is found in the table header""""""
    headers = [chomp(str(x)).lower() for x in table.find_all(""th"")]
    match_found = False
    for header in headers:
        if string in header:
            match_found = True
            break
    if not match_found:
        return False
    return True",_125435.py,5,"for header in headers:
    if string in header:
        match_found = True
        break","if not match_found:
    return False","for header in headers:
    if string in header:
        break
else:
    return False"
https://github.com/openstack/neutron/tree/master/neutron/hacking/checks.py,"def check_builtins_gettext(logical_line, tokens, filename, lines, noqa):
    """"""N341 - Check usage of builtins gettext _().""""""

    if noqa:
        return

    modulename = os.path.normpath(filename).split('/')[0]

    if '%s/tests' % modulename in filename:
        return

    if os.path.basename(filename) in ('i18n.py', '_i18n.py'):
        return

    token_values = [t[1] for t in tokens]
    i18n_wrapper = '%s._i18n' % modulename

    if '_' in token_values:
        i18n_import_line_found = False
        for line in lines:
            split_line = [elm.rstrip(',') for elm in line.split()]
            if (len(split_line) > 1 and split_line[0] == 'from' and
                    split_line[1] == i18n_wrapper and
                    '_' in split_line):
                i18n_import_line_found = True
                break
        if not i18n_import_line_found:
            msg = (""N341: _ from python builtins module is used. ""
                   ""Use _ from %s instead."" % i18n_wrapper)
            yield (0, msg)",_126485.py,20,"for line in lines:
    split_line = [elm.rstrip(',') for elm in line.split()]
    if len(split_line) > 1 and split_line[0] == 'from' and (split_line[1] == i18n_wrapper) and ('_' in split_line):
        i18n_import_line_found = True
        break","if not i18n_import_line_found:
    msg = 'N341: _ from python builtins module is used. Use _ from %s instead.' % i18n_wrapper
    yield (0, msg)","for line in lines:
    split_line = [elm.rstrip(',') for elm in line.split()]
    if len(split_line) > 1 and split_line[0] == 'from' and (split_line[1] == i18n_wrapper) and ('_' in split_line):
        break
else:
    msg = 'N341: _ from python builtins module is used. Use _ from %s instead.' % i18n_wrapper
    yield (0, msg)"
