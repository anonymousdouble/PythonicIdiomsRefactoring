file_html,method_content,file_name,lineno,old_code,new_code
https://github.com/MIC-DKFZ/nnUNet/tree/master/nnunet/inference/pretrained_models/download_pretrained_model.py,"def download_file(url: str, local_filename: str, chunk_size: Optional[int] = 8192 * 16) -> str:
    # borrowed from https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests
    # NOTE the stream=True parameter below
    # OpenRefactory Warning: The 'requests.get' method does not use any 'timeout' threshold which may cause program to hang indefinitely.
    with requests.get(url, stream=True, timeout=100) as r:
        r.raise_for_status()
        # with open(local_filename, 'wb') as f:
        #     for chunk in r.iter_content(chunk_size=chunk_size):
        #         # If you have chunk encoded response uncomment if
        #         # and set chunk_size parameter to None.
        #         #if chunk:
        #         f.write(chunk)
        with tqdm.wrapattr(open(local_filename, 'wb'), ""write"", total=int(r.headers.get(""Content-Length""))) as f:
            for chunk in r.iter_content(chunk_size=chunk_size):
                # If you have chunk encoded response uncomment if
                # and set chunk_size parameter to None.
                # if chunk:
                f.write(chunk)
    return local_filename",_9010.py,13,"with tqdm.wrapattr(open(local_filename, 'wb'), ""write"", total=int(r.headers.get(""Content-Length""))) as f:
    for chunk in r.iter_content(chunk_size=chunk_size):
        # If you have chunk encoded response uncomment if
        # and set chunk_size parameter to None.
        # if chunk:
        f.write(chunk)
","with open(local_filename, 'wb') as f_loc:
    with tqdm.wrapattr(f_loc, ""write"", total=int(r.headers.get(""Content-Length""))) as f:
        for chunk in r.iter_content(chunk_size=chunk_size):
            # If you have chunk encoded response uncomment if
            # and set chunk_size parameter to None.
            # if chunk:
            f.write(chunk)
"
https://github.com/lisa-lab/pylearn2/tree/master/pylearn2/scripts/icml_2013_wrepl/multimodal/make_wordlist.py,"def main():
    base = '${PYLEARN2_DATA_PATH}/esp_game/ESPGame100k/labels/'
    base = preprocess(base)
    paths = sorted(os.listdir(base))
    assert len(paths) == 100000

    words = {}

    for i, path in enumerate(paths):

        if i % 1000 == 0:
            print(i)
        path = base+path
        f = open(path, 'r')
        lines = f.readlines()
        for line in lines:
            word = line[: -1]
            if word not in words:
                words[word] = 1
            else:
                words[word] += 1

    ranked_words = sorted(words.keys(), key=lambda x: -words[x])

    ranked_words = [word_ + '\n' for word_ in ranked_words[0:4000]]

    f = open('wordlist.txt', 'w')
    f.writelines(ranked_words)
    f.close()",_22074.py,14,"f = open(path, 'r')
lines = f.readlines()
for line in lines:
    word = line[: -1]
    if word not in words:
        words[word] = 1
    else:
        words[word] += 1
","with open(path, 'r') as  f:
    lines = f.readlines()
    for line in lines:
        word = line[: -1]
        if word not in words:
            words[word] = 1
        else:
            words[word] += 1
"
https://github.com/ansible/galaxy/tree/master/lib/galaxy/datatypes/data.py,"def split(cls, input_datasets, subdir_generator_function, split_params):
        """"""
        Split the input files by line.
        """"""
        if split_params is None:
            return

        if len(input_datasets) > 1:
            raise Exception(""Text file splitting does not support multiple files"")
        input_files = [ds.file_name for ds in input_datasets]

        lines_per_file = None
        chunk_size = None
        if split_params['split_mode'] == 'number_of_parts':
            lines_per_file = []

            # Computing the length is expensive!
            def _file_len(fname):
                with open(fname) as f:
                    return sum(1 for _ in f)
            length = _file_len(input_files[0])
            parts = int(split_params['split_size'])
            if length < parts:
                parts = length
            len_each, remainder = divmod(length, parts)
            while length > 0:
                chunk = len_each
                if remainder > 0:
                    chunk += 1
                lines_per_file.append(chunk)
                remainder -= 1
                length -= chunk
        elif split_params['split_mode'] == 'to_size':
            chunk_size = int(split_params['split_size'])
        else:
            raise Exception(f""Unsupported split mode {split_params['split_mode']}"")

        f = open(input_files[0])
        try:
            chunk_idx = 0
            file_done = False
            part_file = None
            while not file_done:
                if lines_per_file is None:
                    this_chunk_size = chunk_size
                elif chunk_idx < len(lines_per_file):
                    this_chunk_size = lines_per_file[chunk_idx]
                    chunk_idx += 1
                lines_remaining = this_chunk_size
                part_file = None
                while lines_remaining > 0:
                    a_line = f.readline()
                    if a_line == '':
                        file_done = True
                        break
                    if part_file is None:
                        part_dir = subdir_generator_function()
                        part_path = os.path.join(part_dir, os.path.basename(input_files[0]))
                        part_file = open(part_path, 'w')
                    part_file.write(a_line)
                    lines_remaining -= 1
        except Exception as e:
            log.error('Unable to split files: %s', unicodify(e))
            raise
        finally:
            f.close()
            if part_file:
                part_file.close()",_22.py,38,f = open(input_files[0]),"with open(input_files[0]) as f:
    try:
        chunk_idx = 0
        file_done = False
        part_file = None
        while not file_done:
            if lines_per_file is None:
                this_chunk_size = chunk_size
            elif chunk_idx < len(lines_per_file):
                this_chunk_size = lines_per_file[chunk_idx]
                chunk_idx += 1
            lines_remaining = this_chunk_size
            part_file = None
            while lines_remaining > 0:
                a_line = f.readline()
                if a_line == '':
                    file_done = True
                    break
                if part_file is None:
                    part_dir = subdir_generator_function()
                    part_path = os.path.join(part_dir, os.path.basename(input_files[0]))
                    part_file = open(part_path, 'w')
                part_file.write(a_line)
                lines_remaining -= 1
    except Exception as e:
        log.error('Unable to split files: %s', unicodify(e))
        raise
    finally:
        
        pass
        if part_file:
            part_file.close()"
https://github.com/beancount/beancount/tree/master/experiments/docs/framedocs.py,"def parse_htaccess(filename):
    documents = collections.defaultdict(list)
    redirects = []
    for line in open(filename):
        match = re.match(r'RedirectMatch /doc/(.+?)\$\s+(.+)$', line)
        if match:
            name, url = match.groups()
            url_match = re.match('https://docs.google.com/document/d/(.+)/$', url)
            if url_match:
                docid = url_match.group(1)
                documents[docid].insert(0, name)
            else:
                redirects.append((name, url))

    doc2id = {name[0]: docid for docid, name in documents.items()}
    for name, url in redirects:
        if not url.startswith('/beancount/doc/'):
            continue
        url = re.sub('^/beancount/doc/', '', url)
        try:
            docid = doc2id[url]
            documents[docid].append(name)
        except KeyError:
            pass

    return documents",_99.py,4,"for line in open(filename):
    match = re.match('RedirectMatch /doc/(.+?)\\$\\s+(.+)$', line)
    if match:
        (name, url) = match.groups()
        url_match = re.match('https://docs.google.com/document/d/(.+)/$', url)
        if url_match:
            docid = url_match.group(1)
            documents[docid].insert(0, name)
        else:
            redirects.append((name, url))","with open(filename) as my_f:
    for line in my_f:
        match = re.match('RedirectMatch /doc/(.+?)\\$\\s+(.+)$', line)
        if match:
            (name, url) = match.groups()
            url_match = re.match('https://docs.google.com/document/d/(.+)/$', url)
            if url_match:
                docid = url_match.group(1)
                documents[docid].insert(0, name)
            else:
                redirects.append((name, url))"
https://github.com/beancount/beancount/tree/master/experiments/docs/framedocs.py,"def parse_htaccess(filename):
    documents = collections.defaultdict(list)
    redirects = []
    for line in open(filename):
        match = re.match(r'RedirectMatch /doc/(.+?)\$\s+(.+)$', line)
        if match:
            name, url = match.groups()
            url_match = re.match('https://docs.google.com/document/d/(.+)/$', url)
            if url_match:
                docid = url_match.group(1)
                documents[docid].insert(0, name)
            else:
                redirects.append((name, url))

    doc2id = {name[0]: docid for docid, name in documents.items()}
    for name, url in redirects:
        if not url.startswith('/beancount/doc/'):
            continue
        url = re.sub('^/beancount/doc/', '', url)
        try:
            docid = doc2id[url]
            documents[docid].append(name)
        except KeyError:
            pass

    return documents",_99.py,4,"for line in open(filename):
    match = re.match('RedirectMatch /doc/(.+?)\\$\\s+(.+)$', line)
    if match:
        (name, url) = match.groups()
        url_match = re.match('https://docs.google.com/document/d/(.+)/$', url)
        if url_match:
            docid = url_match.group(1)
            documents[docid].insert(0, name)
        else:
            redirects.append((name, url))","with open(filename) as my_f:
    for line in my_f:
        match = re.match('RedirectMatch /doc/(.+?)\\$\\s+(.+)$', line)
        if match:
            (name, url) = match.groups()
            url_match = re.match('https://docs.google.com/document/d/(.+)/$', url)
            if url_match:
                docid = url_match.group(1)
                documents[docid].insert(0, name)
            else:
                redirects.append((name, url))"
https://github.com/r9y9/deepvoice3_pytorch/tree/master/vctk_preprocess/extract_feats.py,"def extract_intermediate_features(wav_path, txt_path, keep_silences=False,
                                  full_features=False, ehmm_max_n_itr=1):
    basedir = os.getcwd()
    latest_feature_dir = ""latest_features""
    if not os.path.exists(latest_feature_dir):
        os.mkdir(latest_feature_dir)

    os.chdir(latest_feature_dir)
    latest_feature_dir = os.getcwd()

    if not os.path.exists(""merlin""):
        clone_cmd = ""git clone https://github.com/kastnerkyle/merlin""
        pe(clone_cmd, shell=True)

    if keep_silences:
        # REMOVE SILENCES TO MATCH JOSE PREPROC
        os.chdir(""merlin/src"")
        pe(""sed -i.bak -e '708,712d;' run_merlin.py"", shell=True)
        pe(""sed -i.bak -e '695,706d;' run_merlin.py"", shell=True)
        os.chdir(latest_feature_dir)

    os.chdir(""merlin"")
    merlin_dir = os.getcwd()
    os.chdir(""egs/build_your_own_voice/s1"")
    experiment_dir = os.getcwd()

    if not os.path.exists(""database""):
        print(""Creating database and copying in files"")
        pe(""bash -x 01_setup.sh my_new_voice 2>&1"", shell=True)

        # Copy in wav files
        wav_partial_path = wav_path  # vctkdir + ""wav48/""
        """"""
        subfolders = sorted(os.listdir(wav_partial_path))
        # only p294 for now...
        subfolders = subfolder_select(subfolders)
        os.chdir(""database/wav"")
        for sf in subfolders:
            wav_path = wav_partial_path + sf + ""/*.wav""
            pe(""cp %s ."" % wav_path, shell=True)
        """"""
        to_copy = os.listdir(wav_partial_path)
        if len([tc for tc in to_copy if tc[-4:] == "".wav""]) == 0:
            raise IOError(
                ""Unable to find any wav files in %s, make sure the filenames end in .wav!"" % wav_partial_path)
        os.chdir(""database/wav"")
        if wav_partial_path[-1] != ""/"":
            wav_partial_path = wav_partial_path + ""/""
        wav_match_path = wav_partial_path + ""*.wav""
        for fi in glob.glob(wav_match_path):
            pe(""echo %s; cp %s ."" % (fi, fi), shell=True)
        # THIS MAY FAIL IF TOO MANY WAV FILES
        # pe(""cp %s ."" % wav_match_path, shell=True)
        for f in os.listdir("".""):
            # This is only necessary because of corrupted files...
            fs, d = wavfile.read(f)
            wavfile.write(f, fs, d)

        # downsample the files
        get_sr_cmd = 'file `ls *.wav | head -n 1` | cut -d "" "" -f 12'
        sr = pe(get_sr_cmd, shell=True)
        sr_int = int(sr[0].strip())
        print(""Got samplerate {}, converting to 16000"".format(sr_int))
        # was assuming all were 48000
        convert = estdir + \
            ""bin/ch_wave $i -o tmp_$i -itype wav -otype wav -F 16000 -f {}"".format(sr_int)
        pe(""for i in *.wav; do echo %s; %s; mv tmp_$i $i; done"" % (convert, convert), shell=True)

        os.chdir(experiment_dir)
        txt_partial_path = txt_path  # vctkdir + ""txt/""
        """"""
        subfolders = sorted(os.listdir(txt_partial_path))
        # only p294 for now...
        subfolders = subfolder_select(subfolders)
        os.chdir(""database/txt"")
        for sf in subfolders:
            txt_path = txt_partial_path + sf + ""/*.txt""
            pe(""cp %s ."" % txt_path, shell=True)
        """"""
        os.chdir(""database/txt"")
        to_copy = os.listdir(txt_partial_path)
        if len([tc for tc in to_copy if tc[-4:] == "".txt""]) == 0:
            raise IOError(
                ""Unable to find any txt files in %s. Be sure the filenames end in .txt!"" % txt_partial_path)
        txt_match_path = txt_partial_path + ""/*.txt""
        for fi in glob.glob(txt_match_path):
            # escape string...
            fi = re.escape(fi)
            try:
                pe(""echo %s; cp %s ."" % (fi, fi), shell=True)
            except:
                from IPython import embed
                embed()
                raise ValueError()

        #pe(""cp %s ."" % txt_match_path, shell=True)

    do_state_align = False
    if do_state_align:
        raise ValueError(""Replace these lies with something that points at the right place"")
        os.chdir(merlin_dir)
        os.chdir(""misc/scripts/alignment/state_align"")
        pe(""bash -x setup.sh 2>&1"", shell=True)

        with open(""config.cfg"", ""r"") as f:
            config_lines = f.readlines()

        # replace FESTDIR with the correct path
        festdir_replace_line = None
        for n, l in enumerate(config_lines):
            if ""FESTDIR="" in l:
                festdir_replace_line = n
                break

        config_lines[festdir_replace_line] = ""FESTDIR=%s\n"" % festdir

        # replace HTKDIR with the correct path
        htkdir_replace_line = None
        for n, l in enumerate(config_lines):
            if ""HTKDIR="" in l:
                htkdir_replace_line = n
                break

        config_lines[htkdir_replace_line] = ""HTKDIR=%s\n"" % htkdir

        with open(""config.cfg"", ""w"") as f:
            f.writelines(config_lines)

        pe(""bash -x run_aligner.sh config.cfg 2>&1"", shell=True)
    else:
        os.chdir(merlin_dir)
        if not os.path.exists(""misc/scripts/alignment/phone_align/full-context-labels/full""):
            os.chdir(""misc/scripts/alignment/phone_align"")
            pe(""bash -x setup.sh 2>&1"", shell=True)

            with open(""config.cfg"", ""r"") as f:
                config_lines = f.readlines()

            # replace ESTDIR with the correct path
            estdir_replace_line = None
            for n, l in enumerate(config_lines):
                if ""ESTDIR="" in l and l[0] == ""E"":
                    estdir_replace_line = n
                    break

            config_lines[estdir_replace_line] = ""ESTDIR=%s\n"" % estdir

            # replace FESTDIR with the correct path
            festdir_replace_line = None
            for n, l in enumerate(config_lines):
                # EST/FEST
                if ""FESTDIR="" in l and l[0] == ""F"":
                    festdir_replace_line = n
                    break

            config_lines[festdir_replace_line] = ""FESTDIR=%s\n"" % festdir

            # replace FESTVOXDIR with the correct path
            festvoxdir_replace_line = None
            for n, l in enumerate(config_lines):
                if ""FESTVOXDIR="" in l:
                    festvoxdir_replace_line = n
                    break

            config_lines[festvoxdir_replace_line] = ""FESTVOXDIR=%s\n"" % festvoxdir

            with open(""config.cfg"", ""w"") as f:
                f.writelines(config_lines)

            with open(""run_aligner.sh"", ""r"") as f:
                run_aligner_lines = f.readlines()

            replace_line = None
            for n, l in enumerate(run_aligner_lines):
                if ""cp ../cmuarctic.data"" in l:
                    replace_line = n
                    break

            run_aligner_lines[replace_line] = ""cp ../txt.done.data etc/txt.done.data\n""

            # Make the txt.done.data file
            def format_info_tup(info_tup):
                return ""( "" + str(info_tup[0]) + ' ""' + info_tup[1] + '"" )\n'

            # Now we need to get the text info
            txt_partial_path = txt_path  # vctkdir + ""txt/""
            cwd = os.getcwd()
            out_path = ""txt.done.data""
            out_file = open(out_path, ""w"")
            """"""
            subfolders = sorted(os.listdir(txt_partial_path))
            # TODO: Avoid this truncation and have an option to select subfolder(s)...
            subfolders = subfolder_select(subfolders)

            txt_ids = []
            for sf in subfolders:
                print(""Processing subfolder %s"" % sf)
                txt_sf_path = txt_partial_path + sf + ""/""
                for txtpath in os.listdir(txt_sf_path):
                    full_txtpath = txt_sf_path + txtpath
                    with open(full_txtpath, 'r') as f:
                        r = f.readlines()
                        assert len(r) == 1
                        # remove txt extension
                        name = txtpath.split(""."")[0]
                        text = r[0].strip()
                        info_tup = (name, text)
                        txt_ids.append(name)
                        out_file.writelines(format_info_tup(info_tup))
            """"""
            txt_ids = []
            txt_l_path = txt_partial_path
            for txtpath in os.listdir(txt_l_path):
                print(""Processing %s"" % txtpath)
                full_txtpath = txt_l_path + txtpath
                name = txtpath.split(""."")[0]
                wavpath_matches = [fname.split(""."")[0] for fname in os.listdir(wav_partial_path)
                                   if name in fname]
                for name in wavpath_matches:
                    # Need an extra level here for pavoque :/
                    with open(full_txtpath, 'r') as f:
                        r = f.readlines()
                    if len(r) == 0:
                        continue
                    if len(r) != 1:
                        new_r = []
                        for ri in r:
                            if ri != ""\n"":
                                new_r.append(ri)
                        r = new_r
                    if len(r) != 1:
                        print(""Something wrong in text extraction, cowardly bailing to IPython"")
                        from IPython import embed
                        embed()
                        raise ValueError()
                    assert len(r) == 1
                    # remove txt extension
                    text = r[0].strip()
                    info_tup = (name, text)
                    txt_ids.append(name)
                    out_file.writelines(format_info_tup(info_tup))
            out_file.close()
            pe(""cp %s %s/txt.done.data"" % (out_path, latest_feature_dir),
               shell=True)
            os.chdir(cwd)

            replace_line = None
            for n, l in enumerate(run_aligner_lines):
                if ""cp ../slt_wav/*.wav"" in l:
                    replace_line = n
                    break

            run_aligner_lines[replace_line] = ""cp ../wav/*.wav wav\n""

            # Put wav file in the correct place
            wav_partial_path = experiment_dir + ""/database/wav""
            """"""
            subfolders = sorted(os.listdir(wav_partial_path))
            """"""
            if not os.path.exists(""wav""):
                os.mkdir(""wav"")
            cwd = os.getcwd()
            os.chdir(""wav"")
            """"""
            for sf in subfolders:
                wav_path = wav_partial_path + ""/*.wav""
                pe(""cp %s ."" % wav_path, shell=True)
            """"""
            wav_match_path = wav_partial_path + ""/*.wav""
            for fi in glob.glob(wav_match_path):
                fi = re.escape(fi)
                try:
                    pe(""echo %s; cp %s ."" % (fi, fi), shell=True)
                except:
                    from IPython import embed
                    embed()
                    raise ValueError()
                #pe(""echo %s; cp %s ."" % (fi, fi), shell=True)
            #pe(""cp %s ."" % wav_match_path, shell=True)
            os.chdir(cwd)

            replace_line = None
            for n, l in enumerate(run_aligner_lines):
                if ""cat cmuarctic.data |"" in l:
                    replace_line = n
                    break

            run_aligner_lines[replace_line] = 'cat txt.done.data | cut -d "" "" -f 2 > file_id_list.scp\n'

            # FIXME
            # Hackaround to avoid harcoded 30 in festivox do_ehmm
            if not full_features:
                bdir = os.getcwd()

                # need to hack up run_aligner more..
                # do setup manually
                pe(""mkdir cmu_us_slt_arctic"", shell=True)
                os.chdir(""cmu_us_slt_arctic"")

                pe(""%s/src/clustergen/setup_cg cmu us slt_arctic"" % festvoxdir, shell=True)

                pe(""cp ../txt.done.data etc/txt.done.data"", shell=True)
                wmp = ""../wav/*.wav""
                for fi in glob.glob(wmp):
                    fi = re.escape(fi)
                    try:
                        pe(""echo %s; cp %s wav/"" % (fi, fi), shell=True)
                    except:
                        from IPython import embed
                        embed()
                        raise ValueError()
                    #pe(""echo %s; cp %s wav/"" % (fi, fi), shell=True)
                #pe(""cp ../wav/*.wav wav/"", shell=True)

                # remove top part but keep cd call
                run_aligner_lines = run_aligner_lines[:13] + \
                    [""cd cmu_us_slt_arctic\n""] + run_aligner_lines[35:]

                '''
                # need to change do_build
                # NO LONGER NECESSARY DUE TO FESTIVAL DEPENDENCE ON FILENAME

                os.chdir(""bin"")
                with open(""do_build"", ""r"") as f:
                    do_build_lines = f.readlines()

                replace_line = None
                for n, l in enumerate(do_build_lines):
                    if ""$FESTVOXDIR/src/ehmm/bin/do_ehmm"" in l:
                        replace_line = n
                        break

                do_build_lines[replace_line] = ""   $FESTVOXDIR/src/ehmm/bin/do_ehmm\n""

                # FIXME Why does this hang when not overwritten???
                with open(""edit_do_build"", ""w"") as f:
                    f.writelines(do_build_lines)
                '''

                # need to change do_ehmm
                os.chdir(festvoxdir)
                os.chdir(""src/ehmm/bin/"")

                # this is to fix festival if we somehow kill in the middle of training :(
                # all due to festival's apparent dependence on name of script!
                # really, really, REALLY weird
                if os.path.exists(""do_ehmm.bak""):
                    with open(""do_ehmm.bak"", ""r"") as f:
                        fix = f.readlines()

                    with open(""do_ehmm"", ""w"") as f:
                        f.writelines(fix)

                with open(""do_ehmm"", ""r"") as f:
                    do_ehmm_lines = f.readlines()

                with open(""do_ehmm.bak"", ""w"") as f:
                    f.writelines(do_ehmm_lines)

                replace_line = None
                for n, l in enumerate(do_ehmm_lines):
                    if ""$EHMMDIR/bin/ehmm ehmm/etc/ph_list.int"" in l:
                        replace_line = n
                        break

                max_n_itr = ehmm_max_n_itr
                do_ehmm_lines[replace_line] = ""    $EHMMDIR/bin/ehmm ehmm/etc/ph_list.int ehmm/etc/txt.phseq.data.int 1 0 ehmm/binfeat scaledft ehmm/mod 0 0 0 %s $num_cpus\n"" % str(
                    max_n_itr)

                # depends on *name* of the script?????????
                with open(""do_ehmm"", ""w"") as f:
                    f.writelines(do_ehmm_lines)

                # need to edit run_aligner....
                dbn = ""do_build""
                # FIXME
                # WHY DOES IT DEPEND ON FILENAME????!!!!!??????
                # should be able to call only edit_do_build label
                # but hangs indefinitely...
                replace_line = None
                for n, l in enumerate(run_aligner_lines):
                    if ""./bin/do_build build_prompts"" in l:
                        replace_line = n
                        break
                run_aligner_lines[replace_line] = ""./bin/%s build_prompts\n"" % dbn

                replace_line = None
                for n, l in enumerate(run_aligner_lines):
                    if ""./bin/do_build label"" in l:
                        replace_line = n
                        break
                run_aligner_lines[replace_line] = ""./bin/%s label\n"" % dbn

                replace_line = None
                for n, l in enumerate(run_aligner_lines):
                    if ""./bin/do_build build_utts"" in l:
                        replace_line = n
                        break
                run_aligner_lines[replace_line] = ""./bin/%s build_utts\n"" % dbn
                os.chdir(bdir)

            with open(""edit_run_aligner.sh"", ""w"") as f:
                f.writelines(run_aligner_lines)

            # 2>&1 needed to make it work?? really sketchy
            pe(""bash -x edit_run_aligner.sh config.cfg 2>&1"", shell=True)

    # compile vocoder
    os.chdir(merlin_dir)
    # set it to run on cpu
    pe(""sed -i.bak -e s/MERLIN_THEANO_FLAGS=.*/MERLIN_THEANO_FLAGS='device=cpu,floatX=float32,on_unused_input=ignore'/g src/setup_env.sh"", shell=True)
    os.chdir(""tools"")
    if not os.path.exists(""SPTK-3.9""):
        pe(""bash -x compile_tools.sh 2>&1"", shell=True)

    # slt_arctic stuff
    os.chdir(merlin_dir)
    os.chdir(""egs/slt_arctic/s1"")

    # This madness due to autogen configs...
    pe(""bash -x scripts/setup.sh slt_arctic_full 2>&1"", shell=True)

    global_config_file = ""conf/global_settings.cfg""
    replace_write(global_config_file, ""Labels"", ""phone_align"", replace_line=""%s=%s\n"")
    replace_write(global_config_file, ""Train"", ""1132"", replace_line=""%s=%s\n"")
    replace_write(global_config_file, ""Valid"", ""0"", replace_line=""%s=%s\n"")
    replace_write(global_config_file, ""Test"", ""0"", replace_line=""%s=%s\n"")

    pe(""bash -x scripts/prepare_config_files.sh %s 2>&1"" % global_config_file, shell=True)
    pe(""bash -x scripts/prepare_config_files_for_synthesis.sh %s 2>&1"" % global_config_file, shell=True)
    # delete the setup lines from run_full_voice.sh
    pe(""sed -i.bak -e '11d;12d;13d' run_full_voice.sh"", shell=True)

    pushd = os.getcwd()
    os.chdir(""conf"")

    acoustic_conf = ""acoustic_slt_arctic_full.conf""
    replace_write(acoustic_conf, ""train_file_number"", ""1132"")
    replace_write(acoustic_conf, ""valid_file_number"", ""0"")
    replace_write(acoustic_conf, ""test_file_number"", ""0"")

    replace_write(acoustic_conf, ""label_type"", ""phone_align"")
    replace_write(acoustic_conf, ""subphone_feats"", ""coarse_coding"")
    replace_write(acoustic_conf, ""dmgc"", ""60"")
    replace_write(acoustic_conf, ""dbap"", ""1"")
    # hack this to add an extra line in the config
    replace_write(acoustic_conf, ""dlf0"", ""1\ndo_MLPG: False"")

    if not full_features:
        replace_write(acoustic_conf, ""warmup_epoch"", ""1"")
        replace_write(acoustic_conf, ""training_epochs"", ""1"")
    replace_write(acoustic_conf, ""TRAINDNN"", ""False"")
    replace_write(acoustic_conf, ""DNNGEN"", ""False"")
    replace_write(acoustic_conf, ""GENWAV"", ""False"")
    replace_write(acoustic_conf, ""CALMCD"", ""False"")

    duration_conf = ""duration_slt_arctic_full.conf""
    replace_write(duration_conf, ""train_file_number"", ""1132"")
    replace_write(duration_conf, ""valid_file_number"", ""0"")
    replace_write(duration_conf, ""test_file_number"", ""0"")
    replace_write(duration_conf, ""label_type"", ""phone_align"")
    replace_write(duration_conf, ""dur"", ""1"")
    if not full_features:
        replace_write(duration_conf, ""warmup_epoch"", ""1"")
        replace_write(duration_conf, ""training_epochs"", ""1"")

    replace_write(duration_conf, ""TRAINDNN"", ""False"")
    replace_write(duration_conf, ""DNNGEN"", ""False"")
    replace_write(duration_conf, ""CALMCD"", ""False"")

    os.chdir(pushd)
    if not os.path.exists(""slt_arctic_full_data""):
        pe(""bash -x run_full_voice.sh 2>&1"", shell=True)

    pe(""mv run_full_voice.sh.bak run_full_voice.sh"", shell=True)

    os.chdir(merlin_dir)
    os.chdir(""misc/scripts/vocoder/world"")

    with open(""extract_features_for_merlin.sh"", ""r"") as f:
        ex_lines = f.readlines()

    ex_line_replace = None
    for n, l in enumerate(ex_lines):
        if ""merlin_dir="" in l:
            ex_line_replace = n
            break

    ex_lines[ex_line_replace] = 'merlin_dir=""%s""' % merlin_dir

    ex_line_replace = None
    for n, l in enumerate(ex_lines):
        if ""wav_dir="" in l:
            ex_line_replace = n
            break

    ex_lines[ex_line_replace] = 'wav_dir=""%s""' % (experiment_dir + ""/database/wav"")

    with open(""edit_extract_features_for_merlin.sh"", ""w"") as f:
        f.writelines(ex_lines)

    pe(""bash -x edit_extract_features_for_merlin.sh 2>&1"", shell=True)

    os.chdir(basedir)
    os.chdir(""latest_features"")
    os.symlink(merlin_dir + ""/egs/slt_arctic/s1/slt_arctic_full_data/feat"", ""audio_feat"")
    os.symlink(merlin_dir + ""/misc/scripts/alignment/phone_align/full-context-labels/full"", ""text_feat"")

    print(""Audio features in %s (and %s)"" % (os.getcwd() + ""/audio_feat"",
                                             merlin_dir + ""/egs/slt_arctic/s1/slt_arctic_full_data/feat""))
    print(""Text features in %s (and %s)"" % (os.getcwd() + ""/text_feat"", merlin_dir +
                                            ""/misc/scripts/alignment/phone_align/full-context-labels/full""))
    os.chdir(basedir)",_794.py,189,"out_file = open(out_path, 'w')","with open(out_path, 'w') as out_file:
    '\n            subfolders = sorted(os.listdir(txt_partial_path))\n            # TODO: Avoid this truncation and have an option to select subfolder(s)...\n            subfolders = subfolder_select(subfolders)\n\n            txt_ids = []\n            for sf in subfolders:\n                print(""Processing subfolder %s"" % sf)\n                txt_sf_path = txt_partial_path + sf + ""/""\n                for txtpath in os.listdir(txt_sf_path):\n                    full_txtpath = txt_sf_path + txtpath\n                    with open(full_txtpath, \'r\') as f:\n                        r = f.readlines()\n                        assert len(r) == 1\n                        # remove txt extension\n                        name = txtpath.split(""."")[0]\n                        text = r[0].strip()\n                        info_tup = (name, text)\n                        txt_ids.append(name)\n                        out_file.writelines(format_info_tup(info_tup))\n            '
    txt_ids = []
    txt_l_path = txt_partial_path
    for txtpath in os.listdir(txt_l_path):
        print('Processing %s' % txtpath)
        full_txtpath = txt_l_path + txtpath
        name = txtpath.split('.')[0]
        wavpath_matches = [fname.split('.')[0] for fname in os.listdir(wav_partial_path) if name in fname]
        for name in wavpath_matches:
            with open(full_txtpath, 'r') as f:
                r = f.readlines()
            if len(r) == 0:
                continue
            if len(r) != 1:
                new_r = []
                for ri in r:
                    if ri != '\n':
                        new_r.append(ri)
                r = new_r
            if len(r) != 1:
                print('Something wrong in text extraction, cowardly bailing to IPython')
                from IPython import embed
                embed()
                raise ValueError()
            assert len(r) == 1
            text = r[0].strip()
            info_tup = (name, text)
            txt_ids.append(name)
            out_file.writelines(format_info_tup(info_tup))
    
    pass
    pe('cp %s %s/txt.done.data' % (out_path, latest_feature_dir), shell=True)
    os.chdir(cwd)
    replace_line = None
    for (n, l) in enumerate(run_aligner_lines):
        if 'cp ../slt_wav/*.wav' in l:
            replace_line = n
            break
    run_aligner_lines[replace_line] = 'cp ../wav/*.wav wav\n'
    wav_partial_path = experiment_dir + '/database/wav'
    '\n            subfolders = sorted(os.listdir(wav_partial_path))\n            '
    if not os.path.exists('wav'):
        os.mkdir('wav')
    cwd = os.getcwd()
    os.chdir('wav')
    '\n            for sf in subfolders:\n                wav_path = wav_partial_path + ""/*.wav""\n                pe(""cp %s ."" % wav_path, shell=True)\n            '
    wav_match_path = wav_partial_path + '/*.wav'
    for fi in glob.glob(wav_match_path):
        fi = re.escape(fi)
        try:
            pe('echo %s; cp %s .' % (fi, fi), shell=True)
        except:
            from IPython import embed
            embed()
            raise ValueError()
    os.chdir(cwd)
    replace_line = None
    for (n, l) in enumerate(run_aligner_lines):
        if 'cat cmuarctic.data |' in l:
            replace_line = n
            break
    run_aligner_lines[replace_line] = 'cat txt.done.data | cut -d "" "" -f 2 > file_id_list.scp\n'
    if not full_features:
        bdir = os.getcwd()
        pe('mkdir cmu_us_slt_arctic', shell=True)
        os.chdir('cmu_us_slt_arctic')
        pe('%s/src/clustergen/setup_cg cmu us slt_arctic' % festvoxdir, shell=True)
        pe('cp ../txt.done.data etc/txt.done.data', shell=True)
        wmp = '../wav/*.wav'
        for fi in glob.glob(wmp):
            fi = re.escape(fi)
            try:
                pe('echo %s; cp %s wav/' % (fi, fi), shell=True)
            except:
                from IPython import embed
                embed()
                raise ValueError()
        run_aligner_lines = run_aligner_lines[:13] + ['cd cmu_us_slt_arctic\n'] + run_aligner_lines[35:]
        '\n                # need to change do_build\n                # NO LONGER NECESSARY DUE TO FESTIVAL DEPENDENCE ON FILENAME\n\n                os.chdir(""bin"")\n                with open(""do_build"", ""r"") as f:\n                    do_build_lines = f.readlines()\n\n                replace_line = None\n                for n, l in enumerate(do_build_lines):\n                    if ""$FESTVOXDIR/src/ehmm/bin/do_ehmm"" in l:\n                        replace_line = n\n                        break\n\n                do_build_lines[replace_line] = ""   $FESTVOXDIR/src/ehmm/bin/do_ehmm\n""\n\n                # FIXME Why does this hang when not overwritten???\n                with open(""edit_do_build"", ""w"") as f:\n                    f.writelines(do_build_lines)\n                '
        os.chdir(festvoxdir)
        os.chdir('src/ehmm/bin/')
        if os.path.exists('do_ehmm.bak'):
            with open('do_ehmm.bak', 'r') as f:
                fix = f.readlines()
            with open('do_ehmm', 'w') as f:
                f.writelines(fix)
        with open('do_ehmm', 'r') as f:
            do_ehmm_lines = f.readlines()
        with open('do_ehmm.bak', 'w') as f:
            f.writelines(do_ehmm_lines)
        replace_line = None
        for (n, l) in enumerate(do_ehmm_lines):
            if '$EHMMDIR/bin/ehmm ehmm/etc/ph_list.int' in l:
                replace_line = n
                break
        max_n_itr = ehmm_max_n_itr
        do_ehmm_lines[replace_line] = '    $EHMMDIR/bin/ehmm ehmm/etc/ph_list.int ehmm/etc/txt.phseq.data.int 1 0 ehmm/binfeat scaledft ehmm/mod 0 0 0 %s $num_cpus\n' % str(max_n_itr)
        with open('do_ehmm', 'w') as f:
            f.writelines(do_ehmm_lines)
        dbn = 'do_build'
        replace_line = None
        for (n, l) in enumerate(run_aligner_lines):
            if './bin/do_build build_prompts' in l:
                replace_line = n
                break
        run_aligner_lines[replace_line] = './bin/%s build_prompts\n' % dbn
        replace_line = None
        for (n, l) in enumerate(run_aligner_lines):
            if './bin/do_build label' in l:
                replace_line = n
                break
        run_aligner_lines[replace_line] = './bin/%s label\n' % dbn
        replace_line = None
        for (n, l) in enumerate(run_aligner_lines):
            if './bin/do_build build_utts' in l:
                replace_line = n
                break
        run_aligner_lines[replace_line] = './bin/%s build_utts\n' % dbn
        os.chdir(bdir)
    with open('edit_run_aligner.sh', 'w') as f:
        f.writelines(run_aligner_lines)
    pe('bash -x edit_run_aligner.sh config.cfg 2>&1', shell=True)"
https://github.com/eternnoir/pyTelegramBotAPI/tree/master/tests/test_telebot.py,"def test_send_video_formatting_caption(self):
        file_data = open('./test_data/test_video.mp4', 'rb')
        tb = telebot.TeleBot(TOKEN)
        ret_msg = tb.send_video(CHAT_ID, file_data, caption='_italic_', parse_mode='Markdown')
        assert ret_msg.caption_entities[0].type == 'italic'",_861.py,2,"file_data = open('./test_data/test_video.mp4', 'rb')","with open('./test_data/test_video.mp4', 'rb') as file_data:
    tb = telebot.TeleBot(TOKEN)
    ret_msg = tb.send_video(CHAT_ID, file_data, caption='_italic_', parse_mode='Markdown')
    assert ret_msg.caption_entities[0].type == 'italic'"
https://github.com/commixproject/commix/tree/master/src/core/injections/controller/checks.py,"def print_ps_version(ps_version, filename, _):
  try:
    settings.PS_ENABLED = True
    ps_version = """".join(str(p) for p in ps_version)
    if settings.VERBOSITY_LEVEL == 0 and _:
      print(settings.SINGLE_WHITESPACE)
    # Output PowerShell's version number
    info_msg = ""Powershell version: "" + ps_version
    print(settings.print_bold_info_msg(info_msg))
    # Add infos to logs file. 
    output_file = open(filename, ""a"")
    if not menu.options.no_logging:
      info_msg = ""Powershell version: "" + ps_version + ""\n""
      output_file.write(re.compile(re.compile(settings.ANSI_COLOR_REMOVAL)).sub("""",settings.INFO_BOLD_SIGN) + info_msg)
    output_file.close()
  except ValueError:
    warn_msg = ""Heuristics have failed to identify the version of Powershell, ""
    warn_msg += ""which means that some payloads or injection techniques may be failed."" 
    print(settings.print_warning_msg(warn_msg))
    settings.PS_ENABLED = False
    ps_check_failed()",_1572.py,11,"output_file = open(filename, 'a')","with open(filename, 'a') as output_file:
    if not menu.options.no_logging:
        info_msg = 'Powershell version: ' + ps_version + '\n'
        output_file.write(re.compile(re.compile(settings.ANSI_COLOR_REMOVAL)).sub('', settings.INFO_BOLD_SIGN) + info_msg)
    
    pass"
https://github.com/quodlibet/mutagen/tree/master/tests/test__riff.py,"def setUp(self):
        self.file_1 = open(self.has_tags, 'rb')
        self.riff_1 = RiffFile(self.file_1)
        self.file_2 = open(self.no_tags, 'rb')
        self.riff_2 = RiffFile(self.file_2)

        self.tmp_1_name = get_temp_copy(self.has_tags)
        self.file_1_tmp = open(self.tmp_1_name, 'rb+')
        self.riff_1_tmp = RiffFile(self.file_1_tmp)

        self.tmp_2_name = get_temp_copy(self.no_tags)
        self.file_2_tmp = open(self.tmp_2_name, 'rb+')
        self.riff_2_tmp = RiffFile(self.file_2_tmp)",_2015.py,2,"self.file_1 = open(self.has_tags, 'rb')","with open(self.has_tags, 'rb') as self.file_1:
    self.riff_1 = RiffFile(self.file_1)
    self.file_2 = open(self.no_tags, 'rb')
    self.riff_2 = RiffFile(self.file_2)
    self.tmp_1_name = get_temp_copy(self.has_tags)
    self.file_1_tmp = open(self.tmp_1_name, 'rb+')
    self.riff_1_tmp = RiffFile(self.file_1_tmp)
    self.tmp_2_name = get_temp_copy(self.no_tags)
    self.file_2_tmp = open(self.tmp_2_name, 'rb+')
    self.riff_2_tmp = RiffFile(self.file_2_tmp)"
https://github.com/quodlibet/mutagen/tree/master/tests/test__riff.py,"def setUp(self):
        self.file_1 = open(self.has_tags, 'rb')
        self.riff_1 = RiffFile(self.file_1)
        self.file_2 = open(self.no_tags, 'rb')
        self.riff_2 = RiffFile(self.file_2)

        self.tmp_1_name = get_temp_copy(self.has_tags)
        self.file_1_tmp = open(self.tmp_1_name, 'rb+')
        self.riff_1_tmp = RiffFile(self.file_1_tmp)

        self.tmp_2_name = get_temp_copy(self.no_tags)
        self.file_2_tmp = open(self.tmp_2_name, 'rb+')
        self.riff_2_tmp = RiffFile(self.file_2_tmp)",_2015.py,4,"self.file_2 = open(self.no_tags, 'rb')","with open(self.no_tags, 'rb') as self.file_2:
    self.riff_2 = RiffFile(self.file_2)
    self.tmp_1_name = get_temp_copy(self.has_tags)
    self.file_1_tmp = open(self.tmp_1_name, 'rb+')
    self.riff_1_tmp = RiffFile(self.file_1_tmp)
    self.tmp_2_name = get_temp_copy(self.no_tags)
    self.file_2_tmp = open(self.tmp_2_name, 'rb+')
    self.riff_2_tmp = RiffFile(self.file_2_tmp)"
https://github.com/sbaresearch/simcoin/tree/master/code/simulationfiles/network_config.py,"def read_connections():
    utils.check_for_file(config.network_csv)
    connections = {}
    network_config = pandas.read_csv(open(config.network_csv), index_col=0)

    for node_row, row in network_config.iterrows():
        connections[node_row] = []
        for node_column, value in row.iteritems():
            if node_column == node_row:
                pass
            elif value == 1:
                connections[node_row].append(node_column)

    return connections",_2350.py,4,"network_config = pandas.read_csv(open(config.network_csv), index_col=0)","with open(config.network_csv) as my_f:
    network_config = pandas.read_csv(my_f, index_col=0)
    for (node_row, row) in network_config.iterrows():
        connections[node_row] = []
        for (node_column, value) in row.iteritems():
            if node_column == node_row:
                pass
            elif value == 1:
                connections[node_row].append(node_column)
    return connections"
https://github.com/codekansas/keras-language-modeling/tree/master//insurance_qa_eval.py,"def __init__(self, conf, model, optimizer=None):
        try:
            data_path = os.environ['INSURANCE_QA']
        except KeyError:
            print(""INSURANCE_QA is not set. Set it to your clone of https://github.com/codekansas/insurance_qa_python"")
            sys.exit(1)
        if isinstance(conf, str):
            conf = json.load(open(conf, 'rb'))
        self.model = model(conf)
        self.path = data_path
        self.conf = conf
        self.params = conf['training']
        optimizer = self.params['optimizer'] if optimizer is None else optimizer
        self.model.compile(optimizer)
        self.answers = self.load('answers') # self.load('generated')
        self._vocab = None
        self._reverse_vocab = None
        self._eval_sets = None",_2483.py,8,"conf = json.load(open(conf, 'rb'))","with open(conf, 'rb') as my_f:
    conf = json.load(my_f)"
https://github.com/codekansas/keras-language-modeling/tree/master//insurance_qa_eval.py,"def __init__(self, conf, model, optimizer=None):
        try:
            data_path = os.environ['INSURANCE_QA']
        except KeyError:
            print(""INSURANCE_QA is not set. Set it to your clone of https://github.com/codekansas/insurance_qa_python"")
            sys.exit(1)
        if isinstance(conf, str):
            conf = json.load(open(conf, 'rb'))
        self.model = model(conf)
        self.path = data_path
        self.conf = conf
        self.params = conf['training']
        optimizer = self.params['optimizer'] if optimizer is None else optimizer
        self.model.compile(optimizer)
        self.answers = self.load('answers') # self.load('generated')
        self._vocab = None
        self._reverse_vocab = None
        self._eval_sets = None",_2483.py,8,"conf = json.load(open(conf, 'rb'))","with open(conf, 'rb') as my_f:
    conf = json.load(my_f)"
https://github.com/wkentaro/labelme/tree/master/tests/labelme_tests/utils_tests/util.py,"def get_img_and_data():
    json_file = osp.join(data_dir, ""annotated_with_data/apc2016_obj3.json"")
    data = json.load(open(json_file))
    img_b64 = data[""imageData""]
    img = image_module.img_b64_to_arr(img_b64)
    return img, data",_2605.py,3,data = json.load(open(json_file)),"with open(json_file) as my_f:
    data = json.load(my_f)
    img_b64 = data['imageData']
    img = image_module.img_b64_to_arr(img_b64)
    return (img, data)"
https://github.com/kakao/khaiii/tree/master/rsc/bin/compile_model.py,"def _load_cfg_rsc(rsc_src: str, model_size: str) -> Tuple[Namespace, Resource]:
    """"""
    load config and resource from source directory
    Args:
        rsc_src:  source directory
        model_size:  model size (base|large)
    Returns:
        config
        resource
    """"""
    file_path = '{}/{}.config.json'.format(rsc_src, model_size)
    cfg_dic = json.load(open(file_path, 'r', encoding='UTF-8'))
    logging.info('config: %s', json.dumps(cfg_dic, indent=4, sort_keys=True))
    cfg = Namespace()
    for key, val in cfg_dic.items():
        setattr(cfg, key, val)
    setattr(cfg, 'rsc_src', rsc_src)
    rsc = Resource(cfg)
    return cfg, rsc",_2622.py,12,"cfg_dic = json.load(open(file_path, 'r', encoding='UTF-8'))","with open(file_path, 'r', encoding='UTF-8') as my_f:
    cfg_dic = json.load(my_f)
    logging.info('config: %s', json.dumps(cfg_dic, indent=4, sort_keys=True))
    cfg = Namespace()
    for (key, val) in cfg_dic.items():
        setattr(cfg, key, val)
    setattr(cfg, 'rsc_src', rsc_src)
    rsc = Resource(cfg)
    return (cfg, rsc)"
https://github.com/LoRexxar/Kunlun-M/tree/master/core/detection.py,"def count_java_line(filename):
        count = {'count_code': 0, 'count_blank': 0, 'count_pound': 0}
        fi = open(filename, 'r')
        file_line = fi.readline()
        while fi.tell() != os.path.getsize(filename):
            file_line = file_line.lstrip()
            if len(file_line) == 0:
                count['count_blank'] += 1
            elif file_line.startswith('//'):
                count['count_pound'] += 1
            elif file_line.count('/*') == 1 and file_line.count('*/') == 1:
                if file_line.startswith('/*'):
                    count['count_pound'] += 1
                else:
                    count['count_code'] += 1
            elif file_line.count('/*') == 1 and file_line.count('*/') == 0:
                if file_line.startswith('/*'):
                    count['count_pound'] += 1
                    while True:
                        file_line = fi.readline()
                        if len(file_line) == 0 or file_line == ""\n"":
                            count['count_blank'] += 1
                        else:
                            count['count_pound'] += 1
                        if file_line.endswith('*/\n'):
                            break
                else:
                    count['count_code'] += 1
                    while True:
                        file_line = fi.readline()
                        if len(file_line) == 0 or file_line == ""\n"":
                            count['count_blank'] += 1
                        else:
                            count['count_code'] += 1
                        if file_line.find('*/'):
                            break
            else:
                count['count_code'] += 1
            file_line = fi.readline()
        fi.close()
        return count",_2624.py,3,"fi = open(filename, 'r')","with open(filename, 'r') as fi:
    file_line = fi.readline()
    while fi.tell() != os.path.getsize(filename):
        file_line = file_line.lstrip()
        if len(file_line) == 0:
            count['count_blank'] += 1
        elif file_line.startswith('//'):
            count['count_pound'] += 1
        elif file_line.count('/*') == 1 and file_line.count('*/') == 1:
            if file_line.startswith('/*'):
                count['count_pound'] += 1
            else:
                count['count_code'] += 1
        elif file_line.count('/*') == 1 and file_line.count('*/') == 0:
            if file_line.startswith('/*'):
                count['count_pound'] += 1
                while True:
                    file_line = fi.readline()
                    if len(file_line) == 0 or file_line == '\n':
                        count['count_blank'] += 1
                    else:
                        count['count_pound'] += 1
                    if file_line.endswith('*/\n'):
                        break
            else:
                count['count_code'] += 1
                while True:
                    file_line = fi.readline()
                    if len(file_line) == 0 or file_line == '\n':
                        count['count_blank'] += 1
                    else:
                        count['count_code'] += 1
                    if file_line.find('*/'):
                        break
        else:
            count['count_code'] += 1
        file_line = fi.readline()
    
    pass
    return count"
https://github.com/hfaran/slack-export-viewer/tree/master/slackviewer/cli.py,"def export(archive_dir):
    css = pkgutil.get_data('slackviewer', 'static/viewer.css').decode('utf-8')
    tmpl = Environment(loader=PackageLoader('slackviewer')).get_template(""export_single.html"")
    export_file_info = get_export_info(archive_dir)
    r = Reader(export_file_info[""readable_path""])
    channel_list = sorted(
        [{""channel_name"": k, ""messages"": v} for (k, v) in r.compile_channels().items()],
        key=lambda d: d[""channel_name""]
    )

    html = tmpl.render(
        css=css,
        generated_on=datetime.now(),
        workspace_name=export_file_info[""workspace_name""],
        source_file=export_file_info[""basename""],
        channels=channel_list
    )
    outfile = open(export_file_info[""stripped_name""] + '.html', 'w')
    outfile.write(html.encode('utf-8'))",_2986.py,18,"outfile = open(export_file_info['stripped_name'] + '.html', 'w')","with open(export_file_info['stripped_name'] + '.html', 'w') as outfile:
    outfile.write(html.encode('utf-8'))"
https://github.com/songluyi/crawl_wechat/tree/master//new_crawl_wechat.py,"def change_txt(self):
        f=open('Response.txt','r',encoding='gbk',errors='ignore')
        data=f.readlines()
        new_data=[]
        file_write_new=open('New_Response.txt','wb')
        for i in data:
            i=str(i).replace(""b'"",'').replace(""'"",'')
            # i.replace('\x00','')
            hope=''.join(list(filter(lambda x: x in string.printable, i)))
            if hope.startswith('#')or not hope.split():
                continue
            file_write_new.write(bytes(hope,encoding='utf-8'))
            # print(bytes(hope,encoding='utf-8'))
        file_write_new.close()
        f.close()",_3120.py,2,"f = open('Response.txt', 'r', encoding='gbk', errors='ignore')","with open('Response.txt', 'r', encoding='gbk', errors='ignore') as f:
    data = f.readlines()
    new_data = []
    file_write_new = open('New_Response.txt', 'wb')
    for i in data:
        i = str(i).replace(""b'"", '').replace(""'"", '')
        hope = ''.join(list(filter(lambda x: x in string.printable, i)))
        if hope.startswith('#') or not hope.split():
            continue
        file_write_new.write(bytes(hope, encoding='utf-8'))
    file_write_new.close()
    
    pass"
https://github.com/songluyi/crawl_wechat/tree/master//new_crawl_wechat.py,"def change_txt(self):
        f=open('Response.txt','r',encoding='gbk',errors='ignore')
        data=f.readlines()
        new_data=[]
        file_write_new=open('New_Response.txt','wb')
        for i in data:
            i=str(i).replace(""b'"",'').replace(""'"",'')
            # i.replace('\x00','')
            hope=''.join(list(filter(lambda x: x in string.printable, i)))
            if hope.startswith('#')or not hope.split():
                continue
            file_write_new.write(bytes(hope,encoding='utf-8'))
            # print(bytes(hope,encoding='utf-8'))
        file_write_new.close()
        f.close()",_3120.py,5,"file_write_new = open('New_Response.txt', 'wb')","with open('New_Response.txt', 'wb') as file_write_new:
    for i in data:
        i = str(i).replace(""b'"", '').replace(""'"", '')
        hope = ''.join(list(filter(lambda x: x in string.printable, i)))
        if hope.startswith('#') or not hope.split():
            continue
        file_write_new.write(bytes(hope, encoding='utf-8'))
    
    pass
    f.close()"
https://github.com/ChenRocks/UNITER/tree/master//inf_re.py,"def write_to_tmp(txt, tmp_file):
    if tmp_file:
        f = open(tmp_file, ""a"")
        f.write(txt)",_3154.py,3,"f = open(tmp_file, 'a')","with open(tmp_file, 'a') as f:
    f.write(txt)"
https://github.com/lunixbochs/SublimeXiki/tree/master/lib/util.py,"def tmpdir(cmd, files, filename, code):
    filename = os.path.split(filename)[1]
    d = tempfile.mkdtemp()

    for f in files:
        try: os.makedirs(os.path.join(d, os.path.split(f)[0]))
        except: pass

        target = os.path.join(d, f)
        if os.path.split(target)[1] == filename:
            # source file hasn't been saved since change, so update it from our live buffer
            f = open(target, 'wb')
            f.write(code)
            f.close()
        else:
            shutil.copyfile(f, target)

    os.chdir(d)
    out = popen(cmd)
    if out:
        out = out.communicate()
        out = combine_output(out, '\n')

        # filter results from build to just this filename
        # no guarantee all languages are as nice about this as Go
        # may need to improve later or just defer to communicate()
        out = '\n'.join([
            line for line in out.split('\n') if filename in line.split(':', 1)[0]
        ])
    else:
        out = ''

    shutil.rmtree(d, True)
    return out",_3621.py,12,"f = open(target, 'wb')","with open(target, 'wb') as f:
    f.write(code)"
https://github.com/facebookresearch/ic_gan/tree/master/inference/test.py,"if __name__ == ""__main__"":
    parser = biggan_utils.prepare_parser()
    parser = biggan_utils.add_sample_parser(parser)
    parser = inference_utils.add_backbone_parser(parser)
    config = vars(parser.parse_args())
    config[""n_classes""] = 1000
    if config[""json_config""] != """":
        data = json.load(open(config[""json_config""]))
        for key in data.keys():
            if ""exp_name"" in key:
                config[""experiment_name""] = data[key]
            else:
                config[key] = data[key]
    else:
        print(""No json file to load configuration from"")

    tester = Tester(config)

    if config[""run_setup""] == ""local_debug"":  # or LOCAL:
        tester()
    else:
        executor = submitit.SlurmExecutor(
            folder=config[""slurm_logdir""], max_num_timeout=10
        )
        executor.update_parameters(
            gpus_per_node=1,
            partition=config[""partition""],
            cpus_per_task=8,
            mem=128000,
            time=30,
            job_name=""testing_"" + config[""experiment_name""],
        )
        executor.submit(tester)
        import time

        time.sleep(1)",_3921.py,8,data = json.load(open(config['json_config'])),"with open(config['json_config']) as my_f:
    data = json.load(my_f)
    for key in data.keys():
        if 'exp_name' in key:
            config['experiment_name'] = data[key]
        else:
            config[key] = data[key]"
https://github.com/facebookresearch/ic_gan/tree/master/inference/test.py,"if __name__ == ""__main__"":
    parser = biggan_utils.prepare_parser()
    parser = biggan_utils.add_sample_parser(parser)
    parser = inference_utils.add_backbone_parser(parser)
    config = vars(parser.parse_args())
    config[""n_classes""] = 1000
    if config[""json_config""] != """":
        data = json.load(open(config[""json_config""]))
        for key in data.keys():
            if ""exp_name"" in key:
                config[""experiment_name""] = data[key]
            else:
                config[key] = data[key]
    else:
        print(""No json file to load configuration from"")

    tester = Tester(config)

    if config[""run_setup""] == ""local_debug"":  # or LOCAL:
        tester()
    else:
        executor = submitit.SlurmExecutor(
            folder=config[""slurm_logdir""], max_num_timeout=10
        )
        executor.update_parameters(
            gpus_per_node=1,
            partition=config[""partition""],
            cpus_per_task=8,
            mem=128000,
            time=30,
            job_name=""testing_"" + config[""experiment_name""],
        )
        executor.submit(tester)
        import time

        time.sleep(1)",_3921.py,8,data = json.load(open(config['json_config'])),"with open(config['json_config']) as my_f:
    data = json.load(my_f)"
https://github.com/yenchenlin/DeepLearningFlappyBird/tree/master//deep_q_network.py,"def trainNetwork(s, readout, h_fc1, sess):
    # define the cost function
    a = tf.placeholder(""float"", [None, ACTIONS])
    y = tf.placeholder(""float"", [None])
    readout_action = tf.reduce_sum(tf.multiply(readout, a), reduction_indices=1)
    cost = tf.reduce_mean(tf.square(y - readout_action))
    train_step = tf.train.AdamOptimizer(1e-6).minimize(cost)

    # open up a game state to communicate with emulator
    game_state = game.GameState()

    # store the previous observations in replay memory
    D = deque()

    # printing
    a_file = open(""logs_"" + GAME + ""/readout.txt"", 'w')
    h_file = open(""logs_"" + GAME + ""/hidden.txt"", 'w')

    # get the first state by doing nothing and preprocess the image to 80x80x4
    do_nothing = np.zeros(ACTIONS)
    do_nothing[0] = 1
    x_t, r_0, terminal = game_state.frame_step(do_nothing)
    x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)
    ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)
    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)

    # saving and loading networks
    saver = tf.train.Saver()
    sess.run(tf.initialize_all_variables())
    checkpoint = tf.train.get_checkpoint_state(""saved_networks"")
    if checkpoint and checkpoint.model_checkpoint_path:
        saver.restore(sess, checkpoint.model_checkpoint_path)
        print(""Successfully loaded:"", checkpoint.model_checkpoint_path)
    else:
        print(""Could not find old network weights"")

    # start training
    epsilon = INITIAL_EPSILON
    t = 0
    while ""flappy bird"" != ""angry bird"":
        # choose an action epsilon greedily
        readout_t = readout.eval(feed_dict={s : [s_t]})[0]
        a_t = np.zeros([ACTIONS])
        action_index = 0
        if t % FRAME_PER_ACTION == 0:
            if random.random() <= epsilon:
                print(""----------Random Action----------"")
                action_index = random.randrange(ACTIONS)
                a_t[random.randrange(ACTIONS)] = 1
            else:
                action_index = np.argmax(readout_t)
                a_t[action_index] = 1
        else:
            a_t[0] = 1 # do nothing

        # scale down epsilon
        if epsilon > FINAL_EPSILON and t > OBSERVE:
            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE

        # run the selected action and observe next state and reward
        x_t1_colored, r_t, terminal = game_state.frame_step(a_t)
        x_t1 = cv2.cvtColor(cv2.resize(x_t1_colored, (80, 80)), cv2.COLOR_BGR2GRAY)
        ret, x_t1 = cv2.threshold(x_t1, 1, 255, cv2.THRESH_BINARY)
        x_t1 = np.reshape(x_t1, (80, 80, 1))
        #s_t1 = np.append(x_t1, s_t[:,:,1:], axis = 2)
        s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)

        # store the transition in D
        D.append((s_t, a_t, r_t, s_t1, terminal))
        if len(D) > REPLAY_MEMORY:
            D.popleft()

        # only train if done observing
        if t > OBSERVE:
            # sample a minibatch to train on
            minibatch = random.sample(D, BATCH)

            # get the batch variables
            s_j_batch = [d[0] for d in minibatch]
            a_batch = [d[1] for d in minibatch]
            r_batch = [d[2] for d in minibatch]
            s_j1_batch = [d[3] for d in minibatch]

            y_batch = []
            readout_j1_batch = readout.eval(feed_dict = {s : s_j1_batch})
            for i in range(0, len(minibatch)):
                terminal = minibatch[i][4]
                # if terminal, only equals reward
                if terminal:
                    y_batch.append(r_batch[i])
                else:
                    y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))

            # perform gradient step
            train_step.run(feed_dict = {
                y : y_batch,
                a : a_batch,
                s : s_j_batch}
            )

        # update the old values
        s_t = s_t1
        t += 1

        # save progress every 10000 iterations
        if t % 10000 == 0:
            saver.save(sess, 'saved_networks/' + GAME + '-dqn', global_step = t)

        # print info
        state = """"
        if t <= OBSERVE:
            state = ""observe""
        elif t > OBSERVE and t <= OBSERVE + EXPLORE:
            state = ""explore""
        else:
            state = ""train""

        print(""TIMESTEP"", t, ""/ STATE"", state, \
            ""/ EPSILON"", epsilon, ""/ ACTION"", action_index, ""/ REWARD"", r_t, \
            ""/ Q_MAX %e"" % np.max(readout_t))
        # write info to files
        '''
        if t % 10000 <= 100:
            a_file.write("","".join([str(x) for x in readout_t]) + '\n')
            h_file.write("","".join([str(x) for x in h_fc1.eval(feed_dict={s:[s_t]})[0]]) + '\n')
            cv2.imwrite(""logs_tetris/frame"" + str(t) + "".png"", x_t1)
        '''",_4107.py,16,"a_file = open('logs_' + GAME + '/readout.txt', 'w')","with open('logs_' + GAME + '/readout.txt', 'w') as a_file:
    h_file = open('logs_' + GAME + '/hidden.txt', 'w')
    do_nothing = np.zeros(ACTIONS)
    do_nothing[0] = 1
    (x_t, r_0, terminal) = game_state.frame_step(do_nothing)
    x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)
    (ret, x_t) = cv2.threshold(x_t, 1, 255, cv2.THRESH_BINARY)
    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)
    saver = tf.train.Saver()
    sess.run(tf.initialize_all_variables())
    checkpoint = tf.train.get_checkpoint_state('saved_networks')
    if checkpoint and checkpoint.model_checkpoint_path:
        saver.restore(sess, checkpoint.model_checkpoint_path)
        print('Successfully loaded:', checkpoint.model_checkpoint_path)
    else:
        print('Could not find old network weights')
    epsilon = INITIAL_EPSILON
    t = 0
    while 'flappy bird' != 'angry bird':
        readout_t = readout.eval(feed_dict={s: [s_t]})[0]
        a_t = np.zeros([ACTIONS])
        action_index = 0
        if t % FRAME_PER_ACTION == 0:
            if random.random() <= epsilon:
                print('----------Random Action----------')
                action_index = random.randrange(ACTIONS)
                a_t[random.randrange(ACTIONS)] = 1
            else:
                action_index = np.argmax(readout_t)
                a_t[action_index] = 1
        else:
            a_t[0] = 1
        if epsilon > FINAL_EPSILON and t > OBSERVE:
            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE
        (x_t1_colored, r_t, terminal) = game_state.frame_step(a_t)
        x_t1 = cv2.cvtColor(cv2.resize(x_t1_colored, (80, 80)), cv2.COLOR_BGR2GRAY)
        (ret, x_t1) = cv2.threshold(x_t1, 1, 255, cv2.THRESH_BINARY)
        x_t1 = np.reshape(x_t1, (80, 80, 1))
        s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)
        D.append((s_t, a_t, r_t, s_t1, terminal))
        if len(D) > REPLAY_MEMORY:
            D.popleft()
        if t > OBSERVE:
            minibatch = random.sample(D, BATCH)
            s_j_batch = [d[0] for d in minibatch]
            a_batch = [d[1] for d in minibatch]
            r_batch = [d[2] for d in minibatch]
            s_j1_batch = [d[3] for d in minibatch]
            y_batch = []
            readout_j1_batch = readout.eval(feed_dict={s: s_j1_batch})
            for i in range(0, len(minibatch)):
                terminal = minibatch[i][4]
                if terminal:
                    y_batch.append(r_batch[i])
                else:
                    y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))
            train_step.run(feed_dict={y: y_batch, a: a_batch, s: s_j_batch})
        s_t = s_t1
        t += 1
        if t % 10000 == 0:
            saver.save(sess, 'saved_networks/' + GAME + '-dqn', global_step=t)
        state = ''
        if t <= OBSERVE:
            state = 'observe'
        elif t > OBSERVE and t <= OBSERVE + EXPLORE:
            state = 'explore'
        else:
            state = 'train'
        print('TIMESTEP', t, '/ STATE', state, '/ EPSILON', epsilon, '/ ACTION', action_index, '/ REWARD', r_t, '/ Q_MAX %e' % np.max(readout_t))
        '\n        if t % 10000 <= 100:\n            a_file.write("","".join([str(x) for x in readout_t]) + \'\n\')\n            h_file.write("","".join([str(x) for x in h_fc1.eval(feed_dict={s:[s_t]})[0]]) + \'\n\')\n            cv2.imwrite(""logs_tetris/frame"" + str(t) + "".png"", x_t1)\n        '"
https://github.com/yenchenlin/DeepLearningFlappyBird/tree/master//deep_q_network.py,"def trainNetwork(s, readout, h_fc1, sess):
    # define the cost function
    a = tf.placeholder(""float"", [None, ACTIONS])
    y = tf.placeholder(""float"", [None])
    readout_action = tf.reduce_sum(tf.multiply(readout, a), reduction_indices=1)
    cost = tf.reduce_mean(tf.square(y - readout_action))
    train_step = tf.train.AdamOptimizer(1e-6).minimize(cost)

    # open up a game state to communicate with emulator
    game_state = game.GameState()

    # store the previous observations in replay memory
    D = deque()

    # printing
    a_file = open(""logs_"" + GAME + ""/readout.txt"", 'w')
    h_file = open(""logs_"" + GAME + ""/hidden.txt"", 'w')

    # get the first state by doing nothing and preprocess the image to 80x80x4
    do_nothing = np.zeros(ACTIONS)
    do_nothing[0] = 1
    x_t, r_0, terminal = game_state.frame_step(do_nothing)
    x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)
    ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)
    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)

    # saving and loading networks
    saver = tf.train.Saver()
    sess.run(tf.initialize_all_variables())
    checkpoint = tf.train.get_checkpoint_state(""saved_networks"")
    if checkpoint and checkpoint.model_checkpoint_path:
        saver.restore(sess, checkpoint.model_checkpoint_path)
        print(""Successfully loaded:"", checkpoint.model_checkpoint_path)
    else:
        print(""Could not find old network weights"")

    # start training
    epsilon = INITIAL_EPSILON
    t = 0
    while ""flappy bird"" != ""angry bird"":
        # choose an action epsilon greedily
        readout_t = readout.eval(feed_dict={s : [s_t]})[0]
        a_t = np.zeros([ACTIONS])
        action_index = 0
        if t % FRAME_PER_ACTION == 0:
            if random.random() <= epsilon:
                print(""----------Random Action----------"")
                action_index = random.randrange(ACTIONS)
                a_t[random.randrange(ACTIONS)] = 1
            else:
                action_index = np.argmax(readout_t)
                a_t[action_index] = 1
        else:
            a_t[0] = 1 # do nothing

        # scale down epsilon
        if epsilon > FINAL_EPSILON and t > OBSERVE:
            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE

        # run the selected action and observe next state and reward
        x_t1_colored, r_t, terminal = game_state.frame_step(a_t)
        x_t1 = cv2.cvtColor(cv2.resize(x_t1_colored, (80, 80)), cv2.COLOR_BGR2GRAY)
        ret, x_t1 = cv2.threshold(x_t1, 1, 255, cv2.THRESH_BINARY)
        x_t1 = np.reshape(x_t1, (80, 80, 1))
        #s_t1 = np.append(x_t1, s_t[:,:,1:], axis = 2)
        s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)

        # store the transition in D
        D.append((s_t, a_t, r_t, s_t1, terminal))
        if len(D) > REPLAY_MEMORY:
            D.popleft()

        # only train if done observing
        if t > OBSERVE:
            # sample a minibatch to train on
            minibatch = random.sample(D, BATCH)

            # get the batch variables
            s_j_batch = [d[0] for d in minibatch]
            a_batch = [d[1] for d in minibatch]
            r_batch = [d[2] for d in minibatch]
            s_j1_batch = [d[3] for d in minibatch]

            y_batch = []
            readout_j1_batch = readout.eval(feed_dict = {s : s_j1_batch})
            for i in range(0, len(minibatch)):
                terminal = minibatch[i][4]
                # if terminal, only equals reward
                if terminal:
                    y_batch.append(r_batch[i])
                else:
                    y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))

            # perform gradient step
            train_step.run(feed_dict = {
                y : y_batch,
                a : a_batch,
                s : s_j_batch}
            )

        # update the old values
        s_t = s_t1
        t += 1

        # save progress every 10000 iterations
        if t % 10000 == 0:
            saver.save(sess, 'saved_networks/' + GAME + '-dqn', global_step = t)

        # print info
        state = """"
        if t <= OBSERVE:
            state = ""observe""
        elif t > OBSERVE and t <= OBSERVE + EXPLORE:
            state = ""explore""
        else:
            state = ""train""

        print(""TIMESTEP"", t, ""/ STATE"", state, \
            ""/ EPSILON"", epsilon, ""/ ACTION"", action_index, ""/ REWARD"", r_t, \
            ""/ Q_MAX %e"" % np.max(readout_t))
        # write info to files
        '''
        if t % 10000 <= 100:
            a_file.write("","".join([str(x) for x in readout_t]) + '\n')
            h_file.write("","".join([str(x) for x in h_fc1.eval(feed_dict={s:[s_t]})[0]]) + '\n')
            cv2.imwrite(""logs_tetris/frame"" + str(t) + "".png"", x_t1)
        '''",_4107.py,17,"h_file = open('logs_' + GAME + '/hidden.txt', 'w')","with open('logs_' + GAME + '/hidden.txt', 'w') as h_file:
    do_nothing = np.zeros(ACTIONS)
    do_nothing[0] = 1
    (x_t, r_0, terminal) = game_state.frame_step(do_nothing)
    x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)
    (ret, x_t) = cv2.threshold(x_t, 1, 255, cv2.THRESH_BINARY)
    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)
    saver = tf.train.Saver()
    sess.run(tf.initialize_all_variables())
    checkpoint = tf.train.get_checkpoint_state('saved_networks')
    if checkpoint and checkpoint.model_checkpoint_path:
        saver.restore(sess, checkpoint.model_checkpoint_path)
        print('Successfully loaded:', checkpoint.model_checkpoint_path)
    else:
        print('Could not find old network weights')
    epsilon = INITIAL_EPSILON
    t = 0
    while 'flappy bird' != 'angry bird':
        readout_t = readout.eval(feed_dict={s: [s_t]})[0]
        a_t = np.zeros([ACTIONS])
        action_index = 0
        if t % FRAME_PER_ACTION == 0:
            if random.random() <= epsilon:
                print('----------Random Action----------')
                action_index = random.randrange(ACTIONS)
                a_t[random.randrange(ACTIONS)] = 1
            else:
                action_index = np.argmax(readout_t)
                a_t[action_index] = 1
        else:
            a_t[0] = 1
        if epsilon > FINAL_EPSILON and t > OBSERVE:
            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE
        (x_t1_colored, r_t, terminal) = game_state.frame_step(a_t)
        x_t1 = cv2.cvtColor(cv2.resize(x_t1_colored, (80, 80)), cv2.COLOR_BGR2GRAY)
        (ret, x_t1) = cv2.threshold(x_t1, 1, 255, cv2.THRESH_BINARY)
        x_t1 = np.reshape(x_t1, (80, 80, 1))
        s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)
        D.append((s_t, a_t, r_t, s_t1, terminal))
        if len(D) > REPLAY_MEMORY:
            D.popleft()
        if t > OBSERVE:
            minibatch = random.sample(D, BATCH)
            s_j_batch = [d[0] for d in minibatch]
            a_batch = [d[1] for d in minibatch]
            r_batch = [d[2] for d in minibatch]
            s_j1_batch = [d[3] for d in minibatch]
            y_batch = []
            readout_j1_batch = readout.eval(feed_dict={s: s_j1_batch})
            for i in range(0, len(minibatch)):
                terminal = minibatch[i][4]
                if terminal:
                    y_batch.append(r_batch[i])
                else:
                    y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))
            train_step.run(feed_dict={y: y_batch, a: a_batch, s: s_j_batch})
        s_t = s_t1
        t += 1
        if t % 10000 == 0:
            saver.save(sess, 'saved_networks/' + GAME + '-dqn', global_step=t)
        state = ''
        if t <= OBSERVE:
            state = 'observe'
        elif t > OBSERVE and t <= OBSERVE + EXPLORE:
            state = 'explore'
        else:
            state = 'train'
        print('TIMESTEP', t, '/ STATE', state, '/ EPSILON', epsilon, '/ ACTION', action_index, '/ REWARD', r_t, '/ Q_MAX %e' % np.max(readout_t))
        '\n        if t % 10000 <= 100:\n            a_file.write("","".join([str(x) for x in readout_t]) + \'\n\')\n            h_file.write("","".join([str(x) for x in h_fc1.eval(feed_dict={s:[s_t]})[0]]) + \'\n\')\n            cv2.imwrite(""logs_tetris/frame"" + str(t) + "".png"", x_t1)\n        '"
https://github.com/golemhq/golem/tree/master/golem/report/execution_report.py,"def function_test_execution_result(project, execution, timestamp, test_file, test, set_name='',
                                   no_screenshots=False, encode_screenshots=False):
    """"""

    :Args:
      - encode_screenshots: return screenshot files encoded as a base64 string or
                            the screenshot filename (rel to its folder).
      - no_screenshots: convert screenshot values to None
    """"""
    path = execution_report_path(project, execution, timestamp)
    test_json = {
        'has_finished': False
    }
    if has_execution_finished(path):
        json_report = get_execution_data(path)
        for t in json_report['tests']:
            if t['test_file'] == test_file and t['test'] == test and t['set_name'] == set_name:
                test_json = t
                test_json['has_finished'] = True
                break
    else:
        test_json = test_report.get_test_function_report_json(project, execution, timestamp,
                                                              test_file, test, set_name)
        test_json['has_finished'] = False

    test_json['debug_log'] = test_report.get_test_debug_log(project, execution, timestamp,
                                                            test_file, set_name)
    test_json['info_log'] = test_report.get_test_info_log(project, execution, timestamp,
                                                          test_file, set_name)

    if no_screenshots:
        for step in test_json['steps']:
            step['screenshot'] = None
    elif encode_screenshots:
        for step in test_json['steps']:
            if step['screenshot'] is not None:
                image_filename = test_report.screenshot_path(project, execution, timestamp,
                                                             test_file, test, set_name,
                                                             step['screenshot'])
                b64 = base64.b64encode(open(image_filename, ""rb"").read()).decode('utf-8')
                step['screenshot'] = b64
    return test_json",_4136.py,40,"b64 = base64.b64encode(open(image_filename, 'rb').read()).decode('utf-8')","with open(image_filename, 'rb') as my_f:
    b64 = base64.b64encode(my_f.read()).decode('utf-8')
    step['screenshot'] = b64"
https://github.com/golemhq/golem/tree/master/golem/report/execution_report.py,"def function_test_execution_result(project, execution, timestamp, test_file, test, set_name='',
                                   no_screenshots=False, encode_screenshots=False):
    """"""

    :Args:
      - encode_screenshots: return screenshot files encoded as a base64 string or
                            the screenshot filename (rel to its folder).
      - no_screenshots: convert screenshot values to None
    """"""
    path = execution_report_path(project, execution, timestamp)
    test_json = {
        'has_finished': False
    }
    if has_execution_finished(path):
        json_report = get_execution_data(path)
        for t in json_report['tests']:
            if t['test_file'] == test_file and t['test'] == test and t['set_name'] == set_name:
                test_json = t
                test_json['has_finished'] = True
                break
    else:
        test_json = test_report.get_test_function_report_json(project, execution, timestamp,
                                                              test_file, test, set_name)
        test_json['has_finished'] = False

    test_json['debug_log'] = test_report.get_test_debug_log(project, execution, timestamp,
                                                            test_file, set_name)
    test_json['info_log'] = test_report.get_test_info_log(project, execution, timestamp,
                                                          test_file, set_name)

    if no_screenshots:
        for step in test_json['steps']:
            step['screenshot'] = None
    elif encode_screenshots:
        for step in test_json['steps']:
            if step['screenshot'] is not None:
                image_filename = test_report.screenshot_path(project, execution, timestamp,
                                                             test_file, test, set_name,
                                                             step['screenshot'])
                b64 = base64.b64encode(open(image_filename, ""rb"").read()).decode('utf-8')
                step['screenshot'] = b64
    return test_json",_4136.py,40,"b64 = base64.b64encode(open(image_filename, 'rb').read()).decode('utf-8')","with open(image_filename, 'rb') as my_f:
    b64 = base64.b64encode(my_f.read()).decode('utf-8')"
https://github.com/CMA-ES/pycma/tree/master/cma/logger.py,"def downsampling(self, factor=10, first=3, switch=True, verbose=False):
        """"""
        rude downsampling of a `CMADataLogger` data file by `factor`,
        keeping also the first `first` entries. This function is a
        stump and subject to future changes. Return self.

        Arguments
        ---------
           - `factor` -- downsampling factor
           - `first` -- keep first `first` entries
           - `switch` -- switch the new logger to the downsampled logger
                original_name+'down'

        Details
        -------
        ``self.name_prefix+'down'`` files are written

        Example
        -------
        ::

            import cma
            cma.downsampling()  # takes outcma/cma* files
            cma.plot('outcma/cmadown')

        """"""
        newprefix = self.name_prefix + 'down'
        for name in self.file_names:
            with open(newprefix + name + '.dat', 'wt') as f:
                iline = 0
                cwritten = 0
                for line in open(self.name_prefix + name + '.dat'):
                    if iline < first or iline % factor < 1:
                        f.write(line)
                        cwritten += 1
                    iline += 1
            if verbose and iline > first:
                print('%d' % (cwritten) + ' lines written in ' + newprefix + name + '.dat')
        if switch:
            self.name_prefix += 'down'
        return self",_4154.py,32,"for line in open(self.name_prefix + name + '.dat'):
    if iline < first or iline % factor < 1:
        f.write(line)
        cwritten += 1
    iline += 1","with open(self.name_prefix + name + '.dat') as my_f:
    for line in my_f:
        if iline < first or iline % factor < 1:
            f.write(line)
            cwritten += 1
        iline += 1"
https://github.com/CMA-ES/pycma/tree/master/cma/logger.py,"def downsampling(self, factor=10, first=3, switch=True, verbose=False):
        """"""
        rude downsampling of a `CMADataLogger` data file by `factor`,
        keeping also the first `first` entries. This function is a
        stump and subject to future changes. Return self.

        Arguments
        ---------
           - `factor` -- downsampling factor
           - `first` -- keep first `first` entries
           - `switch` -- switch the new logger to the downsampled logger
                original_name+'down'

        Details
        -------
        ``self.name_prefix+'down'`` files are written

        Example
        -------
        ::

            import cma
            cma.downsampling()  # takes outcma/cma* files
            cma.plot('outcma/cmadown')

        """"""
        newprefix = self.name_prefix + 'down'
        for name in self.file_names:
            with open(newprefix + name + '.dat', 'wt') as f:
                iline = 0
                cwritten = 0
                for line in open(self.name_prefix + name + '.dat'):
                    if iline < first or iline % factor < 1:
                        f.write(line)
                        cwritten += 1
                    iline += 1
            if verbose and iline > first:
                print('%d' % (cwritten) + ' lines written in ' + newprefix + name + '.dat')
        if switch:
            self.name_prefix += 'down'
        return self",_4154.py,32,"for line in open(self.name_prefix + name + '.dat'):
    if iline < first or iline % factor < 1:
        f.write(line)
        cwritten += 1
    iline += 1","with open(self.name_prefix + name + '.dat') as my_f:
    for line in my_f:
        if iline < first or iline % factor < 1:
            f.write(line)
            cwritten += 1
        iline += 1"
https://github.com/kakao/khaiii/tree/master/src/main/python/khaiii/resource/vocabulary.py,"def _load(self, path: str, cutoff: int = 1):
        """"""
        load vocabulary from file
        Args:
            path:  file path
            cutoff:  cutoff frequency
        """"""
        append_num = 0
        cutoff_num = 0
        for line in open(path, 'r', encoding='UTF-8'):
            line = line.rstrip('\r\n')
            if not line:
                continue
            try:
                entry, freq = line.split('\t')
                if int(freq) <= cutoff:
                    cutoff_num += 1
                    continue
            except ValueError:
                entry = line
            if entry in self.dic:
                cutoff_num += 1
                continue
            self.dic[entry] = len(self.dic)
            self.rev.append(entry)
            append_num += 1
        logging.info('%s: %d entries, %d cutoff', os.path.basename(path), append_num, cutoff_num)",_4702.py,10,"for line in open(path, 'r', encoding='UTF-8'):
    line = line.rstrip('\r\n')
    if not line:
        continue
    try:
        (entry, freq) = line.split('\t')
        if int(freq) <= cutoff:
            cutoff_num += 1
            continue
    except ValueError:
        entry = line
    if entry in self.dic:
        cutoff_num += 1
        continue
    self.dic[entry] = len(self.dic)
    self.rev.append(entry)
    append_num += 1","with open(path, 'r', encoding='UTF-8') as my_f:
    for line in my_f:
        line = line.rstrip('\r\n')
        if not line:
            continue
        try:
            (entry, freq) = line.split('\t')
            if int(freq) <= cutoff:
                cutoff_num += 1
                continue
        except ValueError:
            entry = line
        if entry in self.dic:
            cutoff_num += 1
            continue
        self.dic[entry] = len(self.dic)
        self.rev.append(entry)
        append_num += 1"
https://github.com/kakao/khaiii/tree/master/src/main/python/khaiii/resource/vocabulary.py,"def _load(self, path: str, cutoff: int = 1):
        """"""
        load vocabulary from file
        Args:
            path:  file path
            cutoff:  cutoff frequency
        """"""
        append_num = 0
        cutoff_num = 0
        for line in open(path, 'r', encoding='UTF-8'):
            line = line.rstrip('\r\n')
            if not line:
                continue
            try:
                entry, freq = line.split('\t')
                if int(freq) <= cutoff:
                    cutoff_num += 1
                    continue
            except ValueError:
                entry = line
            if entry in self.dic:
                cutoff_num += 1
                continue
            self.dic[entry] = len(self.dic)
            self.rev.append(entry)
            append_num += 1
        logging.info('%s: %d entries, %d cutoff', os.path.basename(path), append_num, cutoff_num)",_4702.py,10,"for line in open(path, 'r', encoding='UTF-8'):
    line = line.rstrip('\r\n')
    if not line:
        continue
    try:
        (entry, freq) = line.split('\t')
        if int(freq) <= cutoff:
            cutoff_num += 1
            continue
    except ValueError:
        entry = line
    if entry in self.dic:
        cutoff_num += 1
        continue
    self.dic[entry] = len(self.dic)
    self.rev.append(entry)
    append_num += 1","with open(path, 'r', encoding='UTF-8') as my_f:
    for line in my_f:
        line = line.rstrip('\r\n')
        if not line:
            continue
        try:
            (entry, freq) = line.split('\t')
            if int(freq) <= cutoff:
                cutoff_num += 1
                continue
        except ValueError:
            entry = line
        if entry in self.dic:
            cutoff_num += 1
            continue
        self.dic[entry] = len(self.dic)
        self.rev.append(entry)
        append_num += 1"
https://github.com/aws/sagemaker-python-sdk/tree/master/tests/unit/sagemaker/workflow/test_repack_model_script.py,"def test_repack_with_dependencies(tmp):
    model_name = ""xg-boost-model""
    fake_model_path = os.path.join(tmp, model_name)

    # create a fake model
    open(fake_model_path, ""w"")

    # create model.tar.gz
    model_tar_name = ""model-%s.tar.gz"" % time.time()
    model_tar_location = os.path.join(tmp, model_tar_name)
    with tarfile.open(model_tar_location, mode=""w:gz"") as t:
        t.add(fake_model_path, arcname=model_name)

    # move model.tar.gz to /opt/ml/input/data/training
    Path(""/opt/ml/input/data/training"").mkdir(parents=True, exist_ok=True)
    shutil.move(model_tar_location, os.path.join(""/opt/ml/input/data/training"", model_tar_name))

    # create files that will be added to model.tar.gz
    create_file_tree(
        ""/opt/ml/code"",
        [""inference.py"", ""dependencies/a"", ""bb"", ""dependencies/some/dir/b""],
    )

    # repack
    _repack_model.repack(
        inference_script=""inference.py"",
        model_archive=model_tar_name,
        dependencies=""dependencies/a bb dependencies/some/dir"",
    )

    # /opt/ml/model should now have the original model and the inference script
    assert os.path.exists(os.path.join(""/opt/ml/model"", model_name))
    assert os.path.exists(os.path.join(""/opt/ml/model/code"", ""inference.py""))
    assert os.path.exists(os.path.join(""/opt/ml/model/code/lib"", ""a""))
    assert os.path.exists(os.path.join(""/opt/ml/model/code/lib"", ""bb""))
    assert os.path.exists(os.path.join(""/opt/ml/model/code/lib/dir"", ""b""))",_5116.py,6,"open(fake_model_path, 'w')","with open(fake_model_path, 'w') as my_f:
    my_f
    model_tar_name = 'model-%s.tar.gz' % time.time()
    model_tar_location = os.path.join(tmp, model_tar_name)
    with tarfile.open(model_tar_location, mode='w:gz') as t:
        t.add(fake_model_path, arcname=model_name)
    Path('/opt/ml/input/data/training').mkdir(parents=True, exist_ok=True)
    shutil.move(model_tar_location, os.path.join('/opt/ml/input/data/training', model_tar_name))
    create_file_tree('/opt/ml/code', ['inference.py', 'dependencies/a', 'bb', 'dependencies/some/dir/b'])
    _repack_model.repack(inference_script='inference.py', model_archive=model_tar_name, dependencies='dependencies/a bb dependencies/some/dir')
    assert os.path.exists(os.path.join('/opt/ml/model', model_name))
    assert os.path.exists(os.path.join('/opt/ml/model/code', 'inference.py'))
    assert os.path.exists(os.path.join('/opt/ml/model/code/lib', 'a'))
    assert os.path.exists(os.path.join('/opt/ml/model/code/lib', 'bb'))
    assert os.path.exists(os.path.join('/opt/ml/model/code/lib/dir', 'b'))"
https://github.com/Blazemeter/taurus/tree/master/tests/unit/test_swagger2yaml.py,"def test_convert_security_apikey_multiscenarios(self):
        source = RESOURCES_DIR + ""/swagger/auth-key.json""
        expected = RESOURCES_DIR + ""/swagger/auth-key-multiscenarios-converted.yaml""
        result = self._get_tmp()
        options = FakeOptions(file_name=result, scenarios_from_paths=True)
        self.configure(options, source)
        self.tool.process()
        actual = yaml.full_load(open(result).read())
        expected = yaml.full_load(open(expected).read())
        self.assertEqual(actual, expected)",_5448.py,8,actual = yaml.full_load(open(result).read()),"with open(result) as my_f:
    actual = yaml.full_load(my_f.read())
    expected = yaml.full_load(open(expected).read())
    self.assertEqual(actual, expected)"
https://github.com/Blazemeter/taurus/tree/master/tests/unit/test_swagger2yaml.py,"def test_convert_security_apikey_multiscenarios(self):
        source = RESOURCES_DIR + ""/swagger/auth-key.json""
        expected = RESOURCES_DIR + ""/swagger/auth-key-multiscenarios-converted.yaml""
        result = self._get_tmp()
        options = FakeOptions(file_name=result, scenarios_from_paths=True)
        self.configure(options, source)
        self.tool.process()
        actual = yaml.full_load(open(result).read())
        expected = yaml.full_load(open(expected).read())
        self.assertEqual(actual, expected)",_5448.py,9,expected = yaml.full_load(open(expected).read()),"with open(expected) as my_f:
    expected = yaml.full_load(my_f.read())
    self.assertEqual(actual, expected)"
https://github.com/golemhq/golem/tree/master/golem/report/html_report.py,"def generate_html_report(project, execution, timestamp, destination_folder=None,
                         report_name=None, no_images=False):
    """"""Generate static HTML report.
    Report is generated in <report_directory>/<report_name>
    By default it's generated in <testdir>/projects/<project>/reports/<suite>/<timestamp>
    Default name is 'report.html' and 'report-no-images.html'
    """"""
    execution_directory = exec_report.execution_report_path(project, execution, timestamp)

    if destination_folder is None:
        destination_folder = execution_directory

    if not report_name:
        if no_images:
            report_name = 'report-no-images'
        else:
            report_name = 'report'

    formatted_date = utils.get_date_time_from_timestamp(timestamp)
    app = gui.create_app()
    static_folder = app.static_folder
    # css paths
    css_folder = os.path.join(static_folder, 'css')
    boostrap_css = os.path.join(css_folder, 'bootstrap', 'bootstrap.min.css')
    main_css = os.path.join(css_folder, 'main.css')
    report_css = os.path.join(css_folder, 'report.css')
    # js paths
    js_folder = os.path.join(static_folder, 'js')
    main_js = os.path.join(js_folder, 'main.js')
    jquery_js = os.path.join(js_folder, 'external', 'jquery.min.js')
    datatables_js = os.path.join(js_folder, 'external', 'datatable', 'datatables.min.js')
    bootstrap_js = os.path.join(js_folder, 'external', 'bootstrap.min.js')
    report_execution_js = os.path.join(js_folder, 'report_execution.js')

    css = {
        'bootstrap': open(boostrap_css, encoding='utf-8').read(),
        'main': open(main_css, encoding='utf-8').read(),
        'report': open(report_css, encoding='utf-8').read()
    }
    js = {
        'jquery': open(jquery_js, encoding='utf-8').read(),
        'datatables': open(datatables_js, encoding='utf-8').read(),
        'bootstrap': open(bootstrap_js, encoding='utf-8').read(),
        'main': open(main_js, encoding='utf-8').read(),
        'report_execution': open(report_execution_js).read()
    }

    execution_data = exec_report.get_execution_data(execution_directory)
    detail_test_data = {}
    for test in execution_data['tests']:
        test_detail = exec_report.function_test_execution_result(
            project, execution, timestamp, test['test_file'], test['test'], test['set_name'],
            no_screenshots=no_images, encode_screenshots=True
        )
        # testId is test_file + test + set_name
        test_id = f""{test['test_file']}.{test['test']}""
        if test['set_name']:
            test_id = f""{test_id}.{test['set_name']}""
        detail_test_data[test_id] = test_detail
    with app.app_context():
        html_string = render_template(
            'report/report_execution_static.html', project=project, execution=execution,
            timestamp=timestamp, execution_data=execution_data,
            detail_test_data=detail_test_data, formatted_date=formatted_date,
            css=css, js=js, static=True
        )
    _, file_extension = os.path.splitext(report_name)
    if not file_extension:
        report_name = f'{report_name}.html'
    destination = os.path.join(destination_folder, report_name)

    if not os.path.exists(os.path.dirname(destination)):
        os.makedirs(os.path.dirname(destination), exist_ok=True)

    try:
        with open(destination, 'w', encoding='utf-8') as f:
            f.write(html_string)
    except IOError as e:
        if e.errno == errno.EACCES:
            print(f'ERROR: cannot write to {destination}, PermissionError (Errno 13)')
        else:
            print(f'ERROR: There was an error writing to {destination}')

    return html_string",_5821.py,36,"css = {'bootstrap': open(boostrap_css, encoding='utf-8').read(), 'main': open(main_css, encoding='utf-8').read(), 'report': open(report_css, encoding='utf-8').read()}","with open(boostrap_css, encoding='utf-8') as my_f:
    css = {'bootstrap': my_f.read(), 'main': open(main_css, encoding='utf-8').read(), 'report': open(report_css, encoding='utf-8').read()}
    js = {'jquery': open(jquery_js, encoding='utf-8').read(), 'datatables': open(datatables_js, encoding='utf-8').read(), 'bootstrap': open(bootstrap_js, encoding='utf-8').read(), 'main': open(main_js, encoding='utf-8').read(), 'report_execution': open(report_execution_js).read()}
    execution_data = exec_report.get_execution_data(execution_directory)
    detail_test_data = {}
    for test in execution_data['tests']:
        test_detail = exec_report.function_test_execution_result(project, execution, timestamp, test['test_file'], test['test'], test['set_name'], no_screenshots=no_images, encode_screenshots=True)
        test_id = f""{test['test_file']}.{test['test']}""
        if test['set_name']:
            test_id = f""{test_id}.{test['set_name']}""
        detail_test_data[test_id] = test_detail
    with app.app_context():
        html_string = render_template('report/report_execution_static.html', project=project, execution=execution, timestamp=timestamp, execution_data=execution_data, detail_test_data=detail_test_data, formatted_date=formatted_date, css=css, js=js, static=True)
    (_, file_extension) = os.path.splitext(report_name)
    if not file_extension:
        report_name = f'{report_name}.html'
    destination = os.path.join(destination_folder, report_name)
    if not os.path.exists(os.path.dirname(destination)):
        os.makedirs(os.path.dirname(destination), exist_ok=True)
    try:
        with open(destination, 'w', encoding='utf-8') as f:
            f.write(html_string)
    except IOError as e:
        if e.errno == errno.EACCES:
            print(f'ERROR: cannot write to {destination}, PermissionError (Errno 13)')
        else:
            print(f'ERROR: There was an error writing to {destination}')
    return html_string"
https://github.com/golemhq/golem/tree/master/golem/report/html_report.py,"def generate_html_report(project, execution, timestamp, destination_folder=None,
                         report_name=None, no_images=False):
    """"""Generate static HTML report.
    Report is generated in <report_directory>/<report_name>
    By default it's generated in <testdir>/projects/<project>/reports/<suite>/<timestamp>
    Default name is 'report.html' and 'report-no-images.html'
    """"""
    execution_directory = exec_report.execution_report_path(project, execution, timestamp)

    if destination_folder is None:
        destination_folder = execution_directory

    if not report_name:
        if no_images:
            report_name = 'report-no-images'
        else:
            report_name = 'report'

    formatted_date = utils.get_date_time_from_timestamp(timestamp)
    app = gui.create_app()
    static_folder = app.static_folder
    # css paths
    css_folder = os.path.join(static_folder, 'css')
    boostrap_css = os.path.join(css_folder, 'bootstrap', 'bootstrap.min.css')
    main_css = os.path.join(css_folder, 'main.css')
    report_css = os.path.join(css_folder, 'report.css')
    # js paths
    js_folder = os.path.join(static_folder, 'js')
    main_js = os.path.join(js_folder, 'main.js')
    jquery_js = os.path.join(js_folder, 'external', 'jquery.min.js')
    datatables_js = os.path.join(js_folder, 'external', 'datatable', 'datatables.min.js')
    bootstrap_js = os.path.join(js_folder, 'external', 'bootstrap.min.js')
    report_execution_js = os.path.join(js_folder, 'report_execution.js')

    css = {
        'bootstrap': open(boostrap_css, encoding='utf-8').read(),
        'main': open(main_css, encoding='utf-8').read(),
        'report': open(report_css, encoding='utf-8').read()
    }
    js = {
        'jquery': open(jquery_js, encoding='utf-8').read(),
        'datatables': open(datatables_js, encoding='utf-8').read(),
        'bootstrap': open(bootstrap_js, encoding='utf-8').read(),
        'main': open(main_js, encoding='utf-8').read(),
        'report_execution': open(report_execution_js).read()
    }

    execution_data = exec_report.get_execution_data(execution_directory)
    detail_test_data = {}
    for test in execution_data['tests']:
        test_detail = exec_report.function_test_execution_result(
            project, execution, timestamp, test['test_file'], test['test'], test['set_name'],
            no_screenshots=no_images, encode_screenshots=True
        )
        # testId is test_file + test + set_name
        test_id = f""{test['test_file']}.{test['test']}""
        if test['set_name']:
            test_id = f""{test_id}.{test['set_name']}""
        detail_test_data[test_id] = test_detail
    with app.app_context():
        html_string = render_template(
            'report/report_execution_static.html', project=project, execution=execution,
            timestamp=timestamp, execution_data=execution_data,
            detail_test_data=detail_test_data, formatted_date=formatted_date,
            css=css, js=js, static=True
        )
    _, file_extension = os.path.splitext(report_name)
    if not file_extension:
        report_name = f'{report_name}.html'
    destination = os.path.join(destination_folder, report_name)

    if not os.path.exists(os.path.dirname(destination)):
        os.makedirs(os.path.dirname(destination), exist_ok=True)

    try:
        with open(destination, 'w', encoding='utf-8') as f:
            f.write(html_string)
    except IOError as e:
        if e.errno == errno.EACCES:
            print(f'ERROR: cannot write to {destination}, PermissionError (Errno 13)')
        else:
            print(f'ERROR: There was an error writing to {destination}')

    return html_string",_5821.py,37,"css = {'bootstrap': open(boostrap_css, encoding='utf-8').read(), 'main': open(main_css, encoding='utf-8').read(), 'report': open(report_css, encoding='utf-8').read()}","with open(main_css, encoding='utf-8') as my_f:
    css = {'bootstrap': open(boostrap_css, encoding='utf-8').read(), 'main': my_f.read(), 'report': open(report_css, encoding='utf-8').read()}
    js = {'jquery': open(jquery_js, encoding='utf-8').read(), 'datatables': open(datatables_js, encoding='utf-8').read(), 'bootstrap': open(bootstrap_js, encoding='utf-8').read(), 'main': open(main_js, encoding='utf-8').read(), 'report_execution': open(report_execution_js).read()}
    execution_data = exec_report.get_execution_data(execution_directory)
    detail_test_data = {}
    for test in execution_data['tests']:
        test_detail = exec_report.function_test_execution_result(project, execution, timestamp, test['test_file'], test['test'], test['set_name'], no_screenshots=no_images, encode_screenshots=True)
        test_id = f""{test['test_file']}.{test['test']}""
        if test['set_name']:
            test_id = f""{test_id}.{test['set_name']}""
        detail_test_data[test_id] = test_detail
    with app.app_context():
        html_string = render_template('report/report_execution_static.html', project=project, execution=execution, timestamp=timestamp, execution_data=execution_data, detail_test_data=detail_test_data, formatted_date=formatted_date, css=css, js=js, static=True)
    (_, file_extension) = os.path.splitext(report_name)
    if not file_extension:
        report_name = f'{report_name}.html'
    destination = os.path.join(destination_folder, report_name)
    if not os.path.exists(os.path.dirname(destination)):
        os.makedirs(os.path.dirname(destination), exist_ok=True)
    try:
        with open(destination, 'w', encoding='utf-8') as f:
            f.write(html_string)
    except IOError as e:
        if e.errno == errno.EACCES:
            print(f'ERROR: cannot write to {destination}, PermissionError (Errno 13)')
        else:
            print(f'ERROR: There was an error writing to {destination}')
    return html_string"
https://github.com/golemhq/golem/tree/master/golem/report/html_report.py,"def generate_html_report(project, execution, timestamp, destination_folder=None,
                         report_name=None, no_images=False):
    """"""Generate static HTML report.
    Report is generated in <report_directory>/<report_name>
    By default it's generated in <testdir>/projects/<project>/reports/<suite>/<timestamp>
    Default name is 'report.html' and 'report-no-images.html'
    """"""
    execution_directory = exec_report.execution_report_path(project, execution, timestamp)

    if destination_folder is None:
        destination_folder = execution_directory

    if not report_name:
        if no_images:
            report_name = 'report-no-images'
        else:
            report_name = 'report'

    formatted_date = utils.get_date_time_from_timestamp(timestamp)
    app = gui.create_app()
    static_folder = app.static_folder
    # css paths
    css_folder = os.path.join(static_folder, 'css')
    boostrap_css = os.path.join(css_folder, 'bootstrap', 'bootstrap.min.css')
    main_css = os.path.join(css_folder, 'main.css')
    report_css = os.path.join(css_folder, 'report.css')
    # js paths
    js_folder = os.path.join(static_folder, 'js')
    main_js = os.path.join(js_folder, 'main.js')
    jquery_js = os.path.join(js_folder, 'external', 'jquery.min.js')
    datatables_js = os.path.join(js_folder, 'external', 'datatable', 'datatables.min.js')
    bootstrap_js = os.path.join(js_folder, 'external', 'bootstrap.min.js')
    report_execution_js = os.path.join(js_folder, 'report_execution.js')

    css = {
        'bootstrap': open(boostrap_css, encoding='utf-8').read(),
        'main': open(main_css, encoding='utf-8').read(),
        'report': open(report_css, encoding='utf-8').read()
    }
    js = {
        'jquery': open(jquery_js, encoding='utf-8').read(),
        'datatables': open(datatables_js, encoding='utf-8').read(),
        'bootstrap': open(bootstrap_js, encoding='utf-8').read(),
        'main': open(main_js, encoding='utf-8').read(),
        'report_execution': open(report_execution_js).read()
    }

    execution_data = exec_report.get_execution_data(execution_directory)
    detail_test_data = {}
    for test in execution_data['tests']:
        test_detail = exec_report.function_test_execution_result(
            project, execution, timestamp, test['test_file'], test['test'], test['set_name'],
            no_screenshots=no_images, encode_screenshots=True
        )
        # testId is test_file + test + set_name
        test_id = f""{test['test_file']}.{test['test']}""
        if test['set_name']:
            test_id = f""{test_id}.{test['set_name']}""
        detail_test_data[test_id] = test_detail
    with app.app_context():
        html_string = render_template(
            'report/report_execution_static.html', project=project, execution=execution,
            timestamp=timestamp, execution_data=execution_data,
            detail_test_data=detail_test_data, formatted_date=formatted_date,
            css=css, js=js, static=True
        )
    _, file_extension = os.path.splitext(report_name)
    if not file_extension:
        report_name = f'{report_name}.html'
    destination = os.path.join(destination_folder, report_name)

    if not os.path.exists(os.path.dirname(destination)):
        os.makedirs(os.path.dirname(destination), exist_ok=True)

    try:
        with open(destination, 'w', encoding='utf-8') as f:
            f.write(html_string)
    except IOError as e:
        if e.errno == errno.EACCES:
            print(f'ERROR: cannot write to {destination}, PermissionError (Errno 13)')
        else:
            print(f'ERROR: There was an error writing to {destination}')

    return html_string",_5821.py,38,"css = {'bootstrap': open(boostrap_css, encoding='utf-8').read(), 'main': open(main_css, encoding='utf-8').read(), 'report': open(report_css, encoding='utf-8').read()}","with open(report_css, encoding='utf-8') as my_f:
    css = {'bootstrap': open(boostrap_css, encoding='utf-8').read(), 'main': open(main_css, encoding='utf-8').read(), 'report': my_f.read()}
    js = {'jquery': open(jquery_js, encoding='utf-8').read(), 'datatables': open(datatables_js, encoding='utf-8').read(), 'bootstrap': open(bootstrap_js, encoding='utf-8').read(), 'main': open(main_js, encoding='utf-8').read(), 'report_execution': open(report_execution_js).read()}
    execution_data = exec_report.get_execution_data(execution_directory)
    detail_test_data = {}
    for test in execution_data['tests']:
        test_detail = exec_report.function_test_execution_result(project, execution, timestamp, test['test_file'], test['test'], test['set_name'], no_screenshots=no_images, encode_screenshots=True)
        test_id = f""{test['test_file']}.{test['test']}""
        if test['set_name']:
            test_id = f""{test_id}.{test['set_name']}""
        detail_test_data[test_id] = test_detail
    with app.app_context():
        html_string = render_template('report/report_execution_static.html', project=project, execution=execution, timestamp=timestamp, execution_data=execution_data, detail_test_data=detail_test_data, formatted_date=formatted_date, css=css, js=js, static=True)
    (_, file_extension) = os.path.splitext(report_name)
    if not file_extension:
        report_name = f'{report_name}.html'
    destination = os.path.join(destination_folder, report_name)
    if not os.path.exists(os.path.dirname(destination)):
        os.makedirs(os.path.dirname(destination), exist_ok=True)
    try:
        with open(destination, 'w', encoding='utf-8') as f:
            f.write(html_string)
    except IOError as e:
        if e.errno == errno.EACCES:
            print(f'ERROR: cannot write to {destination}, PermissionError (Errno 13)')
        else:
            print(f'ERROR: There was an error writing to {destination}')
    return html_string"
https://github.com/golemhq/golem/tree/master/golem/report/html_report.py,"def generate_html_report(project, execution, timestamp, destination_folder=None,
                         report_name=None, no_images=False):
    """"""Generate static HTML report.
    Report is generated in <report_directory>/<report_name>
    By default it's generated in <testdir>/projects/<project>/reports/<suite>/<timestamp>
    Default name is 'report.html' and 'report-no-images.html'
    """"""
    execution_directory = exec_report.execution_report_path(project, execution, timestamp)

    if destination_folder is None:
        destination_folder = execution_directory

    if not report_name:
        if no_images:
            report_name = 'report-no-images'
        else:
            report_name = 'report'

    formatted_date = utils.get_date_time_from_timestamp(timestamp)
    app = gui.create_app()
    static_folder = app.static_folder
    # css paths
    css_folder = os.path.join(static_folder, 'css')
    boostrap_css = os.path.join(css_folder, 'bootstrap', 'bootstrap.min.css')
    main_css = os.path.join(css_folder, 'main.css')
    report_css = os.path.join(css_folder, 'report.css')
    # js paths
    js_folder = os.path.join(static_folder, 'js')
    main_js = os.path.join(js_folder, 'main.js')
    jquery_js = os.path.join(js_folder, 'external', 'jquery.min.js')
    datatables_js = os.path.join(js_folder, 'external', 'datatable', 'datatables.min.js')
    bootstrap_js = os.path.join(js_folder, 'external', 'bootstrap.min.js')
    report_execution_js = os.path.join(js_folder, 'report_execution.js')

    css = {
        'bootstrap': open(boostrap_css, encoding='utf-8').read(),
        'main': open(main_css, encoding='utf-8').read(),
        'report': open(report_css, encoding='utf-8').read()
    }
    js = {
        'jquery': open(jquery_js, encoding='utf-8').read(),
        'datatables': open(datatables_js, encoding='utf-8').read(),
        'bootstrap': open(bootstrap_js, encoding='utf-8').read(),
        'main': open(main_js, encoding='utf-8').read(),
        'report_execution': open(report_execution_js).read()
    }

    execution_data = exec_report.get_execution_data(execution_directory)
    detail_test_data = {}
    for test in execution_data['tests']:
        test_detail = exec_report.function_test_execution_result(
            project, execution, timestamp, test['test_file'], test['test'], test['set_name'],
            no_screenshots=no_images, encode_screenshots=True
        )
        # testId is test_file + test + set_name
        test_id = f""{test['test_file']}.{test['test']}""
        if test['set_name']:
            test_id = f""{test_id}.{test['set_name']}""
        detail_test_data[test_id] = test_detail
    with app.app_context():
        html_string = render_template(
            'report/report_execution_static.html', project=project, execution=execution,
            timestamp=timestamp, execution_data=execution_data,
            detail_test_data=detail_test_data, formatted_date=formatted_date,
            css=css, js=js, static=True
        )
    _, file_extension = os.path.splitext(report_name)
    if not file_extension:
        report_name = f'{report_name}.html'
    destination = os.path.join(destination_folder, report_name)

    if not os.path.exists(os.path.dirname(destination)):
        os.makedirs(os.path.dirname(destination), exist_ok=True)

    try:
        with open(destination, 'w', encoding='utf-8') as f:
            f.write(html_string)
    except IOError as e:
        if e.errno == errno.EACCES:
            print(f'ERROR: cannot write to {destination}, PermissionError (Errno 13)')
        else:
            print(f'ERROR: There was an error writing to {destination}')

    return html_string",_5821.py,41,"js = {'jquery': open(jquery_js, encoding='utf-8').read(), 'datatables': open(datatables_js, encoding='utf-8').read(), 'bootstrap': open(bootstrap_js, encoding='utf-8').read(), 'main': open(main_js, encoding='utf-8').read(), 'report_execution': open(report_execution_js).read()}","with open(jquery_js, encoding='utf-8') as my_f:
    js = {'jquery': my_f.read(), 'datatables': open(datatables_js, encoding='utf-8').read(), 'bootstrap': open(bootstrap_js, encoding='utf-8').read(), 'main': open(main_js, encoding='utf-8').read(), 'report_execution': open(report_execution_js).read()}
    execution_data = exec_report.get_execution_data(execution_directory)
    detail_test_data = {}
    for test in execution_data['tests']:
        test_detail = exec_report.function_test_execution_result(project, execution, timestamp, test['test_file'], test['test'], test['set_name'], no_screenshots=no_images, encode_screenshots=True)
        test_id = f""{test['test_file']}.{test['test']}""
        if test['set_name']:
            test_id = f""{test_id}.{test['set_name']}""
        detail_test_data[test_id] = test_detail
    with app.app_context():
        html_string = render_template('report/report_execution_static.html', project=project, execution=execution, timestamp=timestamp, execution_data=execution_data, detail_test_data=detail_test_data, formatted_date=formatted_date, css=css, js=js, static=True)
    (_, file_extension) = os.path.splitext(report_name)
    if not file_extension:
        report_name = f'{report_name}.html'
    destination = os.path.join(destination_folder, report_name)
    if not os.path.exists(os.path.dirname(destination)):
        os.makedirs(os.path.dirname(destination), exist_ok=True)
    try:
        with open(destination, 'w', encoding='utf-8') as f:
            f.write(html_string)
    except IOError as e:
        if e.errno == errno.EACCES:
            print(f'ERROR: cannot write to {destination}, PermissionError (Errno 13)')
        else:
            print(f'ERROR: There was an error writing to {destination}')
    return html_string"
https://github.com/golemhq/golem/tree/master/golem/report/html_report.py,"def generate_html_report(project, execution, timestamp, destination_folder=None,
                         report_name=None, no_images=False):
    """"""Generate static HTML report.
    Report is generated in <report_directory>/<report_name>
    By default it's generated in <testdir>/projects/<project>/reports/<suite>/<timestamp>
    Default name is 'report.html' and 'report-no-images.html'
    """"""
    execution_directory = exec_report.execution_report_path(project, execution, timestamp)

    if destination_folder is None:
        destination_folder = execution_directory

    if not report_name:
        if no_images:
            report_name = 'report-no-images'
        else:
            report_name = 'report'

    formatted_date = utils.get_date_time_from_timestamp(timestamp)
    app = gui.create_app()
    static_folder = app.static_folder
    # css paths
    css_folder = os.path.join(static_folder, 'css')
    boostrap_css = os.path.join(css_folder, 'bootstrap', 'bootstrap.min.css')
    main_css = os.path.join(css_folder, 'main.css')
    report_css = os.path.join(css_folder, 'report.css')
    # js paths
    js_folder = os.path.join(static_folder, 'js')
    main_js = os.path.join(js_folder, 'main.js')
    jquery_js = os.path.join(js_folder, 'external', 'jquery.min.js')
    datatables_js = os.path.join(js_folder, 'external', 'datatable', 'datatables.min.js')
    bootstrap_js = os.path.join(js_folder, 'external', 'bootstrap.min.js')
    report_execution_js = os.path.join(js_folder, 'report_execution.js')

    css = {
        'bootstrap': open(boostrap_css, encoding='utf-8').read(),
        'main': open(main_css, encoding='utf-8').read(),
        'report': open(report_css, encoding='utf-8').read()
    }
    js = {
        'jquery': open(jquery_js, encoding='utf-8').read(),
        'datatables': open(datatables_js, encoding='utf-8').read(),
        'bootstrap': open(bootstrap_js, encoding='utf-8').read(),
        'main': open(main_js, encoding='utf-8').read(),
        'report_execution': open(report_execution_js).read()
    }

    execution_data = exec_report.get_execution_data(execution_directory)
    detail_test_data = {}
    for test in execution_data['tests']:
        test_detail = exec_report.function_test_execution_result(
            project, execution, timestamp, test['test_file'], test['test'], test['set_name'],
            no_screenshots=no_images, encode_screenshots=True
        )
        # testId is test_file + test + set_name
        test_id = f""{test['test_file']}.{test['test']}""
        if test['set_name']:
            test_id = f""{test_id}.{test['set_name']}""
        detail_test_data[test_id] = test_detail
    with app.app_context():
        html_string = render_template(
            'report/report_execution_static.html', project=project, execution=execution,
            timestamp=timestamp, execution_data=execution_data,
            detail_test_data=detail_test_data, formatted_date=formatted_date,
            css=css, js=js, static=True
        )
    _, file_extension = os.path.splitext(report_name)
    if not file_extension:
        report_name = f'{report_name}.html'
    destination = os.path.join(destination_folder, report_name)

    if not os.path.exists(os.path.dirname(destination)):
        os.makedirs(os.path.dirname(destination), exist_ok=True)

    try:
        with open(destination, 'w', encoding='utf-8') as f:
            f.write(html_string)
    except IOError as e:
        if e.errno == errno.EACCES:
            print(f'ERROR: cannot write to {destination}, PermissionError (Errno 13)')
        else:
            print(f'ERROR: There was an error writing to {destination}')

    return html_string",_5821.py,42,"js = {'jquery': open(jquery_js, encoding='utf-8').read(), 'datatables': open(datatables_js, encoding='utf-8').read(), 'bootstrap': open(bootstrap_js, encoding='utf-8').read(), 'main': open(main_js, encoding='utf-8').read(), 'report_execution': open(report_execution_js).read()}","with open(datatables_js, encoding='utf-8') as my_f:
    js = {'jquery': open(jquery_js, encoding='utf-8').read(), 'datatables': my_f.read(), 'bootstrap': open(bootstrap_js, encoding='utf-8').read(), 'main': open(main_js, encoding='utf-8').read(), 'report_execution': open(report_execution_js).read()}
    execution_data = exec_report.get_execution_data(execution_directory)
    detail_test_data = {}
    for test in execution_data['tests']:
        test_detail = exec_report.function_test_execution_result(project, execution, timestamp, test['test_file'], test['test'], test['set_name'], no_screenshots=no_images, encode_screenshots=True)
        test_id = f""{test['test_file']}.{test['test']}""
        if test['set_name']:
            test_id = f""{test_id}.{test['set_name']}""
        detail_test_data[test_id] = test_detail
    with app.app_context():
        html_string = render_template('report/report_execution_static.html', project=project, execution=execution, timestamp=timestamp, execution_data=execution_data, detail_test_data=detail_test_data, formatted_date=formatted_date, css=css, js=js, static=True)
    (_, file_extension) = os.path.splitext(report_name)
    if not file_extension:
        report_name = f'{report_name}.html'
    destination = os.path.join(destination_folder, report_name)
    if not os.path.exists(os.path.dirname(destination)):
        os.makedirs(os.path.dirname(destination), exist_ok=True)
    try:
        with open(destination, 'w', encoding='utf-8') as f:
            f.write(html_string)
    except IOError as e:
        if e.errno == errno.EACCES:
            print(f'ERROR: cannot write to {destination}, PermissionError (Errno 13)')
        else:
            print(f'ERROR: There was an error writing to {destination}')
    return html_string"
https://github.com/golemhq/golem/tree/master/golem/report/html_report.py,"def generate_html_report(project, execution, timestamp, destination_folder=None,
                         report_name=None, no_images=False):
    """"""Generate static HTML report.
    Report is generated in <report_directory>/<report_name>
    By default it's generated in <testdir>/projects/<project>/reports/<suite>/<timestamp>
    Default name is 'report.html' and 'report-no-images.html'
    """"""
    execution_directory = exec_report.execution_report_path(project, execution, timestamp)

    if destination_folder is None:
        destination_folder = execution_directory

    if not report_name:
        if no_images:
            report_name = 'report-no-images'
        else:
            report_name = 'report'

    formatted_date = utils.get_date_time_from_timestamp(timestamp)
    app = gui.create_app()
    static_folder = app.static_folder
    # css paths
    css_folder = os.path.join(static_folder, 'css')
    boostrap_css = os.path.join(css_folder, 'bootstrap', 'bootstrap.min.css')
    main_css = os.path.join(css_folder, 'main.css')
    report_css = os.path.join(css_folder, 'report.css')
    # js paths
    js_folder = os.path.join(static_folder, 'js')
    main_js = os.path.join(js_folder, 'main.js')
    jquery_js = os.path.join(js_folder, 'external', 'jquery.min.js')
    datatables_js = os.path.join(js_folder, 'external', 'datatable', 'datatables.min.js')
    bootstrap_js = os.path.join(js_folder, 'external', 'bootstrap.min.js')
    report_execution_js = os.path.join(js_folder, 'report_execution.js')

    css = {
        'bootstrap': open(boostrap_css, encoding='utf-8').read(),
        'main': open(main_css, encoding='utf-8').read(),
        'report': open(report_css, encoding='utf-8').read()
    }
    js = {
        'jquery': open(jquery_js, encoding='utf-8').read(),
        'datatables': open(datatables_js, encoding='utf-8').read(),
        'bootstrap': open(bootstrap_js, encoding='utf-8').read(),
        'main': open(main_js, encoding='utf-8').read(),
        'report_execution': open(report_execution_js).read()
    }

    execution_data = exec_report.get_execution_data(execution_directory)
    detail_test_data = {}
    for test in execution_data['tests']:
        test_detail = exec_report.function_test_execution_result(
            project, execution, timestamp, test['test_file'], test['test'], test['set_name'],
            no_screenshots=no_images, encode_screenshots=True
        )
        # testId is test_file + test + set_name
        test_id = f""{test['test_file']}.{test['test']}""
        if test['set_name']:
            test_id = f""{test_id}.{test['set_name']}""
        detail_test_data[test_id] = test_detail
    with app.app_context():
        html_string = render_template(
            'report/report_execution_static.html', project=project, execution=execution,
            timestamp=timestamp, execution_data=execution_data,
            detail_test_data=detail_test_data, formatted_date=formatted_date,
            css=css, js=js, static=True
        )
    _, file_extension = os.path.splitext(report_name)
    if not file_extension:
        report_name = f'{report_name}.html'
    destination = os.path.join(destination_folder, report_name)

    if not os.path.exists(os.path.dirname(destination)):
        os.makedirs(os.path.dirname(destination), exist_ok=True)

    try:
        with open(destination, 'w', encoding='utf-8') as f:
            f.write(html_string)
    except IOError as e:
        if e.errno == errno.EACCES:
            print(f'ERROR: cannot write to {destination}, PermissionError (Errno 13)')
        else:
            print(f'ERROR: There was an error writing to {destination}')

    return html_string",_5821.py,43,"js = {'jquery': open(jquery_js, encoding='utf-8').read(), 'datatables': open(datatables_js, encoding='utf-8').read(), 'bootstrap': open(bootstrap_js, encoding='utf-8').read(), 'main': open(main_js, encoding='utf-8').read(), 'report_execution': open(report_execution_js).read()}","with open(bootstrap_js, encoding='utf-8') as my_f:
    js = {'jquery': open(jquery_js, encoding='utf-8').read(), 'datatables': open(datatables_js, encoding='utf-8').read(), 'bootstrap': my_f.read(), 'main': open(main_js, encoding='utf-8').read(), 'report_execution': open(report_execution_js).read()}
    execution_data = exec_report.get_execution_data(execution_directory)
    detail_test_data = {}
    for test in execution_data['tests']:
        test_detail = exec_report.function_test_execution_result(project, execution, timestamp, test['test_file'], test['test'], test['set_name'], no_screenshots=no_images, encode_screenshots=True)
        test_id = f""{test['test_file']}.{test['test']}""
        if test['set_name']:
            test_id = f""{test_id}.{test['set_name']}""
        detail_test_data[test_id] = test_detail
    with app.app_context():
        html_string = render_template('report/report_execution_static.html', project=project, execution=execution, timestamp=timestamp, execution_data=execution_data, detail_test_data=detail_test_data, formatted_date=formatted_date, css=css, js=js, static=True)
    (_, file_extension) = os.path.splitext(report_name)
    if not file_extension:
        report_name = f'{report_name}.html'
    destination = os.path.join(destination_folder, report_name)
    if not os.path.exists(os.path.dirname(destination)):
        os.makedirs(os.path.dirname(destination), exist_ok=True)
    try:
        with open(destination, 'w', encoding='utf-8') as f:
            f.write(html_string)
    except IOError as e:
        if e.errno == errno.EACCES:
            print(f'ERROR: cannot write to {destination}, PermissionError (Errno 13)')
        else:
            print(f'ERROR: There was an error writing to {destination}')
    return html_string"
https://github.com/golemhq/golem/tree/master/golem/report/html_report.py,"def generate_html_report(project, execution, timestamp, destination_folder=None,
                         report_name=None, no_images=False):
    """"""Generate static HTML report.
    Report is generated in <report_directory>/<report_name>
    By default it's generated in <testdir>/projects/<project>/reports/<suite>/<timestamp>
    Default name is 'report.html' and 'report-no-images.html'
    """"""
    execution_directory = exec_report.execution_report_path(project, execution, timestamp)

    if destination_folder is None:
        destination_folder = execution_directory

    if not report_name:
        if no_images:
            report_name = 'report-no-images'
        else:
            report_name = 'report'

    formatted_date = utils.get_date_time_from_timestamp(timestamp)
    app = gui.create_app()
    static_folder = app.static_folder
    # css paths
    css_folder = os.path.join(static_folder, 'css')
    boostrap_css = os.path.join(css_folder, 'bootstrap', 'bootstrap.min.css')
    main_css = os.path.join(css_folder, 'main.css')
    report_css = os.path.join(css_folder, 'report.css')
    # js paths
    js_folder = os.path.join(static_folder, 'js')
    main_js = os.path.join(js_folder, 'main.js')
    jquery_js = os.path.join(js_folder, 'external', 'jquery.min.js')
    datatables_js = os.path.join(js_folder, 'external', 'datatable', 'datatables.min.js')
    bootstrap_js = os.path.join(js_folder, 'external', 'bootstrap.min.js')
    report_execution_js = os.path.join(js_folder, 'report_execution.js')

    css = {
        'bootstrap': open(boostrap_css, encoding='utf-8').read(),
        'main': open(main_css, encoding='utf-8').read(),
        'report': open(report_css, encoding='utf-8').read()
    }
    js = {
        'jquery': open(jquery_js, encoding='utf-8').read(),
        'datatables': open(datatables_js, encoding='utf-8').read(),
        'bootstrap': open(bootstrap_js, encoding='utf-8').read(),
        'main': open(main_js, encoding='utf-8').read(),
        'report_execution': open(report_execution_js).read()
    }

    execution_data = exec_report.get_execution_data(execution_directory)
    detail_test_data = {}
    for test in execution_data['tests']:
        test_detail = exec_report.function_test_execution_result(
            project, execution, timestamp, test['test_file'], test['test'], test['set_name'],
            no_screenshots=no_images, encode_screenshots=True
        )
        # testId is test_file + test + set_name
        test_id = f""{test['test_file']}.{test['test']}""
        if test['set_name']:
            test_id = f""{test_id}.{test['set_name']}""
        detail_test_data[test_id] = test_detail
    with app.app_context():
        html_string = render_template(
            'report/report_execution_static.html', project=project, execution=execution,
            timestamp=timestamp, execution_data=execution_data,
            detail_test_data=detail_test_data, formatted_date=formatted_date,
            css=css, js=js, static=True
        )
    _, file_extension = os.path.splitext(report_name)
    if not file_extension:
        report_name = f'{report_name}.html'
    destination = os.path.join(destination_folder, report_name)

    if not os.path.exists(os.path.dirname(destination)):
        os.makedirs(os.path.dirname(destination), exist_ok=True)

    try:
        with open(destination, 'w', encoding='utf-8') as f:
            f.write(html_string)
    except IOError as e:
        if e.errno == errno.EACCES:
            print(f'ERROR: cannot write to {destination}, PermissionError (Errno 13)')
        else:
            print(f'ERROR: There was an error writing to {destination}')

    return html_string",_5821.py,44,"js = {'jquery': open(jquery_js, encoding='utf-8').read(), 'datatables': open(datatables_js, encoding='utf-8').read(), 'bootstrap': open(bootstrap_js, encoding='utf-8').read(), 'main': open(main_js, encoding='utf-8').read(), 'report_execution': open(report_execution_js).read()}","with open(main_js, encoding='utf-8') as my_f:
    js = {'jquery': open(jquery_js, encoding='utf-8').read(), 'datatables': open(datatables_js, encoding='utf-8').read(), 'bootstrap': open(bootstrap_js, encoding='utf-8').read(), 'main': my_f.read(), 'report_execution': open(report_execution_js).read()}
    execution_data = exec_report.get_execution_data(execution_directory)
    detail_test_data = {}
    for test in execution_data['tests']:
        test_detail = exec_report.function_test_execution_result(project, execution, timestamp, test['test_file'], test['test'], test['set_name'], no_screenshots=no_images, encode_screenshots=True)
        test_id = f""{test['test_file']}.{test['test']}""
        if test['set_name']:
            test_id = f""{test_id}.{test['set_name']}""
        detail_test_data[test_id] = test_detail
    with app.app_context():
        html_string = render_template('report/report_execution_static.html', project=project, execution=execution, timestamp=timestamp, execution_data=execution_data, detail_test_data=detail_test_data, formatted_date=formatted_date, css=css, js=js, static=True)
    (_, file_extension) = os.path.splitext(report_name)
    if not file_extension:
        report_name = f'{report_name}.html'
    destination = os.path.join(destination_folder, report_name)
    if not os.path.exists(os.path.dirname(destination)):
        os.makedirs(os.path.dirname(destination), exist_ok=True)
    try:
        with open(destination, 'w', encoding='utf-8') as f:
            f.write(html_string)
    except IOError as e:
        if e.errno == errno.EACCES:
            print(f'ERROR: cannot write to {destination}, PermissionError (Errno 13)')
        else:
            print(f'ERROR: There was an error writing to {destination}')
    return html_string"
https://github.com/golemhq/golem/tree/master/golem/report/html_report.py,"def generate_html_report(project, execution, timestamp, destination_folder=None,
                         report_name=None, no_images=False):
    """"""Generate static HTML report.
    Report is generated in <report_directory>/<report_name>
    By default it's generated in <testdir>/projects/<project>/reports/<suite>/<timestamp>
    Default name is 'report.html' and 'report-no-images.html'
    """"""
    execution_directory = exec_report.execution_report_path(project, execution, timestamp)

    if destination_folder is None:
        destination_folder = execution_directory

    if not report_name:
        if no_images:
            report_name = 'report-no-images'
        else:
            report_name = 'report'

    formatted_date = utils.get_date_time_from_timestamp(timestamp)
    app = gui.create_app()
    static_folder = app.static_folder
    # css paths
    css_folder = os.path.join(static_folder, 'css')
    boostrap_css = os.path.join(css_folder, 'bootstrap', 'bootstrap.min.css')
    main_css = os.path.join(css_folder, 'main.css')
    report_css = os.path.join(css_folder, 'report.css')
    # js paths
    js_folder = os.path.join(static_folder, 'js')
    main_js = os.path.join(js_folder, 'main.js')
    jquery_js = os.path.join(js_folder, 'external', 'jquery.min.js')
    datatables_js = os.path.join(js_folder, 'external', 'datatable', 'datatables.min.js')
    bootstrap_js = os.path.join(js_folder, 'external', 'bootstrap.min.js')
    report_execution_js = os.path.join(js_folder, 'report_execution.js')

    css = {
        'bootstrap': open(boostrap_css, encoding='utf-8').read(),
        'main': open(main_css, encoding='utf-8').read(),
        'report': open(report_css, encoding='utf-8').read()
    }
    js = {
        'jquery': open(jquery_js, encoding='utf-8').read(),
        'datatables': open(datatables_js, encoding='utf-8').read(),
        'bootstrap': open(bootstrap_js, encoding='utf-8').read(),
        'main': open(main_js, encoding='utf-8').read(),
        'report_execution': open(report_execution_js).read()
    }

    execution_data = exec_report.get_execution_data(execution_directory)
    detail_test_data = {}
    for test in execution_data['tests']:
        test_detail = exec_report.function_test_execution_result(
            project, execution, timestamp, test['test_file'], test['test'], test['set_name'],
            no_screenshots=no_images, encode_screenshots=True
        )
        # testId is test_file + test + set_name
        test_id = f""{test['test_file']}.{test['test']}""
        if test['set_name']:
            test_id = f""{test_id}.{test['set_name']}""
        detail_test_data[test_id] = test_detail
    with app.app_context():
        html_string = render_template(
            'report/report_execution_static.html', project=project, execution=execution,
            timestamp=timestamp, execution_data=execution_data,
            detail_test_data=detail_test_data, formatted_date=formatted_date,
            css=css, js=js, static=True
        )
    _, file_extension = os.path.splitext(report_name)
    if not file_extension:
        report_name = f'{report_name}.html'
    destination = os.path.join(destination_folder, report_name)

    if not os.path.exists(os.path.dirname(destination)):
        os.makedirs(os.path.dirname(destination), exist_ok=True)

    try:
        with open(destination, 'w', encoding='utf-8') as f:
            f.write(html_string)
    except IOError as e:
        if e.errno == errno.EACCES:
            print(f'ERROR: cannot write to {destination}, PermissionError (Errno 13)')
        else:
            print(f'ERROR: There was an error writing to {destination}')

    return html_string",_5821.py,45,"js = {'jquery': open(jquery_js, encoding='utf-8').read(), 'datatables': open(datatables_js, encoding='utf-8').read(), 'bootstrap': open(bootstrap_js, encoding='utf-8').read(), 'main': open(main_js, encoding='utf-8').read(), 'report_execution': open(report_execution_js).read()}","with open(report_execution_js) as my_f:
    js = {'jquery': open(jquery_js, encoding='utf-8').read(), 'datatables': open(datatables_js, encoding='utf-8').read(), 'bootstrap': open(bootstrap_js, encoding='utf-8').read(), 'main': open(main_js, encoding='utf-8').read(), 'report_execution': my_f.read()}
    execution_data = exec_report.get_execution_data(execution_directory)
    detail_test_data = {}
    for test in execution_data['tests']:
        test_detail = exec_report.function_test_execution_result(project, execution, timestamp, test['test_file'], test['test'], test['set_name'], no_screenshots=no_images, encode_screenshots=True)
        test_id = f""{test['test_file']}.{test['test']}""
        if test['set_name']:
            test_id = f""{test_id}.{test['set_name']}""
        detail_test_data[test_id] = test_detail
    with app.app_context():
        html_string = render_template('report/report_execution_static.html', project=project, execution=execution, timestamp=timestamp, execution_data=execution_data, detail_test_data=detail_test_data, formatted_date=formatted_date, css=css, js=js, static=True)
    (_, file_extension) = os.path.splitext(report_name)
    if not file_extension:
        report_name = f'{report_name}.html'
    destination = os.path.join(destination_folder, report_name)
    if not os.path.exists(os.path.dirname(destination)):
        os.makedirs(os.path.dirname(destination), exist_ok=True)
    try:
        with open(destination, 'w', encoding='utf-8') as f:
            f.write(html_string)
    except IOError as e:
        if e.errno == errno.EACCES:
            print(f'ERROR: cannot write to {destination}, PermissionError (Errno 13)')
        else:
            print(f'ERROR: There was an error writing to {destination}')
    return html_string"
https://github.com/carmaa/inception/tree/master/inception/modules/dump.py,"def run(opts, memspace):
    # Ensure that the filename is accessible outside this module
    global filename

    # Set start and end parameters based on user input. If no input is given,
    # start at zero (i.e., the beginning of main memory)
    end = memspace.memsize
    if opts.address and opts.size:
        start, end = calculate(opts.address, opts.size)
    elif opts.address:
        raise InceptionException('Missing parameter ""size""')
    elif opts.size:
        raise InceptionException('Missing parameter ""address""')
    else:
        start = 0  # May be overridden later

    # Make sure that the right mode is set
    # cfg.memdump = True #TODO: do we really need this?
    
    # Ensure correct denomination
    size = end - start
    if size % cfg.GiB == 0:
        s = '{0} GiB'.format(size // cfg.GiB)
    elif size % cfg.MiB == 0:
        s = '{0} MiB'.format(size // cfg.MiB)
    else:
        s = '{0} KiB'.format(size // cfg.KiB)
    
    if opts.prefix:
        prefix = opts.prefix
    else:
        prefix = filename_prefix

    # Open file for writing
    timestr = time.strftime(""%Y%m%d-%H%M%S"")
    filename = '{0}_{1}-{2}_{3}.{4}'.format(prefix,
                                            hex(start), hex(end),
                                            timestr,
                                            filename_ext)
    term.info('Dumping from {0:#x} to {1:#x}, a total of {2}:'
              .format(start, end, s))
    file = open(filename, 'wb')

    # Progress bar
    prog = term.ProgressBar(min_value=start, max_value=end,
                            total_width=term.wrapper.width)

    if size < cfg.max_request_size:
        requestsize = size
    else:
        requestsize = cfg.max_request_size
    try:
        for i in range(start, end, requestsize):
            # Edge case, make sure that we don't read beyond the end
            if i + requestsize > end:
                requestsize = end - i
            data = memspace.read(i, requestsize)
            file.write(data)
            # Print status
            prog.update_amount(i + requestsize, data)
            prog.draw()
        file.close()
        print()  # Filler
        term.info('Dumped memory to file {0}'.format(filename))
        # device.close()
    except KeyboardInterrupt:
        file.close()
        print()  # Filler
        # device.close()
        term.info('Partial memory dumped to file {0}'.format(filename))
        raise KeyboardInterrupt",_6705.py,42,"file = open(filename, 'wb')","with open(filename, 'wb') as file:
    prog = term.ProgressBar(min_value=start, max_value=end, total_width=term.wrapper.width)
    if size < cfg.max_request_size:
        requestsize = size
    else:
        requestsize = cfg.max_request_size
    try:
        for i in range(start, end, requestsize):
            if i + requestsize > end:
                requestsize = end - i
            data = memspace.read(i, requestsize)
            file.write(data)
            prog.update_amount(i + requestsize, data)
            prog.draw()
        
        pass
        print()
        term.info('Dumped memory to file {0}'.format(filename))
    except KeyboardInterrupt:
        
        pass
        print()
        term.info('Partial memory dumped to file {0}'.format(filename))
        raise KeyboardInterrupt"
https://github.com/sony/nnabla/tree/master/python/src/nnabla/utils/image_utils/backend_events/pypng_backend.py,"def imsave(self, path, img, channel_first=False, as_uint16=False, auto_scale=True):
        """"""
        Save image by pypng module.

        Args:
            path (str): output filename
            img (numpy.ndarray): Image array to save. Image shape is considered as (height, width, channel) by default.
            channel_first:
                This argument specifies the shape of img is whether (height, width, channel) or (channel, height, width).
                Default value is False, which means the img shape is (height, width, channel)
            as_uint16 (bool):
                If True, save image as uint16.
            auto_scale (bool) :
                Whether upscale pixel values or not.
                If you want to save float image, this argument must be True.
                In pypng backend, all below are supported.
                    - float ([0, 1]) to uint8 ([0, 255])  (if img.dtype==float and upscale==True and as_uint16==False)
                    - float to uint16 ([0, 65535]) (if img.dtype==float and upscale==True and as_uint16==True)
                    - uint8 to uint16 are supported (if img.dtype==np.uint8 and upscale==True and as_uint16==True)
        """"""

        img = _imsave_before(img, channel_first, auto_scale)

        if auto_scale:
            img = upscale_pixel_intensity(img, as_uint16)

        img = check_type_and_cast_if_necessary(img, as_uint16)

        bitdepth = 8 if img.dtype == np.uint8 else 16
        grayscale = True if len(img.shape) == 2 or (
            len(img.shape) == 3 and img.shape[-1] == 1) else False

        writer = png.Writer(img.shape[1], img.shape[0],
                            greyscale=grayscale, bitdepth=bitdepth)

        writer.write(open(path, ""wb""), img.reshape(img.shape[0], -1))",_7202.py,36,"writer.write(open(path, 'wb'), img.reshape(img.shape[0], -1))","with open(path, 'wb') as my_f:
    writer.write(my_f, img.reshape(img.shape[0], -1))"
https://github.com/microsoft/hummingbird/tree/master/benchmarks/datasets.py,"def prepare_year(dataset_folder, nrows):
    url = ""https://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt"" "".zip""
    local_url = os.path.join(dataset_folder, os.path.basename(url))
    pickle_url = os.path.join(dataset_folder, ""year"" + ("""" if nrows is None else ""-"" + str(nrows)) + ""-pickle.dat"")

    if os.path.exists(pickle_url):
        return pickle.load(open(pickle_url, ""rb""))
    print(""Preparing dataset ..."")

    if not os.path.isfile(local_url):
        urlretrieve(url, local_url)
    year = pd.read_csv(local_url, nrows=nrows, header=None)
    X = year.iloc[:, 1:]
    y = year.iloc[:, 0]

    if nrows is None:
        # this dataset requires a specific train/test split,
        # with the specified number of rows at the start belonging to the train set,
        # and the rest being the test set
        X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, train_size=463715, test_size=51630)
    else:
        print(""Warning: nrows is specified, not using predefined test/train split for "" ""YearPredictionMSD."")
        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=77, test_size=0.2,)

    data = Data(X_train.astype(""|f4"").to_numpy(), X_test.astype(""|f4"").to_numpy(), y_train, y_test, LearningTask.REGRESSION)
    pickle.dump(data, open(pickle_url, ""wb""), protocol=4)
    return data",_7637.py,26,"pickle.dump(data, open(pickle_url, 'wb'), protocol=4)","with open(pickle_url, 'wb') as my_f:
    pickle.dump(data, my_f, protocol=4)
    return data"
https://github.com/microsoft/hummingbird/tree/master/benchmarks/datasets.py,"def prepare_year(dataset_folder, nrows):
    url = ""https://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt"" "".zip""
    local_url = os.path.join(dataset_folder, os.path.basename(url))
    pickle_url = os.path.join(dataset_folder, ""year"" + ("""" if nrows is None else ""-"" + str(nrows)) + ""-pickle.dat"")

    if os.path.exists(pickle_url):
        return pickle.load(open(pickle_url, ""rb""))
    print(""Preparing dataset ..."")

    if not os.path.isfile(local_url):
        urlretrieve(url, local_url)
    year = pd.read_csv(local_url, nrows=nrows, header=None)
    X = year.iloc[:, 1:]
    y = year.iloc[:, 0]

    if nrows is None:
        # this dataset requires a specific train/test split,
        # with the specified number of rows at the start belonging to the train set,
        # and the rest being the test set
        X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, train_size=463715, test_size=51630)
    else:
        print(""Warning: nrows is specified, not using predefined test/train split for "" ""YearPredictionMSD."")
        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=77, test_size=0.2,)

    data = Data(X_train.astype(""|f4"").to_numpy(), X_test.astype(""|f4"").to_numpy(), y_train, y_test, LearningTask.REGRESSION)
    pickle.dump(data, open(pickle_url, ""wb""), protocol=4)
    return data",_7637.py,7,"return pickle.load(open(pickle_url, 'rb'))","with open(pickle_url, 'rb') as my_f:
    return pickle.load(my_f)"
https://github.com/microsoft/hummingbird/tree/master/benchmarks/datasets.py,"def prepare_year(dataset_folder, nrows):
    url = ""https://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt"" "".zip""
    local_url = os.path.join(dataset_folder, os.path.basename(url))
    pickle_url = os.path.join(dataset_folder, ""year"" + ("""" if nrows is None else ""-"" + str(nrows)) + ""-pickle.dat"")

    if os.path.exists(pickle_url):
        return pickle.load(open(pickle_url, ""rb""))
    print(""Preparing dataset ..."")

    if not os.path.isfile(local_url):
        urlretrieve(url, local_url)
    year = pd.read_csv(local_url, nrows=nrows, header=None)
    X = year.iloc[:, 1:]
    y = year.iloc[:, 0]

    if nrows is None:
        # this dataset requires a specific train/test split,
        # with the specified number of rows at the start belonging to the train set,
        # and the rest being the test set
        X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, train_size=463715, test_size=51630)
    else:
        print(""Warning: nrows is specified, not using predefined test/train split for "" ""YearPredictionMSD."")
        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=77, test_size=0.2,)

    data = Data(X_train.astype(""|f4"").to_numpy(), X_test.astype(""|f4"").to_numpy(), y_train, y_test, LearningTask.REGRESSION)
    pickle.dump(data, open(pickle_url, ""wb""), protocol=4)
    return data",_7637.py,7,"return pickle.load(open(pickle_url, 'rb'))","with open(pickle_url, 'rb') as my_f:
    return pickle.load(my_f)"
https://github.com/zake7749/Chatbot/tree/master/Chatbot/QuestionAnswering/chat.py,"def getResponse(self,query,threshold=50):

        title,index = self.matcher.match(query)
        sim = self.matcher.getSimilarity()
        if sim < threshold:
            return self.defaultResponse[random.randrange(0,len(self.defaultResponse))]
        else:
            res = json.load(open(os.path.join(""data/processed/reply/"",str(int(index/1000))+'.json'),'r',encoding='utf-8'))
            targetId = index % 1000
            candiates = self.evaluator.getBestResponse(res[targetId],topk=3)
            reply = self.randomPick(candiates)
            return reply",_7867.py,8,"res = json.load(open(os.path.join('data/processed/reply/', str(int(index / 1000)) + '.json'), 'r', encoding='utf-8'))","with open(os.path.join('data/processed/reply/', str(int(index / 1000)) + '.json'), 'r', encoding='utf-8') as my_f:
    res = json.load(my_f)"
https://github.com/zake7749/Chatbot/tree/master/Chatbot/QuestionAnswering/chat.py,"def getResponse(self,query,threshold=50):

        title,index = self.matcher.match(query)
        sim = self.matcher.getSimilarity()
        if sim < threshold:
            return self.defaultResponse[random.randrange(0,len(self.defaultResponse))]
        else:
            res = json.load(open(os.path.join(""data/processed/reply/"",str(int(index/1000))+'.json'),'r',encoding='utf-8'))
            targetId = index % 1000
            candiates = self.evaluator.getBestResponse(res[targetId],topk=3)
            reply = self.randomPick(candiates)
            return reply",_7867.py,8,"res = json.load(open(os.path.join('data/processed/reply/', str(int(index / 1000)) + '.json'), 'r', encoding='utf-8'))","with open(os.path.join('data/processed/reply/', str(int(index / 1000)) + '.json'), 'r', encoding='utf-8') as my_f:
    res = json.load(my_f)
    targetId = index % 1000
    candiates = self.evaluator.getBestResponse(res[targetId], topk=3)
    reply = self.randomPick(candiates)
    return reply"
https://github.com/ansible/ansible/tree/master/hacking/azp/download.py,"def download_run(args):
    """"""Download a run.""""""

    output_dir = '%s' % args.run

    if not args.test and not os.path.exists(output_dir):
        os.makedirs(output_dir)

    if args.run_metadata:
        run_url = 'https://dev.azure.com/ansible/ansible/_apis/pipelines/%s/runs/%s?api-version=6.0-preview.1' % (args.pipeline_id, args.run)
        run_info_response = requests.get(run_url)
        run_info_response.raise_for_status()
        run = run_info_response.json()

        path = os.path.join(output_dir, 'run.json')
        contents = json.dumps(run, sort_keys=True, indent=4)

        if args.verbose:
            print(path)

        if not args.test:
            with open(path, 'w') as metadata_fd:
                metadata_fd.write(contents)

    timeline_response = requests.get('https://dev.azure.com/ansible/ansible/_apis/build/builds/%s/timeline?api-version=6.0' % args.run)
    timeline_response.raise_for_status()
    timeline = timeline_response.json()
    roots = set()
    by_id = {}
    children_of = {}
    parent_of = {}
    for r in timeline['records']:
        thisId = r['id']
        parentId = r['parentId']

        by_id[thisId] = r

        if parentId is None:
            roots.add(thisId)
        else:
            parent_of[thisId] = parentId
            children_of[parentId] = children_of.get(parentId, []) + [thisId]

    allowed = set()

    def allow_recursive(ei):
        allowed.add(ei)
        for ci in children_of.get(ei, []):
            allow_recursive(ci)

    for ri in roots:
        r = by_id[ri]
        allowed.add(ri)
        for ci in children_of.get(r['id'], []):
            c = by_id[ci]
            if not args.match_job_name.match(""%s %s"" % (r['name'], c['name'])):
                continue
            allow_recursive(c['id'])

    if args.artifacts:
        artifact_list_url = 'https://dev.azure.com/ansible/ansible/_apis/build/builds/%s/artifacts?api-version=6.0' % args.run
        artifact_list_response = requests.get(artifact_list_url)
        artifact_list_response.raise_for_status()
        for artifact in artifact_list_response.json()['value']:
            if artifact['source'] not in allowed or not args.match_artifact_name.match(artifact['name']):
                continue
            if args.verbose:
                print('%s/%s' % (output_dir, artifact['name']))
            if not args.test:
                response = requests.get(artifact['resource']['downloadUrl'])
                response.raise_for_status()
                archive = zipfile.ZipFile(io.BytesIO(response.content))
                archive.extractall(path=output_dir)

    if args.console_logs:
        for r in timeline['records']:
            if not r['log'] or r['id'] not in allowed or not args.match_artifact_name.match(r['name']):
                continue
            names = []
            parent_id = r['id']
            while parent_id is not None:
                p = by_id[parent_id]
                name = p['name']
                if name not in names:
                    names = [name] + names
                parent_id = parent_of.get(p['id'], None)

            path = "" "".join(names)

            # Some job names have the separator in them.
            path = path.replace(os.sep, '_')

            log_path = os.path.join(output_dir, '%s.log' % path)
            if args.verbose:
                print(log_path)
            if not args.test:
                log = requests.get(r['log']['url'])
                log.raise_for_status()
                open(log_path, 'wb').write(log.content)",_7997.py,99,"open(log_path, 'wb').write(log.content)","with open(log_path, 'wb') as my_f:
    my_f.write(log.content)"
https://github.com/ansible/ansible/tree/master/hacking/azp/download.py,"def download_run(args):
    """"""Download a run.""""""

    output_dir = '%s' % args.run

    if not args.test and not os.path.exists(output_dir):
        os.makedirs(output_dir)

    if args.run_metadata:
        run_url = 'https://dev.azure.com/ansible/ansible/_apis/pipelines/%s/runs/%s?api-version=6.0-preview.1' % (args.pipeline_id, args.run)
        run_info_response = requests.get(run_url)
        run_info_response.raise_for_status()
        run = run_info_response.json()

        path = os.path.join(output_dir, 'run.json')
        contents = json.dumps(run, sort_keys=True, indent=4)

        if args.verbose:
            print(path)

        if not args.test:
            with open(path, 'w') as metadata_fd:
                metadata_fd.write(contents)

    timeline_response = requests.get('https://dev.azure.com/ansible/ansible/_apis/build/builds/%s/timeline?api-version=6.0' % args.run)
    timeline_response.raise_for_status()
    timeline = timeline_response.json()
    roots = set()
    by_id = {}
    children_of = {}
    parent_of = {}
    for r in timeline['records']:
        thisId = r['id']
        parentId = r['parentId']

        by_id[thisId] = r

        if parentId is None:
            roots.add(thisId)
        else:
            parent_of[thisId] = parentId
            children_of[parentId] = children_of.get(parentId, []) + [thisId]

    allowed = set()

    def allow_recursive(ei):
        allowed.add(ei)
        for ci in children_of.get(ei, []):
            allow_recursive(ci)

    for ri in roots:
        r = by_id[ri]
        allowed.add(ri)
        for ci in children_of.get(r['id'], []):
            c = by_id[ci]
            if not args.match_job_name.match(""%s %s"" % (r['name'], c['name'])):
                continue
            allow_recursive(c['id'])

    if args.artifacts:
        artifact_list_url = 'https://dev.azure.com/ansible/ansible/_apis/build/builds/%s/artifacts?api-version=6.0' % args.run
        artifact_list_response = requests.get(artifact_list_url)
        artifact_list_response.raise_for_status()
        for artifact in artifact_list_response.json()['value']:
            if artifact['source'] not in allowed or not args.match_artifact_name.match(artifact['name']):
                continue
            if args.verbose:
                print('%s/%s' % (output_dir, artifact['name']))
            if not args.test:
                response = requests.get(artifact['resource']['downloadUrl'])
                response.raise_for_status()
                archive = zipfile.ZipFile(io.BytesIO(response.content))
                archive.extractall(path=output_dir)

    if args.console_logs:
        for r in timeline['records']:
            if not r['log'] or r['id'] not in allowed or not args.match_artifact_name.match(r['name']):
                continue
            names = []
            parent_id = r['id']
            while parent_id is not None:
                p = by_id[parent_id]
                name = p['name']
                if name not in names:
                    names = [name] + names
                parent_id = parent_of.get(p['id'], None)

            path = "" "".join(names)

            # Some job names have the separator in them.
            path = path.replace(os.sep, '_')

            log_path = os.path.join(output_dir, '%s.log' % path)
            if args.verbose:
                print(log_path)
            if not args.test:
                log = requests.get(r['log']['url'])
                log.raise_for_status()
                open(log_path, 'wb').write(log.content)",_7997.py,99,"open(log_path, 'wb').write(log.content)","with open(log_path, 'wb') as my_f:
    my_f.write(log.content)"
https://github.com/korcankaraokcu/PINCE/tree/master/libpince/GDB_Engine.py,"def get_dissect_code_status():
    """"""Returns the current state of dissect code process

    Returns:
        tuple:(current_region, current_region_count, referenced_strings_count,
                               referenced_jumps_count, referenced_calls_count)

        current_region-->(str) Currently scanned memory region
        current_region_count-->(str) ""Region x of y""
        current_range-->(str) Currently scanned memory range(current buffer)
        referenced_strings_count-->(int) Count of referenced strings
        referenced_jumps_count-->(int) Count of referenced jumps
        referenced_calls_count-->(int) Count of referenced calls

        Returns a tuple of ("""", """", """", 0, 0, 0) if fails to gather info
    """"""
    dissect_code_status_file = SysUtils.get_dissect_code_status_file(currentpid)
    try:
        output = pickle.load(open(dissect_code_status_file, ""rb""))
    except:
        output = """", """", """", 0, 0, 0
    return output",_8330.py,19,"output = pickle.load(open(dissect_code_status_file, 'rb'))","with open(dissect_code_status_file, 'rb') as my_f:
    output = pickle.load(my_f)"
https://github.com/korcankaraokcu/PINCE/tree/master/libpince/GDB_Engine.py,"def get_dissect_code_status():
    """"""Returns the current state of dissect code process

    Returns:
        tuple:(current_region, current_region_count, referenced_strings_count,
                               referenced_jumps_count, referenced_calls_count)

        current_region-->(str) Currently scanned memory region
        current_region_count-->(str) ""Region x of y""
        current_range-->(str) Currently scanned memory range(current buffer)
        referenced_strings_count-->(int) Count of referenced strings
        referenced_jumps_count-->(int) Count of referenced jumps
        referenced_calls_count-->(int) Count of referenced calls

        Returns a tuple of ("""", """", """", 0, 0, 0) if fails to gather info
    """"""
    dissect_code_status_file = SysUtils.get_dissect_code_status_file(currentpid)
    try:
        output = pickle.load(open(dissect_code_status_file, ""rb""))
    except:
        output = """", """", """", 0, 0, 0
    return output",_8330.py,19,"output = pickle.load(open(dissect_code_status_file, 'rb'))","with open(dissect_code_status_file, 'rb') as my_f:
    output = pickle.load(my_f)"
https://github.com/SymbiFlow/prjxray/tree/master/fuzzers/032-cmt-pll/top.py,"def main():
    sites = sorted(list(gen_sites()))
    max_sites = len(sites)

    f = open('params.jl', 'w')
    f.write('module,loc,params\n')

    routes_file = open('routes.txt', 'w')

    print(
        """"""
module top(
    input [{N}:0] clkin1,
    input [{N}:0] clkin2,
    input [{N}:0] clkfb,
    input [{N}:0] dclk
);

    (* KEEP, DONT_TOUCH *)
    LUT1 dummy();
"""""".format(N=max_sites - 1))

    for i, (
            tile_name,
            tile_type,
            site,
    ) in enumerate(sorted(gen_sites())):
        params = {
            ""site"":
            site,
            'active':
            random.random() > .2,
            ""clkin1_conn"":
            random.choice(
                (""clkfbout_mult_BUFG_"" + site, ""clkin1[{}]"".format(i), """")),
            ""clkin2_conn"":
            random.choice(
                (""clkfbout_mult_BUFG_"" + site, ""clkin2[{}]"".format(i), """")),
            ""dclk_conn"":
            random.choice((
                ""0"",
                ""dclk[{}]"".format(i),
            )),
            ""dwe_conn"":
            random.choice((
                """",
                ""1"",
                ""0"",
                ""dwe_"" + site,
                ""den_"" + site,
            )),
            ""den_conn"":
            random.choice((
                """",
                ""1"",
                ""0"",
                ""den_"" + site,
            )),
            ""daddr4_conn"":
            random.choice((
                ""0"",
                ""dwe_"" + site,
            )),
            ""IS_RST_INVERTED"":
            random.randint(0, 1),
            ""IS_PWRDWN_INVERTED"":
            random.randint(0, 1),
            ""IS_CLKINSEL_INVERTED"":
            random.randint(0, 1),
            ""CLKFBOUT_MULT"":
            random.randint(2, 4),
            ""CLKOUT0_DIVIDE"":
            random.randint(1, 128),
            ""CLKOUT1_DIVIDE"":
            random.randint(1, 128),
            ""CLKOUT2_DIVIDE"":
            random.randint(1, 128),
            ""CLKOUT3_DIVIDE"":
            random.randint(1, 128),
            ""CLKOUT4_DIVIDE"":
            random.randint(1, 128),
            ""CLKOUT5_DIVIDE"":
            random.randint(1, 128),
            ""DIVCLK_DIVIDE"":
            random.randint(1, 5),
            ""CLKOUT0_DUTY_CYCLE"":
            ""0.500"",
            ""STARTUP_WAIT"":
            verilog.quote('TRUE' if random.randint(0, 1) else 'FALSE'),
            ""COMPENSATION"":
            verilog.quote(
                random.choice((
                    'ZHOLD',
                    'BUF_IN',
                    'EXTERNAL',
                    'INTERNAL',
                ))),
            ""BANDWIDTH"":
            verilog.quote(random.choice((
                'OPTIMIZED',
                'HIGH',
                'LOW',
            ))),
        }

        if verilog.unquote(params['COMPENSATION']) == 'ZHOLD':
            params['clkfbin_conn'] = random.choice(
                (
                    """",
                    ""clkfbout_mult_BUFG_"" + site,
                ))
        elif verilog.unquote(params['COMPENSATION']) == 'INTERNAL':
            params['clkfbin_conn'] = random.choice(
                (
                    """",
                    ""clkfbout_mult_"" + site,
                ))
        else:
            params['clkfbin_conn'] = random.choice(
                ("""", ""clkfb[{}]"".format(i), ""clkfbout_mult_BUFG_"" + site))

        params['clkin1_route'] = random.choice(
            (
                ""{}_CLKIN1"",
                ""{}_FREQ_BB0"",
                ""{}_FREQ_BB1"",
                ""{}_FREQ_BB2"",
                ""{}_FREQ_BB3"",
                ""{}_PLLE2_CLK_IN1_INT"",
            )).format(tile_type)

        params['clkin2_route'] = random.choice(
            (
                ""{}_CLKIN2"",
                ""{}_FREQ_BB0"",
                ""{}_FREQ_BB1"",
                ""{}_FREQ_BB2"",
                ""{}_FREQ_BB3"",
                ""{}_PLLE2_CLK_IN2_INT"",
            )).format(tile_type)

        params['clkfbin_route'] = random.choice(
            (
                ""{}_CLKFBOUT2IN"",
                ""{}_UPPER_T_FREQ_BB0"",
                ""{}_UPPER_T_FREQ_BB1"",
                ""{}_UPPER_T_FREQ_BB2"",
                ""{}_UPPER_T_FREQ_BB3"",
                ""{}_UPPER_T_PLLE2_CLK_FB_INT"",
            )).format(tile_type.replace(""_UPPER_T"", """"))

        f.write('%s\n' % (json.dumps(params)))

        def make_ibuf_net(net):
            p = net.find('[')
            return net[:p] + '_IBUF' + net[p:]

        if params['clkin1_conn'] != """":
            net = make_ibuf_net(params['clkin1_conn'])
            wire = '{}/{}'.format(tile_name, params['clkin1_route'])
            routes_file.write('{} {}\n'.format(net, wire))

        if params['clkin2_conn'] != """":
            net = make_ibuf_net(params['clkin2_conn'])
            wire = '{}/{}'.format(tile_name, params['clkin2_route'])
            routes_file.write('{} {}\n'.format(net, wire))

        if params['clkfbin_conn'] != """" and\
           params['clkfbin_conn'] != (""clkfbout_mult_BUFG_"" + site):
            net = params['clkfbin_conn']
            if ""["" in net and ""]"" in net:
                net = make_ibuf_net(net)
            wire = '{}/{}'.format(tile_name, params['clkfbin_route'])
            routes_file.write('{} {}\n'.format(net, wire))

        if not params['active']:
            continue

        print(
            """"""

    wire den_{site};
    wire dwe_{site};

    LUT1 den_lut_{site} (
        .O(den_{site})
    );

    LUT1 dwe_lut_{site} (
        .O(dwe_{site})
    );

    wire clkfbout_mult_{site};
    wire clkfbout_mult_BUFG_{site};
    wire clkout0_{site};
    wire clkout1_{site};
    wire clkout2_{site};
    wire clkout3_{site};
    wire clkout4_{site};
    wire clkout5_{site};
    (* KEEP, DONT_TOUCH, LOC = ""{site}"" *)
    PLLE2_ADV #(
            .IS_RST_INVERTED({IS_RST_INVERTED}),
            .IS_PWRDWN_INVERTED({IS_PWRDWN_INVERTED}),
            .IS_CLKINSEL_INVERTED({IS_CLKINSEL_INVERTED}),
            .CLKOUT0_DIVIDE({CLKOUT0_DIVIDE}),
            .CLKOUT1_DIVIDE({CLKOUT1_DIVIDE}),
            .CLKOUT2_DIVIDE({CLKOUT2_DIVIDE}),
            .CLKOUT3_DIVIDE({CLKOUT3_DIVIDE}),
            .CLKOUT4_DIVIDE({CLKOUT4_DIVIDE}),
            .CLKOUT5_DIVIDE({CLKOUT5_DIVIDE}),
            .CLKFBOUT_MULT({CLKFBOUT_MULT}),
            .DIVCLK_DIVIDE({DIVCLK_DIVIDE}),
            .STARTUP_WAIT({STARTUP_WAIT}),
            .CLKOUT0_DUTY_CYCLE({CLKOUT0_DUTY_CYCLE}),
            .COMPENSATION({COMPENSATION}),
            .BANDWIDTH({BANDWIDTH}),
            .CLKIN1_PERIOD(10.0),
            .CLKIN2_PERIOD(10.0)
    ) pll_{site} (
            .CLKFBOUT(clkfbout_mult_{site}),
            .CLKOUT0(clkout0_{site}),
            .CLKOUT1(clkout1_{site}),
            .CLKOUT2(clkout2_{site}),
            .CLKOUT3(clkout3_{site}),
            .CLKOUT4(clkout4_{site}),
            .CLKOUT5(clkout5_{site}),
            .DRDY(),
            .LOCKED(),
            .DO(),
            .CLKFBIN({clkfbin_conn}),
            .CLKIN1({clkin1_conn}),
            .CLKIN2({clkin2_conn}),
            .CLKINSEL(),
            .DCLK({dclk_conn}),
            .DEN({den_conn}),
            .DWE({dwe_conn}),
            .PWRDWN(),
            .RST(),
            .DI(),
            .DADDR({{7{{ {daddr4_conn} }} }}));

    (* KEEP, DONT_TOUCH *)
    BUFG bufg_{site} (
        .I(clkfbout_mult_{site}),
        .O(clkfbout_mult_BUFG_{site})
    );

    (* KEEP, DONT_TOUCH *)
    FDRE reg_clkfbout_mult_{site} (
        .C(clkfbout_mult_{site})
    );

    (* KEEP, DONT_TOUCH *)
    FDRE reg_clkout0_{site} (
        .C(clkout0_{site})
    );

    (* KEEP, DONT_TOUCH *)
    FDRE reg_clkout1_{site} (
        .C(clkout1_{site})
    );

    (* KEEP, DONT_TOUCH *)
    FDRE reg_clkout2_{site} (
        .C(clkout2_{site})
    );

    (* KEEP, DONT_TOUCH *)
    FDRE reg_clkout3_{site} (
        .C(clkout3_{site})
    );

    (* KEEP, DONT_TOUCH *)
    FDRE reg_clkout4_{site} (
        .C(clkout4_{site})
    );

    (* KEEP, DONT_TOUCH *)
    FDRE reg_clkout5_{site} (
        .C(clkout5_{site})
    );
            """""".format(**params))

    print('endmodule')

    f.close()",_8331.py,5,"f = open('params.jl', 'w')","with open('params.jl', 'w') as f:
    f.write('module,loc,params\n')
    routes_file = open('routes.txt', 'w')
    print('\nmodule top(\n    input [{N}:0] clkin1,\n    input [{N}:0] clkin2,\n    input [{N}:0] clkfb,\n    input [{N}:0] dclk\n);\n\n    (* KEEP, DONT_TOUCH *)\n    LUT1 dummy();\n'.format(N=max_sites - 1))
    for (i, (tile_name, tile_type, site)) in enumerate(sorted(gen_sites())):
        params = {'site': site, 'active': random.random() > 0.2, 'clkin1_conn': random.choice(('clkfbout_mult_BUFG_' + site, 'clkin1[{}]'.format(i), '')), 'clkin2_conn': random.choice(('clkfbout_mult_BUFG_' + site, 'clkin2[{}]'.format(i), '')), 'dclk_conn': random.choice(('0', 'dclk[{}]'.format(i))), 'dwe_conn': random.choice(('', '1', '0', 'dwe_' + site, 'den_' + site)), 'den_conn': random.choice(('', '1', '0', 'den_' + site)), 'daddr4_conn': random.choice(('0', 'dwe_' + site)), 'IS_RST_INVERTED': random.randint(0, 1), 'IS_PWRDWN_INVERTED': random.randint(0, 1), 'IS_CLKINSEL_INVERTED': random.randint(0, 1), 'CLKFBOUT_MULT': random.randint(2, 4), 'CLKOUT0_DIVIDE': random.randint(1, 128), 'CLKOUT1_DIVIDE': random.randint(1, 128), 'CLKOUT2_DIVIDE': random.randint(1, 128), 'CLKOUT3_DIVIDE': random.randint(1, 128), 'CLKOUT4_DIVIDE': random.randint(1, 128), 'CLKOUT5_DIVIDE': random.randint(1, 128), 'DIVCLK_DIVIDE': random.randint(1, 5), 'CLKOUT0_DUTY_CYCLE': '0.500', 'STARTUP_WAIT': verilog.quote('TRUE' if random.randint(0, 1) else 'FALSE'), 'COMPENSATION': verilog.quote(random.choice(('ZHOLD', 'BUF_IN', 'EXTERNAL', 'INTERNAL'))), 'BANDWIDTH': verilog.quote(random.choice(('OPTIMIZED', 'HIGH', 'LOW')))}
        if verilog.unquote(params['COMPENSATION']) == 'ZHOLD':
            params['clkfbin_conn'] = random.choice(('', 'clkfbout_mult_BUFG_' + site))
        elif verilog.unquote(params['COMPENSATION']) == 'INTERNAL':
            params['clkfbin_conn'] = random.choice(('', 'clkfbout_mult_' + site))
        else:
            params['clkfbin_conn'] = random.choice(('', 'clkfb[{}]'.format(i), 'clkfbout_mult_BUFG_' + site))
        params['clkin1_route'] = random.choice(('{}_CLKIN1', '{}_FREQ_BB0', '{}_FREQ_BB1', '{}_FREQ_BB2', '{}_FREQ_BB3', '{}_PLLE2_CLK_IN1_INT')).format(tile_type)
        params['clkin2_route'] = random.choice(('{}_CLKIN2', '{}_FREQ_BB0', '{}_FREQ_BB1', '{}_FREQ_BB2', '{}_FREQ_BB3', '{}_PLLE2_CLK_IN2_INT')).format(tile_type)
        params['clkfbin_route'] = random.choice(('{}_CLKFBOUT2IN', '{}_UPPER_T_FREQ_BB0', '{}_UPPER_T_FREQ_BB1', '{}_UPPER_T_FREQ_BB2', '{}_UPPER_T_FREQ_BB3', '{}_UPPER_T_PLLE2_CLK_FB_INT')).format(tile_type.replace('_UPPER_T', ''))
        f.write('%s\n' % json.dumps(params))

        def make_ibuf_net(net):
            p = net.find('[')
            return net[:p] + '_IBUF' + net[p:]
        if params['clkin1_conn'] != '':
            net = make_ibuf_net(params['clkin1_conn'])
            wire = '{}/{}'.format(tile_name, params['clkin1_route'])
            routes_file.write('{} {}\n'.format(net, wire))
        if params['clkin2_conn'] != '':
            net = make_ibuf_net(params['clkin2_conn'])
            wire = '{}/{}'.format(tile_name, params['clkin2_route'])
            routes_file.write('{} {}\n'.format(net, wire))
        if params['clkfbin_conn'] != '' and params['clkfbin_conn'] != 'clkfbout_mult_BUFG_' + site:
            net = params['clkfbin_conn']
            if '[' in net and ']' in net:
                net = make_ibuf_net(net)
            wire = '{}/{}'.format(tile_name, params['clkfbin_route'])
            routes_file.write('{} {}\n'.format(net, wire))
        if not params['active']:
            continue
        print('\n\n    wire den_{site};\n    wire dwe_{site};\n\n    LUT1 den_lut_{site} (\n        .O(den_{site})\n    );\n\n    LUT1 dwe_lut_{site} (\n        .O(dwe_{site})\n    );\n\n    wire clkfbout_mult_{site};\n    wire clkfbout_mult_BUFG_{site};\n    wire clkout0_{site};\n    wire clkout1_{site};\n    wire clkout2_{site};\n    wire clkout3_{site};\n    wire clkout4_{site};\n    wire clkout5_{site};\n    (* KEEP, DONT_TOUCH, LOC = ""{site}"" *)\n    PLLE2_ADV #(\n            .IS_RST_INVERTED({IS_RST_INVERTED}),\n            .IS_PWRDWN_INVERTED({IS_PWRDWN_INVERTED}),\n            .IS_CLKINSEL_INVERTED({IS_CLKINSEL_INVERTED}),\n            .CLKOUT0_DIVIDE({CLKOUT0_DIVIDE}),\n            .CLKOUT1_DIVIDE({CLKOUT1_DIVIDE}),\n            .CLKOUT2_DIVIDE({CLKOUT2_DIVIDE}),\n            .CLKOUT3_DIVIDE({CLKOUT3_DIVIDE}),\n            .CLKOUT4_DIVIDE({CLKOUT4_DIVIDE}),\n            .CLKOUT5_DIVIDE({CLKOUT5_DIVIDE}),\n            .CLKFBOUT_MULT({CLKFBOUT_MULT}),\n            .DIVCLK_DIVIDE({DIVCLK_DIVIDE}),\n            .STARTUP_WAIT({STARTUP_WAIT}),\n            .CLKOUT0_DUTY_CYCLE({CLKOUT0_DUTY_CYCLE}),\n            .COMPENSATION({COMPENSATION}),\n            .BANDWIDTH({BANDWIDTH}),\n            .CLKIN1_PERIOD(10.0),\n            .CLKIN2_PERIOD(10.0)\n    ) pll_{site} (\n            .CLKFBOUT(clkfbout_mult_{site}),\n            .CLKOUT0(clkout0_{site}),\n            .CLKOUT1(clkout1_{site}),\n            .CLKOUT2(clkout2_{site}),\n            .CLKOUT3(clkout3_{site}),\n            .CLKOUT4(clkout4_{site}),\n            .CLKOUT5(clkout5_{site}),\n            .DRDY(),\n            .LOCKED(),\n            .DO(),\n            .CLKFBIN({clkfbin_conn}),\n            .CLKIN1({clkin1_conn}),\n            .CLKIN2({clkin2_conn}),\n            .CLKINSEL(),\n            .DCLK({dclk_conn}),\n            .DEN({den_conn}),\n            .DWE({dwe_conn}),\n            .PWRDWN(),\n            .RST(),\n            .DI(),\n            .DADDR({{7{{ {daddr4_conn} }} }}));\n\n    (* KEEP, DONT_TOUCH *)\n    BUFG bufg_{site} (\n        .I(clkfbout_mult_{site}),\n        .O(clkfbout_mult_BUFG_{site})\n    );\n\n    (* KEEP, DONT_TOUCH *)\n    FDRE reg_clkfbout_mult_{site} (\n        .C(clkfbout_mult_{site})\n    );\n\n    (* KEEP, DONT_TOUCH *)\n    FDRE reg_clkout0_{site} (\n        .C(clkout0_{site})\n    );\n\n    (* KEEP, DONT_TOUCH *)\n    FDRE reg_clkout1_{site} (\n        .C(clkout1_{site})\n    );\n\n    (* KEEP, DONT_TOUCH *)\n    FDRE reg_clkout2_{site} (\n        .C(clkout2_{site})\n    );\n\n    (* KEEP, DONT_TOUCH *)\n    FDRE reg_clkout3_{site} (\n        .C(clkout3_{site})\n    );\n\n    (* KEEP, DONT_TOUCH *)\n    FDRE reg_clkout4_{site} (\n        .C(clkout4_{site})\n    );\n\n    (* KEEP, DONT_TOUCH *)\n    FDRE reg_clkout5_{site} (\n        .C(clkout5_{site})\n    );\n            '.format(**params))
    print('endmodule')
    
    pass"
https://github.com/SymbiFlow/prjxray/tree/master/fuzzers/032-cmt-pll/top.py,"def main():
    sites = sorted(list(gen_sites()))
    max_sites = len(sites)

    f = open('params.jl', 'w')
    f.write('module,loc,params\n')

    routes_file = open('routes.txt', 'w')

    print(
        """"""
module top(
    input [{N}:0] clkin1,
    input [{N}:0] clkin2,
    input [{N}:0] clkfb,
    input [{N}:0] dclk
);

    (* KEEP, DONT_TOUCH *)
    LUT1 dummy();
"""""".format(N=max_sites - 1))

    for i, (
            tile_name,
            tile_type,
            site,
    ) in enumerate(sorted(gen_sites())):
        params = {
            ""site"":
            site,
            'active':
            random.random() > .2,
            ""clkin1_conn"":
            random.choice(
                (""clkfbout_mult_BUFG_"" + site, ""clkin1[{}]"".format(i), """")),
            ""clkin2_conn"":
            random.choice(
                (""clkfbout_mult_BUFG_"" + site, ""clkin2[{}]"".format(i), """")),
            ""dclk_conn"":
            random.choice((
                ""0"",
                ""dclk[{}]"".format(i),
            )),
            ""dwe_conn"":
            random.choice((
                """",
                ""1"",
                ""0"",
                ""dwe_"" + site,
                ""den_"" + site,
            )),
            ""den_conn"":
            random.choice((
                """",
                ""1"",
                ""0"",
                ""den_"" + site,
            )),
            ""daddr4_conn"":
            random.choice((
                ""0"",
                ""dwe_"" + site,
            )),
            ""IS_RST_INVERTED"":
            random.randint(0, 1),
            ""IS_PWRDWN_INVERTED"":
            random.randint(0, 1),
            ""IS_CLKINSEL_INVERTED"":
            random.randint(0, 1),
            ""CLKFBOUT_MULT"":
            random.randint(2, 4),
            ""CLKOUT0_DIVIDE"":
            random.randint(1, 128),
            ""CLKOUT1_DIVIDE"":
            random.randint(1, 128),
            ""CLKOUT2_DIVIDE"":
            random.randint(1, 128),
            ""CLKOUT3_DIVIDE"":
            random.randint(1, 128),
            ""CLKOUT4_DIVIDE"":
            random.randint(1, 128),
            ""CLKOUT5_DIVIDE"":
            random.randint(1, 128),
            ""DIVCLK_DIVIDE"":
            random.randint(1, 5),
            ""CLKOUT0_DUTY_CYCLE"":
            ""0.500"",
            ""STARTUP_WAIT"":
            verilog.quote('TRUE' if random.randint(0, 1) else 'FALSE'),
            ""COMPENSATION"":
            verilog.quote(
                random.choice((
                    'ZHOLD',
                    'BUF_IN',
                    'EXTERNAL',
                    'INTERNAL',
                ))),
            ""BANDWIDTH"":
            verilog.quote(random.choice((
                'OPTIMIZED',
                'HIGH',
                'LOW',
            ))),
        }

        if verilog.unquote(params['COMPENSATION']) == 'ZHOLD':
            params['clkfbin_conn'] = random.choice(
                (
                    """",
                    ""clkfbout_mult_BUFG_"" + site,
                ))
        elif verilog.unquote(params['COMPENSATION']) == 'INTERNAL':
            params['clkfbin_conn'] = random.choice(
                (
                    """",
                    ""clkfbout_mult_"" + site,
                ))
        else:
            params['clkfbin_conn'] = random.choice(
                ("""", ""clkfb[{}]"".format(i), ""clkfbout_mult_BUFG_"" + site))

        params['clkin1_route'] = random.choice(
            (
                ""{}_CLKIN1"",
                ""{}_FREQ_BB0"",
                ""{}_FREQ_BB1"",
                ""{}_FREQ_BB2"",
                ""{}_FREQ_BB3"",
                ""{}_PLLE2_CLK_IN1_INT"",
            )).format(tile_type)

        params['clkin2_route'] = random.choice(
            (
                ""{}_CLKIN2"",
                ""{}_FREQ_BB0"",
                ""{}_FREQ_BB1"",
                ""{}_FREQ_BB2"",
                ""{}_FREQ_BB3"",
                ""{}_PLLE2_CLK_IN2_INT"",
            )).format(tile_type)

        params['clkfbin_route'] = random.choice(
            (
                ""{}_CLKFBOUT2IN"",
                ""{}_UPPER_T_FREQ_BB0"",
                ""{}_UPPER_T_FREQ_BB1"",
                ""{}_UPPER_T_FREQ_BB2"",
                ""{}_UPPER_T_FREQ_BB3"",
                ""{}_UPPER_T_PLLE2_CLK_FB_INT"",
            )).format(tile_type.replace(""_UPPER_T"", """"))

        f.write('%s\n' % (json.dumps(params)))

        def make_ibuf_net(net):
            p = net.find('[')
            return net[:p] + '_IBUF' + net[p:]

        if params['clkin1_conn'] != """":
            net = make_ibuf_net(params['clkin1_conn'])
            wire = '{}/{}'.format(tile_name, params['clkin1_route'])
            routes_file.write('{} {}\n'.format(net, wire))

        if params['clkin2_conn'] != """":
            net = make_ibuf_net(params['clkin2_conn'])
            wire = '{}/{}'.format(tile_name, params['clkin2_route'])
            routes_file.write('{} {}\n'.format(net, wire))

        if params['clkfbin_conn'] != """" and\
           params['clkfbin_conn'] != (""clkfbout_mult_BUFG_"" + site):
            net = params['clkfbin_conn']
            if ""["" in net and ""]"" in net:
                net = make_ibuf_net(net)
            wire = '{}/{}'.format(tile_name, params['clkfbin_route'])
            routes_file.write('{} {}\n'.format(net, wire))

        if not params['active']:
            continue

        print(
            """"""

    wire den_{site};
    wire dwe_{site};

    LUT1 den_lut_{site} (
        .O(den_{site})
    );

    LUT1 dwe_lut_{site} (
        .O(dwe_{site})
    );

    wire clkfbout_mult_{site};
    wire clkfbout_mult_BUFG_{site};
    wire clkout0_{site};
    wire clkout1_{site};
    wire clkout2_{site};
    wire clkout3_{site};
    wire clkout4_{site};
    wire clkout5_{site};
    (* KEEP, DONT_TOUCH, LOC = ""{site}"" *)
    PLLE2_ADV #(
            .IS_RST_INVERTED({IS_RST_INVERTED}),
            .IS_PWRDWN_INVERTED({IS_PWRDWN_INVERTED}),
            .IS_CLKINSEL_INVERTED({IS_CLKINSEL_INVERTED}),
            .CLKOUT0_DIVIDE({CLKOUT0_DIVIDE}),
            .CLKOUT1_DIVIDE({CLKOUT1_DIVIDE}),
            .CLKOUT2_DIVIDE({CLKOUT2_DIVIDE}),
            .CLKOUT3_DIVIDE({CLKOUT3_DIVIDE}),
            .CLKOUT4_DIVIDE({CLKOUT4_DIVIDE}),
            .CLKOUT5_DIVIDE({CLKOUT5_DIVIDE}),
            .CLKFBOUT_MULT({CLKFBOUT_MULT}),
            .DIVCLK_DIVIDE({DIVCLK_DIVIDE}),
            .STARTUP_WAIT({STARTUP_WAIT}),
            .CLKOUT0_DUTY_CYCLE({CLKOUT0_DUTY_CYCLE}),
            .COMPENSATION({COMPENSATION}),
            .BANDWIDTH({BANDWIDTH}),
            .CLKIN1_PERIOD(10.0),
            .CLKIN2_PERIOD(10.0)
    ) pll_{site} (
            .CLKFBOUT(clkfbout_mult_{site}),
            .CLKOUT0(clkout0_{site}),
            .CLKOUT1(clkout1_{site}),
            .CLKOUT2(clkout2_{site}),
            .CLKOUT3(clkout3_{site}),
            .CLKOUT4(clkout4_{site}),
            .CLKOUT5(clkout5_{site}),
            .DRDY(),
            .LOCKED(),
            .DO(),
            .CLKFBIN({clkfbin_conn}),
            .CLKIN1({clkin1_conn}),
            .CLKIN2({clkin2_conn}),
            .CLKINSEL(),
            .DCLK({dclk_conn}),
            .DEN({den_conn}),
            .DWE({dwe_conn}),
            .PWRDWN(),
            .RST(),
            .DI(),
            .DADDR({{7{{ {daddr4_conn} }} }}));

    (* KEEP, DONT_TOUCH *)
    BUFG bufg_{site} (
        .I(clkfbout_mult_{site}),
        .O(clkfbout_mult_BUFG_{site})
    );

    (* KEEP, DONT_TOUCH *)
    FDRE reg_clkfbout_mult_{site} (
        .C(clkfbout_mult_{site})
    );

    (* KEEP, DONT_TOUCH *)
    FDRE reg_clkout0_{site} (
        .C(clkout0_{site})
    );

    (* KEEP, DONT_TOUCH *)
    FDRE reg_clkout1_{site} (
        .C(clkout1_{site})
    );

    (* KEEP, DONT_TOUCH *)
    FDRE reg_clkout2_{site} (
        .C(clkout2_{site})
    );

    (* KEEP, DONT_TOUCH *)
    FDRE reg_clkout3_{site} (
        .C(clkout3_{site})
    );

    (* KEEP, DONT_TOUCH *)
    FDRE reg_clkout4_{site} (
        .C(clkout4_{site})
    );

    (* KEEP, DONT_TOUCH *)
    FDRE reg_clkout5_{site} (
        .C(clkout5_{site})
    );
            """""".format(**params))

    print('endmodule')

    f.close()",_8331.py,8,"routes_file = open('routes.txt', 'w')","with open('routes.txt', 'w') as routes_file:
    print('\nmodule top(\n    input [{N}:0] clkin1,\n    input [{N}:0] clkin2,\n    input [{N}:0] clkfb,\n    input [{N}:0] dclk\n);\n\n    (* KEEP, DONT_TOUCH *)\n    LUT1 dummy();\n'.format(N=max_sites - 1))
    for (i, (tile_name, tile_type, site)) in enumerate(sorted(gen_sites())):
        params = {'site': site, 'active': random.random() > 0.2, 'clkin1_conn': random.choice(('clkfbout_mult_BUFG_' + site, 'clkin1[{}]'.format(i), '')), 'clkin2_conn': random.choice(('clkfbout_mult_BUFG_' + site, 'clkin2[{}]'.format(i), '')), 'dclk_conn': random.choice(('0', 'dclk[{}]'.format(i))), 'dwe_conn': random.choice(('', '1', '0', 'dwe_' + site, 'den_' + site)), 'den_conn': random.choice(('', '1', '0', 'den_' + site)), 'daddr4_conn': random.choice(('0', 'dwe_' + site)), 'IS_RST_INVERTED': random.randint(0, 1), 'IS_PWRDWN_INVERTED': random.randint(0, 1), 'IS_CLKINSEL_INVERTED': random.randint(0, 1), 'CLKFBOUT_MULT': random.randint(2, 4), 'CLKOUT0_DIVIDE': random.randint(1, 128), 'CLKOUT1_DIVIDE': random.randint(1, 128), 'CLKOUT2_DIVIDE': random.randint(1, 128), 'CLKOUT3_DIVIDE': random.randint(1, 128), 'CLKOUT4_DIVIDE': random.randint(1, 128), 'CLKOUT5_DIVIDE': random.randint(1, 128), 'DIVCLK_DIVIDE': random.randint(1, 5), 'CLKOUT0_DUTY_CYCLE': '0.500', 'STARTUP_WAIT': verilog.quote('TRUE' if random.randint(0, 1) else 'FALSE'), 'COMPENSATION': verilog.quote(random.choice(('ZHOLD', 'BUF_IN', 'EXTERNAL', 'INTERNAL'))), 'BANDWIDTH': verilog.quote(random.choice(('OPTIMIZED', 'HIGH', 'LOW')))}
        if verilog.unquote(params['COMPENSATION']) == 'ZHOLD':
            params['clkfbin_conn'] = random.choice(('', 'clkfbout_mult_BUFG_' + site))
        elif verilog.unquote(params['COMPENSATION']) == 'INTERNAL':
            params['clkfbin_conn'] = random.choice(('', 'clkfbout_mult_' + site))
        else:
            params['clkfbin_conn'] = random.choice(('', 'clkfb[{}]'.format(i), 'clkfbout_mult_BUFG_' + site))
        params['clkin1_route'] = random.choice(('{}_CLKIN1', '{}_FREQ_BB0', '{}_FREQ_BB1', '{}_FREQ_BB2', '{}_FREQ_BB3', '{}_PLLE2_CLK_IN1_INT')).format(tile_type)
        params['clkin2_route'] = random.choice(('{}_CLKIN2', '{}_FREQ_BB0', '{}_FREQ_BB1', '{}_FREQ_BB2', '{}_FREQ_BB3', '{}_PLLE2_CLK_IN2_INT')).format(tile_type)
        params['clkfbin_route'] = random.choice(('{}_CLKFBOUT2IN', '{}_UPPER_T_FREQ_BB0', '{}_UPPER_T_FREQ_BB1', '{}_UPPER_T_FREQ_BB2', '{}_UPPER_T_FREQ_BB3', '{}_UPPER_T_PLLE2_CLK_FB_INT')).format(tile_type.replace('_UPPER_T', ''))
        f.write('%s\n' % json.dumps(params))

        def make_ibuf_net(net):
            p = net.find('[')
            return net[:p] + '_IBUF' + net[p:]
        if params['clkin1_conn'] != '':
            net = make_ibuf_net(params['clkin1_conn'])
            wire = '{}/{}'.format(tile_name, params['clkin1_route'])
            routes_file.write('{} {}\n'.format(net, wire))
        if params['clkin2_conn'] != '':
            net = make_ibuf_net(params['clkin2_conn'])
            wire = '{}/{}'.format(tile_name, params['clkin2_route'])
            routes_file.write('{} {}\n'.format(net, wire))
        if params['clkfbin_conn'] != '' and params['clkfbin_conn'] != 'clkfbout_mult_BUFG_' + site:
            net = params['clkfbin_conn']
            if '[' in net and ']' in net:
                net = make_ibuf_net(net)
            wire = '{}/{}'.format(tile_name, params['clkfbin_route'])
            routes_file.write('{} {}\n'.format(net, wire))
        if not params['active']:
            continue
        print('\n\n    wire den_{site};\n    wire dwe_{site};\n\n    LUT1 den_lut_{site} (\n        .O(den_{site})\n    );\n\n    LUT1 dwe_lut_{site} (\n        .O(dwe_{site})\n    );\n\n    wire clkfbout_mult_{site};\n    wire clkfbout_mult_BUFG_{site};\n    wire clkout0_{site};\n    wire clkout1_{site};\n    wire clkout2_{site};\n    wire clkout3_{site};\n    wire clkout4_{site};\n    wire clkout5_{site};\n    (* KEEP, DONT_TOUCH, LOC = ""{site}"" *)\n    PLLE2_ADV #(\n            .IS_RST_INVERTED({IS_RST_INVERTED}),\n            .IS_PWRDWN_INVERTED({IS_PWRDWN_INVERTED}),\n            .IS_CLKINSEL_INVERTED({IS_CLKINSEL_INVERTED}),\n            .CLKOUT0_DIVIDE({CLKOUT0_DIVIDE}),\n            .CLKOUT1_DIVIDE({CLKOUT1_DIVIDE}),\n            .CLKOUT2_DIVIDE({CLKOUT2_DIVIDE}),\n            .CLKOUT3_DIVIDE({CLKOUT3_DIVIDE}),\n            .CLKOUT4_DIVIDE({CLKOUT4_DIVIDE}),\n            .CLKOUT5_DIVIDE({CLKOUT5_DIVIDE}),\n            .CLKFBOUT_MULT({CLKFBOUT_MULT}),\n            .DIVCLK_DIVIDE({DIVCLK_DIVIDE}),\n            .STARTUP_WAIT({STARTUP_WAIT}),\n            .CLKOUT0_DUTY_CYCLE({CLKOUT0_DUTY_CYCLE}),\n            .COMPENSATION({COMPENSATION}),\n            .BANDWIDTH({BANDWIDTH}),\n            .CLKIN1_PERIOD(10.0),\n            .CLKIN2_PERIOD(10.0)\n    ) pll_{site} (\n            .CLKFBOUT(clkfbout_mult_{site}),\n            .CLKOUT0(clkout0_{site}),\n            .CLKOUT1(clkout1_{site}),\n            .CLKOUT2(clkout2_{site}),\n            .CLKOUT3(clkout3_{site}),\n            .CLKOUT4(clkout4_{site}),\n            .CLKOUT5(clkout5_{site}),\n            .DRDY(),\n            .LOCKED(),\n            .DO(),\n            .CLKFBIN({clkfbin_conn}),\n            .CLKIN1({clkin1_conn}),\n            .CLKIN2({clkin2_conn}),\n            .CLKINSEL(),\n            .DCLK({dclk_conn}),\n            .DEN({den_conn}),\n            .DWE({dwe_conn}),\n            .PWRDWN(),\n            .RST(),\n            .DI(),\n            .DADDR({{7{{ {daddr4_conn} }} }}));\n\n    (* KEEP, DONT_TOUCH *)\n    BUFG bufg_{site} (\n        .I(clkfbout_mult_{site}),\n        .O(clkfbout_mult_BUFG_{site})\n    );\n\n    (* KEEP, DONT_TOUCH *)\n    FDRE reg_clkfbout_mult_{site} (\n        .C(clkfbout_mult_{site})\n    );\n\n    (* KEEP, DONT_TOUCH *)\n    FDRE reg_clkout0_{site} (\n        .C(clkout0_{site})\n    );\n\n    (* KEEP, DONT_TOUCH *)\n    FDRE reg_clkout1_{site} (\n        .C(clkout1_{site})\n    );\n\n    (* KEEP, DONT_TOUCH *)\n    FDRE reg_clkout2_{site} (\n        .C(clkout2_{site})\n    );\n\n    (* KEEP, DONT_TOUCH *)\n    FDRE reg_clkout3_{site} (\n        .C(clkout3_{site})\n    );\n\n    (* KEEP, DONT_TOUCH *)\n    FDRE reg_clkout4_{site} (\n        .C(clkout4_{site})\n    );\n\n    (* KEEP, DONT_TOUCH *)\n    FDRE reg_clkout5_{site} (\n        .C(clkout5_{site})\n    );\n            '.format(**params))
    print('endmodule')
    f.close()"
https://github.com/astraw/stdeb/tree/master/stdeb/downloader.py,"def get_source_tarball(package_name, verbose=0, allow_unsafe_download=False,
                       release=None):
    download_url, expected_md5_digest = find_tar_gz(package_name,
                                                    verbose=verbose,
                                                    release=release)
    if not download_url.startswith('https://'):
        if allow_unsafe_download:
            warnings.warn('downloading from unsafe url: %r' % download_url)
        else:
            raise ValueError('PYPI returned unsafe url: %r' % download_url)

    fname = download_url.split('/')[-1]
    if expected_md5_digest is not None:
        if os.path.exists(fname):
            actual_md5_digest = md5sum(fname)
            if actual_md5_digest == expected_md5_digest:
                if verbose >= 1:
                    myprint('Download URL: %s' % download_url)
                    myprint(
                        'File ""%s"" already exists with correct checksum.' %
                        fname)
                return fname
            else:
                raise ValueError(
                    'File ""%s"" exists but has wrong checksum.' % fname)
    if verbose >= 1:
        myprint('downloading %s' % download_url)
    headers = {'User-Agent': USER_AGENT}
    r = requests.get(download_url, headers=headers)
    r.raise_for_status()
    package_tar_gz = r.content
    if verbose >= 1:
        myprint('done downloading %d bytes.' % (len(package_tar_gz), ))
    if expected_md5_digest is not None:
        m = hashlib.md5()
        m.update(package_tar_gz)
        actual_md5_digest = m.hexdigest()
        if verbose >= 2:
            myprint('md5:   actual %s\n     expected %s' %
                    (actual_md5_digest, expected_md5_digest))
        if actual_md5_digest != expected_md5_digest:
            raise ValueError('actual and expected md5 digests do not match')
    else:
        warnings.warn('no md5 digest found -- cannot verify source file')

    fd = open(fname, mode='wb')
    fd.write(package_tar_gz)
    fd.close()
    return fname",_8399.py,46,"fd = open(fname, mode='wb')","with open(fname, mode='wb') as fd:
    fd.write(package_tar_gz)
    
    pass
    return fname"
https://github.com/andabi/music-source-separation/tree/master//utils.py,"def nd_array_to_txt(filename, data):
    path = filename + '.txt'
    file = open(path, 'w')
    with file as outfile:
        # I'm writing a header here just for the sake of readability
        # Any line starting with ""#"" will be ignored by numpy.loadtxt
        outfile.write('# Array shape: {0}\n'.format(data.shape))

        # Iterating through a ndimensional array produces slices along
        # the last axis. This is equivalent to data[i,:,:] in this case
        for data_slice in data:

            # The formatting string indicates that I'm writing out
            # the values in left-justified columns 7 characters in width
            # with 2 decimal places.
            np.savetxt(outfile, data_slice, fmt='%-7.2f')

            # Writing out a break to indicate different slices...
            outfile.write('# New slice\n')",_8660.py,3,"file = open(path, 'w')","with open(path, 'w') as file:
    with file as outfile:
        outfile.write('# Array shape: {0}\n'.format(data.shape))
        for data_slice in data:
            np.savetxt(outfile, data_slice, fmt='%-7.2f')
            outfile.write('# New slice\n')"
https://github.com/pypa/warehouse/tree/master//gunicorn-uploads.conf.py,"def when_ready(server):
    open(""/tmp/app-initialized"", ""w"").close()",_8718.py,2,"open('/tmp/app-initialized', 'w').close()","with open('/tmp/app-initialized', 'w') as my_f:
    my_f.close()"
https://github.com/pyenchant/pyenchant/tree/master/enchant/pypwl.py,"def add(self, word: str) -> None:
        """"""Add a word to the user's personal dictionary.
        For a PWL, this means appending it to the file.
        """"""
        if self.pwl is not None:
            pwl_f = open(self.pwl, ""a"")
            pwl_f.write(""%s\n"" % (word.strip(),))
            pwl_f.close()
        self.add_to_session(word)",_8834.py,6,"pwl_f = open(self.pwl, 'a')","with open(self.pwl, 'a') as pwl_f:
    pwl_f.write('%s\n' % (word.strip(),))"
https://github.com/easezyc/deep-transfer-learning/tree/master/Application/cross-domain fraud detection/data/dataset.py,"def __init__(self, path, ratio):
        """"""
            :param
            path: the path of dataset
            ratio: the ritio of oversample
        """"""
        self.path = path
        file = open(self.path, 'r')
        self.lines = file.readlines()
        file.close()
        self.categorical_columns = 50
        self.numerical_columns = 6
        self.maxnum_events = 11  # 10 historical event 1 target payment event
        self.nb_features_per_event = self.categorical_columns + self.numerical_columns

        length = len(self.lines)
        self.weight = []
        black_num = 0
        for i in range(length):
            tmp = self.lines[i].split(' ')
            if int(tmp[2]) == 1:
                self.weight.append(ratio)
                black_num += 1
            else:
                self.weight.append(1)
        print(path, 'black num:', black_num, 'white num:', length - black_num)",_9197.py,8,"file = open(self.path, 'r')","with open(self.path, 'r') as file:
    self.lines = file.readlines()
    self.categorical_columns = 50
    self.numerical_columns = 6
    self.maxnum_events = 11
    self.nb_features_per_event = self.categorical_columns + self.numerical_columns
    length = len(self.lines)
    self.weight = []
    black_num = 0
    for i in range(length):
        tmp = self.lines[i].split(' ')
        if int(tmp[2]) == 1:
            self.weight.append(ratio)
            black_num += 1
        else:
            self.weight.append(1)
    print(path, 'black num:', black_num, 'white num:', length - black_num)"
https://github.com/NVIDIAGameWorks/kaolin/tree/master/kaolin/io/off.py,"def import_mesh(path, with_face_colors=False):
    r""""""Load data from an off file as a single mesh.

    Args:
        path (str): path to the obj file (with extension).
        with_face_colors (bool): if True, load face colors. Default: False.

    Returns:
        (off.return_type):
            nametuple of:

            - **vertices** (torch.FloatTensor): of shape :math:`(\text{num_vertices}, 3)`.
            - **faces** (torch.LongTensor): of shape :math:`(\text{num_faces}, \text{face_size})`.
            - **face_colors** (torch.LongTensor):
              in the range :math:`[0, 255]`, of shape :math:`(\text{num_faces}, 3)`.
    """"""
    vertices = []
    uvs = []
    f = open(path, 'r', encoding='utf-8')
    # Get metadata (number of vertices / faces (/ edges))
    for line in f:
        data = line.split()
        if _is_void(data):
            continue
        if data[0].startswith('OFF'):
            # ModelNet40 have some OFFnum_vertices num_faces
            if len(data[0][3:]) > 0:
                num_vertices = int(data[0][3:])
                num_faces = int(data[1])
                break
            elif len(data) > 1:
                num_vertices = int(data[1])
                num_faces = int(data[2])
                break
            continue
        num_vertices = int(data[0])
        num_faces = int(data[1])
        break

    # Get vertices
    for line in f:
        data = line.split()
        if _is_void(data):
            continue
        vertices.append([float(d) for d in data[:3]])
        if len(vertices) == num_vertices:
            break
    vertices = torch.FloatTensor(vertices)

    # Get faces
    faces = []
    face_colors = []
    for line in f:
        data = line.split()
        if _is_void(data):
            continue
        face_size = int(data[0])
        faces.append([int(d) for d in data[1:face_size + 1]])
        if with_face_colors:
            face_colors.append([
                float(d) for d in data[face_size + 1:face_size + 4]
            ])
        if len(faces) == num_faces:
            break
    faces = torch.LongTensor(faces)
    if with_face_colors:
        face_colors = torch.LongTensor(face_colors)
    else:
        face_colors = None

    f.close()
    return return_type(vertices, faces, face_colors)",_9615.py,19,"f = open(path, 'r', encoding='utf-8')","with open(path, 'r', encoding='utf-8') as f:
    for line in f:
        data = line.split()
        if _is_void(data):
            continue
        if data[0].startswith('OFF'):
            if len(data[0][3:]) > 0:
                num_vertices = int(data[0][3:])
                num_faces = int(data[1])
                break
            elif len(data) > 1:
                num_vertices = int(data[1])
                num_faces = int(data[2])
                break
            continue
        num_vertices = int(data[0])
        num_faces = int(data[1])
        break
    for line in f:
        data = line.split()
        if _is_void(data):
            continue
        vertices.append([float(d) for d in data[:3]])
        if len(vertices) == num_vertices:
            break
    vertices = torch.FloatTensor(vertices)
    faces = []
    face_colors = []
    for line in f:
        data = line.split()
        if _is_void(data):
            continue
        face_size = int(data[0])
        faces.append([int(d) for d in data[1:face_size + 1]])
        if with_face_colors:
            face_colors.append([float(d) for d in data[face_size + 1:face_size + 4]])
        if len(faces) == num_faces:
            break
    faces = torch.LongTensor(faces)
    if with_face_colors:
        face_colors = torch.LongTensor(face_colors)
    else:
        face_colors = None
    
    pass
    return return_type(vertices, faces, face_colors)"
https://github.com/GoVanguard/legion/tree/master/ui/helpDialog.py,"def __init__(self,parent = None):
        super(License, self).__init__(parent)
        self.setReadOnly(True)
        self.setWindowTitle('License')
        self.setGeometry(0, 0, 300, 300)
        self.center()
        self.setPlainText(open('LICENSE','r').read())",_9652.py,7,"self.setPlainText(open('LICENSE', 'r').read())","with open('LICENSE', 'r') as my_f:
    self.setPlainText(my_f.read())"
https://github.com/bhoov/exbert/tree/master/server/spacyface/spacyface/utils/sentence_extracting.py,"def extract_sentences_to_file(infile, outfname:str):
    """"""Extract sentences from a file into a new file indicated by `outfname`.""""""
    out = open(outfname, 'x')

    linegen = extract_lines(infile)

    for line in linegen:
        out.write(line + ""\n"")

    out.close()",_10307.py,3,"out = open(outfname, 'x')","with open(outfname, 'x') as out:
    linegen = extract_lines(infile)
    for line in linegen:
        out.write(line + '\n')"
https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/serverlib.py,"def record_torrent(self, item, hist_folder):
        tmp_dir = TMPDIR
        name = ''
        if not os.path.exists(hist_folder):
            os.makedirs(hist_folder)
        if item.startswith('http') or os.path.isfile(item):
            home = hist_folder
            name1 = os.path.basename(item).replace('.torrent', '')
            torrent_dest1 = os.path.join(tmp_dir, name1+'.torrent')
            if not os.path.exists(torrent_dest1):
                if item.startswith('http'):
                    ccurl(item+'#'+'-o'+'#'+torrent_dest1)
                else:
                    shutil.copy(item, torrent_dest1)
            if os.path.exists(torrent_dest1):
                info = lt.torrent_info(torrent_dest1)
                name = info.name()
                torrent_dest = os.path.join(home, name+'.torrent')
                shutil.copy(torrent_dest1, torrent_dest)
            logger.info(name)
        elif item.startswith('magnet:'):
            torrent_handle, stream_session, info = get_torrent_info_magnet(
                item, tmp_dir, self, ui.progress, tmp_dir)
            torrent_file = lt.create_torrent(info)
            home = hist_folder
            name = info.name()
            torrent_dest = os.path.join(home, name+'.torrent')
            with open(torrent_dest, ""wb"") as f:
                f.write(lt.bencode(torrent_file.generate()))
            torrent_handle.pause()
            stream_session.pause()
            ui.stop_torrent_forcefully()
        if name:
            torrent_dest = os.path.join(home, name+'.torrent')
            info = lt.torrent_info(torrent_dest)
            file_arr = []
            for f in info.files():
                file_path = f.path
                file_path = os.path.basename(file_path)	
                file_arr.append(file_path)
            if file_arr:
                hist_path = os.path.join(home, 'history.txt')
                if not os.path.isfile(hist_path):
                    hist_dir, last_field = os.path.split(hist_path)
                    if not os.path.exists(hist_dir):
                        os.makedirs(hist_dir)
                    f = open(hist_path, 'w').close()
                if os.path.isfile(hist_path):
                    if (os.stat(hist_path).st_size == 0):
                        write_files(hist_path, name, line_by_line=True)
                    else:
                        lines = open_files(hist_path, True)
                        line_list = []
                        for i in lines:
                            i = i.strip()
                            line_list.append(i)
                        if name not in line_list:
                            write_files(hist_path, name, line_by_line=True)
                
                hist_site = os.path.join(hist_folder, name)
                if not os.path.exists(hist_site):
                    try:
                        os.makedirs(hist_site)
                        hist_epn = os.path.join(hist_site, 'Ep.txt')
                        write_files(hist_epn, file_arr, line_by_line=True)
                        torrent_extra = os.path.join(hist_site, 'title.torrent')
                        shutil.copy(torrent_dest, torrent_extra)
                    except Exception as e:
                        print(e)
        return name",_10615.py,47,"f = open(hist_path, 'w').close()","with open(hist_path, 'w') as my_f:
    f = my_f.close()"
https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/serverlib.py,"def record_torrent(self, item, hist_folder):
        tmp_dir = TMPDIR
        name = ''
        if not os.path.exists(hist_folder):
            os.makedirs(hist_folder)
        if item.startswith('http') or os.path.isfile(item):
            home = hist_folder
            name1 = os.path.basename(item).replace('.torrent', '')
            torrent_dest1 = os.path.join(tmp_dir, name1+'.torrent')
            if not os.path.exists(torrent_dest1):
                if item.startswith('http'):
                    ccurl(item+'#'+'-o'+'#'+torrent_dest1)
                else:
                    shutil.copy(item, torrent_dest1)
            if os.path.exists(torrent_dest1):
                info = lt.torrent_info(torrent_dest1)
                name = info.name()
                torrent_dest = os.path.join(home, name+'.torrent')
                shutil.copy(torrent_dest1, torrent_dest)
            logger.info(name)
        elif item.startswith('magnet:'):
            torrent_handle, stream_session, info = get_torrent_info_magnet(
                item, tmp_dir, self, ui.progress, tmp_dir)
            torrent_file = lt.create_torrent(info)
            home = hist_folder
            name = info.name()
            torrent_dest = os.path.join(home, name+'.torrent')
            with open(torrent_dest, ""wb"") as f:
                f.write(lt.bencode(torrent_file.generate()))
            torrent_handle.pause()
            stream_session.pause()
            ui.stop_torrent_forcefully()
        if name:
            torrent_dest = os.path.join(home, name+'.torrent')
            info = lt.torrent_info(torrent_dest)
            file_arr = []
            for f in info.files():
                file_path = f.path
                file_path = os.path.basename(file_path)	
                file_arr.append(file_path)
            if file_arr:
                hist_path = os.path.join(home, 'history.txt')
                if not os.path.isfile(hist_path):
                    hist_dir, last_field = os.path.split(hist_path)
                    if not os.path.exists(hist_dir):
                        os.makedirs(hist_dir)
                    f = open(hist_path, 'w').close()
                if os.path.isfile(hist_path):
                    if (os.stat(hist_path).st_size == 0):
                        write_files(hist_path, name, line_by_line=True)
                    else:
                        lines = open_files(hist_path, True)
                        line_list = []
                        for i in lines:
                            i = i.strip()
                            line_list.append(i)
                        if name not in line_list:
                            write_files(hist_path, name, line_by_line=True)
                
                hist_site = os.path.join(hist_folder, name)
                if not os.path.exists(hist_site):
                    try:
                        os.makedirs(hist_site)
                        hist_epn = os.path.join(hist_site, 'Ep.txt')
                        write_files(hist_epn, file_arr, line_by_line=True)
                        torrent_extra = os.path.join(hist_site, 'title.torrent')
                        shutil.copy(torrent_dest, torrent_extra)
                    except Exception as e:
                        print(e)
        return name",_10615.py,47,"f = open(hist_path, 'w').close()","with open(hist_path, 'w') as my_f:
    f = my_f.close()"
https://github.com/chubin/wttr.in/tree/master/lib/location.py,"def _ipcache(ip_addr):
    """""" Retrieve a location from cache by ip addr
        Returns a triple of (CITY, REGION, COUNTRY) or None
        TODO: When cache becomes more robust, transition to using latlong
    """"""
    cachefile = os.path.join(IP2LCACHE, ip_addr)

    if os.path.exists(cachefile):
        try:
            _, country, region, city, *_ = open(cachefile, 'r').read().split(';')
            return city, region, country
        except ValueError:
            # cache entry is malformed: should be
            # [ccode];country;region;city;[lat];[long];...
            return None
    else:
        _debug_log(""[_ipcache] %s not found"" % ip_addr)
    return None",_10896.py,10,"(_, country, region, city, *_) = open(cachefile, 'r').read().split(';')","with open(cachefile, 'r') as my_f:
    (_, country, region, city, *_) = my_f.read().split(';')
    return (city, region, country)"
https://github.com/chubin/wttr.in/tree/master/lib/location.py,"def _ipcache(ip_addr):
    """""" Retrieve a location from cache by ip addr
        Returns a triple of (CITY, REGION, COUNTRY) or None
        TODO: When cache becomes more robust, transition to using latlong
    """"""
    cachefile = os.path.join(IP2LCACHE, ip_addr)

    if os.path.exists(cachefile):
        try:
            _, country, region, city, *_ = open(cachefile, 'r').read().split(';')
            return city, region, country
        except ValueError:
            # cache entry is malformed: should be
            # [ccode];country;region;city;[lat];[long];...
            return None
    else:
        _debug_log(""[_ipcache] %s not found"" % ip_addr)
    return None",_10896.py,10,"(_, country, region, city, *_) = open(cachefile, 'r').read().split(';')","with open(cachefile, 'r') as my_f:
    (_, country, region, city, *_) = my_f.read().split(';')"
https://github.com/zllrunning/video-object-removal/tree/master/inpainting/utils.py,"def __init__(self, path, header):
        self.log_file = open(path, 'w')
        self.logger = csv.writer(self.log_file, delimiter='\t')

        self.logger.writerow(header)
        self.header = header",_11022.py,2,"self.log_file = open(path, 'w')","with open(path, 'w') as self.log_file:
    self.logger = csv.writer(self.log_file, delimiter='\t')
    self.logger.writerow(header)
    self.header = header"
https://github.com/flexxui/flexx/tree/master/flexx/app/_app.py,"def __init__(self, cls, *args, **kwargs):
        if not isinstance(cls, type) and issubclass(type, (PyComponent, JsComponent)):
            raise ValueError('App needs a PyComponent or JsComponent class '
                             'as its first argument.')
        self._cls = cls
        self.args = args
        self.kwargs = kwargs
        self._path = cls.__name__  # can be overloaded by serve()
        self._is_served = False

        # Handle good defaults
        if hasattr(cls, 'title') and self.kwargs.get('title', None) is None:
            self.kwargs['title'] = 'Flexx app - ' + cls.__name__
        if hasattr(cls, 'set_icon') and self.kwargs.get('icon', None) is None:
            # Set icon as base64 str; exported apps can still be standalone
            fname = os.path.abspath(os.path.join(__file__, '..', '..',
                                                    'resources', 'flexx.ico'))
            icon_str = encodebytes(open(fname, 'rb').read()).decode()
            self.kwargs['icon'] = 'data:image/ico;base64,' + icon_str",_11134.py,18,"icon_str = encodebytes(open(fname, 'rb').read()).decode()","with open(fname, 'rb') as my_f:
    icon_str = encodebytes(my_f.read()).decode()
    self.kwargs['icon'] = 'data:image/ico;base64,' + icon_str"
https://github.com/flexxui/flexx/tree/master/flexx/app/_app.py,"def __init__(self, cls, *args, **kwargs):
        if not isinstance(cls, type) and issubclass(type, (PyComponent, JsComponent)):
            raise ValueError('App needs a PyComponent or JsComponent class '
                             'as its first argument.')
        self._cls = cls
        self.args = args
        self.kwargs = kwargs
        self._path = cls.__name__  # can be overloaded by serve()
        self._is_served = False

        # Handle good defaults
        if hasattr(cls, 'title') and self.kwargs.get('title', None) is None:
            self.kwargs['title'] = 'Flexx app - ' + cls.__name__
        if hasattr(cls, 'set_icon') and self.kwargs.get('icon', None) is None:
            # Set icon as base64 str; exported apps can still be standalone
            fname = os.path.abspath(os.path.join(__file__, '..', '..',
                                                    'resources', 'flexx.ico'))
            icon_str = encodebytes(open(fname, 'rb').read()).decode()
            self.kwargs['icon'] = 'data:image/ico;base64,' + icon_str",_11134.py,18,"icon_str = encodebytes(open(fname, 'rb').read()).decode()","with open(fname, 'rb') as my_f:
    icon_str = encodebytes(my_f.read()).decode()"
https://github.com/open-mmlab/mmfashion/tree/master/data/prepare_in_shop.py,"def split_label():
    id2label = {}
    labelf = open(os.path.join(PREFIX, 'list_attr_items.txt')).readlines()
    for line in labelf[2:]:
        aline = line.strip('\n').split()
        id, label = aline[0], aline[1:]
        id2label[id] = label

    def get_label(fn, prefix):
        rf = open(fn).readlines()
        wf = open(os.path.join(PREFIX, '%s_labels.txt' % prefix), 'w')
        for line in rf:
            aline = line.strip('\n').split('/')
            id = aline[3]
            label = id2label[id]
            for element in label:
                if element == '1':
                    wf.write('1 ')
                else:
                    wf.write('0 ')
            wf.write('\n')
        wf.close()

    get_label(os.path.join(PREFIX, 'train_img.txt'), 'train')
    get_label(os.path.join(PREFIX, 'gallery_img.txt'), 'gallery')
    get_label(os.path.join(PREFIX, 'query_img.txt'), 'query')",_11319.py,11,"wf = open(os.path.join(PREFIX, '%s_labels.txt' % prefix), 'w')","with open(os.path.join(PREFIX, '%s_labels.txt' % prefix), 'w') as wf:
    for line in rf:
        aline = line.strip('\n').split('/')
        id = aline[3]
        label = id2label[id]
        for element in label:
            if element == '1':
                wf.write('1 ')
            else:
                wf.write('0 ')
        wf.write('\n')"
https://github.com/open-mmlab/mmfashion/tree/master/data/prepare_in_shop.py,"def split_label():
    id2label = {}
    labelf = open(os.path.join(PREFIX, 'list_attr_items.txt')).readlines()
    for line in labelf[2:]:
        aline = line.strip('\n').split()
        id, label = aline[0], aline[1:]
        id2label[id] = label

    def get_label(fn, prefix):
        rf = open(fn).readlines()
        wf = open(os.path.join(PREFIX, '%s_labels.txt' % prefix), 'w')
        for line in rf:
            aline = line.strip('\n').split('/')
            id = aline[3]
            label = id2label[id]
            for element in label:
                if element == '1':
                    wf.write('1 ')
                else:
                    wf.write('0 ')
            wf.write('\n')
        wf.close()

    get_label(os.path.join(PREFIX, 'train_img.txt'), 'train')
    get_label(os.path.join(PREFIX, 'gallery_img.txt'), 'gallery')
    get_label(os.path.join(PREFIX, 'query_img.txt'), 'query')",_11319.py,3,"labelf = open(os.path.join(PREFIX, 'list_attr_items.txt')).readlines()","with open(os.path.join(PREFIX, 'list_attr_items.txt')) as my_f:
    labelf = my_f.readlines()
    for line in labelf[2:]:
        aline = line.strip('\n').split()
        (id, label) = (aline[0], aline[1:])
        id2label[id] = label

    def get_label(fn, prefix):
        rf = open(fn).readlines()
        wf = open(os.path.join(PREFIX, '%s_labels.txt' % prefix), 'w')
        for line in rf:
            aline = line.strip('\n').split('/')
            id = aline[3]
            label = id2label[id]
            for element in label:
                if element == '1':
                    wf.write('1 ')
                else:
                    wf.write('0 ')
            wf.write('\n')
        wf.close()
    get_label(os.path.join(PREFIX, 'train_img.txt'), 'train')
    get_label(os.path.join(PREFIX, 'gallery_img.txt'), 'gallery')
    get_label(os.path.join(PREFIX, 'query_img.txt'), 'query')"
https://github.com/open-mmlab/mmfashion/tree/master/data/prepare_in_shop.py,"def split_label():
    id2label = {}
    labelf = open(os.path.join(PREFIX, 'list_attr_items.txt')).readlines()
    for line in labelf[2:]:
        aline = line.strip('\n').split()
        id, label = aline[0], aline[1:]
        id2label[id] = label

    def get_label(fn, prefix):
        rf = open(fn).readlines()
        wf = open(os.path.join(PREFIX, '%s_labels.txt' % prefix), 'w')
        for line in rf:
            aline = line.strip('\n').split('/')
            id = aline[3]
            label = id2label[id]
            for element in label:
                if element == '1':
                    wf.write('1 ')
                else:
                    wf.write('0 ')
            wf.write('\n')
        wf.close()

    get_label(os.path.join(PREFIX, 'train_img.txt'), 'train')
    get_label(os.path.join(PREFIX, 'gallery_img.txt'), 'gallery')
    get_label(os.path.join(PREFIX, 'query_img.txt'), 'query')",_11319.py,10,rf = open(fn).readlines(),"with open(fn) as my_f:
    rf = my_f.readlines()
    wf = open(os.path.join(PREFIX, '%s_labels.txt' % prefix), 'w')
    for line in rf:
        aline = line.strip('\n').split('/')
        id = aline[3]
        label = id2label[id]
        for element in label:
            if element == '1':
                wf.write('1 ')
            else:
                wf.write('0 ')
        wf.write('\n')
    wf.close()"
https://github.com/astropy/astropy/tree/master/astropy/io/fits/tests/test_convenience.py,"def test_fileobj_not_closed(self):
        """"""
        Tests that file-like objects are not closed after being passed
        to convenience functions.

        Regression test for https://github.com/astropy/astropy/issues/5063
        """"""

        f = open(self.data(""test0.fits""), ""rb"")
        _ = fits.getdata(f)
        assert not f.closed

        f.seek(0)
        _ = fits.getheader(f)
        assert not f.closed

        f.close()",_11900.py,9,"f = open(self.data('test0.fits'), 'rb')","with open(self.data('test0.fits'), 'rb') as f:
    _ = fits.getdata(f)
    assert not f.closed
    f.seek(0)
    _ = fits.getheader(f)
    assert not f.closed
    
    pass"
https://github.com/tribe29/checkmk/tree/master/omd/packages/omd/omdlib/contexts.py,"def read_site_config(self) -> Config:
        """"""Read and parse the file site.conf of a site into a dictionary and returns it""""""
        config: Config = {}
        confpath = ""%s/etc/omd/site.conf"" % (self.dir)
        if not os.path.exists(confpath):
            return {}

        for line in open(confpath):
            line = line.strip()
            if line == """" or line[0] == ""#"":
                continue
            var, value = line.split(""="", 1)
            if not var.startswith(""CONFIG_""):
                sys.stderr.write(""Ignoring invalid variable %s.\n"" % var)
            else:
                config[var[7:].strip()] = value.strip().strip(""'"")

        return config",_12507.py,8,"for line in open(confpath):
    line = line.strip()
    if line == '' or line[0] == '#':
        continue
    (var, value) = line.split('=', 1)
    if not var.startswith('CONFIG_'):
        sys.stderr.write('Ignoring invalid variable %s.\n' % var)
    else:
        config[var[7:].strip()] = value.strip().strip(""'"")","with open(confpath) as my_f:
    for line in my_f:
        line = line.strip()
        if line == '' or line[0] == '#':
            continue
        (var, value) = line.split('=', 1)
        if not var.startswith('CONFIG_'):
            sys.stderr.write('Ignoring invalid variable %s.\n' % var)
        else:
            config[var[7:].strip()] = value.strip().strip(""'"")"
https://github.com/tribe29/checkmk/tree/master/omd/packages/omd/omdlib/contexts.py,"def read_site_config(self) -> Config:
        """"""Read and parse the file site.conf of a site into a dictionary and returns it""""""
        config: Config = {}
        confpath = ""%s/etc/omd/site.conf"" % (self.dir)
        if not os.path.exists(confpath):
            return {}

        for line in open(confpath):
            line = line.strip()
            if line == """" or line[0] == ""#"":
                continue
            var, value = line.split(""="", 1)
            if not var.startswith(""CONFIG_""):
                sys.stderr.write(""Ignoring invalid variable %s.\n"" % var)
            else:
                config[var[7:].strip()] = value.strip().strip(""'"")

        return config",_12507.py,8,"for line in open(confpath):
    line = line.strip()
    if line == '' or line[0] == '#':
        continue
    (var, value) = line.split('=', 1)
    if not var.startswith('CONFIG_'):
        sys.stderr.write('Ignoring invalid variable %s.\n' % var)
    else:
        config[var[7:].strip()] = value.strip().strip(""'"")","with open(confpath) as my_f:
    for line in my_f:
        line = line.strip()
        if line == '' or line[0] == '#':
            continue
        (var, value) = line.split('=', 1)
        if not var.startswith('CONFIG_'):
            sys.stderr.write('Ignoring invalid variable %s.\n' % var)
        else:
            config[var[7:].strip()] = value.strip().strip(""'"")"
https://github.com/Qianlitp/WatchAD/tree/master/libs/kek/ccache.py,"def load(cls, filename):
        fp = open(filename, 'rb')
        version, headerlen = unpack('>HH', fp.read(4))
        if version != VERSION:
            raise ValueError('Unsupported version: 0x%04x' % version)
        header = fp.read(headerlen)
        primary_principal = cls.read_principal(fp)
        credentials = []
        while True:
            try:
                credentials.append(cls.read_credential(fp))
            except struct.error:
                break
        fp.close()
        return cls(primary_principal, credentials, header)",_12533.py,2,"fp = open(filename, 'rb')","with open(filename, 'rb') as fp:
    (version, headerlen) = unpack('>HH', fp.read(4))
    if version != VERSION:
        raise ValueError('Unsupported version: 0x%04x' % version)
    header = fp.read(headerlen)
    primary_principal = cls.read_principal(fp)
    credentials = []
    while True:
        try:
            credentials.append(cls.read_credential(fp))
        except struct.error:
            break
    return cls(primary_principal, credentials, header)"
https://github.com/bhoov/exbert/tree/master/server/spacyface/spacyface/checker/against_corpus.py,"def check_against_corpus(alnr, corpus_name, hard_assert=True):
    """"""Go through every sentence of the corpus and see if the meta tokenization is different than base transformer tokenization

    Args:
        alnr: Aligner
        corpus_name: Name of text file to parse
        hard_assert: If True, break on first error. Otherwise, print error msg and continue
    """"""
    src = open(corpus_name)
    chunk_gen = extract_chars(src, 100000)
    for c, chunk in enumerate(chunk_gen):
        doc = alnr.spacy_nlp(chunk)
        sents = [sent.text for sent in doc.sents]
        for i, sent in enumerate(sents):
            if i % 100 == 0: print(f""Chunk {c}. Sentence {i}"")
            alnr.check_tokenization(sent, hard_assert)

    src.close()",_12761.py,9,src = open(corpus_name),"with open(corpus_name) as src:
    chunk_gen = extract_chars(src, 100000)
    for (c, chunk) in enumerate(chunk_gen):
        doc = alnr.spacy_nlp(chunk)
        sents = [sent.text for sent in doc.sents]
        for (i, sent) in enumerate(sents):
            if i % 100 == 0:
                print(f'Chunk {c}. Sentence {i}')
            alnr.check_tokenization(sent, hard_assert)"
https://github.com/cisco/mindmeld/tree/master/mindmeld/models/taggers/embeddings.py,"def _add_historic_embeddings(self):
        historic_word_embeddings = {}

        # load historic word embeddings
        if os.path.exists(PREVIOUSLY_USED_WORD_EMBEDDINGS_FILE_PATH):
            pkl_file = open(PREVIOUSLY_USED_WORD_EMBEDDINGS_FILE_PATH, ""rb"")
            historic_word_embeddings = pickle.load(pkl_file)
            pkl_file.close()

        for word in historic_word_embeddings:
            if len(historic_word_embeddings[word]) == self.token_embedding_dimension:
                self.token_to_embedding_mapping[word] = historic_word_embeddings.get(
                    word
                )",_13242.py,6,"pkl_file = open(PREVIOUSLY_USED_WORD_EMBEDDINGS_FILE_PATH, 'rb')","with open(PREVIOUSLY_USED_WORD_EMBEDDINGS_FILE_PATH, 'rb') as pkl_file:
    historic_word_embeddings = pickle.load(pkl_file)"
https://github.com/ansible/galaxy/tree/master/lib/galaxy_test/api/test_dataset_collections.py,"def test_upload_collection(self):
        elements = [{""src"": ""files"", ""dbkey"": ""hg19"", ""info"": ""my cool bed"", ""tags"": [""name:data1"", ""group:condition:treated"", ""machine:illumina""]}]
        targets = [{
            ""destination"": {""type"": ""hdca""},
            ""elements"": elements,
            ""collection_type"": ""list"",
            ""name"": ""Test upload"",
            ""tags"": [""name:collection1""]
        }]
        payload = {
            ""history_id"": self.history_id,
            ""targets"": json.dumps(targets),
            ""__files"": {""files_0|file_data"": open(self.test_data_resolver.get_filename(""4.bed""))},
        }
        self.dataset_populator.fetch(payload)
        hdca = self._assert_one_collection_created_in_history()
        self.assertEqual(hdca[""name""], ""Test upload"")
        hdca_tags = hdca[""tags""]
        assert len(hdca_tags) == 1
        assert ""name:collection1"" in hdca_tags
        assert len(hdca[""elements""]) == 1, hdca
        element0 = hdca[""elements""][0]
        assert element0[""element_identifier""] == ""4.bed""
        dataset0 = element0[""object""]
        assert dataset0[""file_size""] == 61
        dataset_tags = dataset0[""tags""]
        assert len(dataset_tags) == 3, dataset0",_13569.py,13,"payload = {'history_id': self.history_id, 'targets': json.dumps(targets), '__files': {'files_0|file_data': open(self.test_data_resolver.get_filename('4.bed'))}}","with open(self.test_data_resolver.get_filename('4.bed')) as my_f:
    payload = {'history_id': self.history_id, 'targets': json.dumps(targets), '__files': {'files_0|file_data': my_f}}
    self.dataset_populator.fetch(payload)
    hdca = self._assert_one_collection_created_in_history()
    self.assertEqual(hdca['name'], 'Test upload')
    hdca_tags = hdca['tags']
    assert len(hdca_tags) == 1
    assert 'name:collection1' in hdca_tags
    assert len(hdca['elements']) == 1, hdca
    element0 = hdca['elements'][0]
    assert element0['element_identifier'] == '4.bed'
    dataset0 = element0['object']
    assert dataset0['file_size'] == 61
    dataset_tags = dataset0['tags']
    assert len(dataset_tags) == 3, dataset0"
https://github.com/locuslab/deq/tree/master/MDEQ-Vision/lib/datasets/cityscapes.py,"def __init__(self, 
                 root, 
                 list_path, 
                 num_samples=None, 
                 num_classes=19,
                 multi_scale=True, 
                 flip=True, 
                 ignore_label=-1, 
                 base_size=2048, 
                 crop_size=(512, 1024), 
                 center_crop_test=False,
                 downsample_rate=1,
                 scale_factor=16,
                 mean=[0.485, 0.456, 0.406], 
                 std=[0.229, 0.224, 0.225]):

        super(Cityscapes, self).__init__(ignore_label, base_size,
                crop_size, downsample_rate, scale_factor, mean, std,)

        self.root = root
        self.list_path = list_path
        self.num_classes = num_classes
        self.class_weights = torch.FloatTensor([0.8373, 0.918, 0.866, 1.0345, 
                                        1.0166, 0.9969, 0.9754, 1.0489,
                                        0.8786, 1.0023, 0.9539, 0.9843, 
                                        1.1116, 0.9037, 1.0865, 1.0955, 
                                        1.0865, 1.1529, 1.0507]).cuda()

        self.multi_scale = multi_scale
        self.flip = flip
        self.center_crop_test = center_crop_test
        
        self.img_list = [line.strip().split() for line in open(root+list_path)]

        self.files = self.read_files()
        if num_samples:
            self.files = self.files[:num_samples]

        self.label_mapping = {-1: ignore_label, 0: ignore_label, 
                              1: ignore_label, 2: ignore_label, 
                              3: ignore_label, 4: ignore_label, 
                              5: ignore_label, 6: ignore_label, 
                              7: 0, 8: 1, 9: ignore_label, 
                              10: ignore_label, 11: 2, 12: 3, 
                              13: 4, 14: ignore_label, 15: ignore_label, 
                              16: ignore_label, 17: 5, 18: ignore_label, 
                              19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11,
                              25: 12, 26: 13, 27: 14, 28: 15, 
                              29: ignore_label, 30: ignore_label, 
                              31: 16, 32: 17, 33: 18}",_13913.py,33,self.img_list = [line.strip().split() for line in open(root + list_path)],"with open(root + list_path) as my_f:
    self.img_list = [line.strip().split() for line in my_f]
    self.files = self.read_files()
    if num_samples:
        self.files = self.files[:num_samples]
    self.label_mapping = {-1: ignore_label, 0: ignore_label, 1: ignore_label, 2: ignore_label, 3: ignore_label, 4: ignore_label, 5: ignore_label, 6: ignore_label, 7: 0, 8: 1, 9: ignore_label, 10: ignore_label, 11: 2, 12: 3, 13: 4, 14: ignore_label, 15: ignore_label, 16: ignore_label, 17: 5, 18: ignore_label, 19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12, 26: 13, 27: 14, 28: 15, 29: ignore_label, 30: ignore_label, 31: 16, 32: 17, 33: 18}"
https://github.com/Blazemeter/taurus/tree/master/tests/unit/modules/jmeter/test_JMeterExecutor.py,"def test_requests(self):
        self.configure(json.loads(open(RESOURCES_DIR + ""json/get-post.json"").read()))
        self.obj.prepare()
        self.obj.log.debug(""%s: %s"", self.obj.modified_jmx, open(self.obj.modified_jmx).read())
        self.obj.log.debug(""%s"", json.dumps(self.obj.execution, indent=True))
        try:
            self.obj.startup()
            while not self.obj.check():
                self.obj.log.debug(""Check..."")
                time.sleep(self.obj.engine.check_interval)
            self.obj.shutdown()
            self.obj.post_process()
        except:
            pass
        finally:
            if self.obj.jmeter_log and os.path.exists(self.obj.jmeter_log):
                self.obj.log.debug(""%s"", open(self.obj.jmeter_log).read())",_13918.py,4,"self.obj.log.debug('%s: %s', self.obj.modified_jmx, open(self.obj.modified_jmx).read())","with open(self.obj.modified_jmx) as my_f:
    self.obj.log.debug('%s: %s', self.obj.modified_jmx, my_f.read())
    self.obj.log.debug('%s', json.dumps(self.obj.execution, indent=True))
    try:
        self.obj.startup()
        while not self.obj.check():
            self.obj.log.debug('Check...')
            time.sleep(self.obj.engine.check_interval)
        self.obj.shutdown()
        self.obj.post_process()
    except:
        pass
    finally:
        if self.obj.jmeter_log and os.path.exists(self.obj.jmeter_log):
            self.obj.log.debug('%s', open(self.obj.jmeter_log).read())"
https://github.com/Blazemeter/taurus/tree/master/tests/unit/modules/jmeter/test_JMeterExecutor.py,"def test_requests(self):
        self.configure(json.loads(open(RESOURCES_DIR + ""json/get-post.json"").read()))
        self.obj.prepare()
        self.obj.log.debug(""%s: %s"", self.obj.modified_jmx, open(self.obj.modified_jmx).read())
        self.obj.log.debug(""%s"", json.dumps(self.obj.execution, indent=True))
        try:
            self.obj.startup()
            while not self.obj.check():
                self.obj.log.debug(""Check..."")
                time.sleep(self.obj.engine.check_interval)
            self.obj.shutdown()
            self.obj.post_process()
        except:
            pass
        finally:
            if self.obj.jmeter_log and os.path.exists(self.obj.jmeter_log):
                self.obj.log.debug(""%s"", open(self.obj.jmeter_log).read())",_13918.py,2,self.configure(json.loads(open(RESOURCES_DIR + 'json/get-post.json').read())),"with open(RESOURCES_DIR + 'json/get-post.json') as my_f:
    self.configure(json.loads(my_f.read()))
    self.obj.prepare()
    self.obj.log.debug('%s: %s', self.obj.modified_jmx, open(self.obj.modified_jmx).read())
    self.obj.log.debug('%s', json.dumps(self.obj.execution, indent=True))
    try:
        self.obj.startup()
        while not self.obj.check():
            self.obj.log.debug('Check...')
            time.sleep(self.obj.engine.check_interval)
        self.obj.shutdown()
        self.obj.post_process()
    except:
        pass
    finally:
        if self.obj.jmeter_log and os.path.exists(self.obj.jmeter_log):
            self.obj.log.debug('%s', open(self.obj.jmeter_log).read())"
https://github.com/Blazemeter/taurus/tree/master/tests/unit/modules/jmeter/test_JMeterExecutor.py,"def test_requests(self):
        self.configure(json.loads(open(RESOURCES_DIR + ""json/get-post.json"").read()))
        self.obj.prepare()
        self.obj.log.debug(""%s: %s"", self.obj.modified_jmx, open(self.obj.modified_jmx).read())
        self.obj.log.debug(""%s"", json.dumps(self.obj.execution, indent=True))
        try:
            self.obj.startup()
            while not self.obj.check():
                self.obj.log.debug(""Check..."")
                time.sleep(self.obj.engine.check_interval)
            self.obj.shutdown()
            self.obj.post_process()
        except:
            pass
        finally:
            if self.obj.jmeter_log and os.path.exists(self.obj.jmeter_log):
                self.obj.log.debug(""%s"", open(self.obj.jmeter_log).read())",_13918.py,17,"self.obj.log.debug('%s', open(self.obj.jmeter_log).read())","with open(self.obj.jmeter_log) as my_f:
    self.obj.log.debug('%s', my_f.read())"
https://github.com/Blazemeter/taurus/tree/master/tests/unit/modules/jmeter/test_JMeterExecutor.py,"def test_requests(self):
        self.configure(json.loads(open(RESOURCES_DIR + ""json/get-post.json"").read()))
        self.obj.prepare()
        self.obj.log.debug(""%s: %s"", self.obj.modified_jmx, open(self.obj.modified_jmx).read())
        self.obj.log.debug(""%s"", json.dumps(self.obj.execution, indent=True))
        try:
            self.obj.startup()
            while not self.obj.check():
                self.obj.log.debug(""Check..."")
                time.sleep(self.obj.engine.check_interval)
            self.obj.shutdown()
            self.obj.post_process()
        except:
            pass
        finally:
            if self.obj.jmeter_log and os.path.exists(self.obj.jmeter_log):
                self.obj.log.debug(""%s"", open(self.obj.jmeter_log).read())",_13918.py,17,"self.obj.log.debug('%s', open(self.obj.jmeter_log).read())","with open(self.obj.jmeter_log) as my_f:
    self.obj.log.debug('%s', my_f.read())"
https://github.com/zhunzhong07/Random-Erasing/tree/master/utils/logger.py,"def __init__(self, fpath, title=None, resume=False): 
        self.file = None
        self.resume = resume
        self.title = '' if title == None else title
        if fpath is not None:
            if resume: 
                self.file = open(fpath, 'r') 
                name = self.file.readline()
                self.names = name.rstrip().split('\t')
                self.numbers = {}
                for _, name in enumerate(self.names):
                    self.numbers[name] = []

                for numbers in self.file:
                    numbers = numbers.rstrip().split('\t')
                    for i in range(0, len(numbers)):
                        self.numbers[self.names[i]].append(numbers[i])
                self.file.close()
                self.file = open(fpath, 'a')  
            else:
                self.file = open(fpath, 'w')",_14006.py,7,"self.file = open(fpath, 'r')","with open(fpath, 'r') as self.file:
    name = self.file.readline()
    self.names = name.rstrip().split('\t')
    self.numbers = {}
    for (_, name) in enumerate(self.names):
        self.numbers[name] = []
    for numbers in self.file:
        numbers = numbers.rstrip().split('\t')
        for i in range(0, len(numbers)):
            self.numbers[self.names[i]].append(numbers[i])
    
    pass
    self.file = open(fpath, 'a')"
https://github.com/yt-dlp/yt-dlp/tree/master//pyinst.py,"def read_version():
    exec(compile(open('yt_dlp/version.py').read(), 'yt_dlp/version.py', 'exec'))
    return locals()['__version__']",_14444.py,2,"exec(compile(open('yt_dlp/version.py').read(), 'yt_dlp/version.py', 'exec'))","with open('yt_dlp/version.py') as my_f:
    exec(compile(my_f.read(), 'yt_dlp/version.py', 'exec'))
    return locals()['__version__']"
https://github.com/mail-in-a-box/mailinabox/tree/master/management/status_checks.py,"def check_ssh_password(env, output):
	# Check that SSH login with password is disabled. The openssh-server
	# package may not be installed so check that before trying to access
	# the configuration file.
	if not os.path.exists(""/etc/ssh/sshd_config""):
		return
	sshd = open(""/etc/ssh/sshd_config"").read()
	if re.search(""\nPasswordAuthentication\s+yes"", sshd) \
		or not re.search(""\nPasswordAuthentication\s+no"", sshd):
		output.print_error(""""""The SSH server on this machine permits password-based login. A more secure
			way to log in is using a public key. Add your SSH public key to $HOME/.ssh/authorized_keys, check
			that you can log in without a password, set the option 'PasswordAuthentication no' in
			/etc/ssh/sshd_config, and then restart the openssh via 'sudo service ssh restart'."""""")
	else:
		output.print_ok(""SSH disallows password-based login."")",_14586.py,7,sshd = open('/etc/ssh/sshd_config').read(),"with open('/etc/ssh/sshd_config') as my_f:
    sshd = my_f.read()
    if re.search('\nPasswordAuthentication\\s+yes', sshd) or not re.search('\nPasswordAuthentication\\s+no', sshd):
        output.print_error(""The SSH server on this machine permits password-based login. A more secure\n\t\t\tway to log in is using a public key. Add your SSH public key to $HOME/.ssh/authorized_keys, check\n\t\t\tthat you can log in without a password, set the option 'PasswordAuthentication no' in\n\t\t\t/etc/ssh/sshd_config, and then restart the openssh via 'sudo service ssh restart'."")
    else:
        output.print_ok('SSH disallows password-based login.')"
https://github.com/skywind3000/ECDICT/tree/master//stardict.py,"def load (self, filename, encoding = None):
        content = open(filename, 'rb').read()
        if content[:3] == b'\xef\xbb\xbf':
            content = content[3:].decode('utf-8', 'ignore')
        elif encoding is not None:
            text = content.decode(encoding, 'ignore')
        else:
            text = None
            match = ['utf-8', sys.getdefaultencoding(), 'ascii']
            for encoding in match + ['gbk', 'latin1']:
                try:
                    text = content.decode(encoding)
                    break
                except:
                    pass
            if text is None:
                text = content.decode('utf-8', 'ignore')
        number = 0
        for line in text.split('\n'):
            number += 1
            line = line.strip('\r\n ')
            if (not line) or (line[:1] == ';'):
                continue
            pos = line.find('->')
            if not pos:
                continue
            stem = line[:pos].strip()
            p1 = stem.find('/')
            frq = 0
            if p1 >= 0:
                frq = int(stem[p1 + 1:].strip())
                stem = stem[:p1].strip()
            if not stem:
                continue
            if frq > 0:
                self._frqs[stem] = frq
            for word in line[pos + 2:].strip().split(','):
                p1 = word.find('/')
                if p1 >= 0:
                    word = word[:p1].strip()
                if not word:
                    continue
                self.add(stem, word.strip())
        return True",_15127.py,2,"content = open(filename, 'rb').read()","with open(filename, 'rb') as my_f:
    content = my_f.read()
    if content[:3] == b'\xef\xbb\xbf':
        content = content[3:].decode('utf-8', 'ignore')
    elif encoding is not None:
        text = content.decode(encoding, 'ignore')
    else:
        text = None
        match = ['utf-8', sys.getdefaultencoding(), 'ascii']
        for encoding in match + ['gbk', 'latin1']:
            try:
                text = content.decode(encoding)
                break
            except:
                pass
        if text is None:
            text = content.decode('utf-8', 'ignore')
    number = 0
    for line in text.split('\n'):
        number += 1
        line = line.strip('\r\n ')
        if not line or line[:1] == ';':
            continue
        pos = line.find('->')
        if not pos:
            continue
        stem = line[:pos].strip()
        p1 = stem.find('/')
        frq = 0
        if p1 >= 0:
            frq = int(stem[p1 + 1:].strip())
            stem = stem[:p1].strip()
        if not stem:
            continue
        if frq > 0:
            self._frqs[stem] = frq
        for word in line[pos + 2:].strip().split(','):
            p1 = word.find('/')
            if p1 >= 0:
                word = word[:p1].strip()
            if not word:
                continue
            self.add(stem, word.strip())
    return True"
https://github.com/PaddlePaddle/PaddleX/tree/master/paddlex_restful/restful/project/operate.py,"def _call_paddlex_predict(task_path,
                          predict_status_path,
                          params,
                          img_list,
                          img_data,
                          save_dir,
                          score_thresh,
                          epoch=None):
    total_num = open(
        osp.join(predict_status_path, 'total_num'), 'w', encoding='utf-8')

    def write_file_num(total_file_num):
        total_num.write(str(total_file_num))
        total_num.close()

    sys.stdout = open(
        osp.join(predict_status_path, 'out.log'), 'w', encoding='utf-8')
    sys.stderr = open(
        osp.join(predict_status_path, 'err.log'), 'w', encoding='utf-8')

    import paddlex as pdx
    pdx.log_level = 3
    task_type = params['task_type']
    dataset_path = params['dataset_path']
    if epoch is None:
        model_path = osp.join(task_path, 'output', 'best_model')
    else:
        model_path = osp.join(task_path, 'output', 'epoch_{}'.format(epoch))
    model = pdx.load_model(model_path)
    file_list = dict()
    predicted_num = 0
    if task_type == ""classification"":
        if img_data is None:
            if len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                with open(osp.join(dataset_path, ""test_list.txt"")) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = items[1]
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            for image, label_id in file_list.items():
                pred_result = {}
                if label_id is not None:
                    pred_result[""gt_label""] = model.labels[int(label_id)]
                results = model.predict(img_file=image)
                pred_result[""label""] = []
                pred_result[""score""] = []
                pred_result[""topk""] = len(results)
                for res in results:
                    pred_result[""label""].append(res['category'])
                    pred_result[""score""].append(res['score'])
                visualize_classified_result(save_dir, image, pred_result)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            pred_result = {}
            pred_result[""label""] = []
            pred_result[""score""] = []
            pred_result[""topk""] = len(results)
            for res in results:
                pred_result[""label""].append(res['category'])
                pred_result[""score""].append(res['score'])
            visualize_classified_result(save_dir, img, pred_result)
    elif task_type in [""detection"", ""instance_segmentation""]:
        if img_data is None:
            if task_type == ""detection"" and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                if len(img_list) == 0 and osp.exists(
                        osp.join(dataset_path, ""test_list.txt"")):
                    with open(
                            osp.join(dataset_path, ""test_list.txt""),
                            encoding=get_encoding(
                                osp.join(dataset_path, ""test_list.txt""))) as f:
                        for line in f:
                            items = line.strip().split()
                            file_list[osp.join(dataset_path, items[0])] = \
                                osp.join(dataset_path, items[1])
                else:
                    for image in img_list:
                        file_list[image] = None
                total_file_num = len(file_list)
                write_file_num(total_file_num)
                for image, anno in file_list.items():
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(
                        image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    image_gt = None
                    if anno is not None:
                        image_gt = plot_det_label(image, anno, model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            elif len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test.json"")):
                from pycocotools.coco import COCO
                anno_path = osp.join(dataset_path, ""test.json"")
                coco = COCO(anno_path)
                img_ids = coco.getImgIds()
                total_file_num = len(img_ids)
                write_file_num(total_file_num)
                for img_id in img_ids:
                    img_anno = coco.loadImgs(img_id)[0]
                    file_name = img_anno['file_name']
                    name = (osp.split(file_name)[-1]).split(""."")[0]
                    anno = osp.join(dataset_path, ""Annotations"", name + "".npy"")
                    img_file = osp.join(dataset_path, ""JPEGImages"", file_name)
                    results = model.predict(img_file=img_file)
                    image_pred = pdx.det.visualize(
                        img_file,
                        results,
                        threshold=score_thresh,
                        save_dir=None)
                    save_name = osp.join(save_dir, osp.split(img_file)[-1])
                    if task_type == ""detection"":
                        image_gt = plot_det_label(img_file, anno, model.labels)
                    else:
                        image_gt = plot_insseg_label(img_file, anno,
                                                     model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            else:
                total_file_num = len(img_list)
                write_file_num(total_file_num)
                for image in img_list:
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(
                        image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    visualize_detected_result(save_name, None, image_pred)
                    predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            image_pred = pdx.det.visualize(
                img, results, threshold=score_thresh, save_dir=None)
            image_gt = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_detected_result(save_name, image_gt, image_pred)

    elif task_type == ""segmentation"":
        if img_data is None:
            if len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                with open(
                        osp.join(dataset_path, ""test_list.txt""),
                        encoding=get_encoding(
                            osp.join(dataset_path, ""test_list.txt""))) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = \
                            osp.join(dataset_path, items[1])
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            for image, anno in file_list.items():
                results = model.predict(img_file=image)
                image_pred = pdx.seg.visualize(image, results, save_dir=None)
                pse_pred = pdx.seg.visualize(
                    image, results, weight=0, save_dir=None)
                image_ground = None
                pse_label = None
                if anno is not None:
                    label = np.asarray(Image.open(anno)).astype('uint8')
                    image_ground = pdx.seg.visualize(
                        image, {'label_map': label}, save_dir=None)
                    pse_label = pdx.seg.visualize(
                        image, {'label_map': label}, weight=0, save_dir=None)
                save_name = osp.join(save_dir, osp.split(image)[-1])
                visualize_segmented_result(save_name, image_ground, pse_label,
                                           image_pred, pse_pred, legend)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            results = model.predict(img)
            image_pred = pdx.seg.visualize(img, results, save_dir=None)
            pse_pred = pdx.seg.visualize(img, results, weight=0, save_dir=None)
            image_ground = None
            pse_label = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_segmented_result(save_name, image_ground, pse_label,
                                       image_pred, pse_pred, legend)
    set_folder_status(predict_status_path, PredictStatus.XPREDONE)",_15501.py,9,"total_num = open(osp.join(predict_status_path, 'total_num'), 'w', encoding='utf-8')","with open(osp.join(predict_status_path, 'total_num'), 'w', encoding='utf-8') as total_num:

    def write_file_num(total_file_num):
        total_num.write(str(total_file_num))
        
        pass
    sys.stdout = open(osp.join(predict_status_path, 'out.log'), 'w', encoding='utf-8')
    sys.stderr = open(osp.join(predict_status_path, 'err.log'), 'w', encoding='utf-8')
    import paddlex as pdx
    pdx.log_level = 3
    task_type = params['task_type']
    dataset_path = params['dataset_path']
    if epoch is None:
        model_path = osp.join(task_path, 'output', 'best_model')
    else:
        model_path = osp.join(task_path, 'output', 'epoch_{}'.format(epoch))
    model = pdx.load_model(model_path)
    file_list = dict()
    predicted_num = 0
    if task_type == 'classification':
        if img_data is None:
            if len(img_list) == 0 and osp.exists(osp.join(dataset_path, 'test_list.txt')):
                with open(osp.join(dataset_path, 'test_list.txt')) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = items[1]
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            for (image, label_id) in file_list.items():
                pred_result = {}
                if label_id is not None:
                    pred_result['gt_label'] = model.labels[int(label_id)]
                results = model.predict(img_file=image)
                pred_result['label'] = []
                pred_result['score'] = []
                pred_result['topk'] = len(results)
                for res in results:
                    pred_result['label'].append(res['category'])
                    pred_result['score'].append(res['score'])
                visualize_classified_result(save_dir, image, pred_result)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            pred_result = {}
            pred_result['label'] = []
            pred_result['score'] = []
            pred_result['topk'] = len(results)
            for res in results:
                pred_result['label'].append(res['category'])
                pred_result['score'].append(res['score'])
            visualize_classified_result(save_dir, img, pred_result)
    elif task_type in ['detection', 'instance_segmentation']:
        if img_data is None:
            if task_type == 'detection' and osp.exists(osp.join(dataset_path, 'test_list.txt')):
                if len(img_list) == 0 and osp.exists(osp.join(dataset_path, 'test_list.txt')):
                    with open(osp.join(dataset_path, 'test_list.txt'), encoding=get_encoding(osp.join(dataset_path, 'test_list.txt'))) as f:
                        for line in f:
                            items = line.strip().split()
                            file_list[osp.join(dataset_path, items[0])] = osp.join(dataset_path, items[1])
                else:
                    for image in img_list:
                        file_list[image] = None
                total_file_num = len(file_list)
                write_file_num(total_file_num)
                for (image, anno) in file_list.items():
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    image_gt = None
                    if anno is not None:
                        image_gt = plot_det_label(image, anno, model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            elif len(img_list) == 0 and osp.exists(osp.join(dataset_path, 'test.json')):
                from pycocotools.coco import COCO
                anno_path = osp.join(dataset_path, 'test.json')
                coco = COCO(anno_path)
                img_ids = coco.getImgIds()
                total_file_num = len(img_ids)
                write_file_num(total_file_num)
                for img_id in img_ids:
                    img_anno = coco.loadImgs(img_id)[0]
                    file_name = img_anno['file_name']
                    name = osp.split(file_name)[-1].split('.')[0]
                    anno = osp.join(dataset_path, 'Annotations', name + '.npy')
                    img_file = osp.join(dataset_path, 'JPEGImages', file_name)
                    results = model.predict(img_file=img_file)
                    image_pred = pdx.det.visualize(img_file, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(img_file)[-1])
                    if task_type == 'detection':
                        image_gt = plot_det_label(img_file, anno, model.labels)
                    else:
                        image_gt = plot_insseg_label(img_file, anno, model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            else:
                total_file_num = len(img_list)
                write_file_num(total_file_num)
                for image in img_list:
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    visualize_detected_result(save_name, None, image_pred)
                    predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            image_pred = pdx.det.visualize(img, results, threshold=score_thresh, save_dir=None)
            image_gt = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_detected_result(save_name, image_gt, image_pred)
    elif task_type == 'segmentation':
        if img_data is None:
            if len(img_list) == 0 and osp.exists(osp.join(dataset_path, 'test_list.txt')):
                with open(osp.join(dataset_path, 'test_list.txt'), encoding=get_encoding(osp.join(dataset_path, 'test_list.txt'))) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = osp.join(dataset_path, items[1])
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            for (image, anno) in file_list.items():
                results = model.predict(img_file=image)
                image_pred = pdx.seg.visualize(image, results, save_dir=None)
                pse_pred = pdx.seg.visualize(image, results, weight=0, save_dir=None)
                image_ground = None
                pse_label = None
                if anno is not None:
                    label = np.asarray(Image.open(anno)).astype('uint8')
                    image_ground = pdx.seg.visualize(image, {'label_map': label}, save_dir=None)
                    pse_label = pdx.seg.visualize(image, {'label_map': label}, weight=0, save_dir=None)
                save_name = osp.join(save_dir, osp.split(image)[-1])
                visualize_segmented_result(save_name, image_ground, pse_label, image_pred, pse_pred, legend)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            results = model.predict(img)
            image_pred = pdx.seg.visualize(img, results, save_dir=None)
            pse_pred = pdx.seg.visualize(img, results, weight=0, save_dir=None)
            image_ground = None
            pse_label = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_segmented_result(save_name, image_ground, pse_label, image_pred, pse_pred, legend)
    set_folder_status(predict_status_path, PredictStatus.XPREDONE)"
https://github.com/PaddlePaddle/PaddleX/tree/master/paddlex_restful/restful/project/operate.py,"def _call_paddlex_predict(task_path,
                          predict_status_path,
                          params,
                          img_list,
                          img_data,
                          save_dir,
                          score_thresh,
                          epoch=None):
    total_num = open(
        osp.join(predict_status_path, 'total_num'), 'w', encoding='utf-8')

    def write_file_num(total_file_num):
        total_num.write(str(total_file_num))
        total_num.close()

    sys.stdout = open(
        osp.join(predict_status_path, 'out.log'), 'w', encoding='utf-8')
    sys.stderr = open(
        osp.join(predict_status_path, 'err.log'), 'w', encoding='utf-8')

    import paddlex as pdx
    pdx.log_level = 3
    task_type = params['task_type']
    dataset_path = params['dataset_path']
    if epoch is None:
        model_path = osp.join(task_path, 'output', 'best_model')
    else:
        model_path = osp.join(task_path, 'output', 'epoch_{}'.format(epoch))
    model = pdx.load_model(model_path)
    file_list = dict()
    predicted_num = 0
    if task_type == ""classification"":
        if img_data is None:
            if len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                with open(osp.join(dataset_path, ""test_list.txt"")) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = items[1]
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            for image, label_id in file_list.items():
                pred_result = {}
                if label_id is not None:
                    pred_result[""gt_label""] = model.labels[int(label_id)]
                results = model.predict(img_file=image)
                pred_result[""label""] = []
                pred_result[""score""] = []
                pred_result[""topk""] = len(results)
                for res in results:
                    pred_result[""label""].append(res['category'])
                    pred_result[""score""].append(res['score'])
                visualize_classified_result(save_dir, image, pred_result)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            pred_result = {}
            pred_result[""label""] = []
            pred_result[""score""] = []
            pred_result[""topk""] = len(results)
            for res in results:
                pred_result[""label""].append(res['category'])
                pred_result[""score""].append(res['score'])
            visualize_classified_result(save_dir, img, pred_result)
    elif task_type in [""detection"", ""instance_segmentation""]:
        if img_data is None:
            if task_type == ""detection"" and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                if len(img_list) == 0 and osp.exists(
                        osp.join(dataset_path, ""test_list.txt"")):
                    with open(
                            osp.join(dataset_path, ""test_list.txt""),
                            encoding=get_encoding(
                                osp.join(dataset_path, ""test_list.txt""))) as f:
                        for line in f:
                            items = line.strip().split()
                            file_list[osp.join(dataset_path, items[0])] = \
                                osp.join(dataset_path, items[1])
                else:
                    for image in img_list:
                        file_list[image] = None
                total_file_num = len(file_list)
                write_file_num(total_file_num)
                for image, anno in file_list.items():
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(
                        image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    image_gt = None
                    if anno is not None:
                        image_gt = plot_det_label(image, anno, model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            elif len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test.json"")):
                from pycocotools.coco import COCO
                anno_path = osp.join(dataset_path, ""test.json"")
                coco = COCO(anno_path)
                img_ids = coco.getImgIds()
                total_file_num = len(img_ids)
                write_file_num(total_file_num)
                for img_id in img_ids:
                    img_anno = coco.loadImgs(img_id)[0]
                    file_name = img_anno['file_name']
                    name = (osp.split(file_name)[-1]).split(""."")[0]
                    anno = osp.join(dataset_path, ""Annotations"", name + "".npy"")
                    img_file = osp.join(dataset_path, ""JPEGImages"", file_name)
                    results = model.predict(img_file=img_file)
                    image_pred = pdx.det.visualize(
                        img_file,
                        results,
                        threshold=score_thresh,
                        save_dir=None)
                    save_name = osp.join(save_dir, osp.split(img_file)[-1])
                    if task_type == ""detection"":
                        image_gt = plot_det_label(img_file, anno, model.labels)
                    else:
                        image_gt = plot_insseg_label(img_file, anno,
                                                     model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            else:
                total_file_num = len(img_list)
                write_file_num(total_file_num)
                for image in img_list:
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(
                        image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    visualize_detected_result(save_name, None, image_pred)
                    predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            image_pred = pdx.det.visualize(
                img, results, threshold=score_thresh, save_dir=None)
            image_gt = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_detected_result(save_name, image_gt, image_pred)

    elif task_type == ""segmentation"":
        if img_data is None:
            if len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                with open(
                        osp.join(dataset_path, ""test_list.txt""),
                        encoding=get_encoding(
                            osp.join(dataset_path, ""test_list.txt""))) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = \
                            osp.join(dataset_path, items[1])
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            for image, anno in file_list.items():
                results = model.predict(img_file=image)
                image_pred = pdx.seg.visualize(image, results, save_dir=None)
                pse_pred = pdx.seg.visualize(
                    image, results, weight=0, save_dir=None)
                image_ground = None
                pse_label = None
                if anno is not None:
                    label = np.asarray(Image.open(anno)).astype('uint8')
                    image_ground = pdx.seg.visualize(
                        image, {'label_map': label}, save_dir=None)
                    pse_label = pdx.seg.visualize(
                        image, {'label_map': label}, weight=0, save_dir=None)
                save_name = osp.join(save_dir, osp.split(image)[-1])
                visualize_segmented_result(save_name, image_ground, pse_label,
                                           image_pred, pse_pred, legend)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            results = model.predict(img)
            image_pred = pdx.seg.visualize(img, results, save_dir=None)
            pse_pred = pdx.seg.visualize(img, results, weight=0, save_dir=None)
            image_ground = None
            pse_label = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_segmented_result(save_name, image_ground, pse_label,
                                       image_pred, pse_pred, legend)
    set_folder_status(predict_status_path, PredictStatus.XPREDONE)",_15501.py,16,"sys.stdout = open(osp.join(predict_status_path, 'out.log'), 'w', encoding='utf-8')","with open(osp.join(predict_status_path, 'out.log'), 'w', encoding='utf-8') as sys.stdout:
    sys.stderr = open(osp.join(predict_status_path, 'err.log'), 'w', encoding='utf-8')
    import paddlex as pdx
    pdx.log_level = 3
    task_type = params['task_type']
    dataset_path = params['dataset_path']
    if epoch is None:
        model_path = osp.join(task_path, 'output', 'best_model')
    else:
        model_path = osp.join(task_path, 'output', 'epoch_{}'.format(epoch))
    model = pdx.load_model(model_path)
    file_list = dict()
    predicted_num = 0
    if task_type == 'classification':
        if img_data is None:
            if len(img_list) == 0 and osp.exists(osp.join(dataset_path, 'test_list.txt')):
                with open(osp.join(dataset_path, 'test_list.txt')) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = items[1]
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            for (image, label_id) in file_list.items():
                pred_result = {}
                if label_id is not None:
                    pred_result['gt_label'] = model.labels[int(label_id)]
                results = model.predict(img_file=image)
                pred_result['label'] = []
                pred_result['score'] = []
                pred_result['topk'] = len(results)
                for res in results:
                    pred_result['label'].append(res['category'])
                    pred_result['score'].append(res['score'])
                visualize_classified_result(save_dir, image, pred_result)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            pred_result = {}
            pred_result['label'] = []
            pred_result['score'] = []
            pred_result['topk'] = len(results)
            for res in results:
                pred_result['label'].append(res['category'])
                pred_result['score'].append(res['score'])
            visualize_classified_result(save_dir, img, pred_result)
    elif task_type in ['detection', 'instance_segmentation']:
        if img_data is None:
            if task_type == 'detection' and osp.exists(osp.join(dataset_path, 'test_list.txt')):
                if len(img_list) == 0 and osp.exists(osp.join(dataset_path, 'test_list.txt')):
                    with open(osp.join(dataset_path, 'test_list.txt'), encoding=get_encoding(osp.join(dataset_path, 'test_list.txt'))) as f:
                        for line in f:
                            items = line.strip().split()
                            file_list[osp.join(dataset_path, items[0])] = osp.join(dataset_path, items[1])
                else:
                    for image in img_list:
                        file_list[image] = None
                total_file_num = len(file_list)
                write_file_num(total_file_num)
                for (image, anno) in file_list.items():
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    image_gt = None
                    if anno is not None:
                        image_gt = plot_det_label(image, anno, model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            elif len(img_list) == 0 and osp.exists(osp.join(dataset_path, 'test.json')):
                from pycocotools.coco import COCO
                anno_path = osp.join(dataset_path, 'test.json')
                coco = COCO(anno_path)
                img_ids = coco.getImgIds()
                total_file_num = len(img_ids)
                write_file_num(total_file_num)
                for img_id in img_ids:
                    img_anno = coco.loadImgs(img_id)[0]
                    file_name = img_anno['file_name']
                    name = osp.split(file_name)[-1].split('.')[0]
                    anno = osp.join(dataset_path, 'Annotations', name + '.npy')
                    img_file = osp.join(dataset_path, 'JPEGImages', file_name)
                    results = model.predict(img_file=img_file)
                    image_pred = pdx.det.visualize(img_file, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(img_file)[-1])
                    if task_type == 'detection':
                        image_gt = plot_det_label(img_file, anno, model.labels)
                    else:
                        image_gt = plot_insseg_label(img_file, anno, model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            else:
                total_file_num = len(img_list)
                write_file_num(total_file_num)
                for image in img_list:
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    visualize_detected_result(save_name, None, image_pred)
                    predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            image_pred = pdx.det.visualize(img, results, threshold=score_thresh, save_dir=None)
            image_gt = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_detected_result(save_name, image_gt, image_pred)
    elif task_type == 'segmentation':
        if img_data is None:
            if len(img_list) == 0 and osp.exists(osp.join(dataset_path, 'test_list.txt')):
                with open(osp.join(dataset_path, 'test_list.txt'), encoding=get_encoding(osp.join(dataset_path, 'test_list.txt'))) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = osp.join(dataset_path, items[1])
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            for (image, anno) in file_list.items():
                results = model.predict(img_file=image)
                image_pred = pdx.seg.visualize(image, results, save_dir=None)
                pse_pred = pdx.seg.visualize(image, results, weight=0, save_dir=None)
                image_ground = None
                pse_label = None
                if anno is not None:
                    label = np.asarray(Image.open(anno)).astype('uint8')
                    image_ground = pdx.seg.visualize(image, {'label_map': label}, save_dir=None)
                    pse_label = pdx.seg.visualize(image, {'label_map': label}, weight=0, save_dir=None)
                save_name = osp.join(save_dir, osp.split(image)[-1])
                visualize_segmented_result(save_name, image_ground, pse_label, image_pred, pse_pred, legend)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            results = model.predict(img)
            image_pred = pdx.seg.visualize(img, results, save_dir=None)
            pse_pred = pdx.seg.visualize(img, results, weight=0, save_dir=None)
            image_ground = None
            pse_label = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_segmented_result(save_name, image_ground, pse_label, image_pred, pse_pred, legend)
    set_folder_status(predict_status_path, PredictStatus.XPREDONE)"
https://github.com/PaddlePaddle/PaddleX/tree/master/paddlex_restful/restful/project/operate.py,"def _call_paddlex_predict(task_path,
                          predict_status_path,
                          params,
                          img_list,
                          img_data,
                          save_dir,
                          score_thresh,
                          epoch=None):
    total_num = open(
        osp.join(predict_status_path, 'total_num'), 'w', encoding='utf-8')

    def write_file_num(total_file_num):
        total_num.write(str(total_file_num))
        total_num.close()

    sys.stdout = open(
        osp.join(predict_status_path, 'out.log'), 'w', encoding='utf-8')
    sys.stderr = open(
        osp.join(predict_status_path, 'err.log'), 'w', encoding='utf-8')

    import paddlex as pdx
    pdx.log_level = 3
    task_type = params['task_type']
    dataset_path = params['dataset_path']
    if epoch is None:
        model_path = osp.join(task_path, 'output', 'best_model')
    else:
        model_path = osp.join(task_path, 'output', 'epoch_{}'.format(epoch))
    model = pdx.load_model(model_path)
    file_list = dict()
    predicted_num = 0
    if task_type == ""classification"":
        if img_data is None:
            if len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                with open(osp.join(dataset_path, ""test_list.txt"")) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = items[1]
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            for image, label_id in file_list.items():
                pred_result = {}
                if label_id is not None:
                    pred_result[""gt_label""] = model.labels[int(label_id)]
                results = model.predict(img_file=image)
                pred_result[""label""] = []
                pred_result[""score""] = []
                pred_result[""topk""] = len(results)
                for res in results:
                    pred_result[""label""].append(res['category'])
                    pred_result[""score""].append(res['score'])
                visualize_classified_result(save_dir, image, pred_result)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            pred_result = {}
            pred_result[""label""] = []
            pred_result[""score""] = []
            pred_result[""topk""] = len(results)
            for res in results:
                pred_result[""label""].append(res['category'])
                pred_result[""score""].append(res['score'])
            visualize_classified_result(save_dir, img, pred_result)
    elif task_type in [""detection"", ""instance_segmentation""]:
        if img_data is None:
            if task_type == ""detection"" and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                if len(img_list) == 0 and osp.exists(
                        osp.join(dataset_path, ""test_list.txt"")):
                    with open(
                            osp.join(dataset_path, ""test_list.txt""),
                            encoding=get_encoding(
                                osp.join(dataset_path, ""test_list.txt""))) as f:
                        for line in f:
                            items = line.strip().split()
                            file_list[osp.join(dataset_path, items[0])] = \
                                osp.join(dataset_path, items[1])
                else:
                    for image in img_list:
                        file_list[image] = None
                total_file_num = len(file_list)
                write_file_num(total_file_num)
                for image, anno in file_list.items():
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(
                        image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    image_gt = None
                    if anno is not None:
                        image_gt = plot_det_label(image, anno, model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            elif len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test.json"")):
                from pycocotools.coco import COCO
                anno_path = osp.join(dataset_path, ""test.json"")
                coco = COCO(anno_path)
                img_ids = coco.getImgIds()
                total_file_num = len(img_ids)
                write_file_num(total_file_num)
                for img_id in img_ids:
                    img_anno = coco.loadImgs(img_id)[0]
                    file_name = img_anno['file_name']
                    name = (osp.split(file_name)[-1]).split(""."")[0]
                    anno = osp.join(dataset_path, ""Annotations"", name + "".npy"")
                    img_file = osp.join(dataset_path, ""JPEGImages"", file_name)
                    results = model.predict(img_file=img_file)
                    image_pred = pdx.det.visualize(
                        img_file,
                        results,
                        threshold=score_thresh,
                        save_dir=None)
                    save_name = osp.join(save_dir, osp.split(img_file)[-1])
                    if task_type == ""detection"":
                        image_gt = plot_det_label(img_file, anno, model.labels)
                    else:
                        image_gt = plot_insseg_label(img_file, anno,
                                                     model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            else:
                total_file_num = len(img_list)
                write_file_num(total_file_num)
                for image in img_list:
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(
                        image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    visualize_detected_result(save_name, None, image_pred)
                    predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            image_pred = pdx.det.visualize(
                img, results, threshold=score_thresh, save_dir=None)
            image_gt = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_detected_result(save_name, image_gt, image_pred)

    elif task_type == ""segmentation"":
        if img_data is None:
            if len(img_list) == 0 and osp.exists(
                    osp.join(dataset_path, ""test_list.txt"")):
                with open(
                        osp.join(dataset_path, ""test_list.txt""),
                        encoding=get_encoding(
                            osp.join(dataset_path, ""test_list.txt""))) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = \
                            osp.join(dataset_path, items[1])
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            for image, anno in file_list.items():
                results = model.predict(img_file=image)
                image_pred = pdx.seg.visualize(image, results, save_dir=None)
                pse_pred = pdx.seg.visualize(
                    image, results, weight=0, save_dir=None)
                image_ground = None
                pse_label = None
                if anno is not None:
                    label = np.asarray(Image.open(anno)).astype('uint8')
                    image_ground = pdx.seg.visualize(
                        image, {'label_map': label}, save_dir=None)
                    pse_label = pdx.seg.visualize(
                        image, {'label_map': label}, weight=0, save_dir=None)
                save_name = osp.join(save_dir, osp.split(image)[-1])
                visualize_segmented_result(save_name, image_ground, pse_label,
                                           image_pred, pse_pred, legend)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            results = model.predict(img)
            image_pred = pdx.seg.visualize(img, results, save_dir=None)
            pse_pred = pdx.seg.visualize(img, results, weight=0, save_dir=None)
            image_ground = None
            pse_label = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_segmented_result(save_name, image_ground, pse_label,
                                       image_pred, pse_pred, legend)
    set_folder_status(predict_status_path, PredictStatus.XPREDONE)",_15501.py,18,"sys.stderr = open(osp.join(predict_status_path, 'err.log'), 'w', encoding='utf-8')","with open(osp.join(predict_status_path, 'err.log'), 'w', encoding='utf-8') as sys.stderr:
    import paddlex as pdx
    pdx.log_level = 3
    task_type = params['task_type']
    dataset_path = params['dataset_path']
    if epoch is None:
        model_path = osp.join(task_path, 'output', 'best_model')
    else:
        model_path = osp.join(task_path, 'output', 'epoch_{}'.format(epoch))
    model = pdx.load_model(model_path)
    file_list = dict()
    predicted_num = 0
    if task_type == 'classification':
        if img_data is None:
            if len(img_list) == 0 and osp.exists(osp.join(dataset_path, 'test_list.txt')):
                with open(osp.join(dataset_path, 'test_list.txt')) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = items[1]
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            for (image, label_id) in file_list.items():
                pred_result = {}
                if label_id is not None:
                    pred_result['gt_label'] = model.labels[int(label_id)]
                results = model.predict(img_file=image)
                pred_result['label'] = []
                pred_result['score'] = []
                pred_result['topk'] = len(results)
                for res in results:
                    pred_result['label'].append(res['category'])
                    pred_result['score'].append(res['score'])
                visualize_classified_result(save_dir, image, pred_result)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            pred_result = {}
            pred_result['label'] = []
            pred_result['score'] = []
            pred_result['topk'] = len(results)
            for res in results:
                pred_result['label'].append(res['category'])
                pred_result['score'].append(res['score'])
            visualize_classified_result(save_dir, img, pred_result)
    elif task_type in ['detection', 'instance_segmentation']:
        if img_data is None:
            if task_type == 'detection' and osp.exists(osp.join(dataset_path, 'test_list.txt')):
                if len(img_list) == 0 and osp.exists(osp.join(dataset_path, 'test_list.txt')):
                    with open(osp.join(dataset_path, 'test_list.txt'), encoding=get_encoding(osp.join(dataset_path, 'test_list.txt'))) as f:
                        for line in f:
                            items = line.strip().split()
                            file_list[osp.join(dataset_path, items[0])] = osp.join(dataset_path, items[1])
                else:
                    for image in img_list:
                        file_list[image] = None
                total_file_num = len(file_list)
                write_file_num(total_file_num)
                for (image, anno) in file_list.items():
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    image_gt = None
                    if anno is not None:
                        image_gt = plot_det_label(image, anno, model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            elif len(img_list) == 0 and osp.exists(osp.join(dataset_path, 'test.json')):
                from pycocotools.coco import COCO
                anno_path = osp.join(dataset_path, 'test.json')
                coco = COCO(anno_path)
                img_ids = coco.getImgIds()
                total_file_num = len(img_ids)
                write_file_num(total_file_num)
                for img_id in img_ids:
                    img_anno = coco.loadImgs(img_id)[0]
                    file_name = img_anno['file_name']
                    name = osp.split(file_name)[-1].split('.')[0]
                    anno = osp.join(dataset_path, 'Annotations', name + '.npy')
                    img_file = osp.join(dataset_path, 'JPEGImages', file_name)
                    results = model.predict(img_file=img_file)
                    image_pred = pdx.det.visualize(img_file, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(img_file)[-1])
                    if task_type == 'detection':
                        image_gt = plot_det_label(img_file, anno, model.labels)
                    else:
                        image_gt = plot_insseg_label(img_file, anno, model.labels)
                    visualize_detected_result(save_name, image_gt, image_pred)
                    predicted_num += 1
            else:
                total_file_num = len(img_list)
                write_file_num(total_file_num)
                for image in img_list:
                    results = model.predict(img_file=image)
                    image_pred = pdx.det.visualize(image, results, threshold=score_thresh, save_dir=None)
                    save_name = osp.join(save_dir, osp.split(image)[-1])
                    visualize_detected_result(save_name, None, image_pred)
                    predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            results = model.predict(img)
            image_pred = pdx.det.visualize(img, results, threshold=score_thresh, save_dir=None)
            image_gt = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_detected_result(save_name, image_gt, image_pred)
    elif task_type == 'segmentation':
        if img_data is None:
            if len(img_list) == 0 and osp.exists(osp.join(dataset_path, 'test_list.txt')):
                with open(osp.join(dataset_path, 'test_list.txt'), encoding=get_encoding(osp.join(dataset_path, 'test_list.txt'))) as f:
                    for line in f:
                        items = line.strip().split()
                        file_list[osp.join(dataset_path, items[0])] = osp.join(dataset_path, items[1])
            else:
                for image in img_list:
                    file_list[image] = None
            total_file_num = len(file_list)
            write_file_num(total_file_num)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            for (image, anno) in file_list.items():
                results = model.predict(img_file=image)
                image_pred = pdx.seg.visualize(image, results, save_dir=None)
                pse_pred = pdx.seg.visualize(image, results, weight=0, save_dir=None)
                image_ground = None
                pse_label = None
                if anno is not None:
                    label = np.asarray(Image.open(anno)).astype('uint8')
                    image_ground = pdx.seg.visualize(image, {'label_map': label}, save_dir=None)
                    pse_label = pdx.seg.visualize(image, {'label_map': label}, weight=0, save_dir=None)
                save_name = osp.join(save_dir, osp.split(image)[-1])
                visualize_segmented_result(save_name, image_ground, pse_label, image_pred, pse_pred, legend)
                predicted_num += 1
        else:
            img_data = base64.b64decode(img_data)
            img_array = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR)
            color_map = get_color_map_list(256)
            legend = {}
            for i in range(len(model.labels)):
                legend[model.labels[i]] = color_map[i]
            results = model.predict(img)
            image_pred = pdx.seg.visualize(img, results, save_dir=None)
            pse_pred = pdx.seg.visualize(img, results, weight=0, save_dir=None)
            image_ground = None
            pse_label = None
            save_name = osp.join(save_dir, 'predict_result.png')
            visualize_segmented_result(save_name, image_ground, pse_label, image_pred, pse_pred, legend)
    set_folder_status(predict_status_path, PredictStatus.XPREDONE)"
https://github.com/haiwen/seahub/tree/master/seahub/group/tests/tests.py,"def test_leave_500_chars_msg(self):
        f = open(os.path.join(self.testdatapath, ""valid_message""), ""rb"")
        message = f.read()
        response = self.client.post('/group/1/', {
                'message': message,
                })
        # Redirect only if it worked
        self.assertEqual(response.status_code, 302)
        self.assertEqual(GroupMessage.objects.all().count(), 1)",_15574.py,2,"f = open(os.path.join(self.testdatapath, 'valid_message'), 'rb')","with open(os.path.join(self.testdatapath, 'valid_message'), 'rb') as f:
    message = f.read()
    response = self.client.post('/group/1/', {'message': message})
    self.assertEqual(response.status_code, 302)
    self.assertEqual(GroupMessage.objects.all().count(), 1)"
https://github.com/ticarpi/jwt_tool/tree/master//jwt_tool.py,"def parseJWKS(jwksfile):
    jwks = open(jwksfile, ""r"").read()
    jwksDict = json.loads(jwks, object_pairs_hook=OrderedDict)
    nowtime = int(datetime.now().timestamp())
    cprintc(""JWKS Contents:"", ""cyan"")
    try:
        keyLen = len(jwksDict[""keys""])
        cprintc(""Number of keys: ""+str(keyLen), ""cyan"")
        i = -1
        for jkey in range(0,keyLen):
            i += 1
            cprintc(""\n--------"", ""white"")
            try:
                cprintc(""Key ""+str(i+1), ""cyan"")
                kid = str(jwksDict[""keys""][i][""kid""])
                cprintc(""kid: ""+kid, ""cyan"")
            except:
                kid = i
                cprintc(""Key ""+str(i+1), ""cyan"")
            for keyVal in jwksDict[""keys""][i].items():
                keyVal = keyVal[0]
                cprintc(""[+] ""+keyVal+"" = ""+str(jwksDict[""keys""][i][keyVal]), ""green"")
            try:
                x = str(jwksDict[""keys""][i][""x""])
                y = str(jwksDict[""keys""][i][""y""])
                cprintc(""\nFound ECC key factors, generating a public key"", ""cyan"")
                pubkeyName = genECPubFromJWKS(x, y, kid, nowtime)
                cprintc(""[+] ""+pubkeyName, ""green"")
                cprintc(""\nAttempting to verify token using ""+pubkeyName, ""cyan"")
                valid = verifyTokenEC(headDict, paylDict, sig, pubkeyName)
            except:
                pass
            try:
                n = str(jwksDict[""keys""][i][""n""])
                e = str(jwksDict[""keys""][i][""e""])
                cprintc(""\nFound RSA key factors, generating a public key"", ""cyan"")
                pubkeyName = genRSAPubFromJWKS(n, e, kid, nowtime)
                cprintc(""[+] ""+pubkeyName, ""green"")
                cprintc(""\nAttempting to verify token using ""+pubkeyName, ""cyan"")
                valid = verifyTokenRSA(headDict, paylDict, sig, pubkeyName)
            except:
                pass
    except:
        cprintc(""Single key file"", ""white"")
        for jkey in jwksDict:
            cprintc(""[+] ""+jkey+"" = ""+str(jwksDict[jkey]), ""green"")
        try:
            kid = 1
            x = str(jwksDict[""x""])
            y = str(jwksDict[""y""])
            cprintc(""\nFound ECC key factors, generating a public key"", ""cyan"")
            pubkeyName = genECPubFromJWKS(x, y, kid, nowtime)
            cprintc(""[+] ""+pubkeyName, ""green"")
            cprintc(""\nAttempting to verify token using ""+pubkeyName, ""cyan"")
            valid = verifyTokenEC(headDict, paylDict, sig, pubkeyName)
        except:
            pass
        try:
            kid = 1
            n = str(jwksDict[""n""])
            e = str(jwksDict[""e""])
            cprintc(""\nFound RSA key factors, generating a public key"", ""cyan"")
            pubkeyName = genRSAPubFromJWKS(n, e, kid, nowtime)
            cprintc(""[+] ""+pubkeyName, ""green"")
            cprintc(""\nAttempting to verify token using ""+pubkeyName, ""cyan"")
            valid = verifyTokenRSA(headDict, paylDict, sig, pubkeyName)
        except:
            pass",_15905.py,2,"jwks = open(jwksfile, 'r').read()","with open(jwksfile, 'r') as my_f:
    jwks = my_f.read()
    jwksDict = json.loads(jwks, object_pairs_hook=OrderedDict)
    nowtime = int(datetime.now().timestamp())
    cprintc('JWKS Contents:', 'cyan')
    try:
        keyLen = len(jwksDict['keys'])
        cprintc('Number of keys: ' + str(keyLen), 'cyan')
        i = -1
        for jkey in range(0, keyLen):
            i += 1
            cprintc('\n--------', 'white')
            try:
                cprintc('Key ' + str(i + 1), 'cyan')
                kid = str(jwksDict['keys'][i]['kid'])
                cprintc('kid: ' + kid, 'cyan')
            except:
                kid = i
                cprintc('Key ' + str(i + 1), 'cyan')
            for keyVal in jwksDict['keys'][i].items():
                keyVal = keyVal[0]
                cprintc('[+] ' + keyVal + ' = ' + str(jwksDict['keys'][i][keyVal]), 'green')
            try:
                x = str(jwksDict['keys'][i]['x'])
                y = str(jwksDict['keys'][i]['y'])
                cprintc('\nFound ECC key factors, generating a public key', 'cyan')
                pubkeyName = genECPubFromJWKS(x, y, kid, nowtime)
                cprintc('[+] ' + pubkeyName, 'green')
                cprintc('\nAttempting to verify token using ' + pubkeyName, 'cyan')
                valid = verifyTokenEC(headDict, paylDict, sig, pubkeyName)
            except:
                pass
            try:
                n = str(jwksDict['keys'][i]['n'])
                e = str(jwksDict['keys'][i]['e'])
                cprintc('\nFound RSA key factors, generating a public key', 'cyan')
                pubkeyName = genRSAPubFromJWKS(n, e, kid, nowtime)
                cprintc('[+] ' + pubkeyName, 'green')
                cprintc('\nAttempting to verify token using ' + pubkeyName, 'cyan')
                valid = verifyTokenRSA(headDict, paylDict, sig, pubkeyName)
            except:
                pass
    except:
        cprintc('Single key file', 'white')
        for jkey in jwksDict:
            cprintc('[+] ' + jkey + ' = ' + str(jwksDict[jkey]), 'green')
        try:
            kid = 1
            x = str(jwksDict['x'])
            y = str(jwksDict['y'])
            cprintc('\nFound ECC key factors, generating a public key', 'cyan')
            pubkeyName = genECPubFromJWKS(x, y, kid, nowtime)
            cprintc('[+] ' + pubkeyName, 'green')
            cprintc('\nAttempting to verify token using ' + pubkeyName, 'cyan')
            valid = verifyTokenEC(headDict, paylDict, sig, pubkeyName)
        except:
            pass
        try:
            kid = 1
            n = str(jwksDict['n'])
            e = str(jwksDict['e'])
            cprintc('\nFound RSA key factors, generating a public key', 'cyan')
            pubkeyName = genRSAPubFromJWKS(n, e, kid, nowtime)
            cprintc('[+] ' + pubkeyName, 'green')
            cprintc('\nAttempting to verify token using ' + pubkeyName, 'cyan')
            valid = verifyTokenRSA(headDict, paylDict, sig, pubkeyName)
        except:
            pass"
https://github.com/Blazemeter/taurus/tree/master/tests/unit/modules/test_cloudProvisioning.py,"def test_cloud_paths(self):
        """"""
        Test different executor/path combinations for correct return values of get_resources_files
        """"""
        self.configure(
            add_config=False, add_settings=False,
        )  # upload files

        # FIXME: refactor this method!
        self.sniff_log(self.obj.log)
        self.obj.engine.configure([BASE_CONFIG, RESOURCES_DIR + 'yaml/resource_files.yml'], read_config_files=False)
        self.obj.engine.unify_config()
        self.obj.settings = self.obj.engine.config['modules']['cloud']
        self.obj.settings.merge({'delete-test-files': False})

        # list of existing files in $HOME
        pref = 'file-in-home-'
        files_in_home = ['00.jmx', '01.csv', '02.res', '03.java', '04.scala', '05.jar', '06.py',
                         '07.properties', '08.py', '09.siege', '10.xml', '11.ds', '12.xml',
                         '13.src', '14.java', '15.xml', '16.java', '17.js', '18.rb', '19.jar', '20.jar']
        files_in_home = [pref + _file for _file in files_in_home]

        back_home = os.environ.get('HOME', '')
        temp_home = tempfile.mkdtemp()
        try:
            os.environ['HOME'] = temp_home
            files_in_home = [{'shortname': os.path.join('~', _file),
                              'fullname': get_full_path(os.path.join('~', _file))}
                             for _file in files_in_home]

            shutil.copyfile(RESOURCES_DIR + 'jmeter/jmx/dummy.jmx', files_in_home[0]['fullname'])

            dir_path = get_full_path(os.path.join('~', 'example-of-directory'))
            os.mkdir(dir_path)

            for _file in files_in_home[1:]:
                open(_file['fullname'], 'a').close()

            self.obj.engine.file_search_paths = ['tests', os.path.join('tests', 'unit')]  # config not in cwd

            # 'files' are treated similar in all executors so check only one
            self.obj.engine.config[EXEC][0]['files'] = [
                os.path.join(BZT_DIR, 'tests', 'unit', 'test_CLI.py'),  # full path
                files_in_home[2]['shortname'],  # path from ~
                os.path.join('resources', 'jmeter', 'jmeter-loader.bat'),  # relative path
                'mocks.py',  # only basename (look at file_search_paths)
                '~/example-of-directory']  # dir

            self.obj.prepare()

            debug = self.log_recorder.debug_buff.getvalue().split('\n')
            str_files = [line for line in debug if 'Replace file names in config' in line]
            self.assertEqual(1, len(str_files))
            res_files = [_file for _file in str_files[0].split('\'')[1::2]]
            half = int(len(res_files) / 2)
            old_names = res_files[:half]
            new_names = res_files[half:]
            names = list(zip(old_names, new_names))

            with open(self.obj.engine.artifacts_dir + '/cloud.yml') as cl_file:
                str_cfg = cl_file.read()

            archive_found = False
            for old_name, new_name in names:
                if new_name.endswith('.zip'):
                    archive_found = True

                # all resources on the disk, dir has been packed
                path_to_file = get_full_path(self.obj.engine.find_file(old_name))
                msg = 'File %s (%s) not found on disk' % (old_name, path_to_file)
                self.assertTrue(os.path.exists(path_to_file), msg)
                msg = 'Short name %s not found in modified config' % new_name
                self.assertIn(new_name, str_cfg, msg)  # all short names in config
                if new_name != old_name:
                    msg = 'Long name %s found in config' % old_name
                    self.assertNotIn(old_name, str_cfg, msg)  # no one long name in config

            self.assertTrue(archive_found)

            target_names = {  # source:
                'dummy.jmx',  # execution 0 (script)
                'test_CLI.py', 'file-in-home-02.res',  # 0 (files)
                'jmeter-loader.bat', 'mocks.py',  # 0 (files)
                'example-of-directory.zip',  # 0 (files)
                'files_paths.jmx',  # 1 (script)
                'file-in-home-01.csv', 'body-file.dat',  # 1 (from jmx)
                'BlazeDemo.java',  # 2 (script)
                'file-in-home-05.jar', 'dummy.jar',  # 2 (additional-classpath)
                'not-jmx.xml',  # 2 (testng-xml)
                'file-in-home-03.java',  # 3 (script)
                'file-in-home-12.xml',  # 3 (testng-xml)
                'BasicSimulation.scala',  # 4 (script)
                'file-in-home-04.scala',  # 5 (script)
                'simple.py',  # 6 (script)
                'file-in-home-08.py',  # 7 (script)
                'jmeter-loader.bat',  # 8 (data-sources)
                'file-in-home-11.ds',  # 8 (data-sources)
                'url-file',  # 9 (script)
                'file-in-home-09.siege',  # 10 (script)
                'http_simple.xml',  # 11 (script)
                'file-in-home-10.xml',  # 12 (script)
                'file-in-home-00.jmx',  # 13 (script)
                'TestBlazemeterFail.java',  # 14 (script)
                'file-in-home-20.jar',  # 14 (additional-classpath)
                'file-in-home-14.java',  # 15 (script)
                'TestNGSuite.java',  # 16 (script)
                'testng.xml',  # 16 (detected testng-xml from 'files')
                'file-in-home-15.xml',  # 17 (testng-xml)
                'file-in-home-16.java',  # 17 (script)
                'bd_scenarios.js',  # 18 (script)
                'file-in-home-17.js',  # 19 (sript)
                'example_spec.rb',  # 20 (script)
                'file-in-home-18.rb',  # 21 (sript)
                'file-in-home-19.jar',  # global testng settings (additional-classpath)
                'variable_file_upload.jmx',
            }
            self.assertEqual(set(new_names), target_names)
        finally:
            os.environ['HOME'] = back_home
            shutil.rmtree(temp_home)",_16193.py,37,"open(_file['fullname'], 'a').close()","with open(_file['fullname'], 'a') as my_f:
    my_f.close()"
https://github.com/Blazemeter/taurus/tree/master/tests/unit/modules/test_cloudProvisioning.py,"def test_cloud_paths(self):
        """"""
        Test different executor/path combinations for correct return values of get_resources_files
        """"""
        self.configure(
            add_config=False, add_settings=False,
        )  # upload files

        # FIXME: refactor this method!
        self.sniff_log(self.obj.log)
        self.obj.engine.configure([BASE_CONFIG, RESOURCES_DIR + 'yaml/resource_files.yml'], read_config_files=False)
        self.obj.engine.unify_config()
        self.obj.settings = self.obj.engine.config['modules']['cloud']
        self.obj.settings.merge({'delete-test-files': False})

        # list of existing files in $HOME
        pref = 'file-in-home-'
        files_in_home = ['00.jmx', '01.csv', '02.res', '03.java', '04.scala', '05.jar', '06.py',
                         '07.properties', '08.py', '09.siege', '10.xml', '11.ds', '12.xml',
                         '13.src', '14.java', '15.xml', '16.java', '17.js', '18.rb', '19.jar', '20.jar']
        files_in_home = [pref + _file for _file in files_in_home]

        back_home = os.environ.get('HOME', '')
        temp_home = tempfile.mkdtemp()
        try:
            os.environ['HOME'] = temp_home
            files_in_home = [{'shortname': os.path.join('~', _file),
                              'fullname': get_full_path(os.path.join('~', _file))}
                             for _file in files_in_home]

            shutil.copyfile(RESOURCES_DIR + 'jmeter/jmx/dummy.jmx', files_in_home[0]['fullname'])

            dir_path = get_full_path(os.path.join('~', 'example-of-directory'))
            os.mkdir(dir_path)

            for _file in files_in_home[1:]:
                open(_file['fullname'], 'a').close()

            self.obj.engine.file_search_paths = ['tests', os.path.join('tests', 'unit')]  # config not in cwd

            # 'files' are treated similar in all executors so check only one
            self.obj.engine.config[EXEC][0]['files'] = [
                os.path.join(BZT_DIR, 'tests', 'unit', 'test_CLI.py'),  # full path
                files_in_home[2]['shortname'],  # path from ~
                os.path.join('resources', 'jmeter', 'jmeter-loader.bat'),  # relative path
                'mocks.py',  # only basename (look at file_search_paths)
                '~/example-of-directory']  # dir

            self.obj.prepare()

            debug = self.log_recorder.debug_buff.getvalue().split('\n')
            str_files = [line for line in debug if 'Replace file names in config' in line]
            self.assertEqual(1, len(str_files))
            res_files = [_file for _file in str_files[0].split('\'')[1::2]]
            half = int(len(res_files) / 2)
            old_names = res_files[:half]
            new_names = res_files[half:]
            names = list(zip(old_names, new_names))

            with open(self.obj.engine.artifacts_dir + '/cloud.yml') as cl_file:
                str_cfg = cl_file.read()

            archive_found = False
            for old_name, new_name in names:
                if new_name.endswith('.zip'):
                    archive_found = True

                # all resources on the disk, dir has been packed
                path_to_file = get_full_path(self.obj.engine.find_file(old_name))
                msg = 'File %s (%s) not found on disk' % (old_name, path_to_file)
                self.assertTrue(os.path.exists(path_to_file), msg)
                msg = 'Short name %s not found in modified config' % new_name
                self.assertIn(new_name, str_cfg, msg)  # all short names in config
                if new_name != old_name:
                    msg = 'Long name %s found in config' % old_name
                    self.assertNotIn(old_name, str_cfg, msg)  # no one long name in config

            self.assertTrue(archive_found)

            target_names = {  # source:
                'dummy.jmx',  # execution 0 (script)
                'test_CLI.py', 'file-in-home-02.res',  # 0 (files)
                'jmeter-loader.bat', 'mocks.py',  # 0 (files)
                'example-of-directory.zip',  # 0 (files)
                'files_paths.jmx',  # 1 (script)
                'file-in-home-01.csv', 'body-file.dat',  # 1 (from jmx)
                'BlazeDemo.java',  # 2 (script)
                'file-in-home-05.jar', 'dummy.jar',  # 2 (additional-classpath)
                'not-jmx.xml',  # 2 (testng-xml)
                'file-in-home-03.java',  # 3 (script)
                'file-in-home-12.xml',  # 3 (testng-xml)
                'BasicSimulation.scala',  # 4 (script)
                'file-in-home-04.scala',  # 5 (script)
                'simple.py',  # 6 (script)
                'file-in-home-08.py',  # 7 (script)
                'jmeter-loader.bat',  # 8 (data-sources)
                'file-in-home-11.ds',  # 8 (data-sources)
                'url-file',  # 9 (script)
                'file-in-home-09.siege',  # 10 (script)
                'http_simple.xml',  # 11 (script)
                'file-in-home-10.xml',  # 12 (script)
                'file-in-home-00.jmx',  # 13 (script)
                'TestBlazemeterFail.java',  # 14 (script)
                'file-in-home-20.jar',  # 14 (additional-classpath)
                'file-in-home-14.java',  # 15 (script)
                'TestNGSuite.java',  # 16 (script)
                'testng.xml',  # 16 (detected testng-xml from 'files')
                'file-in-home-15.xml',  # 17 (testng-xml)
                'file-in-home-16.java',  # 17 (script)
                'bd_scenarios.js',  # 18 (script)
                'file-in-home-17.js',  # 19 (sript)
                'example_spec.rb',  # 20 (script)
                'file-in-home-18.rb',  # 21 (sript)
                'file-in-home-19.jar',  # global testng settings (additional-classpath)
                'variable_file_upload.jmx',
            }
            self.assertEqual(set(new_names), target_names)
        finally:
            os.environ['HOME'] = back_home
            shutil.rmtree(temp_home)",_16193.py,37,"open(_file['fullname'], 'a').close()","with open(_file['fullname'], 'a') as my_f:
    my_f.close()"
https://github.com/Cloud-CV/Fabrik/tree/master/tests/unit/ide/test_views.py,"def test_json_to_prototxt(self):
        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'ide',
                                  'caffe_export_test.json'), 'r')
        response = json.load(tests)
        tests.close()
        net = yaml.safe_load(json.dumps(response['net']))
        net = {'l0': net['Input'], 'l1': net['MultinomialLogisticLoss']}
        net['l0']['connection']['output'].append('l1')
        prototxt, input_dim = json_to_prototxt(net, response['net_name'])
        self.assertGreater(len(prototxt), 9)
        self.assertEqual(net['l1']['info']['type'], 'MultinomialLogisticLoss')",_16287.py,2,"tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'ide', 'caffe_export_test.json'), 'r')","with open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'ide', 'caffe_export_test.json'), 'r') as tests:
    response = json.load(tests)
    
    pass
    net = yaml.safe_load(json.dumps(response['net']))
    net = {'l0': net['Input'], 'l1': net['MultinomialLogisticLoss']}
    net['l0']['connection']['output'].append('l1')
    (prototxt, input_dim) = json_to_prototxt(net, response['net_name'])
    self.assertGreater(len(prototxt), 9)
    self.assertEqual(net['l1']['info']['type'], 'MultinomialLogisticLoss')"
https://github.com/shunliz/Machine-Learning/tree/master/Regression/regression.py,"def loadDataSet(fileName):
	""""""
	函数说明:加载数据
	Parameters:
		fileName - 文件名
	Returns:
		xArr - x数据集
		yArr - y数据集
	Website:
		http://www.cuijiahua.com/
	Modify:
		2017-11-20
	""""""
	numFeat = len(open(fileName).readline().split('\t')) - 1
	xArr = []; yArr = []
	fr = open(fileName)
	for line in fr.readlines():
		lineArr =[]
		curLine = line.strip().split('\t')
		for i in range(numFeat):
			lineArr.append(float(curLine[i]))
		xArr.append(lineArr)
		yArr.append(float(curLine[-1]))
	return xArr, yArr",_16682.py,16,fr = open(fileName),"with open(fileName) as fr:
    for line in fr.readlines():
        lineArr = []
        curLine = line.strip().split('\t')
        for i in range(numFeat):
            lineArr.append(float(curLine[i]))
        xArr.append(lineArr)
        yArr.append(float(curLine[-1]))
    return (xArr, yArr)"
https://github.com/shunliz/Machine-Learning/tree/master/Regression/regression.py,"def loadDataSet(fileName):
	""""""
	函数说明:加载数据
	Parameters:
		fileName - 文件名
	Returns:
		xArr - x数据集
		yArr - y数据集
	Website:
		http://www.cuijiahua.com/
	Modify:
		2017-11-20
	""""""
	numFeat = len(open(fileName).readline().split('\t')) - 1
	xArr = []; yArr = []
	fr = open(fileName)
	for line in fr.readlines():
		lineArr =[]
		curLine = line.strip().split('\t')
		for i in range(numFeat):
			lineArr.append(float(curLine[i]))
		xArr.append(lineArr)
		yArr.append(float(curLine[-1]))
	return xArr, yArr",_16682.py,14,numFeat = len(open(fileName).readline().split('\t')) - 1,"with open(fileName) as my_f:
    numFeat = len(my_f.readline().split('\t')) - 1
    xArr = []
    yArr = []
    fr = open(fileName)
    for line in fr.readlines():
        lineArr = []
        curLine = line.strip().split('\t')
        for i in range(numFeat):
            lineArr.append(float(curLine[i]))
        xArr.append(lineArr)
        yArr.append(float(curLine[-1]))
    return (xArr, yArr)"
https://github.com/astropy/astroquery/tree/master/astroquery/utils/tap/xmlparser/utils.py,"def read_file_content(filePath):
    fileHandler = open(filePath, 'r')
    fileContent = fileHandler.read()
    fileHandler.close()
    return fileContent",_17536.py,2,"fileHandler = open(filePath, 'r')","with open(filePath, 'r') as fileHandler:
    fileContent = fileHandler.read()
    return fileContent"
https://github.com/tkrajina/gpxpy/tree/master//test.py,"def test_split(self) -> None:
        f = open('test_files/cerknicko-jezero.gpx')
        parser = mod_parser.GPXParser(f)
        gpx = parser.parse()
        f.close()

        track = gpx.tracks[1]

        track_points_no = track.get_points_no()

        before = len(track.segments)
        track.split(0, 10)
        after = len(track.segments)

        self.assertTrue(before + 1 == after)
        print('Points in first (split) part:', len(track.segments[0].points))

        # From 0 to 10th point == 11 points:
        self.assertTrue(len(track.segments[0].points) == 11)
        self.assertTrue(len(track.segments[0].points) + len(track.segments[1].points) == track_points_no)

        # Now split the second track
        track.split(1, 20)
        self.assertTrue(len(track.segments[1].points) == 21)
        self.assertTrue(len(track.segments[0].points) + len(track.segments[1].points) + len(track.segments[2].points) == track_points_no)",_17586.py,2,f = open('test_files/cerknicko-jezero.gpx'),"with open('test_files/cerknicko-jezero.gpx') as f:
    parser = mod_parser.GPXParser(f)
    gpx = parser.parse()
    
    track = gpx.tracks[1]
    track_points_no = track.get_points_no()
    before = len(track.segments)
    track.split(0, 10)
    after = len(track.segments)
    self.assertTrue(before + 1 == after)
    print('Points in first (split) part:', len(track.segments[0].points))
    self.assertTrue(len(track.segments[0].points) == 11)
    self.assertTrue(len(track.segments[0].points) + len(track.segments[1].points) == track_points_no)
    track.split(1, 20)
    self.assertTrue(len(track.segments[1].points) == 21)
    self.assertTrue(len(track.segments[0].points) + len(track.segments[1].points) + len(track.segments[2].points) == track_points_no)"
https://github.com/edx/edx-platform/tree/master/openedx/features/enterprise_support/tests/test_admin.py,"def test_post_with_incorrect_csv_header(self):
        """"""
        Tests that HTTP POST with incorrect csv header is working as expected.
        """"""
        csv_path = self.create_csv(header=['a', 'b'])
        post_data = {'csv_file': open(csv_path)}
        response = self.client.post(self.view_url, data=post_data)
        assert response.context['form'].errors == {
            'csv_file': [
                'Expected a CSV file with [lms_user_id, course_id, opportunity_id] '
                'columns, but found [a, b] columns instead.'
            ]
        }",_17702.py,6,post_data = {'csv_file': open(csv_path)},"with open(csv_path) as my_f:
    post_data = {'csv_file': my_f}
    response = self.client.post(self.view_url, data=post_data)
    assert response.context['form'].errors == {'csv_file': ['Expected a CSV file with [lms_user_id, course_id, opportunity_id] columns, but found [a, b] columns instead.']}"
https://github.com/dcos/dcos/tree/master/gen/calc.py,"def calculate_fault_domain_detect_contents(fault_domain_detect_filename):
    if os.path.exists(fault_domain_detect_filename):
        return yaml.dump(open(fault_domain_detect_filename, encoding='utf-8').read())
    return ''",_17804.py,3,"return yaml.dump(open(fault_domain_detect_filename, encoding='utf-8').read())","with open(fault_domain_detect_filename, encoding='utf-8') as my_f:
    return yaml.dump(my_f.read())"
https://github.com/dcos/dcos/tree/master/gen/calc.py,"def calculate_fault_domain_detect_contents(fault_domain_detect_filename):
    if os.path.exists(fault_domain_detect_filename):
        return yaml.dump(open(fault_domain_detect_filename, encoding='utf-8').read())
    return ''",_17804.py,3,"return yaml.dump(open(fault_domain_detect_filename, encoding='utf-8').read())","with open(fault_domain_detect_filename, encoding='utf-8') as my_f:
    return yaml.dump(my_f.read())"
https://github.com/tanghaibao/jcvi/tree/master/jcvi/compara/catalog.py,"def sort_layout(thread, listfile, column=0):
    """"""
    Sort the syntelog table according to chromomomal positions. First orient the
    contents against threadbed, then for contents not in threadbed, insert to
    the nearest neighbor.
    """"""
    from jcvi.formats.base import DictFile

    outfile = listfile.rsplit(""."", 1)[0] + "".sorted.list""
    threadorder = thread.order
    fw = open(outfile, ""w"")
    lt = DictFile(listfile, keypos=column, valuepos=None)
    threaded = []
    imported = set()
    for t in thread:
        accn = t.accn
        if accn not in lt:
            continue

        imported.add(accn)
        atoms = lt[accn]
        threaded.append(atoms)

    assert len(threaded) == len(imported)

    total = sum(1 for x in open(listfile))
    logging.debug(""Total: {0}, currently threaded: {1}"".format(total, len(threaded)))
    fp = open(listfile)
    for row in fp:
        atoms = row.split()
        accn = atoms[0]
        if accn in imported:
            continue
        insert_into_threaded(atoms, threaded, threadorder)

    for atoms in threaded:
        print(""\t"".join(atoms), file=fw)

    fw.close()
    logging.debug(""File `{0}` sorted to `{1}`."".format(outfile, thread.filename))",_18060.py,11,"fw = open(outfile, 'w')","with open(outfile, 'w') as fw:
    lt = DictFile(listfile, keypos=column, valuepos=None)
    threaded = []
    imported = set()
    for t in thread:
        accn = t.accn
        if accn not in lt:
            continue
        imported.add(accn)
        atoms = lt[accn]
        threaded.append(atoms)
    assert len(threaded) == len(imported)
    total = sum((1 for x in open(listfile)))
    logging.debug('Total: {0}, currently threaded: {1}'.format(total, len(threaded)))
    fp = open(listfile)
    for row in fp:
        atoms = row.split()
        accn = atoms[0]
        if accn in imported:
            continue
        insert_into_threaded(atoms, threaded, threadorder)
    for atoms in threaded:
        print('\t'.join(atoms), file=fw)
    
    pass
    logging.debug('File `{0}` sorted to `{1}`.'.format(outfile, thread.filename))"
https://github.com/tanghaibao/jcvi/tree/master/jcvi/compara/catalog.py,"def sort_layout(thread, listfile, column=0):
    """"""
    Sort the syntelog table according to chromomomal positions. First orient the
    contents against threadbed, then for contents not in threadbed, insert to
    the nearest neighbor.
    """"""
    from jcvi.formats.base import DictFile

    outfile = listfile.rsplit(""."", 1)[0] + "".sorted.list""
    threadorder = thread.order
    fw = open(outfile, ""w"")
    lt = DictFile(listfile, keypos=column, valuepos=None)
    threaded = []
    imported = set()
    for t in thread:
        accn = t.accn
        if accn not in lt:
            continue

        imported.add(accn)
        atoms = lt[accn]
        threaded.append(atoms)

    assert len(threaded) == len(imported)

    total = sum(1 for x in open(listfile))
    logging.debug(""Total: {0}, currently threaded: {1}"".format(total, len(threaded)))
    fp = open(listfile)
    for row in fp:
        atoms = row.split()
        accn = atoms[0]
        if accn in imported:
            continue
        insert_into_threaded(atoms, threaded, threadorder)

    for atoms in threaded:
        print(""\t"".join(atoms), file=fw)

    fw.close()
    logging.debug(""File `{0}` sorted to `{1}`."".format(outfile, thread.filename))",_18060.py,28,fp = open(listfile),"with open(listfile) as fp:
    for row in fp:
        atoms = row.split()
        accn = atoms[0]
        if accn in imported:
            continue
        insert_into_threaded(atoms, threaded, threadorder)
    for atoms in threaded:
        print('\t'.join(atoms), file=fw)
    fw.close()
    logging.debug('File `{0}` sorted to `{1}`.'.format(outfile, thread.filename))"
https://github.com/tanghaibao/jcvi/tree/master/jcvi/compara/catalog.py,"def sort_layout(thread, listfile, column=0):
    """"""
    Sort the syntelog table according to chromomomal positions. First orient the
    contents against threadbed, then for contents not in threadbed, insert to
    the nearest neighbor.
    """"""
    from jcvi.formats.base import DictFile

    outfile = listfile.rsplit(""."", 1)[0] + "".sorted.list""
    threadorder = thread.order
    fw = open(outfile, ""w"")
    lt = DictFile(listfile, keypos=column, valuepos=None)
    threaded = []
    imported = set()
    for t in thread:
        accn = t.accn
        if accn not in lt:
            continue

        imported.add(accn)
        atoms = lt[accn]
        threaded.append(atoms)

    assert len(threaded) == len(imported)

    total = sum(1 for x in open(listfile))
    logging.debug(""Total: {0}, currently threaded: {1}"".format(total, len(threaded)))
    fp = open(listfile)
    for row in fp:
        atoms = row.split()
        accn = atoms[0]
        if accn in imported:
            continue
        insert_into_threaded(atoms, threaded, threadorder)

    for atoms in threaded:
        print(""\t"".join(atoms), file=fw)

    fw.close()
    logging.debug(""File `{0}` sorted to `{1}`."".format(outfile, thread.filename))",_18060.py,26,total = sum((1 for x in open(listfile))),"with open(listfile) as my_f:
    total = sum((1 for x in my_f))
    logging.debug('Total: {0}, currently threaded: {1}'.format(total, len(threaded)))
    fp = open(listfile)
    for row in fp:
        atoms = row.split()
        accn = atoms[0]
        if accn in imported:
            continue
        insert_into_threaded(atoms, threaded, threadorder)
    for atoms in threaded:
        print('\t'.join(atoms), file=fw)
    fw.close()
    logging.debug('File `{0}` sorted to `{1}`.'.format(outfile, thread.filename))"
https://github.com/zillow/luminaire/tree/master/luminaire/tests/conftest.py,"def lad_structural_model():
    """"""
    Model to test lad structural result
    """"""
    import pickle

    model_file = open(get_model_path('lad_structural_model'), 'rb')
    model = pickle.load(model_file)

    model_file.close()

    return model",_18471.py,7,"model_file = open(get_model_path('lad_structural_model'), 'rb')","with open(get_model_path('lad_structural_model'), 'rb') as model_file:
    model = pickle.load(model_file)
    
    return model"
https://github.com/NervanaSystems/neon/tree/master/examples/ssd/datasets/ingest_spacenet.py,"def plot_image(im_path, json_path, save_path):
    im = Image.open(im_path)
    img = np.array(im).astype(np.float32)

    im = Image.fromarray(img.astype(np.uint8))
    bboxes = json.load(open(json_path))

    draw = ImageDraw.Draw(im)

    for obj in bboxes['object']:
        bbox = obj['bndbox']
        draw.rectangle([bbox['xmin'], bbox['ymin'],
                        bbox['xmax'], bbox['ymax']], outline=(255, 0, 0))

    im.save(save_path)",_18504.py,6,bboxes = json.load(open(json_path)),"with open(json_path) as my_f:
    bboxes = json.load(my_f)
    draw = ImageDraw.Draw(im)
    for obj in bboxes['object']:
        bbox = obj['bndbox']
        draw.rectangle([bbox['xmin'], bbox['ymin'], bbox['xmax'], bbox['ymax']], outline=(255, 0, 0))
    im.save(save_path)"
https://github.com/scipy/scipy/tree/master/scipy/io/matlab/tests/test_mio.py,"def test_read_both_endian():
    # make sure big- and little- endian data is read correctly
    for fname in ('big_endian.mat', 'little_endian.mat'):
        fp = open(pjoin(test_data_path, fname), 'rb')
        rdr = MatFile5Reader(fp)
        d = rdr.get_variables()
        fp.close()
        assert_array_equal(d['strings'],
                           np.array([['hello'],
                                     ['world']], dtype=object))
        assert_array_equal(d['floats'],
                           np.array([[2., 3.],
                                     [3., 4.]], dtype=np.float32))",_18566.py,4,"fp = open(pjoin(test_data_path, fname), 'rb')","with open(pjoin(test_data_path, fname), 'rb') as fp:
    rdr = MatFile5Reader(fp)
    d = rdr.get_variables()
    
    assert_array_equal(d['strings'], np.array([['hello'], ['world']], dtype=object))
    assert_array_equal(d['floats'], np.array([[2.0, 3.0], [3.0, 4.0]], dtype=np.float32))"
https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-aws/dagster_aws_tests/emr_tests/test_configs_spark.py,"def test_spark_configs_same_as_in_dagster_spark():
    aws_contents = open(aws_configs_spark.__file__, encoding=""utf8"").read()
    spark_contents = open(spark_configs_spark.__file__, encoding=""utf8"").read()
    assert aws_contents == spark_contents",_18816.py,2,"aws_contents = open(aws_configs_spark.__file__, encoding='utf8').read()","with open(aws_configs_spark.__file__, encoding='utf8') as my_f:
    aws_contents = my_f.read()
    spark_contents = open(spark_configs_spark.__file__, encoding='utf8').read()
    assert aws_contents == spark_contents"
https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-aws/dagster_aws_tests/emr_tests/test_configs_spark.py,"def test_spark_configs_same_as_in_dagster_spark():
    aws_contents = open(aws_configs_spark.__file__, encoding=""utf8"").read()
    spark_contents = open(spark_configs_spark.__file__, encoding=""utf8"").read()
    assert aws_contents == spark_contents",_18816.py,3,"spark_contents = open(spark_configs_spark.__file__, encoding='utf8').read()","with open(spark_configs_spark.__file__, encoding='utf8') as my_f:
    spark_contents = my_f.read()
    assert aws_contents == spark_contents"
https://github.com/subuser-security/subuser/tree/master/subuserlib/subprocessExtras.py,"def callBackground(args,cwd=None,suppressOutput=True,collectStdout=False,collectStderr=False):
  """"""
  Same as subprocess.call except here you can specify the cwd.
  Returns imediately with the subprocess
  """"""
  stdout = None
  stderr = None

  if suppressOutput:
    devnull = open(os.devnull,""a"")
    stdout = devnull
    stderr = devnull

  if collectStdout:
    temp_stdout = tempfile.TemporaryFile(mode=""r"")
    stdout = temp_stdout.fileno()
  if collectStderr:
    temp_stderr = tempfile.TemporaryFile(mode=""r"")
    stderr = temp_stderr.fileno()

  process = subprocess.Popen(args,cwd=cwd,stdout=stdout,stderr=stderr,close_fds=True)

  if collectStdout:
    process.stdout_file = temp_stdout
  if collectStderr:
    process.stderr_file = temp_stderr

  return process",_18916.py,10,"devnull = open(os.devnull, 'a')","with open(os.devnull, 'a') as devnull:
    stdout = devnull
    stderr = devnull"
https://github.com/sevagas/macro_pack/tree/master/src/modules/wsf_gen.py,"def generate(self):
        logging.info("" [+] Generating %s file..."" % self.outputFileType)
        self.vbScriptConvert()
        f = open(self.getMainVBAFile()+"".vbs"")
        vbsContent = f.read()
        f.close()
        
        #vbsContent = vbsContent.replace(""WScript.Echo "", ""MsgBox "")
        
        # Write VBS in template
        wsfContent = WSF_TEMPLATE
        wsfContent = wsfContent.replace(""<<<random>>>"", randomAlpha(8))
        wsfContent = wsfContent.replace(""<<<VBS>>>"", vbsContent)
        wsfContent = wsfContent.replace(""<<<MAIN>>>"", self.startFunction)
        # Write in new HTA file
        f = open(self.outputFilePath, 'w')
        f.writelines(wsfContent)
        f.close()
        logging.info(""   [-] Generated Windows Script File: %s"" % self.outputFilePath)
        logging.info(""   [-] Test with : \nwscript %s\n"" % self.outputFilePath)
        if os.path.getsize(self.outputFilePath)> (1024*512):
            logging.warning(""   [!] Warning: The resulted %s file seems to be bigger than 512k, it will probably not work!"" % self.outputFileType)",_18993.py,4,f = open(self.getMainVBAFile() + '.vbs'),"with open(self.getMainVBAFile() + '.vbs') as f:
    vbsContent = f.read()
    
    pass
    wsfContent = WSF_TEMPLATE
    wsfContent = wsfContent.replace('<<<random>>>', randomAlpha(8))
    wsfContent = wsfContent.replace('<<<VBS>>>', vbsContent)
    wsfContent = wsfContent.replace('<<<MAIN>>>', self.startFunction)
    f = open(self.outputFilePath, 'w')
    f.writelines(wsfContent)
    logging.info('   [-] Generated Windows Script File: %s' % self.outputFilePath)
    logging.info('   [-] Test with : \nwscript %s\n' % self.outputFilePath)
    if os.path.getsize(self.outputFilePath) > 1024 * 512:
        logging.warning('   [!] Warning: The resulted %s file seems to be bigger than 512k, it will probably not work!' % self.outputFileType)"
https://github.com/sevagas/macro_pack/tree/master/src/modules/wsf_gen.py,"def generate(self):
        logging.info("" [+] Generating %s file..."" % self.outputFileType)
        self.vbScriptConvert()
        f = open(self.getMainVBAFile()+"".vbs"")
        vbsContent = f.read()
        f.close()
        
        #vbsContent = vbsContent.replace(""WScript.Echo "", ""MsgBox "")
        
        # Write VBS in template
        wsfContent = WSF_TEMPLATE
        wsfContent = wsfContent.replace(""<<<random>>>"", randomAlpha(8))
        wsfContent = wsfContent.replace(""<<<VBS>>>"", vbsContent)
        wsfContent = wsfContent.replace(""<<<MAIN>>>"", self.startFunction)
        # Write in new HTA file
        f = open(self.outputFilePath, 'w')
        f.writelines(wsfContent)
        f.close()
        logging.info(""   [-] Generated Windows Script File: %s"" % self.outputFilePath)
        logging.info(""   [-] Test with : \nwscript %s\n"" % self.outputFilePath)
        if os.path.getsize(self.outputFilePath)> (1024*512):
            logging.warning(""   [!] Warning: The resulted %s file seems to be bigger than 512k, it will probably not work!"" % self.outputFileType)",_18993.py,16,"f = open(self.outputFilePath, 'w')","with open(self.outputFilePath, 'w') as f:
    f.writelines(wsfContent)
    logging.info('   [-] Generated Windows Script File: %s' % self.outputFilePath)
    logging.info('   [-] Test with : \nwscript %s\n' % self.outputFilePath)
    if os.path.getsize(self.outputFilePath) > 1024 * 512:
        logging.warning('   [!] Warning: The resulted %s file seems to be bigger than 512k, it will probably not work!' % self.outputFileType)"
https://github.com/freeipa/freeipa/tree/master/ipatests/test_ipalib/test_config.py,"def test_merge_from_file(self):
        """"""
        Test the `ipalib.config.Env._merge_from_file` method.
        """"""
        tmp = TempDir()
        assert callable(tmp.join)

        # Test a config file that doesn't exist
        no_exist = tmp.join('no_exist.conf')
        assert not path.exists(no_exist)
        o = self.cls()
        o._bootstrap()
        keys = tuple(o)
        orig = dict((k, o[k]) for k in o)
        assert o._merge_from_file(no_exist) is None
        assert tuple(o) == keys

        # Test an empty config file
        empty = tmp.touch('empty.conf')
        assert path.isfile(empty)
        assert o._merge_from_file(empty) == (0, 0)
        assert tuple(o) == keys

        # Test a mal-formed config file:
        bad = tmp.join('bad.conf')
        open(bad, 'w').write(config_bad)
        assert path.isfile(bad)
        assert o._merge_from_file(bad) is None
        assert tuple(o) == keys

        # Test a valid config file that tries to override
        override = tmp.join('override.conf')
        open(override, 'w').write(config_override)
        assert path.isfile(override)
        assert o._merge_from_file(override) == (4, 6)
        for (k, v) in orig.items():
            assert o[k] is v
        assert list(o) == sorted(keys + ('key0', 'key1', 'key2', 'key3', 'config_loaded'))
        for i in range(4):
            assert o['key%d' % i] == ('var%d' % i)
        keys = tuple(o)

        # Test a valid config file with type conversion
        good = tmp.join('good.conf')
        open(good, 'w').write(config_good)
        assert path.isfile(good)
        assert o._merge_from_file(good) == (6, 6)
        added = ('string', 'null', 'yes', 'no', 'number', 'floating')
        assert list(o) == sorted(keys + added)

        # pylint: disable=no-member
        assert o.string == 'Hello world!'
        assert o.null is None
        assert o.yes is True
        assert o.no is False
        assert o.number == 42
        assert o.floating == '3.14'",_19487.py,26,"open(bad, 'w').write(config_bad)","with open(bad, 'w') as my_f:
    my_f.write(config_bad)
    assert path.isfile(bad)
    assert o._merge_from_file(bad) is None
    assert tuple(o) == keys
    override = tmp.join('override.conf')
    open(override, 'w').write(config_override)
    assert path.isfile(override)
    assert o._merge_from_file(override) == (4, 6)
    for (k, v) in orig.items():
        assert o[k] is v
    assert list(o) == sorted(keys + ('key0', 'key1', 'key2', 'key3', 'config_loaded'))
    for i in range(4):
        assert o['key%d' % i] == 'var%d' % i
    keys = tuple(o)
    good = tmp.join('good.conf')
    open(good, 'w').write(config_good)
    assert path.isfile(good)
    assert o._merge_from_file(good) == (6, 6)
    added = ('string', 'null', 'yes', 'no', 'number', 'floating')
    assert list(o) == sorted(keys + added)
    assert o.string == 'Hello world!'
    assert o.null is None
    assert o.yes is True
    assert o.no is False
    assert o.number == 42
    assert o.floating == '3.14'"
https://github.com/freeipa/freeipa/tree/master/ipatests/test_ipalib/test_config.py,"def test_merge_from_file(self):
        """"""
        Test the `ipalib.config.Env._merge_from_file` method.
        """"""
        tmp = TempDir()
        assert callable(tmp.join)

        # Test a config file that doesn't exist
        no_exist = tmp.join('no_exist.conf')
        assert not path.exists(no_exist)
        o = self.cls()
        o._bootstrap()
        keys = tuple(o)
        orig = dict((k, o[k]) for k in o)
        assert o._merge_from_file(no_exist) is None
        assert tuple(o) == keys

        # Test an empty config file
        empty = tmp.touch('empty.conf')
        assert path.isfile(empty)
        assert o._merge_from_file(empty) == (0, 0)
        assert tuple(o) == keys

        # Test a mal-formed config file:
        bad = tmp.join('bad.conf')
        open(bad, 'w').write(config_bad)
        assert path.isfile(bad)
        assert o._merge_from_file(bad) is None
        assert tuple(o) == keys

        # Test a valid config file that tries to override
        override = tmp.join('override.conf')
        open(override, 'w').write(config_override)
        assert path.isfile(override)
        assert o._merge_from_file(override) == (4, 6)
        for (k, v) in orig.items():
            assert o[k] is v
        assert list(o) == sorted(keys + ('key0', 'key1', 'key2', 'key3', 'config_loaded'))
        for i in range(4):
            assert o['key%d' % i] == ('var%d' % i)
        keys = tuple(o)

        # Test a valid config file with type conversion
        good = tmp.join('good.conf')
        open(good, 'w').write(config_good)
        assert path.isfile(good)
        assert o._merge_from_file(good) == (6, 6)
        added = ('string', 'null', 'yes', 'no', 'number', 'floating')
        assert list(o) == sorted(keys + added)

        # pylint: disable=no-member
        assert o.string == 'Hello world!'
        assert o.null is None
        assert o.yes is True
        assert o.no is False
        assert o.number == 42
        assert o.floating == '3.14'",_19487.py,33,"open(override, 'w').write(config_override)","with open(override, 'w') as my_f:
    my_f.write(config_override)
    assert path.isfile(override)
    assert o._merge_from_file(override) == (4, 6)
    for (k, v) in orig.items():
        assert o[k] is v
    assert list(o) == sorted(keys + ('key0', 'key1', 'key2', 'key3', 'config_loaded'))
    for i in range(4):
        assert o['key%d' % i] == 'var%d' % i
    keys = tuple(o)
    good = tmp.join('good.conf')
    open(good, 'w').write(config_good)
    assert path.isfile(good)
    assert o._merge_from_file(good) == (6, 6)
    added = ('string', 'null', 'yes', 'no', 'number', 'floating')
    assert list(o) == sorted(keys + added)
    assert o.string == 'Hello world!'
    assert o.null is None
    assert o.yes is True
    assert o.no is False
    assert o.number == 42
    assert o.floating == '3.14'"
https://github.com/freeipa/freeipa/tree/master/ipatests/test_ipalib/test_config.py,"def test_merge_from_file(self):
        """"""
        Test the `ipalib.config.Env._merge_from_file` method.
        """"""
        tmp = TempDir()
        assert callable(tmp.join)

        # Test a config file that doesn't exist
        no_exist = tmp.join('no_exist.conf')
        assert not path.exists(no_exist)
        o = self.cls()
        o._bootstrap()
        keys = tuple(o)
        orig = dict((k, o[k]) for k in o)
        assert o._merge_from_file(no_exist) is None
        assert tuple(o) == keys

        # Test an empty config file
        empty = tmp.touch('empty.conf')
        assert path.isfile(empty)
        assert o._merge_from_file(empty) == (0, 0)
        assert tuple(o) == keys

        # Test a mal-formed config file:
        bad = tmp.join('bad.conf')
        open(bad, 'w').write(config_bad)
        assert path.isfile(bad)
        assert o._merge_from_file(bad) is None
        assert tuple(o) == keys

        # Test a valid config file that tries to override
        override = tmp.join('override.conf')
        open(override, 'w').write(config_override)
        assert path.isfile(override)
        assert o._merge_from_file(override) == (4, 6)
        for (k, v) in orig.items():
            assert o[k] is v
        assert list(o) == sorted(keys + ('key0', 'key1', 'key2', 'key3', 'config_loaded'))
        for i in range(4):
            assert o['key%d' % i] == ('var%d' % i)
        keys = tuple(o)

        # Test a valid config file with type conversion
        good = tmp.join('good.conf')
        open(good, 'w').write(config_good)
        assert path.isfile(good)
        assert o._merge_from_file(good) == (6, 6)
        added = ('string', 'null', 'yes', 'no', 'number', 'floating')
        assert list(o) == sorted(keys + added)

        # pylint: disable=no-member
        assert o.string == 'Hello world!'
        assert o.null is None
        assert o.yes is True
        assert o.no is False
        assert o.number == 42
        assert o.floating == '3.14'",_19487.py,45,"open(good, 'w').write(config_good)","with open(good, 'w') as my_f:
    my_f.write(config_good)
    assert path.isfile(good)
    assert o._merge_from_file(good) == (6, 6)
    added = ('string', 'null', 'yes', 'no', 'number', 'floating')
    assert list(o) == sorted(keys + added)
    assert o.string == 'Hello world!'
    assert o.null is None
    assert o.yes is True
    assert o.no is False
    assert o.number == 42
    assert o.floating == '3.14'"
https://github.com/uber-common/metta/tree/master//run_simulation_yaml.py,"def run_scenario(ioc_filename):
    try:
        print(""### Running the Scenario ###"")
        raw_iocs = yaml.load_all(open(ioc_filename, 'r').read())

        timenow = datetime.datetime.utcnow()

        for raw_ioc in raw_iocs:
            scenario = raw_ioc.get('meta').get('scenario_actions')
            rule_name = raw_ioc.get('name')
            print(""### {} ###"".format(rule_name))

            scenario_actions = []
            # read the steps from purple_actions in yaml and load them into purple_actions
            for x in range(1, len(scenario)+1):
                scenario_actions.append(raw_ioc.get('meta').get('scenario_actions').get(x))

            for uuid_file in scenario_actions:
                run_uuid(uuid_file)

    except Exception as e:
        print(e)",_19745.py,4,"raw_iocs = yaml.load_all(open(ioc_filename, 'r').read())","with open(ioc_filename, 'r') as my_f:
    raw_iocs = yaml.load_all(my_f.read())
    timenow = datetime.datetime.utcnow()
    for raw_ioc in raw_iocs:
        scenario = raw_ioc.get('meta').get('scenario_actions')
        rule_name = raw_ioc.get('name')
        print('### {} ###'.format(rule_name))
        scenario_actions = []
        for x in range(1, len(scenario) + 1):
            scenario_actions.append(raw_ioc.get('meta').get('scenario_actions').get(x))
        for uuid_file in scenario_actions:
            run_uuid(uuid_file)"
https://github.com/uber-common/metta/tree/master//run_simulation_yaml.py,"def run_scenario(ioc_filename):
    try:
        print(""### Running the Scenario ###"")
        raw_iocs = yaml.load_all(open(ioc_filename, 'r').read())

        timenow = datetime.datetime.utcnow()

        for raw_ioc in raw_iocs:
            scenario = raw_ioc.get('meta').get('scenario_actions')
            rule_name = raw_ioc.get('name')
            print(""### {} ###"".format(rule_name))

            scenario_actions = []
            # read the steps from purple_actions in yaml and load them into purple_actions
            for x in range(1, len(scenario)+1):
                scenario_actions.append(raw_ioc.get('meta').get('scenario_actions').get(x))

            for uuid_file in scenario_actions:
                run_uuid(uuid_file)

    except Exception as e:
        print(e)",_19745.py,4,"raw_iocs = yaml.load_all(open(ioc_filename, 'r').read())","with open(ioc_filename, 'r') as my_f:
    raw_iocs = yaml.load_all(my_f.read())"
https://github.com/JiaRenChang/PSMNet/tree/master/dataloader/readpfm.py,"def readPFM(file):
    file = open(file, 'rb')

    color = None
    width = None
    height = None
    scale = None
    endian = None

    header = file.readline().rstrip()
    encode_type = chardet.detect(header)  
    header = header.decode(encode_type['encoding'])
    if header == 'PF':
        color = True
    elif header == 'Pf':
        color = False
    else:
        raise Exception('Not a PFM file.')

    dim_match = re.match(r'^(\d+)\s(\d+)\s$', file.readline().decode(encode_type['encoding']))
    if dim_match:
        width, height = map(int, dim_match.groups())
    else:
        raise Exception('Malformed PFM header.')

    scale = float(file.readline().rstrip().decode(encode_type['encoding']))
    if scale < 0: # little-endian
        endian = '<'
        scale = -scale
    else:
        endian = '>' # big-endian

    data = np.fromfile(file, endian + 'f')
    shape = (height, width, 3) if color else (height, width)

    data = np.reshape(data, shape)
    data = np.flipud(data)
    return data, scale",_20527.py,2,"file = open(file, 'rb')","with open(file, 'rb') as file:
    color = None
    width = None
    height = None
    scale = None
    endian = None
    header = file.readline().rstrip()
    encode_type = chardet.detect(header)
    header = header.decode(encode_type['encoding'])
    if header == 'PF':
        color = True
    elif header == 'Pf':
        color = False
    else:
        raise Exception('Not a PFM file.')
    dim_match = re.match('^(\\d+)\\s(\\d+)\\s$', file.readline().decode(encode_type['encoding']))
    if dim_match:
        (width, height) = map(int, dim_match.groups())
    else:
        raise Exception('Malformed PFM header.')
    scale = float(file.readline().rstrip().decode(encode_type['encoding']))
    if scale < 0:
        endian = '<'
        scale = -scale
    else:
        endian = '>'
    data = np.fromfile(file, endian + 'f')
    shape = (height, width, 3) if color else (height, width)
    data = np.reshape(data, shape)
    data = np.flipud(data)
    return (data, scale)"
https://github.com/cisagov/Malcolm/tree/master/shared/bin/zeek_carve_utils.py,"def touch(filename):
  open(filename, 'a').close()
  os.utime(filename, None)",_20580.py,2,"open(filename, 'a').close()","with open(filename, 'a') as my_f:
    my_f.close()
    os.utime(filename, None)"
https://github.com/microsoft/maro/tree/master/maro/cli/inspector/env_data_process.py,"def _get_index_index_name_conversion(scenario: GlobalScenarios, source_path: str, conversion_path: str):
    """"""Generate a CSV File which indicates the relationship between resource holder's index and name.

    Args:
        scenario (GlobalScenarios): Current scenario. Different scenario has different type of mapping file.
        source_path (str): The root path of the dumped snapshots data for the corresponding experiment.
        conversion_path (str): Path of original mapping file.
    """"""
    conversion_path = os.path.join(source_path, conversion_path)
    if os.path.exists(os.path.join(source_path, GlobalFileNames.name_convert)):
        os.remove(os.path.join(source_path, GlobalFileNames.name_convert))
    if scenario == GlobalScenarios.CITI_BIKE:
        # TODO: the commented out code are older version which will cause errors.
        # TODO: the updated code could work but the fix is temporary.
        # TODO: we need to refactor the dump logic in citi bike scenario and make a stable solution later.

        # with open(conversion_path, ""r"", encoding=""utf8"")as mapping_file:
        #     mapping_json_data = json.load(mapping_file)
        #     name_list = []
        #     for item in mapping_json_data[""data""][""stations""]:
        #         name_list.append(item[""name""])
        #     df = pd.DataFrame({""name"": name_list})
        #     df.to_csv(os.path.join(source_path, GlobalFileNames.name_convert), index=False)

        df_station = pd.read_csv(os.path.join(source_path, ""epoch_0"", ""stations.csv""))
        name_list = df_station[""name""].unique()
        df = pd.DataFrame({""name"": name_list})
        df.to_csv(os.path.join(source_path, GlobalFileNames.name_convert), index=False)

    elif scenario == GlobalScenarios.CIM:
        cim_information = yaml.load(
            open(conversion_path, ""r"").read(),
            Loader=yaml.FullLoader,
        )
        conversion = cim_information[""ports""].keys()
        df = pd.DataFrame(list(conversion))
        df.to_csv(os.path.join(source_path, GlobalFileNames.name_convert), index=False)",_20858.py,32,"cim_information = yaml.load(open(conversion_path, 'r').read(), Loader=yaml.FullLoader)","with open(conversion_path, 'r') as my_f:
    cim_information = yaml.load(my_f.read(), Loader=yaml.FullLoader)
    conversion = cim_information['ports'].keys()
    df = pd.DataFrame(list(conversion))
    df.to_csv(os.path.join(source_path, GlobalFileNames.name_convert), index=False)"
https://github.com/microsoft/maro/tree/master/maro/cli/inspector/env_data_process.py,"def _get_index_index_name_conversion(scenario: GlobalScenarios, source_path: str, conversion_path: str):
    """"""Generate a CSV File which indicates the relationship between resource holder's index and name.

    Args:
        scenario (GlobalScenarios): Current scenario. Different scenario has different type of mapping file.
        source_path (str): The root path of the dumped snapshots data for the corresponding experiment.
        conversion_path (str): Path of original mapping file.
    """"""
    conversion_path = os.path.join(source_path, conversion_path)
    if os.path.exists(os.path.join(source_path, GlobalFileNames.name_convert)):
        os.remove(os.path.join(source_path, GlobalFileNames.name_convert))
    if scenario == GlobalScenarios.CITI_BIKE:
        # TODO: the commented out code are older version which will cause errors.
        # TODO: the updated code could work but the fix is temporary.
        # TODO: we need to refactor the dump logic in citi bike scenario and make a stable solution later.

        # with open(conversion_path, ""r"", encoding=""utf8"")as mapping_file:
        #     mapping_json_data = json.load(mapping_file)
        #     name_list = []
        #     for item in mapping_json_data[""data""][""stations""]:
        #         name_list.append(item[""name""])
        #     df = pd.DataFrame({""name"": name_list})
        #     df.to_csv(os.path.join(source_path, GlobalFileNames.name_convert), index=False)

        df_station = pd.read_csv(os.path.join(source_path, ""epoch_0"", ""stations.csv""))
        name_list = df_station[""name""].unique()
        df = pd.DataFrame({""name"": name_list})
        df.to_csv(os.path.join(source_path, GlobalFileNames.name_convert), index=False)

    elif scenario == GlobalScenarios.CIM:
        cim_information = yaml.load(
            open(conversion_path, ""r"").read(),
            Loader=yaml.FullLoader,
        )
        conversion = cim_information[""ports""].keys()
        df = pd.DataFrame(list(conversion))
        df.to_csv(os.path.join(source_path, GlobalFileNames.name_convert), index=False)",_20858.py,32,"cim_information = yaml.load(open(conversion_path, 'r').read(), Loader=yaml.FullLoader)","with open(conversion_path, 'r') as my_f:
    cim_information = yaml.load(my_f.read(), Loader=yaml.FullLoader)"
https://github.com/akfamily/akshare/tree/master/akshare/option/cons.py,"def get_calendar():
    """"""
    获取交易日历至 2019 年结束, 这里的交易日历需要按年更新
    :return: json
    """"""
    setting_file_name = ""calendar.json""
    setting_file_path = get_json_path(setting_file_name, __file__)
    return json.load(open(setting_file_path, ""r""))",_21393.py,8,"return json.load(open(setting_file_path, 'r'))","with open(setting_file_path, 'r') as my_f:
    return json.load(my_f)"
https://github.com/saeeddhqan/Maryam/tree/master/maryam/core/core.py,"def _is_readable(self, filename, flag='r'):
		try:
			return open(filename, flag)
		except IOError as e:
			self.error('IOError: ' + str(e))
			return False",_21459.py,3,"return open(filename, flag)","with open(filename, flag) as my_f:
    return my_f"
https://github.com/saeeddhqan/Maryam/tree/master/maryam/core/core.py,"def _is_readable(self, filename, flag='r'):
		try:
			return open(filename, flag)
		except IOError as e:
			self.error('IOError: ' + str(e))
			return False",_21459.py,3,"return open(filename, flag)","with open(filename, flag) as my_f:
    return my_f"
https://github.com/yandex/yandex-tank/tree/master/yandextank/plugins/Phantom/widget.py,"def render(self, screen):
        res = """"

        dur_seconds = int(time.time()) - int(self.owner.start_time)

        eta_time = 'N/A'
        eta_secs = -1
        progress = 0
        color_bg = screen.markup.BG_CYAN
        color_fg = screen.markup.CYAN
        if self.test_duration and self.test_duration >= dur_seconds:
            color_bg = screen.markup.BG_GREEN
            color_fg = screen.markup.GREEN
            eta_secs = self.test_duration - dur_seconds
            eta_time = datetime.timedelta(seconds=eta_secs)
            progress = float(dur_seconds) / self.test_duration
        elif self.ammo_progress:
            left_part = self.ammo_count - self.ammo_progress
            if left_part > 0:
                eta_secs = int(
                    float(dur_seconds) / float(self.ammo_progress) * float(left_part))
            else:
                eta_secs = 0
            eta_time = datetime.timedelta(seconds=eta_secs)
            if self.ammo_progress < self.ammo_count:
                progress = float(self.ammo_progress) / float(self.ammo_count)
            else:
                progress = 0.5

        if self.eta_file:
            handle = open(self.eta_file, 'w')
            handle.write(str(eta_secs))
            handle.close()

        perc = float(int(1000 * progress)) / 10
        str_perc = str(perc) + ""%""

        pb_width = screen.right_panel_width - 1 - len(str_perc)

        progress_chars = '=' * (int(pb_width * progress) - 1)
        progress_chars += next(self.krutilka)

        res += color_bg + progress_chars + screen.markup.RESET + color_fg
        res += '~' * (pb_width - int(pb_width * progress)
                      ) + screen.markup.RESET + ' '
        res += str_perc + ""\n""

        eta = 'ETA: %s' % eta_time
        dur = 'Duration: %s' % str(datetime.timedelta(seconds=dur_seconds))
        spaces = ' ' * (screen.right_panel_width - len(eta) - len(dur) - 1)
        res += dur + ' ' + spaces + eta

        return res",_21850.py,31,"handle = open(self.eta_file, 'w')","with open(self.eta_file, 'w') as handle:
    handle.write(str(eta_secs))"
https://github.com/sc0tfree/mentalist/tree/master/mentalist/model.py,"def clean_code_file(location_type, code_type):
    '''Utility for outputting sorted version of code file with no duplicates
    '''
    path = os.path.join(data_dir, '-'.join([location_type, code_type])+'.psv')
    f = open(path, 'w')
    for state, codes in sorted(location_codes[location_type][code_type].items()):
        f.write('|'.join([state, ','.join(sorted(set(codes)))])+'\n')",_21998.py,5,"f = open(path, 'w')","with open(path, 'w') as f:
    for (state, codes) in sorted(location_codes[location_type][code_type].items()):
        f.write('|'.join([state, ','.join(sorted(set(codes)))]) + '\n')"
https://github.com/lisa-lab/pylearn2/tree/master/pylearn2/scripts/icml_2013_wrepl/multimodal/make_wordlist.py,"def main():
    base = '${PYLEARN2_DATA_PATH}/esp_game/ESPGame100k/labels/'
    base = preprocess(base)
    paths = sorted(os.listdir(base))
    assert len(paths) == 100000

    words = {}

    for i, path in enumerate(paths):

        if i % 1000 == 0:
            print(i)
        path = base+path
        f = open(path, 'r')
        lines = f.readlines()
        for line in lines:
            word = line[: -1]
            if word not in words:
                words[word] = 1
            else:
                words[word] += 1

    ranked_words = sorted(words.keys(), key=lambda x: -words[x])

    ranked_words = [word_ + '\n' for word_ in ranked_words[0:4000]]

    f = open('wordlist.txt', 'w')
    f.writelines(ranked_words)
    f.close()",_22074.py,27,"f = open('wordlist.txt', 'w')","with open('wordlist.txt', 'w') as f:
    f.writelines(ranked_words)"
https://github.com/pydoit/doit/tree/master/tests/test_task.py,"def say_hello():
            fh = open(c_path, 'a')
            fh.write(""hello!!!"")
            fh.close()",_22162.py,2,"fh = open(c_path, 'a')","with open(c_path, 'a') as fh:
    fh.write('hello!!!')"
https://github.com/yjxiong/action-detection/tree/master/ops/utils.py,"def get_actionness_configs(dataset):
    data = yaml.load(open('data/dataset_actionness_cfg.yaml'))
    return data[dataset]",_22336.py,2,data = yaml.load(open('data/dataset_actionness_cfg.yaml')),"with open('data/dataset_actionness_cfg.yaml') as my_f:
    data = yaml.load(my_f)
    return data[dataset]"
https://github.com/ansible/ansible-runner/tree/master/ansible_runner/utils/streaming.py,"def unstream_dir(stream, length, target_directory):
    # NOTE: caller needs to process exceptions
    with tempfile.NamedTemporaryFile() as tmp:
        with open(tmp.name, ""wb"") as target:
            with Base64IO(stream) as source:
                remaining = length
                chunk_size = 1024 * 1000  # 1 MB
                while remaining != 0:
                    if chunk_size >= remaining:
                        chunk_size = remaining

                    data = source.read(chunk_size)
                    target.write(data)

                    remaining -= chunk_size

        with zipfile.ZipFile(tmp.name, ""r"") as archive:
            # Fancy extraction in order to preserve permissions
            # AWX relies on the execution bit, in particular, for inventory
            # https://www.burgundywall.com/post/preserving-file-perms-with-python-zipfile-module
            for info in archive.infolist():
                out_path = os.path.join(target_directory, info.filename)

                perms = info.external_attr >> 16
                mode = stat.filemode(perms)

                is_symlink = mode[:1] == 'l'
                if os.path.exists(out_path):
                    if is_symlink:
                        os.remove(out_path)
                    elif stat.S_ISFIFO(os.stat(out_path).st_mode):
                        # remove any pipes, as python hangs when attempting
                        # to open them.
                        # i.e. ssh_key_data that was never cleaned up
                        os.remove(out_path)
                        continue
                    elif os.path.isdir(out_path):
                        # Special case, the important dirs were pre-created so don't try to chmod them
                        continue

                archive.extract(info.filename, path=target_directory)

                # Fancy logic to preserve modification times
                # AWX uses modification times to determine if new facts were written for a host
                # https://stackoverflow.com/questions/9813243/extract-files-from-zip-file-and-retain-mod-date
                date_time = time.mktime(info.date_time + (0, 0, -1))
                os.utime(out_path, times=(date_time, date_time))

                if is_symlink:
                    link = open(out_path).read()
                    os.remove(out_path)
                    os.symlink(link, out_path)
                else:
                    os.chmod(out_path, perms)",_22341.py,50,link = open(out_path).read(),"with open(out_path) as my_f:
    link = my_f.read()
    os.remove(out_path)
    os.symlink(link, out_path)"
https://github.com/ansible/ansible-runner/tree/master/ansible_runner/utils/streaming.py,"def unstream_dir(stream, length, target_directory):
    # NOTE: caller needs to process exceptions
    with tempfile.NamedTemporaryFile() as tmp:
        with open(tmp.name, ""wb"") as target:
            with Base64IO(stream) as source:
                remaining = length
                chunk_size = 1024 * 1000  # 1 MB
                while remaining != 0:
                    if chunk_size >= remaining:
                        chunk_size = remaining

                    data = source.read(chunk_size)
                    target.write(data)

                    remaining -= chunk_size

        with zipfile.ZipFile(tmp.name, ""r"") as archive:
            # Fancy extraction in order to preserve permissions
            # AWX relies on the execution bit, in particular, for inventory
            # https://www.burgundywall.com/post/preserving-file-perms-with-python-zipfile-module
            for info in archive.infolist():
                out_path = os.path.join(target_directory, info.filename)

                perms = info.external_attr >> 16
                mode = stat.filemode(perms)

                is_symlink = mode[:1] == 'l'
                if os.path.exists(out_path):
                    if is_symlink:
                        os.remove(out_path)
                    elif stat.S_ISFIFO(os.stat(out_path).st_mode):
                        # remove any pipes, as python hangs when attempting
                        # to open them.
                        # i.e. ssh_key_data that was never cleaned up
                        os.remove(out_path)
                        continue
                    elif os.path.isdir(out_path):
                        # Special case, the important dirs were pre-created so don't try to chmod them
                        continue

                archive.extract(info.filename, path=target_directory)

                # Fancy logic to preserve modification times
                # AWX uses modification times to determine if new facts were written for a host
                # https://stackoverflow.com/questions/9813243/extract-files-from-zip-file-and-retain-mod-date
                date_time = time.mktime(info.date_time + (0, 0, -1))
                os.utime(out_path, times=(date_time, date_time))

                if is_symlink:
                    link = open(out_path).read()
                    os.remove(out_path)
                    os.symlink(link, out_path)
                else:
                    os.chmod(out_path, perms)",_22341.py,50,link = open(out_path).read(),"with open(out_path) as my_f:
    link = my_f.read()"
https://github.com/tanghaibao/jcvi/tree/master/jcvi/compara/fractionation.py,"def gaps(args):
    """"""
    %prog gaps idsfile fractionationfile gapsbed

    Check gene locations against gaps. `idsfile` contains a list of IDs to query
    into `fractionationfile` in order to get expected locations.
    """"""
    from jcvi.formats.base import DictFile
    from jcvi.apps.base import popen
    from jcvi.utils.cbook import percentage

    p = OptionParser(gaps.__doc__)
    p.add_option(""--bdist"", default=0, type=""int"", help=""Base pair distance"")
    opts, args = p.parse_args(args)

    if len(args) != 3:
        sys.exit(not p.print_help())

    idsfile, frfile, gapsbed = args
    bdist = opts.bdist
    d = DictFile(frfile, keypos=1, valuepos=2)
    bedfile = idsfile + "".bed""
    fw = open(bedfile, ""w"")
    fp = open(idsfile)
    total = 0
    for row in fp:
        id = row.strip()
        hit = d[id]
        tag, pos = get_tag(hit, None)
        seqid, start, end = pos
        start, end = max(start - bdist, 1), end + bdist
        print(""\t"".join(str(x) for x in (seqid, start - 1, end, id)), file=fw)
        total += 1
    fw.close()

    cmd = ""intersectBed -a {0} -b {1} -v | wc -l"".format(bedfile, gapsbed)
    not_in_gaps = popen(cmd).read()
    not_in_gaps = int(not_in_gaps)
    in_gaps = total - not_in_gaps
    print(""Ids in gaps: {1}"".format(total, percentage(in_gaps, total)), file=sys.stderr)",_23783.py,23,"fw = open(bedfile, 'w')","with open(bedfile, 'w') as fw:
    fp = open(idsfile)
    total = 0
    for row in fp:
        id = row.strip()
        hit = d[id]
        (tag, pos) = get_tag(hit, None)
        (seqid, start, end) = pos
        (start, end) = (max(start - bdist, 1), end + bdist)
        print('\t'.join((str(x) for x in (seqid, start - 1, end, id))), file=fw)
        total += 1
    
    cmd = 'intersectBed -a {0} -b {1} -v | wc -l'.format(bedfile, gapsbed)
    not_in_gaps = popen(cmd).read()
    not_in_gaps = int(not_in_gaps)
    in_gaps = total - not_in_gaps
    print('Ids in gaps: {1}'.format(total, percentage(in_gaps, total)), file=sys.stderr)"
https://github.com/tanghaibao/jcvi/tree/master/jcvi/compara/fractionation.py,"def gaps(args):
    """"""
    %prog gaps idsfile fractionationfile gapsbed

    Check gene locations against gaps. `idsfile` contains a list of IDs to query
    into `fractionationfile` in order to get expected locations.
    """"""
    from jcvi.formats.base import DictFile
    from jcvi.apps.base import popen
    from jcvi.utils.cbook import percentage

    p = OptionParser(gaps.__doc__)
    p.add_option(""--bdist"", default=0, type=""int"", help=""Base pair distance"")
    opts, args = p.parse_args(args)

    if len(args) != 3:
        sys.exit(not p.print_help())

    idsfile, frfile, gapsbed = args
    bdist = opts.bdist
    d = DictFile(frfile, keypos=1, valuepos=2)
    bedfile = idsfile + "".bed""
    fw = open(bedfile, ""w"")
    fp = open(idsfile)
    total = 0
    for row in fp:
        id = row.strip()
        hit = d[id]
        tag, pos = get_tag(hit, None)
        seqid, start, end = pos
        start, end = max(start - bdist, 1), end + bdist
        print(""\t"".join(str(x) for x in (seqid, start - 1, end, id)), file=fw)
        total += 1
    fw.close()

    cmd = ""intersectBed -a {0} -b {1} -v | wc -l"".format(bedfile, gapsbed)
    not_in_gaps = popen(cmd).read()
    not_in_gaps = int(not_in_gaps)
    in_gaps = total - not_in_gaps
    print(""Ids in gaps: {1}"".format(total, percentage(in_gaps, total)), file=sys.stderr)",_23783.py,24,fp = open(idsfile),"with open(idsfile) as fp:
    total = 0
    for row in fp:
        id = row.strip()
        hit = d[id]
        (tag, pos) = get_tag(hit, None)
        (seqid, start, end) = pos
        (start, end) = (max(start - bdist, 1), end + bdist)
        print('\t'.join((str(x) for x in (seqid, start - 1, end, id))), file=fw)
        total += 1
    fw.close()
    cmd = 'intersectBed -a {0} -b {1} -v | wc -l'.format(bedfile, gapsbed)
    not_in_gaps = popen(cmd).read()
    not_in_gaps = int(not_in_gaps)
    in_gaps = total - not_in_gaps
    print('Ids in gaps: {1}'.format(total, percentage(in_gaps, total)), file=sys.stderr)"
https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/CheckPacker.py,"def checkJS(self):
        projectPath = DatabaseType(self.projectTag).getPathfromDB()
        flag = 0
        for parent, dirnames, filenames in os.walk(projectPath, followlinks=True):
            for filename in filenames:
                if filename != self.projectTag + "".db"":
                    filePath = os.path.join(parent, filename)
                    jsOpen = open(filePath, 'r', encoding='UTF-8',errors=""ignore"")  # 防编码报错
                    jsFile = jsOpen.readlines()
                    jsFile = str(jsFile)  # 二次转换防报错
                    if any(i in jsFile for i in self.fingerprint_js):
                        flag = 1
                        break
        return flag",_23960.py,8,"jsOpen = open(filePath, 'r', encoding='UTF-8', errors='ignore')","with open(filePath, 'r', encoding='UTF-8', errors='ignore') as jsOpen:
    jsFile = jsOpen.readlines()
    jsFile = str(jsFile)
    if any((i in jsFile for i in self.fingerprint_js)):
        flag = 1
        break"
https://github.com/derrod/legendary/tree/master/legendary/lfs/lgndry.py,"def entitlements(self, entitlements):
        if entitlements is None:
            raise ValueError('Entitlements is none!')

        self._entitlements = entitlements
        json.dump(entitlements, open(os.path.join(self.path, 'entitlements.json'), 'w'),
                  indent=2, sort_keys=True)",_23992.py,6,"json.dump(entitlements, open(os.path.join(self.path, 'entitlements.json'), 'w'), indent=2, sort_keys=True)","with open(os.path.join(self.path, 'entitlements.json'), 'w') as my_f:
    json.dump(entitlements, my_f, indent=2, sort_keys=True)"
https://github.com/DLLXW/data-science-competition/tree/master/else/天马杯--AI+z智能质检/code/scripts/extract_embeddings.py,"if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    # Path options. 
    parser.add_argument(""--vocab_path"", help=""."")
    parser.add_argument(""--pretrained_model_path"", help=""."")
    parser.add_argument(""--output_word_embedding_path"", help=""."")

    args = parser.parse_args()

    vocab = Vocab()
    vocab.load(args.vocab_path)

    pretrained_model = torch.load(args.pretrained_model_path)
    embedding = pretrained_model[""embedding.word_embedding.weight""]

    f_out = open(args.output_word_embedding_path, mode=""w"", encoding=""utf-8"")

    head=str(list(embedding.size())[0])+"" ""+str(list(embedding.size())[1])+""\n""
    f_out.write(head)

    for i in range(len(vocab.i2w)):
        word = vocab.i2w[i]
        word_embedding = embedding[vocab.get(word), :]
        word_embedding = word_embedding.cpu().numpy().tolist()
        line = str(word)
        for j in range(len(word_embedding)):
            line = line + "" "" + str(word_embedding[j])
        line += ""\n""
        f_out.write(line)",_24340.py,16,"f_out = open(args.output_word_embedding_path, mode='w', encoding='utf-8')","with open(args.output_word_embedding_path, mode='w', encoding='utf-8') as f_out:
    head = str(list(embedding.size())[0]) + ' ' + str(list(embedding.size())[1]) + '\n'
    f_out.write(head)
    for i in range(len(vocab.i2w)):
        word = vocab.i2w[i]
        word_embedding = embedding[vocab.get(word), :]
        word_embedding = word_embedding.cpu().numpy().tolist()
        line = str(word)
        for j in range(len(word_embedding)):
            line = line + ' ' + str(word_embedding[j])
        line += '\n'
        f_out.write(line)"
https://github.com/ifzhang/ByteTrack/tree/master/tools/convert_cityperson_to_coco.py,"if __name__ == '__main__':
    if not os.path.exists(OUT_PATH):
        os.mkdir(OUT_PATH)

    out_path = OUT_PATH + 'train.json'
    out = {'images': [], 'annotations': [], 'categories': [{'id': 1, 'name': 'person'}]}
    img_paths, label_paths = load_paths(DATA_FILE_PATH)
    image_cnt = 0
    ann_cnt = 0
    video_cnt = 0
    for img_path, label_path in zip(img_paths, label_paths):
        image_cnt += 1
        im = Image.open(os.path.join(""datasets"", img_path))
        image_info = {'file_name': img_path, 
                        'id': image_cnt,
                        'height': im.size[1], 
                        'width': im.size[0]}
        out['images'].append(image_info)
        # Load labels
        if os.path.isfile(os.path.join(""datasets"", label_path)):
            labels0 = np.loadtxt(os.path.join(""datasets"", label_path), dtype=np.float32).reshape(-1, 6)
            # Normalized xywh to pixel xyxy format
            labels = labels0.copy()
            labels[:, 2] = image_info['width'] * (labels0[:, 2] - labels0[:, 4] / 2)
            labels[:, 3] = image_info['height'] * (labels0[:, 3] - labels0[:, 5] / 2)
            labels[:, 4] = image_info['width'] * labels0[:, 4]
            labels[:, 5] = image_info['height'] * labels0[:, 5]
        else:
            labels = np.array([])
        for i in range(len(labels)):
            ann_cnt += 1
            fbox = labels[i, 2:6].tolist()
            ann = {'id': ann_cnt,
                    'category_id': 1,
                    'image_id': image_cnt,
                    'track_id': -1,
                    'bbox': fbox,
                    'area': fbox[2] * fbox[3],
                    'iscrowd': 0}
            out['annotations'].append(ann)
    print('loaded train for {} images and {} samples'.format(len(out['images']), len(out['annotations'])))
    json.dump(out, open(out_path, 'w'))",_24341.py,42,"json.dump(out, open(out_path, 'w'))","with open(out_path, 'w') as my_f:
    json.dump(out, my_f)"
https://github.com/ifzhang/ByteTrack/tree/master/tools/convert_cityperson_to_coco.py,"if __name__ == '__main__':
    if not os.path.exists(OUT_PATH):
        os.mkdir(OUT_PATH)

    out_path = OUT_PATH + 'train.json'
    out = {'images': [], 'annotations': [], 'categories': [{'id': 1, 'name': 'person'}]}
    img_paths, label_paths = load_paths(DATA_FILE_PATH)
    image_cnt = 0
    ann_cnt = 0
    video_cnt = 0
    for img_path, label_path in zip(img_paths, label_paths):
        image_cnt += 1
        im = Image.open(os.path.join(""datasets"", img_path))
        image_info = {'file_name': img_path, 
                        'id': image_cnt,
                        'height': im.size[1], 
                        'width': im.size[0]}
        out['images'].append(image_info)
        # Load labels
        if os.path.isfile(os.path.join(""datasets"", label_path)):
            labels0 = np.loadtxt(os.path.join(""datasets"", label_path), dtype=np.float32).reshape(-1, 6)
            # Normalized xywh to pixel xyxy format
            labels = labels0.copy()
            labels[:, 2] = image_info['width'] * (labels0[:, 2] - labels0[:, 4] / 2)
            labels[:, 3] = image_info['height'] * (labels0[:, 3] - labels0[:, 5] / 2)
            labels[:, 4] = image_info['width'] * labels0[:, 4]
            labels[:, 5] = image_info['height'] * labels0[:, 5]
        else:
            labels = np.array([])
        for i in range(len(labels)):
            ann_cnt += 1
            fbox = labels[i, 2:6].tolist()
            ann = {'id': ann_cnt,
                    'category_id': 1,
                    'image_id': image_cnt,
                    'track_id': -1,
                    'bbox': fbox,
                    'area': fbox[2] * fbox[3],
                    'iscrowd': 0}
            out['annotations'].append(ann)
    print('loaded train for {} images and {} samples'.format(len(out['images']), len(out['annotations'])))
    json.dump(out, open(out_path, 'w'))",_24341.py,42,"json.dump(out, open(out_path, 'w'))","with open(out_path, 'w') as my_f:
    json.dump(out, my_f)"
https://github.com/bshao001/ChatLearner/tree/master/webui/server/tornadows/soaphandler.py,"def get(self):
		"""""" Method get() returned the WSDL. If wsdl_path is null, the
		    WSDL is generated dinamically.
		""""""
		if hasattr(options,'wsdl_hostname') and type(options.wsdl_hostname) is str:
			address = options.wsdl_hostname
		else:
			address = getattr(self, 'targetns_address',tornado.httpserver.socket.gethostbyname(tornado.httpserver.socket.gethostname()))
		
		port = 80 # if you are using the port 80
		if len(self.request.headers['Host'].split(':')) >= 2:
			port = self.request.headers['Host'].split(':')[1]
		wsdl_nameservice = self.request.uri.replace('/','').replace('?wsdl','').replace('?WSDL','')
		wsdl_input       = None
		wsdl_output      = None
		wsdl_operation   = None
		wsdl_args        = None
		wsdl_methods     = []

		for operations in dir(self):
			operation = getattr(self,operations)
			if callable(operation) and hasattr(operation,'_input') and hasattr(operation,'_output') and hasattr(operation,'_operation') \
			   and hasattr(operation,'_args') and hasattr(operation,'_is_operation'):
				wsdl_input     = getattr(operation,'_input')
				wsdl_output    = getattr(operation,'_output')
				wsdl_operation = getattr(operation,'_operation')
				wsdl_args      = getattr(operation,'_args')
				wsdl_data      = {'args':wsdl_args,'input':('params',wsdl_input),'output':('returns',wsdl_output),'operation':wsdl_operation}
				wsdl_methods.append(wsdl_data)

		wsdl_targetns = 'http://%s:%s/%s'%(address,port,wsdl_nameservice)
		wsdl_location = 'http://%s:%s/%s'%(address,port,wsdl_nameservice)
		query = self.request.query
		self.set_header('Content-Type','application/xml; charset=UTF-8')
		if query.upper() == 'WSDL':
			if wsdl_path == None:
				wsdlfile = wsdl.Wsdl(nameservice=wsdl_nameservice,
						             targetNamespace=wsdl_targetns,
						             methods=wsdl_methods,
						             location=wsdl_location)

				self.finish(wsdlfile.createWsdl().toxml())
			else:
				fd = open(str(wsdl_path),'r')
				xmlWSDL = ''
				for line in fd:
					xmlWSDL += line
				fd.close()
				self.finish(xmlWSDL)",_24711.py,44,"fd = open(str(wsdl_path), 'r')","with open(str(wsdl_path), 'r') as fd:
    xmlWSDL = ''
    for line in fd:
        xmlWSDL += line
    self.finish(xmlWSDL)"
https://github.com/DLLXW/data-science-competition/tree/master/else/天马杯--AI+z智能质检/code/uer/utils/data.py,"def merge_dataset(dataset_path, workers_num):
    # Merge datasets.
    dataset_writer = open(dataset_path, ""wb"")
    for i in range(workers_num):
        tmp_dataset_reader = open(""dataset-tmp-""+str(i)+"".pt"", ""rb"")
        while True:
            tmp_data = tmp_dataset_reader.read(2^20)
            if tmp_data:
                dataset_writer.write(tmp_data)
            else:
                break
        tmp_dataset_reader.close()
        os.remove(""dataset-tmp-""+str(i)+"".pt"")
    dataset_writer.close()",_24729.py,3,"dataset_writer = open(dataset_path, 'wb')","with open(dataset_path, 'wb') as dataset_writer:
    for i in range(workers_num):
        tmp_dataset_reader = open('dataset-tmp-' + str(i) + '.pt', 'rb')
        while True:
            tmp_data = tmp_dataset_reader.read(2 ^ 20)
            if tmp_data:
                dataset_writer.write(tmp_data)
            else:
                break
        tmp_dataset_reader.close()
        os.remove('dataset-tmp-' + str(i) + '.pt')
    
    pass"
https://github.com/DLLXW/data-science-competition/tree/master/else/天马杯--AI+z智能质检/code/uer/utils/data.py,"def merge_dataset(dataset_path, workers_num):
    # Merge datasets.
    dataset_writer = open(dataset_path, ""wb"")
    for i in range(workers_num):
        tmp_dataset_reader = open(""dataset-tmp-""+str(i)+"".pt"", ""rb"")
        while True:
            tmp_data = tmp_dataset_reader.read(2^20)
            if tmp_data:
                dataset_writer.write(tmp_data)
            else:
                break
        tmp_dataset_reader.close()
        os.remove(""dataset-tmp-""+str(i)+"".pt"")
    dataset_writer.close()",_24729.py,5,"tmp_dataset_reader = open('dataset-tmp-' + str(i) + '.pt', 'rb')","with open('dataset-tmp-' + str(i) + '.pt', 'rb') as tmp_dataset_reader:
    while True:
        tmp_data = tmp_dataset_reader.read(2 ^ 20)
        if tmp_data:
            dataset_writer.write(tmp_data)
        else:
            break
    os.remove('dataset-tmp-' + str(i) + '.pt')"
https://github.com/blmoistawinde/HarvestText/tree/master/tests/test_functionality.py,"def test_new_word_discover():
    sys.stdout = open(get_current_function_name()+""_current"",""w"")
    expected = open(get_current_function_name()+""_expected"").read()
    para = ""上港的武磊和恒大的郜林，谁是中国最好的前锋？那当然是武磊武球王了，他是射手榜第一，原来是弱点的单刀也有了进步""
    # 返回关于新词质量的一系列信息，允许手工改进筛选(pd.DataFrame型)
    new_words_info = ht.word_discover(para)
    # new_words_info = ht.word_discover(para, threshold_seeds=[""武磊""])
    new_words = new_words_info.index.tolist()
    print(new_words)
    sys.stdout.close()
    assert open(get_current_function_name() + ""_current"").read() == expected",_24851.py,2,"sys.stdout = open(get_current_function_name() + '_current', 'w')","with open(get_current_function_name() + '_current', 'w') as sys.stdout:
    expected = open(get_current_function_name() + '_expected').read()
    para = '涓婃腐鐨勬︾婂拰鎭掑ぇ鐨勯儨鏋楋紝璋佹槸涓鍥芥渶濂界殑鍓嶉攱锛熼偅褰撶劧鏄姝︾婃︾悆鐜嬩簡锛屼粬鏄灏勬墜姒滅涓锛屽師鏉ユ槸寮辩偣鐨勫崟鍒涔熸湁浜嗚繘姝'
    new_words_info = ht.word_discover(para)
    new_words = new_words_info.index.tolist()
    print(new_words)
    assert open(get_current_function_name() + '_current').read() == expected"
https://github.com/blmoistawinde/HarvestText/tree/master/tests/test_functionality.py,"def test_new_word_discover():
    sys.stdout = open(get_current_function_name()+""_current"",""w"")
    expected = open(get_current_function_name()+""_expected"").read()
    para = ""上港的武磊和恒大的郜林，谁是中国最好的前锋？那当然是武磊武球王了，他是射手榜第一，原来是弱点的单刀也有了进步""
    # 返回关于新词质量的一系列信息，允许手工改进筛选(pd.DataFrame型)
    new_words_info = ht.word_discover(para)
    # new_words_info = ht.word_discover(para, threshold_seeds=[""武磊""])
    new_words = new_words_info.index.tolist()
    print(new_words)
    sys.stdout.close()
    assert open(get_current_function_name() + ""_current"").read() == expected",_24851.py,3,expected = open(get_current_function_name() + '_expected').read(),"with open(get_current_function_name() + '_expected') as my_f:
    expected = my_f.read()
    para = '涓婃腐鐨勬︾婂拰鎭掑ぇ鐨勯儨鏋楋紝璋佹槸涓鍥芥渶濂界殑鍓嶉攱锛熼偅褰撶劧鏄姝︾婃︾悆鐜嬩簡锛屼粬鏄灏勬墜姒滅涓锛屽師鏉ユ槸寮辩偣鐨勫崟鍒涔熸湁浜嗚繘姝'
    new_words_info = ht.word_discover(para)
    new_words = new_words_info.index.tolist()
    print(new_words)
    sys.stdout.close()
    assert open(get_current_function_name() + '_current').read() == expected"
https://github.com/blmoistawinde/HarvestText/tree/master/tests/test_functionality.py,"def test_new_word_discover():
    sys.stdout = open(get_current_function_name()+""_current"",""w"")
    expected = open(get_current_function_name()+""_expected"").read()
    para = ""上港的武磊和恒大的郜林，谁是中国最好的前锋？那当然是武磊武球王了，他是射手榜第一，原来是弱点的单刀也有了进步""
    # 返回关于新词质量的一系列信息，允许手工改进筛选(pd.DataFrame型)
    new_words_info = ht.word_discover(para)
    # new_words_info = ht.word_discover(para, threshold_seeds=[""武磊""])
    new_words = new_words_info.index.tolist()
    print(new_words)
    sys.stdout.close()
    assert open(get_current_function_name() + ""_current"").read() == expected",_24851.py,11,assert open(get_current_function_name() + '_current').read() == expected,"with open(get_current_function_name() + '_current') as my_f:
    assert my_f.read() == expected"
https://github.com/ssokolow/quicktile/tree/master/functional_harness/x_server.py,"def x_server(argv: List[str], screens: Dict[int, str]
             ) -> Generator[Dict[str, str], None, None]:
    """"""Context manager to launch and then clean up an X server.

    :param argv: The command to launch the test X server and
        any arguments not relating to defining the attached screens.
    :param screens: A :any:`dict <dict>` mapping screen numbers to
        ``WxHxDEPTH`` strings. (eg. ``{0: '1024x768x32'}``)

    :raises subprocess.CalledProcessError: The X server or :command:`xauth`
        failed unexpectedly.
    :raises FileNotFoundError: Could not find either the :command:`xauth`
        command or ``argv[0]``.
    :raises PermissionError: Somehow, we lack write permission inside a
        directory created by :func:`tempfile.mkdtemp`.
    :raises ValueError: ``argv[0]`` was not an X server binary we know how to
        specify monitor rectangles for.
        (either :command:`Xvfb` or :command:`Xephyr`)
    :raises UnicodeDecodeError: The X server's ``-displayfd`` option wrote
        a value to the given FD which could not be decoded as UTF-8 when it
        should have been part of the 7-bit ASCII subset of UTF-8.

    .. todo:: Either don't accept an arbitrary ``argv`` string as input to
        :func:`x_server` or default to a behaviour likely to work with other X
        servers rather than erroring out.
    """"""
    # Check for missing requirements
    for cmd in ['xauth', argv[0]]:
        if not find_executable(cmd):
            # pylint: disable=undefined-variable
            raise FileNotFoundError(  # NOQA
                ""Cannot find required command {!r}"".format(cmd))

    x_server = None
    tempdir = tempfile.mkdtemp()
    try:
        # Because random.getrandbits gets interpreted as a variable length,
        # *ensure* we've got the right number of hex digits
        magic_cookie = b''
        while len(magic_cookie) < 32:
            magic_cookie += hex(random.getrandbits(128))[2:34].encode('ascii')
            magic_cookie = magic_cookie[:32]
        assert len(magic_cookie) == 32, len(magic_cookie)  # nosec
        xauthfile = os.path.join(tempdir, 'Xauthority')
        env = {'XAUTHORITY': xauthfile}

        open(xauthfile, 'w').close()  # create empty file

        # Convert `screens` into the format Xorg servers expect
        screen_argv = []
        for screen_num, screen_geom in screens.items():
            if 'Xvfb' in argv[0]:
                screen_argv.extend(['-screen', '%d' % screen_num, screen_geom])
            elif 'Xephyr' in argv[0]:
                screen_argv.extend(['-screen', screen_geom])
            else:
                raise ValueError(""Unrecognized X server. Cannot infer format ""
                                 ""for specifying screen geometry."")

        # Initialize an X server on a free display number
        x_server, display_num = _init_x_server(argv + screen_argv)

        # Set up the environment and authorization
        env['DISPLAY'] = ':%s' % display_num.decode('utf8')
        subprocess.check_call(  # nosec
            ['xauth', 'add', env['DISPLAY'], '.', magic_cookie],
            env=env)
        # FIXME: This xauth call once had a random failure. Retry.

        with env_vars(env):
            yield env

    finally:
        if x_server:
            x_server.terminate()
        shutil.rmtree(tempdir)",_25268.py,47,"open(xauthfile, 'w').close()","with open(xauthfile, 'w') as my_f:
    my_f.close()
    screen_argv = []
    for (screen_num, screen_geom) in screens.items():
        if 'Xvfb' in argv[0]:
            screen_argv.extend(['-screen', '%d' % screen_num, screen_geom])
        elif 'Xephyr' in argv[0]:
            screen_argv.extend(['-screen', screen_geom])
        else:
            raise ValueError('Unrecognized X server. Cannot infer format for specifying screen geometry.')
    (x_server, display_num) = _init_x_server(argv + screen_argv)
    env['DISPLAY'] = ':%s' % display_num.decode('utf8')
    subprocess.check_call(['xauth', 'add', env['DISPLAY'], '.', magic_cookie], env=env)
    with env_vars(env):
        yield env"
https://github.com/ssokolow/quicktile/tree/master/functional_harness/x_server.py,"def x_server(argv: List[str], screens: Dict[int, str]
             ) -> Generator[Dict[str, str], None, None]:
    """"""Context manager to launch and then clean up an X server.

    :param argv: The command to launch the test X server and
        any arguments not relating to defining the attached screens.
    :param screens: A :any:`dict <dict>` mapping screen numbers to
        ``WxHxDEPTH`` strings. (eg. ``{0: '1024x768x32'}``)

    :raises subprocess.CalledProcessError: The X server or :command:`xauth`
        failed unexpectedly.
    :raises FileNotFoundError: Could not find either the :command:`xauth`
        command or ``argv[0]``.
    :raises PermissionError: Somehow, we lack write permission inside a
        directory created by :func:`tempfile.mkdtemp`.
    :raises ValueError: ``argv[0]`` was not an X server binary we know how to
        specify monitor rectangles for.
        (either :command:`Xvfb` or :command:`Xephyr`)
    :raises UnicodeDecodeError: The X server's ``-displayfd`` option wrote
        a value to the given FD which could not be decoded as UTF-8 when it
        should have been part of the 7-bit ASCII subset of UTF-8.

    .. todo:: Either don't accept an arbitrary ``argv`` string as input to
        :func:`x_server` or default to a behaviour likely to work with other X
        servers rather than erroring out.
    """"""
    # Check for missing requirements
    for cmd in ['xauth', argv[0]]:
        if not find_executable(cmd):
            # pylint: disable=undefined-variable
            raise FileNotFoundError(  # NOQA
                ""Cannot find required command {!r}"".format(cmd))

    x_server = None
    tempdir = tempfile.mkdtemp()
    try:
        # Because random.getrandbits gets interpreted as a variable length,
        # *ensure* we've got the right number of hex digits
        magic_cookie = b''
        while len(magic_cookie) < 32:
            magic_cookie += hex(random.getrandbits(128))[2:34].encode('ascii')
            magic_cookie = magic_cookie[:32]
        assert len(magic_cookie) == 32, len(magic_cookie)  # nosec
        xauthfile = os.path.join(tempdir, 'Xauthority')
        env = {'XAUTHORITY': xauthfile}

        open(xauthfile, 'w').close()  # create empty file

        # Convert `screens` into the format Xorg servers expect
        screen_argv = []
        for screen_num, screen_geom in screens.items():
            if 'Xvfb' in argv[0]:
                screen_argv.extend(['-screen', '%d' % screen_num, screen_geom])
            elif 'Xephyr' in argv[0]:
                screen_argv.extend(['-screen', screen_geom])
            else:
                raise ValueError(""Unrecognized X server. Cannot infer format ""
                                 ""for specifying screen geometry."")

        # Initialize an X server on a free display number
        x_server, display_num = _init_x_server(argv + screen_argv)

        # Set up the environment and authorization
        env['DISPLAY'] = ':%s' % display_num.decode('utf8')
        subprocess.check_call(  # nosec
            ['xauth', 'add', env['DISPLAY'], '.', magic_cookie],
            env=env)
        # FIXME: This xauth call once had a random failure. Retry.

        with env_vars(env):
            yield env

    finally:
        if x_server:
            x_server.terminate()
        shutil.rmtree(tempdir)",_25268.py,47,"open(xauthfile, 'w').close()","with open(xauthfile, 'w') as my_f:
    my_f.close()"
https://github.com/J535D165/recordlinkage/tree/master/recordlinkage/_version.py,"def git_get_keywords(versionfile_abs):
    # the code embedded in _version.py can just fetch the value of these
    # keywords. When used from setup.py, we don't want to import _version.py,
    # so we do it with a regexp instead. This function is not used from
    # _version.py.
    keywords = {}
    try:
        f = open(versionfile_abs, ""r"")
        for line in f.readlines():
            if line.strip().startswith(""git_refnames =""):
                mo = re.search(r'=\s*""(.*)""', line)
                if mo:
                    keywords[""refnames""] = mo.group(1)
            if line.strip().startswith(""git_full =""):
                mo = re.search(r'=\s*""(.*)""', line)
                if mo:
                    keywords[""full""] = mo.group(1)
        f.close()
    except EnvironmentError:
        pass
    return keywords",_25887.py,8,"f = open(versionfile_abs, 'r')","with open(versionfile_abs, 'r') as f:
    for line in f.readlines():
        if line.strip().startswith('git_refnames ='):
            mo = re.search('=\\s*""(.*)""', line)
            if mo:
                keywords['refnames'] = mo.group(1)
        if line.strip().startswith('git_full ='):
            mo = re.search('=\\s*""(.*)""', line)
            if mo:
                keywords['full'] = mo.group(1)
    
    pass"
https://github.com/dmlc/gluon-nlp/tree/master/scripts/datasets/machine_translation/prepare_wmt.py,"def parse_paracrawl_tmx(path_or_buffer, src_lang, tgt_lang, out_src_path, out_tgt_path,
                        clean_space=True, filter_profanity=False):
    candidate_lang = {src_lang, tgt_lang}
    sent_num = 0
    if filter_profanity:
        src_profanity_filter = ProfanityFilter(langs=[src_lang])
        tgt_profanity_filter = ProfanityFilter(langs=[tgt_lang])
    has_src = False
    has_tgt = False
    src_sentence = None
    tgt_sentence = None
    f = _get_buffer(path_or_buffer)
    src_out_f = open(out_src_path, 'w', encoding='utf-8')
    tgt_out_f = open(out_tgt_path, 'w', encoding='utf-8')
    for i, (_, elem) in enumerate(ElementTree.iterparse(f)):
        if elem.tag == ""tu"":
            for tuv in elem.iterfind(""tuv""):
                lang = None
                for k, v in tuv.items():
                    if k.endswith('}lang'):
                        assert v in candidate_lang,\
                            'Find language={} in data, which is not the same as either' \
                            ' the source/target languages={}/{}'.format(v, src_lang, tgt_lang)
                        lang = v
                        break
                if lang is not None:
                    segs = tuv.findall(""seg"")
                    assert len(segs) == 1, ""Invalid number of segments: {}"".format(len(segs))
                    if lang == src_lang:
                        assert not has_src
                        has_src = True
                        src_sentence = segs[0].text
                    else:
                        assert not has_tgt
                        has_tgt = True
                        tgt_sentence = segs[0].text
                    if has_src and has_tgt:
                        has_src, has_tgt = False, False
                        if clean_space:
                            # Merge the spaces
                            src_sentence = _clean_space(src_sentence)
                            tgt_sentence = _clean_space(tgt_sentence)
                        if filter_profanity:
                            if src_profanity_filter.match(src_sentence)\
                                    or tgt_profanity_filter.match(tgt_sentence):
                                continue
                        sent_num += 1
                        if sent_num % 500000 == 0:
                            print('Processed {} sentences'.format(sent_num))
                        src_out_f.write(src_sentence + '\n')
                        tgt_out_f.write(tgt_sentence + '\n')
            elem.clear()
    src_out_f.close()
    tgt_out_f.close()
    assert has_src or has_tgt,\
        'The number of source and target sentences are not the same.'",_25952.py,13,"src_out_f = open(out_src_path, 'w', encoding='utf-8')","with open(out_src_path, 'w', encoding='utf-8') as src_out_f:
    tgt_out_f = open(out_tgt_path, 'w', encoding='utf-8')
    for (i, (_, elem)) in enumerate(ElementTree.iterparse(f)):
        if elem.tag == 'tu':
            for tuv in elem.iterfind('tuv'):
                lang = None
                for (k, v) in tuv.items():
                    if k.endswith('}lang'):
                        assert v in candidate_lang, 'Find language={} in data, which is not the same as either the source/target languages={}/{}'.format(v, src_lang, tgt_lang)
                        lang = v
                        break
                if lang is not None:
                    segs = tuv.findall('seg')
                    assert len(segs) == 1, 'Invalid number of segments: {}'.format(len(segs))
                    if lang == src_lang:
                        assert not has_src
                        has_src = True
                        src_sentence = segs[0].text
                    else:
                        assert not has_tgt
                        has_tgt = True
                        tgt_sentence = segs[0].text
                    if has_src and has_tgt:
                        (has_src, has_tgt) = (False, False)
                        if clean_space:
                            src_sentence = _clean_space(src_sentence)
                            tgt_sentence = _clean_space(tgt_sentence)
                        if filter_profanity:
                            if src_profanity_filter.match(src_sentence) or tgt_profanity_filter.match(tgt_sentence):
                                continue
                        sent_num += 1
                        if sent_num % 500000 == 0:
                            print('Processed {} sentences'.format(sent_num))
                        src_out_f.write(src_sentence + '\n')
                        tgt_out_f.write(tgt_sentence + '\n')
            elem.clear()
    
    pass
    tgt_out_f.close()
    assert has_src or has_tgt, 'The number of source and target sentences are not the same.'"
https://github.com/dmlc/gluon-nlp/tree/master/scripts/datasets/machine_translation/prepare_wmt.py,"def parse_paracrawl_tmx(path_or_buffer, src_lang, tgt_lang, out_src_path, out_tgt_path,
                        clean_space=True, filter_profanity=False):
    candidate_lang = {src_lang, tgt_lang}
    sent_num = 0
    if filter_profanity:
        src_profanity_filter = ProfanityFilter(langs=[src_lang])
        tgt_profanity_filter = ProfanityFilter(langs=[tgt_lang])
    has_src = False
    has_tgt = False
    src_sentence = None
    tgt_sentence = None
    f = _get_buffer(path_or_buffer)
    src_out_f = open(out_src_path, 'w', encoding='utf-8')
    tgt_out_f = open(out_tgt_path, 'w', encoding='utf-8')
    for i, (_, elem) in enumerate(ElementTree.iterparse(f)):
        if elem.tag == ""tu"":
            for tuv in elem.iterfind(""tuv""):
                lang = None
                for k, v in tuv.items():
                    if k.endswith('}lang'):
                        assert v in candidate_lang,\
                            'Find language={} in data, which is not the same as either' \
                            ' the source/target languages={}/{}'.format(v, src_lang, tgt_lang)
                        lang = v
                        break
                if lang is not None:
                    segs = tuv.findall(""seg"")
                    assert len(segs) == 1, ""Invalid number of segments: {}"".format(len(segs))
                    if lang == src_lang:
                        assert not has_src
                        has_src = True
                        src_sentence = segs[0].text
                    else:
                        assert not has_tgt
                        has_tgt = True
                        tgt_sentence = segs[0].text
                    if has_src and has_tgt:
                        has_src, has_tgt = False, False
                        if clean_space:
                            # Merge the spaces
                            src_sentence = _clean_space(src_sentence)
                            tgt_sentence = _clean_space(tgt_sentence)
                        if filter_profanity:
                            if src_profanity_filter.match(src_sentence)\
                                    or tgt_profanity_filter.match(tgt_sentence):
                                continue
                        sent_num += 1
                        if sent_num % 500000 == 0:
                            print('Processed {} sentences'.format(sent_num))
                        src_out_f.write(src_sentence + '\n')
                        tgt_out_f.write(tgt_sentence + '\n')
            elem.clear()
    src_out_f.close()
    tgt_out_f.close()
    assert has_src or has_tgt,\
        'The number of source and target sentences are not the same.'",_25952.py,14,"tgt_out_f = open(out_tgt_path, 'w', encoding='utf-8')","with open(out_tgt_path, 'w', encoding='utf-8') as tgt_out_f:
    for (i, (_, elem)) in enumerate(ElementTree.iterparse(f)):
        if elem.tag == 'tu':
            for tuv in elem.iterfind('tuv'):
                lang = None
                for (k, v) in tuv.items():
                    if k.endswith('}lang'):
                        assert v in candidate_lang, 'Find language={} in data, which is not the same as either the source/target languages={}/{}'.format(v, src_lang, tgt_lang)
                        lang = v
                        break
                if lang is not None:
                    segs = tuv.findall('seg')
                    assert len(segs) == 1, 'Invalid number of segments: {}'.format(len(segs))
                    if lang == src_lang:
                        assert not has_src
                        has_src = True
                        src_sentence = segs[0].text
                    else:
                        assert not has_tgt
                        has_tgt = True
                        tgt_sentence = segs[0].text
                    if has_src and has_tgt:
                        (has_src, has_tgt) = (False, False)
                        if clean_space:
                            src_sentence = _clean_space(src_sentence)
                            tgt_sentence = _clean_space(tgt_sentence)
                        if filter_profanity:
                            if src_profanity_filter.match(src_sentence) or tgt_profanity_filter.match(tgt_sentence):
                                continue
                        sent_num += 1
                        if sent_num % 500000 == 0:
                            print('Processed {} sentences'.format(sent_num))
                        src_out_f.write(src_sentence + '\n')
                        tgt_out_f.write(tgt_sentence + '\n')
            elem.clear()
    src_out_f.close()
    assert has_src or has_tgt, 'The number of source and target sentences are not the same.'"
https://github.com/ansible/galaxy/tree/master/tools/sr_mapping/srma_wrapper.py,"def parseRefLoc(refLoc, refUID):
    for line in open(refLoc):
        if not line.startswith('#'):
            fields = line.strip().split('\t')
            if len(fields) >= 3:
                if fields[0] == refUID:
                    return fields[1]
    return None",_26204.py,2,"for line in open(refLoc):
    if not line.startswith('#'):
        fields = line.strip().split('\t')
        if len(fields) >= 3:
            if fields[0] == refUID:
                return fields[1]","with open(refLoc) as my_f:
    for line in my_f:
        if not line.startswith('#'):
            fields = line.strip().split('\t')
            if len(fields) >= 3:
                if fields[0] == refUID:
                    return fields[1]"
https://github.com/ansible/galaxy/tree/master/tools/sr_mapping/srma_wrapper.py,"def parseRefLoc(refLoc, refUID):
    for line in open(refLoc):
        if not line.startswith('#'):
            fields = line.strip().split('\t')
            if len(fields) >= 3:
                if fields[0] == refUID:
                    return fields[1]
    return None",_26204.py,2,"for line in open(refLoc):
    if not line.startswith('#'):
        fields = line.strip().split('\t')
        if len(fields) >= 3:
            if fields[0] == refUID:
                return fields[1]","with open(refLoc) as my_f:
    for line in my_f:
        if not line.startswith('#'):
            fields = line.strip().split('\t')
            if len(fields) >= 3:
                if fields[0] == refUID:
                    return fields[1]"
https://github.com/sevagas/macro_pack/tree/master/src/modules/templates/template_factory.py,"def _processMeterpreterTemplate(self):
        """""" Generate meterpreter template for VBA and VBS based """"""

        paramArray = [MPParam(""rhost""), MPParam(""rport"")]
        self.fillInputParams(paramArray)
         
        content = vbLib.templates.METERPRETER
        content = content.replace(""<<<RHOST>>>"", getParamValue(paramArray, ""rhost""))
        content = content.replace(""<<<RPORT>>>"", getParamValue(paramArray, ""rport""))
        if self.outputFileType in MSTypes.VBSCRIPTS_FORMATS:
            content = content + vbLib.Meterpreter.VBS
        else:
            content = content + vbLib.Meterpreter.VBA
        vbaFile = self.addVBAModule(content)
        logging.debug(""   [-] Template %s VBA generated in %s"" % (self.template, vbaFile)) 
        rc_content = vbLib.templates.METERPRETER_RC
        rc_content = rc_content.replace(""<<<LHOST>>>"", getParamValue(paramArray, ""rhost""))
        rc_content = rc_content.replace(""<<<LPORT>>>"", getParamValue(paramArray, ""rport""))
        # Write in RC file
        rcFilePath = os.path.join(os.path.dirname(self.outputFilePath), ""meterpreter.rc"")
        f = open(rcFilePath, 'w')
        f.writelines(rc_content)
        f.close()
        logging.info(""   [-] Meterpreter resource file generated in %s"" % rcFilePath)
        logging.info(""   [-] Execute listener with 'msfconsole -r %s'"" % rcFilePath)
        logging.info(""   [-] OK!"")",_26277.py,21,"f = open(rcFilePath, 'w')","with open(rcFilePath, 'w') as f:
    f.writelines(rc_content)
    logging.info('   [-] Meterpreter resource file generated in %s' % rcFilePath)
    logging.info(""   [-] Execute listener with 'msfconsole -r %s'"" % rcFilePath)
    logging.info('   [-] OK!')"
https://github.com/Alexey-T/CudaText/tree/master/app/cudatext.app/Contents/Resources/py/cuda_prefs/tests/test_options_editor.py,"def test_rgb(self):
        info_file = os.path.dirname(__file__)+os.sep+'test_rgb.json'

        open(info_file, 'w').write(json.dumps([
        {   ""opt"": ""my_color_with_empty"",
            ""cmt"": [""Comment""],
            ""def"": '',
            ""frm"": ""#rgb-e"",
        },
        {   ""opt"": ""my_color_not_empty"",
            ""cmt"": [""Comment""],
            ""def"": '#aaccff',
            ""frm"": ""#rgb"",
        },
        ]))

        title = 'Test to store into ""oped_test.json""' # Dialog caption
        op_ed.OptEdD(path_keys_info=info_file, subset=subset
                    ,how=dict(stor_json='oped_test.json',
                              hide_fil=True,
                              )
                    ).show(title)
        
        self.assertTrue(True)",_26490.py,4,"open(info_file, 'w').write(json.dumps([{'opt': 'my_color_with_empty', 'cmt': ['Comment'], 'def': '', 'frm': '#rgb-e'}, {'opt': 'my_color_not_empty', 'cmt': ['Comment'], 'def': '#aaccff', 'frm': '#rgb'}]))","with open(info_file, 'w') as my_f:
    my_f.write(json.dumps([{'opt': 'my_color_with_empty', 'cmt': ['Comment'], 'def': '', 'frm': '#rgb-e'}, {'opt': 'my_color_not_empty', 'cmt': ['Comment'], 'def': '#aaccff', 'frm': '#rgb'}]))
    title = 'Test to store into ""oped_test.json""'
    op_ed.OptEdD(path_keys_info=info_file, subset=subset, how=dict(stor_json='oped_test.json', hide_fil=True)).show(title)
    self.assertTrue(True)"